{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict\n\n\nclass HierarchicalRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Root Mean Square Layer Normalization (HierarchicalRMSNorm).\n\n    This layer extends RMSNorm by incorporating multi-scale normalization.\n    It processes input embeddings at multiple scales and integrates them\n    to produce the normalized output while ensuring causality.\n\n    **Core Idea:**\n\n    - The input embeddings are downsampled to multiple scales using causal operations.\n    - Each scale has its own normalization parameters.\n    - The normalized embeddings at each scale are upsampled causally and combined.\n\n    **Mathematical Formulation:**\n\n        For each scale s:\n\n        x_s = causal_downsample(x, scale=s)\n\n        rms_s(x) = sqrt(mean(x_s^2) + eps)\n\n        y_s = x_s / rms_s(x) * gamma_s\n\n        y = sum(causal_upsample(y_s) * w_s for s in scales)\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n\n    Attributes:\n        scales (list of int): The scales being used.\n        eps (float): The epsilon value for numerical stability.\n        gammas (nn.ParameterDict): Scale-specific gamma parameters.\n        scale_weights (nn.Parameter): Weights for each scale.\n\n    Shape:\n        - Input: (batch_size, sequence_length, embed_dim)\n        - Output: Same as input.\n\n    Example:\n\n        norm = HierarchicalRMSNorm(embed_dim=512, scales=[1,2,4])\n        x = torch.randn(32, 128, 512)\n        y, _ = norm(x)\n\n    References:\n\n    - Proposal for HierarchicalRMSNorm.\n\n    Note:\n\n        This implementation ensures causality by using causal downsampling and upsampling operations.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.scales = kwargs.pop('scales', kwarg_all.get('scales', [1, 2, 4]))\n        self.eps = kwargs.pop('eps', kwarg_all.get('eps', 1e-05))\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in self.scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(self.scales), **\n            self.factory_kwargs))\n\n    def _decompose_scales(self, X: Tensor) ->Dict[int, Tensor]:\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_s = self._causal_downsample(X, s)\n                x_scales[s] = x_s\n        return x_scales\n\n    def _causal_downsample(self, X: Tensor, scale: int) ->Tensor:\n        batch_size, seq_length, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale)) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _integrate_scales(self, y_scales: Dict[int, Tensor]) ->Tensor:\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = 0\n        target_length = y_scales[1].size(1)\n        for i, (s, y_s) in enumerate(y_scales.items()):\n            if s == 1:\n                upsampled_y_s = y_s\n            else:\n                upsampled_y_s = self._causal_upsample(y_s, s, target_length)\n            Y = Y + upsampled_y_s * weights[i]\n        return Y\n\n    def _causal_upsample(self, y_s: Tensor, scale: int, target_length: int\n        ) ->Tensor:\n        upsampled_y_s = y_s.repeat_interleave(scale, dim=1)\n        upsampled_y_s = upsampled_y_s[:, :target_length, :]\n        return upsampled_y_s\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(torch.mean(x_s.pow(2), dim=-1, keepdim=True) +\n                self.eps)\n            gamma_s = self.gammas[f's{s}']\n            y_s = x_s / rms_s * gamma_s\n            y_scales[s] = y_s\n        Y = self._integrate_scales(y_scales)\n        return Y, {}\n\n\ngab_config = {'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'mlp_dim': 0, 'qkv_proj_bias': \n    True, 'out_proj_bias': True, 'softmax_scale': None, 'd_conv': 0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict\n\n\nclass HierarchicalRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Root Mean Square Layer Normalization (HierarchicalRMSNorm).\n\n    This layer extends RMSNorm by incorporating multi-scale normalization.\n    It processes input embeddings at multiple scales and integrates them\n    to produce the normalized output while ensuring causality.\n\n    **Core Idea:**\n\n    - The input embeddings are downsampled to multiple scales using causal operations.\n    - Each scale has its own normalization parameters.\n    - The normalized embeddings at each scale are upsampled causally and combined.\n\n    **Mathematical Formulation:**\n\n        For each scale s:\n\n        x_s = causal_downsample(x, scale=s)\n\n        rms_s(x) = sqrt(mean(x_s^2) + eps)\n\n        y_s = x_s / rms_s(x) * gamma_s\n\n        y = sum(causal_upsample(y_s) * w_s for s in scales)\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n\n    Attributes:\n        scales (list of int): The scales being used.\n        eps (float): The epsilon value for numerical stability.\n        gammas (nn.ParameterDict): Scale-specific gamma parameters.\n        scale_weights (nn.Parameter): Weights for each scale.\n\n    Shape:\n        - Input: (batch_size, sequence_length, embed_dim)\n        - Output: Same as input.\n\n    Example:\n\n        norm = HierarchicalRMSNorm(embed_dim=512, scales=[1,2,4])\n        x = torch.randn(32, 128, 512)\n        y, _ = norm(x)\n\n    References:\n\n    - Proposal for HierarchicalRMSNorm.\n\n    Note:\n\n        This implementation ensures causality by using causal downsampling and upsampling operations.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.scales = kwargs.pop('scales', kwarg_all.get('scales', [1, 2, 4]))\n        self.eps = kwargs.pop('eps', kwarg_all.get('eps', 1e-05))\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in self.scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(self.scales), **\n            self.factory_kwargs))\n\n    def _decompose_scales(self, X: Tensor) ->Dict[int, Tensor]:\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_s = self._causal_downsample(X, s)\n                x_scales[s] = x_s\n        return x_scales\n\n    def _causal_downsample(self, X: Tensor, scale: int) ->Tensor:\n        batch_size, seq_length, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale)) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _integrate_scales(self, y_scales: Dict[int, Tensor]) ->Tensor:\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = 0\n        target_length = y_scales[1].size(1)\n        for i, (s, y_s) in enumerate(y_scales.items()):\n            if s == 1:\n                upsampled_y_s = y_s\n            else:\n                upsampled_y_s = self._causal_upsample(y_s, s, target_length)\n            Y = Y + upsampled_y_s * weights[i]\n        return Y\n\n    def _causal_upsample(self, y_s: Tensor, scale: int, target_length: int\n        ) ->Tensor:\n        upsampled_y_s = y_s.repeat_interleave(scale, dim=1)\n        upsampled_y_s = upsampled_y_s[:, :target_length, :]\n        return upsampled_y_s\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(torch.mean(x_s.pow(2), dim=-1, keepdim=True) +\n                self.eps)\n            gamma_s = self.gammas[f's{s}']\n            y_s = x_s / rms_s * gamma_s\n            y_scales[s] = y_s\n        Y = self._integrate_scales(y_scales)\n        return Y, {}\n\n\ngab_config = {'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'mlp_dim': 0, 'qkv_proj_bias': \n    True, 'out_proj_bias': True, 'softmax_scale': None, 'd_conv': 0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict\n\n\nclass HierarchicalRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Root Mean Square Layer Normalization (HierarchicalRMSNorm).\n\n    This layer extends RMSNorm by incorporating multi-scale normalization.\n    It processes input embeddings at multiple scales and integrates them\n    to produce the normalized output while ensuring causality.\n\n    **Core Idea:**\n\n    - The input embeddings are downsampled to multiple scales using causal operations.\n    - Each scale has its own normalization parameters.\n    - The normalized embeddings at each scale are upsampled causally and combined.\n\n    **Mathematical Formulation:**\n\n        For each scale s:\n\n        x_s = causal_downsample(x, scale=s)\n\n        rms_s(x) = sqrt(mean(x_s^2) + eps)\n\n        y_s = x_s / rms_s(x) * gamma_s\n\n        y = sum(causal_upsample(y_s) * w_s for s in scales)\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n\n    Attributes:\n        scales (list of int): The scales being used.\n        eps (float): The epsilon value for numerical stability.\n        gammas (nn.ParameterDict): Scale-specific gamma parameters.\n        scale_weights (nn.Parameter): Weights for each scale.\n\n    Shape:\n        - Input: (batch_size, sequence_length, embed_dim)\n        - Output: Same as input.\n\n    Example:\n\n        norm = HierarchicalRMSNorm(embed_dim=512, scales=[1,2,4])\n        x = torch.randn(32, 128, 512)\n        y, _ = norm(x)\n\n    References:\n\n    - Proposal for HierarchicalRMSNorm.\n\n    Note:\n\n        This implementation ensures causality by using causal downsampling and upsampling operations.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.scales = kwargs.pop('scales', kwarg_all.get('scales', [1, 2, 4]))\n        self.eps = kwargs.pop('eps', kwarg_all.get('eps', 1e-05))\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in self.scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(self.scales), **\n            self.factory_kwargs))\n\n    def _decompose_scales(self, X: Tensor) ->Dict[int, Tensor]:\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_s = self._causal_downsample(X, s)\n                x_scales[s] = x_s\n        return x_scales\n\n    def _causal_downsample(self, X: Tensor, scale: int) ->Tensor:\n        batch_size, seq_length, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale)) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _integrate_scales(self, y_scales: Dict[int, Tensor]) ->Tensor:\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = 0\n        target_length = y_scales[1].size(1)\n        for i, (s, y_s) in enumerate(y_scales.items()):\n            if s == 1:\n                upsampled_y_s = y_s\n            else:\n                upsampled_y_s = self._causal_upsample(y_s, s, target_length)\n            Y = Y + upsampled_y_s * weights[i]\n        return Y\n\n    def _causal_upsample(self, y_s: Tensor, scale: int, target_length: int\n        ) ->Tensor:\n        upsampled_y_s = y_s.repeat_interleave(scale, dim=1)\n        upsampled_y_s = upsampled_y_s[:, :target_length, :]\n        return upsampled_y_s\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(torch.mean(x_s.pow(2), dim=-1, keepdim=True) +\n                self.eps)\n            gamma_s = self.gammas[f's{s}']\n            y_s = x_s / rms_s * gamma_s\n            y_scales[s] = y_s\n        Y = self._integrate_scales(y_scales)\n        return Y, {}\n\n\ngab_config = {'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'mlp_dim': 0, 'qkv_proj_bias': \n    True, 'out_proj_bias': True, 'softmax_scale': None, 'd_conv': 0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict\n\n\nclass HierarchicalRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Root Mean Square Layer Normalization (HierarchicalRMSNorm).\n\n    This layer extends RMSNorm by incorporating multi-scale normalization.\n    It processes input embeddings at multiple scales and integrates them\n    to produce the normalized output while ensuring causality.\n\n    **Core Idea:**\n\n    - The input embeddings are downsampled to multiple scales using causal operations.\n    - Each scale has its own normalization parameters.\n    - The normalized embeddings at each scale are upsampled causally and combined.\n\n    **Mathematical Formulation:**\n\n        For each scale s:\n\n        x_s = causal_downsample(x, scale=s)\n\n        rms_s(x) = sqrt(mean(x_s^2) + eps)\n\n        y_s = x_s / rms_s(x) * gamma_s\n\n        y = sum(causal_upsample(y_s) * w_s for s in scales)\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n\n    Attributes:\n        scales (list of int): The scales being used.\n        eps (float): The epsilon value for numerical stability.\n        gammas (nn.ParameterDict): Scale-specific gamma parameters.\n        scale_weights (nn.Parameter): Weights for each scale.\n\n    Shape:\n        - Input: (batch_size, sequence_length, embed_dim)\n        - Output: Same as input.\n\n    Example:\n\n        norm = HierarchicalRMSNorm(embed_dim=512, scales=[1,2,4])\n        x = torch.randn(32, 128, 512)\n        y, _ = norm(x)\n\n    References:\n\n    - Proposal for HierarchicalRMSNorm.\n\n    Note:\n\n        This implementation ensures causality by using causal downsampling and upsampling operations.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.scales = kwargs.pop('scales', kwarg_all.get('scales', [1, 2, 4]))\n        self.eps = kwargs.pop('eps', kwarg_all.get('eps', 1e-05))\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in self.scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(self.scales), **\n            self.factory_kwargs))\n\n    def _decompose_scales(self, X: Tensor) ->Dict[int, Tensor]:\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_s = self._causal_downsample(X, s)\n                x_scales[s] = x_s\n        return x_scales\n\n    def _causal_downsample(self, X: Tensor, scale: int) ->Tensor:\n        batch_size, seq_length, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale)) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _integrate_scales(self, y_scales: Dict[int, Tensor]) ->Tensor:\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = 0\n        target_length = y_scales[1].size(1)\n        for i, (s, y_s) in enumerate(y_scales.items()):\n            if s == 1:\n                upsampled_y_s = y_s\n            else:\n                upsampled_y_s = self._causal_upsample(y_s, s, target_length)\n            Y = Y + upsampled_y_s * weights[i]\n        return Y\n\n    def _causal_upsample(self, y_s: Tensor, scale: int, target_length: int\n        ) ->Tensor:\n        upsampled_y_s = y_s.repeat_interleave(scale, dim=1)\n        upsampled_y_s = upsampled_y_s[:, :target_length, :]\n        return upsampled_y_s\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(torch.mean(x_s.pow(2), dim=-1, keepdim=True) +\n                self.eps)\n            gamma_s = self.gammas[f's{s}']\n            y_s = x_s / rms_s * gamma_s\n            y_scales[s] = y_s\n        Y = self._integrate_scales(y_scales)\n        return Y, {}\n\n\ngab_config = {'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'mlp_dim': 0, 'qkv_proj_bias': \n    True, 'out_proj_bias': True, 'softmax_scale': None, 'd_conv': 0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict\n\n\nclass HierarchicalRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Root Mean Square Layer Normalization (HierarchicalRMSNorm).\n\n    This layer extends RMSNorm by incorporating multi-scale normalization.\n    It processes input embeddings at multiple scales and integrates them\n    to produce the normalized output while ensuring causality.\n\n    **Core Idea:**\n\n    - The input embeddings are downsampled to multiple scales using causal operations.\n    - Each scale has its own normalization parameters.\n    - The normalized embeddings at each scale are upsampled causally and combined.\n\n    **Mathematical Formulation:**\n\n        For each scale s:\n\n        x_s = causal_downsample(x, scale=s)\n\n        rms_s(x) = sqrt(mean(x_s^2) + eps)\n\n        y_s = x_s / rms_s(x) * gamma_s\n\n        y = sum(causal_upsample(y_s) * w_s for s in scales)\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n\n    Attributes:\n        scales (list of int): The scales being used.\n        eps (float): The epsilon value for numerical stability.\n        gammas (nn.ParameterDict): Scale-specific gamma parameters.\n        scale_weights (nn.Parameter): Weights for each scale.\n\n    Shape:\n        - Input: (batch_size, sequence_length, embed_dim)\n        - Output: Same as input.\n\n    Example:\n\n        norm = HierarchicalRMSNorm(embed_dim=512, scales=[1,2,4])\n        x = torch.randn(32, 128, 512)\n        y, _ = norm(x)\n\n    References:\n\n    - Proposal for HierarchicalRMSNorm.\n\n    Note:\n\n        This implementation ensures causality by using causal downsampling and upsampling operations.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.scales = kwargs.pop('scales', kwarg_all.get('scales', [1, 2, 4]))\n        self.eps = kwargs.pop('eps', kwarg_all.get('eps', 1e-05))\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in self.scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(self.scales), **\n            self.factory_kwargs))\n\n    def _decompose_scales(self, X: Tensor) ->Dict[int, Tensor]:\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_s = self._causal_downsample(X, s)\n                x_scales[s] = x_s\n        return x_scales\n\n    def _causal_downsample(self, X: Tensor, scale: int) ->Tensor:\n        batch_size, seq_length, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale)) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _integrate_scales(self, y_scales: Dict[int, Tensor]) ->Tensor:\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = 0\n        target_length = y_scales[1].size(1)\n        for i, (s, y_s) in enumerate(y_scales.items()):\n            if s == 1:\n                upsampled_y_s = y_s\n            else:\n                upsampled_y_s = self._causal_upsample(y_s, s, target_length)\n            Y = Y + upsampled_y_s * weights[i]\n        return Y\n\n    def _causal_upsample(self, y_s: Tensor, scale: int, target_length: int\n        ) ->Tensor:\n        upsampled_y_s = y_s.repeat_interleave(scale, dim=1)\n        upsampled_y_s = upsampled_y_s[:, :target_length, :]\n        return upsampled_y_s\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(torch.mean(x_s.pow(2), dim=-1, keepdim=True) +\n                self.eps)\n            gamma_s = self.gammas[f's{s}']\n            y_s = x_s / rms_s * gamma_s\n            y_scales[s] = y_s\n        Y = self._integrate_scales(y_scales)\n        return Y, {}\n\n\ngab_config = {'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'mlp_dim': 0, 'qkv_proj_bias': \n    True, 'out_proj_bias': True, 'softmax_scale': None, 'd_conv': 0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict\n\n\nclass HierarchicalRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Root Mean Square Layer Normalization (HierarchicalRMSNorm).\n\n    This layer extends RMSNorm by incorporating multi-scale normalization.\n    It processes input embeddings at multiple scales and integrates them\n    to produce the normalized output while ensuring causality.\n\n    **Core Idea:**\n\n    - The input embeddings are downsampled to multiple scales using causal operations.\n    - Each scale has its own normalization parameters.\n    - The normalized embeddings at each scale are upsampled causally and combined.\n\n    **Mathematical Formulation:**\n\n        For each scale s:\n\n        x_s = causal_downsample(x, scale=s)\n\n        rms_s(x) = sqrt(mean(x_s^2) + eps)\n\n        y_s = x_s / rms_s(x) * gamma_s\n\n        y = sum(causal_upsample(y_s) * w_s for s in scales)\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n\n    Attributes:\n        scales (list of int): The scales being used.\n        eps (float): The epsilon value for numerical stability.\n        gammas (nn.ParameterDict): Scale-specific gamma parameters.\n        scale_weights (nn.Parameter): Weights for each scale.\n\n    Shape:\n        - Input: (batch_size, sequence_length, embed_dim)\n        - Output: Same as input.\n\n    Example:\n\n        norm = HierarchicalRMSNorm(embed_dim=512, scales=[1,2,4])\n        x = torch.randn(32, 128, 512)\n        y, _ = norm(x)\n\n    References:\n\n    - Proposal for HierarchicalRMSNorm.\n\n    Note:\n\n        This implementation ensures causality by using causal downsampling and upsampling operations.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.scales = kwargs.pop('scales', kwarg_all.get('scales', [1, 2, 4]))\n        self.eps = kwargs.pop('eps', kwarg_all.get('eps', 1e-05))\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in self.scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(self.scales), **\n            self.factory_kwargs))\n\n    def _decompose_scales(self, X: Tensor) ->Dict[int, Tensor]:\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_s = self._causal_downsample(X, s)\n                x_scales[s] = x_s\n        return x_scales\n\n    def _causal_downsample(self, X: Tensor, scale: int) ->Tensor:\n        batch_size, seq_length, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale)) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _integrate_scales(self, y_scales: Dict[int, Tensor]) ->Tensor:\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = 0\n        target_length = y_scales[1].size(1)\n        for i, (s, y_s) in enumerate(y_scales.items()):\n            if s == 1:\n                upsampled_y_s = y_s\n            else:\n                upsampled_y_s = self._causal_upsample(y_s, s, target_length)\n            Y = Y + upsampled_y_s * weights[i]\n        return Y\n\n    def _causal_upsample(self, y_s: Tensor, scale: int, target_length: int\n        ) ->Tensor:\n        upsampled_y_s = y_s.repeat_interleave(scale, dim=1)\n        upsampled_y_s = upsampled_y_s[:, :target_length, :]\n        return upsampled_y_s\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(torch.mean(x_s.pow(2), dim=-1, keepdim=True) +\n                self.eps)\n            gamma_s = self.gammas[f's{s}']\n            y_s = x_s / rms_s * gamma_s\n            y_scales[s] = y_s\n        Y = self._integrate_scales(y_scales)\n        return Y, {}\n\n\ngab_config = {'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'mlp_dim': 0, 'qkv_proj_bias': \n    True, 'out_proj_bias': True, 'softmax_scale': None, 'd_conv': 0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = HierarchicalRMSNorm(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Dict\n\n\nclass HierarchicalRMSNorm(GAUBase):\n    \"\"\"\n    Hierarchical Root Mean Square Layer Normalization (HierarchicalRMSNorm).\n\n    This layer extends RMSNorm by incorporating multi-scale normalization.\n    It processes input embeddings at multiple scales and integrates them\n    to produce the normalized output while ensuring causality.\n\n    **Core Idea:**\n\n    - The input embeddings are downsampled to multiple scales using causal operations.\n    - Each scale has its own normalization parameters.\n    - The normalized embeddings at each scale are upsampled causally and combined.\n\n    **Mathematical Formulation:**\n\n        For each scale s:\n\n        x_s = causal_downsample(x, scale=s)\n\n        rms_s(x) = sqrt(mean(x_s^2) + eps)\n\n        y_s = x_s / rms_s(x) * gamma_s\n\n        y = sum(causal_upsample(y_s) * w_s for s in scales)\n\n    Args:\n        embed_dim (int): Dimensionality of the input embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to use.\n        dtype (torch.dtype, optional): Data type to use.\n\n    Attributes:\n        scales (list of int): The scales being used.\n        eps (float): The epsilon value for numerical stability.\n        gammas (nn.ParameterDict): Scale-specific gamma parameters.\n        scale_weights (nn.Parameter): Weights for each scale.\n\n    Shape:\n        - Input: (batch_size, sequence_length, embed_dim)\n        - Output: Same as input.\n\n    Example:\n\n        norm = HierarchicalRMSNorm(embed_dim=512, scales=[1,2,4])\n        x = torch.randn(32, 128, 512)\n        y, _ = norm(x)\n\n    References:\n\n    - Proposal for HierarchicalRMSNorm.\n\n    Note:\n\n        This implementation ensures causality by using causal downsampling and upsampling operations.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.scales = kwargs.pop('scales', kwarg_all.get('scales', [1, 2, 4]))\n        self.eps = kwargs.pop('eps', kwarg_all.get('eps', 1e-05))\n        self.gammas = nn.ParameterDict({f's{s}': nn.Parameter(torch.ones(\n            embed_dim, **self.factory_kwargs)) for s in self.scales})\n        self.scale_weights = nn.Parameter(torch.ones(len(self.scales), **\n            self.factory_kwargs))\n\n    def _decompose_scales(self, X: Tensor) ->Dict[int, Tensor]:\n        x_scales = {}\n        for s in self.scales:\n            if s == 1:\n                x_scales[s] = X\n            else:\n                x_s = self._causal_downsample(X, s)\n                x_scales[s] = x_s\n        return x_scales\n\n    def _causal_downsample(self, X: Tensor, scale: int) ->Tensor:\n        batch_size, seq_length, embed_dim = X.size()\n        padding = scale - 1, 0\n        X_padded = F.pad(X.transpose(1, 2), padding)\n        weight = X.new_ones((embed_dim, 1, scale)) / scale\n        x_s = F.conv1d(X_padded, weight, stride=scale, groups=embed_dim\n            ).transpose(1, 2)\n        return x_s\n\n    def _integrate_scales(self, y_scales: Dict[int, Tensor]) ->Tensor:\n        weights = F.softmax(self.scale_weights, dim=0)\n        Y = 0\n        target_length = y_scales[1].size(1)\n        for i, (s, y_s) in enumerate(y_scales.items()):\n            if s == 1:\n                upsampled_y_s = y_s\n            else:\n                upsampled_y_s = self._causal_upsample(y_s, s, target_length)\n            Y = Y + upsampled_y_s * weights[i]\n        return Y\n\n    def _causal_upsample(self, y_s: Tensor, scale: int, target_length: int\n        ) ->Tensor:\n        upsampled_y_s = y_s.repeat_interleave(scale, dim=1)\n        upsampled_y_s = upsampled_y_s[:, :target_length, :]\n        return upsampled_y_s\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        x_scales = self._decompose_scales(X)\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(torch.mean(x_s.pow(2), dim=-1, keepdim=True) +\n                self.eps)\n            gamma_s = self.gammas[f's{s}']\n            y_s = x_s / rms_s * gamma_s\n            y_scales[s] = y_s\n        Y = self._integrate_scales(y_scales)\n        return Y, {}\n\n\ngab_config = {'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'mlp_dim': 0, 'qkv_proj_bias': \n    True, 'out_proj_bias': True, 'softmax_scale': None, 'd_conv': 0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}