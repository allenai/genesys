{
    "variantname": "AdaptiveGatedHRMSNorm",
    "review": "The HierarchicalRMSNorm (HiRMSNorm) proposal presents an innovative approach to enhancing normalization in language models through multi-scale processing and adaptive computation. After thorough analysis of the proposal, existing research, and similar approaches, here is a comprehensive review:\n\nCLARITY (4.5/5):\n+ Well-structured presentation with clear motivation and objectives\n+ Detailed mathematical formulations and implementation guidelines\n+ Comprehensive theoretical analysis and justification\n+ Clear connection between components and their purposes\n- Some implementation details about memory hierarchy optimization could be more explicit\n\nINNOVATION (4.3/5):\n+ Novel integration of multi-scale processing with RMSNorm\n+ Unique approach to adaptive parameter initialization\n+ Innovative memory optimization through hierarchical grouping\n+ Distinguished from existing work through scale-specific normalization paths\n- Some concepts build on existing work in multi-scale processing\n- Similar ideas exist in recent papers about memory-efficient normalization\n\nFEASIBILITY (4.0/5):\n+ Clear implementation path with detailed mathematical formulations\n+ Careful consideration of hardware constraints\n+ Thoughtful approach to memory management\n+ Well-defined integration with existing architectures\nConcerns:\n- Complex interaction between multiple scales needs careful handling\n- Memory hierarchy optimization requires detailed implementation\n- Parameter initialization strategy needs validation\n\nSCALABILITY (4.2/5):\n+ Linear complexity through efficient scale integration\n+ Memory-efficient design through hierarchical grouping\n+ Parallel processing capabilities across scales\n+ Clear scaling strategy for different model sizes\nConcerns:\n- Memory access patterns need optimization for very large models\n- Communication overhead in distributed settings needs consideration\n- Trade-offs between number of scales and efficiency\n\nACCURACY AND ROBUSTNESS (4.4/5):\n+ Enhanced feature representation through multi-scale processing\n+ Improved stability through adaptive initialization\n+ Better handling of varying input distributions\n+ Scale-specific normalization improves feature capture\nConcerns:\n- Impact of scale selection on model performance\n- Stability under extreme input conditions\n- Trade-offs between adaptivity and consistency\n\nEFFICIENCY (4.3/5):\n+ Reduced memory requirements through hierarchical grouping\n+ Efficient parallel processing across scales\n+ Hardware-aware implementation strategy\n+ Optimized memory access patterns\nLimitations:\n- Additional computation for scale integration\n- Memory-computation trade-offs need careful balancing\n- Implementation complexity may affect practical efficiency\n\nSTRENGTHS:\n\n1. Technical Innovation:\n- Novel integration of multi-scale processing with RMSNorm\n- Adaptive parameter initialization strategy\n- Efficient memory management through hierarchical grouping\n- Scale-specific normalization paths\n\n2. Implementation Design:\n- Detailed mathematical formulations\n- Clear integration guidelines\n- Hardware-aware optimization strategies\n- Parallel processing capabilities\n\n3. Performance Potential:\n- Enhanced feature representation\n- Improved stability through adaptive initialization\n- Better handling of varying input distributions\n- Efficient memory utilization\n\n4. Scalability Features:\n- Linear complexity scaling\n- Memory-efficient architecture\n- Parallel processing support\n- Clear scaling strategy\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex interaction between scales\n- Memory hierarchy optimization needs\n- Parameter initialization complexity\n- Integration challenges with existing systems\n\n2. Performance Trade-offs:\n- Memory-computation balance\n- Scale selection impact\n- Communication overhead\n- Stability under extreme conditions\n\n3. Practical Considerations:\n- Implementation overhead\n- Hardware-specific optimizations\n- Memory access patterns\n- Distributed computing challenges",
    "search_stack": [
        {
            "ready": false,
            "query": "selective attention mechanisms, efficient multi-scale processing, hardware optimization",
            "detail": "Find papers discussing:\n1. Selective attention mechanisms that efficiently handle multi-scale information\n2. Hardware-efficient implementations of normalization and attention\n3. Theoretical analysis of multi-scale processing in neural networks",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Selective attention mechanisms that efficiently handle multi-scale information\n2. Hardware-efficient implementations of normalization and attention\n3. Theoretical analysis of multi-scale processing in neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.84)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.84)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 2. Perceiver IO: A General Architecture for Structured Inputs & Outputs (Avg. Score: 0.82)\n\n*Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Andrew Brock, Evan Shelhamer, Olivier J. H'enaff, M. Botvinick, Andrew Zisserman, O. Vinyals, Jo\u00e3o Carreira*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 458  (*Influential: 46*)\n\n**TL;DR:** This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.\n\n**Abstract:** A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain&task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.\n\n##### *Relevant Chunk: No. 2/54 (Score: 0.82)*\n\n```\n4.1), estimate optical flow without relying on explicit architectural features for multiscale correspondence (Sec. 4.2, learn joint representations of video, audio, and labels without separate network trunks (Sec. 4.3), and perform image classification with no information about the 2D structure of images ( Sec . A. ## 2 RELATEd WORK\n\nNeural network research has long sought architectures that can handle large, arbitrarily structured inputs and outputs. Autoencoding (Hinton \\& Zemel, 1994) was among the first attempts to build\nrepresentation which could encode and reproduce high-dimensional inputs like images. As hardware grew more powerful, neural nets led to breakthroughs in image understanding (Krizhevsky et al., 2012, Zeiler \\& Fergus, 2014; Szegedy et al., 2015) and interest intensified: autoregressive models that could process and complete samples of handwriting were developed (Graves 2013), and new convolutional network designs led to good results in structured output spaces like semantic segmentation (Farabet et al., 2012, Long et al., 2015, Ronneberger et al., 2015), pose estimation (Toshev \\& Szegedy, 2014), detection (Sermanet et al., 2014), captioning (You et al., 2016), and optical flow (Fischer et al. 2015). At the same time, natural language applications research has made extensive progressive in capturing the structured nature of language, typically via autoregressive models (Collobert et al. 2011; Sutskever et al., 2014, Vaswani et al., 2017, Radford et al., 2019, Brown et al, 2020) or context prediction (Mikolov et al., 2013; Pennington et al., 2014, Devlin et al., 2019). Similar to our work, several groups have proposed to solve tasks in multiple domains (e.g. Kaiser et al. 2017, Alayrac et al. 2020; Akbari et al. 2021, but typically across a fixed and predefined set of modalities by means of domain-specific networks. Although single-task specialist networks remain dominant in vision, multi-task learning has become popular (Misra et al., 2016, Doersch \\& Zisserman, 2017, Kokkinos, 2017, Zamir et al., 2018) and individual models achieve generality in a restricted domain: e.g. Mask-RCNN (He et al., 2017) handles object detection, segmentation, and pose estimation. In language, training or evaluation on multiple tasks has also become common (Collobert \\& Weston, 2008, Luong et al., 2016, Devlin et al., 2019, Liu et al., 2019, Raffel et al., 2020). Several groups have demonstrated that Transformers (originally designed for language) can be used or adapted to non-language tasks (e.g. Chen et al. 2020, Lu et al. 2021), but the limited scalability of Transformers limits their usefulness as general-purpose architectures. Several groups have proposed to use attention to manipulate the size of arrays or to introduce bottlenecks in processing. Set Transformers and related work (Lee et al., 2019, Goyal et al., 2022) use a learned query (\"inducing points\") to induce local bottlenecks by mapping a set back and forth from a set with fewer elements and learned decoder queries (\"seed vectors\") to map to outputs (\"pooling by multiheaded attention'). Each layer of these networks has complexity linear in the input size, while Perceivers use a deep latent network with complexity independent of the input and output. Our work uses attention over inputs and outputs of different sizes in part to produce an efficient attention architecture, and several other efficient attention architectures have been proposed, largely for language or small-scale problems (e.g.\n```\n\n#### 3. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.75)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.75)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 4. Rethinking Attention with Performers (Avg. Score: 0.74)\n\n*K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, Adrian Weller*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1210  (*Influential: 176*)\n\n**TL;DR:** Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity, without relying on any priors such as sparsity or low-rankness are introduced.\n\n**Abstract:** We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\n##### *Relevant Chunk: No. 28/48 (Score: 0.74)*\n\n```\nIn Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4335-4344, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30, pp. 5998-6008. Curran Associates, Inc., 2017. URL http: / / papers. nips.cc/paper/7181-attention-is-all-you-need.pdf. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URLhttps://openreview.net/forum?id=rJXMpikCZ. Jesse Vig. A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714, 2019. Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language model.\n```\n\n#### 5. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.70)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.70)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: selective attention mechanisms, efficient multi-scale processing, hardware optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation\n\n*From Search Query: selective attention mechanisms*\n\n*Kai Li, Run Yang, Fuchun Sun, Xiaolin Hu*\n\n**TL;DR:** The fast version of IIANet (IIANet-fast) has only 7% of CTCNet's MACs and is 40% faster than CTCNet on CPUs while achieving better separation quality, showing the great potential of attention mechanism for efficient and effective multimodal fusion.\n\n**Abstract:** Recent research has made significant progress in designing fusion modules for audio-visual speech separation. However, they predominantly focus on multi-modal fusion at a single temporal scale of auditory and visual features without employing selective attention mechanisms, which is in sharp contrast with the brain. To address this issue, We propose a novel model called Intra- and Inter-Attention Network (IIANet), which leverages the attention mechanism for efficient audio-visual feature fusion. IIANet consists of two types of attention blocks: intra-attention (IntraA) and inter-attention (InterA) blocks, where the InterA blocks are distributed at the top, middle and bottom of IIANet. Heavily inspired by the way how human brain selectively focuses on relevant content at various temporal scales, these blocks maintain the ability to learn modality-specific features and enable the extraction of different semantics from audio-visual features. Comprehensive experiments on three standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2) demonstrate the effectiveness of IIANet, outperforming previous state-of-the-art methods while maintaining comparable inference time. In particular, the fast version of IIANet (IIANet-fast) has only 7% of CTCNet's MACs and is 40% faster than CTCNet on CPUs while achieving better separation quality, showing the great potential of attention mechanism for efficient and effective multimodal fusion.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 2. SPECTRA: Sparse Structured Text Rationalization\n\n*From Search Query: selective attention mechanisms*\n\n*Nuno M. Guerreiro, Andr\u00e9 F. T. Martins*\n\n**TL;DR:** This paper presents a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer and provides a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks.\n\n**Abstract:** Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction (e.g., to control the sparsity of a text highlight or the number of alignments). In this paper, we present a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer. Our approach greatly eases training and rationale regularization, generally outperforming previous work on what comes to performance and plausibility of the extracted rationales. We further provide a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks, jointly assessing their predictive power, quality of the explanations, and model variability.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 24  (*Influential: 4*)\n\n#### 3. Closed-Book Training to Improve Summarization Encoder Memory\n\n*From Search Query: selective attention mechanisms*\n\n*Yichen Jiang, Mohit Bansal*\n\n**TL;DR:** This paper presents a memory ability test, two saliency metrics, as well as several sanity-check ablations to prove that the encoder of the 2-decoder model does in fact learn stronger memory representations than the baseline encoder.\n\n**Abstract:** A good neural sequence-to-sequence summarization model should have a strong encoder that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the encoder\u2019s memory. In this paper, we aim to improve the memorization capabilities of the encoder of a pointer-generator model by adding an additional \u2018closed-book\u2019 decoder without attention and pointer mechanisms. Such a decoder forces the encoder to be more selective in the information encoded in its memory state because the decoder can\u2019t rely on the extra information provided by the attention and possibly copy modules, and hence improves the entire model. On the CNN/Daily Mail dataset, our 2-decoder model outperforms the baseline significantly in terms of ROUGE and METEOR metrics, for both cross-entropy and reinforced setups (and on human evaluation). Moreover, our model also achieves higher scores in a test-only DUC-2002 generalizability setup. We further present a memory ability test, two saliency metrics, as well as several sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to prove that the encoder of our 2-decoder model does in fact learn stronger memory representations than the baseline encoder.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 27  (*Influential: 0*)\n\n#### 4. Searching for Efficient Multi-Scale Architectures for Dense Image Prediction\n\n*From Search Query: efficient multi-scale processing*\n\n*Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, G. Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jonathon Shlens*\n\n**TL;DR:** This work constructs a recursive search space for meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation and demonstrates that even with efficient random search, this architecture can outperform human-invented architectures.\n\n**Abstract:** The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7% on Cityscapes (street scene parsing), 71.3% on PASCAL-Person-Part (person-part segmentation), and 87.9% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 394  (*Influential: 28*)\n\n#### 5. ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts\n\n*From Search Query: efficient multi-scale processing*\n\n*Akari Asai, Mohammadreza Salehi, Matthew E. Peters, Hannaneh Hajishirzi*\n\n**TL;DR:** A new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts\u2014small prefix embedding vectors pre-trained for different tasks that significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter- efficient tuning approaches that use 10 times more parameters.\n\n**Abstract:** This work introduces a new multi-task, parameter-efficient language model (LM) tuning method that learns to transfer knowledge across different tasks via a mixture of soft prompts\u2014small prefix embedding vectors pre-trained for different tasks. Our method, called ATTEMPT (ATTEntional Mixtures of Prompt Tuning), obtains source prompts as encodings of large-scale source tasks into a small number of parameters and trains an attention module to interpolate the source prompts and a newly initialized target prompt for every instance in the target task. During training, only the target task prompt and the attention weights, which are shared between tasks in multi-task training, are updated, while the original LM and source prompts are intact. ATTEMPT is highly parameter-efficient (e.g., updates 2,300 times fewer parameters than full fine-tuning), while it overcomes instability of prompt tuning and achieves high task performance using learned knowledge from high-resource tasks. Moreover, it is modular using pre-trained soft prompts, and can flexibly add or remove source prompts for effective knowledge transfer. Our experimental results across 21 diverse NLP datasets show that ATTEMPT significantly outperforms prompt tuning and outperforms or matches fully fine-tuned or other parameter-efficient tuning approaches that use 10 times more parameters. Finally, ATTEMPT outperforms previous work in few-shot learning settings.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 85  (*Influential: 23*)\n\n#### 6. MODE-LSTM: A Parameter-efficient Recurrent Network with Multi-Scale for Sentence Classification\n\n*From Search Query: efficient multi-scale processing*\n\n*Qianli Ma, Zhenxi Lin, Jiangyue Yan, Zipeng Chen, Liuhong Yu*\n\n**TL;DR:** This paper proposes a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features.\n\n**Abstract:** The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences. Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets. In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features. We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices. We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features. Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets. We also combine our model with BERT to further boost the generalization performance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 7. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks\n\n*From Search Query: hardware optimization*\n\n*Y. Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi, Vikas Chandra, Yingyan Lin*\n\n**TL;DR:** This work opens up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy, and proposes a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient Dnns that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real- hardware efficiency.\n\n**Abstract:** Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 8. Bounce: Reliable High-Dimensional Bayesian Optimization for Combinatorial and Mixed Spaces\n\n*From Search Query: hardware optimization*\n\n*Leonard Papenmeier, Luigi Nardi, Matthias Poloczek*\n\n**TL;DR:** This paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality that reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems.\n\n**Abstract:** Impactful applications such as materials discovery, hardware design, neural architecture search, or portfolio optimization require optimizing high-dimensional black-box functions with mixed and combinatorial input spaces. While Bayesian optimization has recently made significant progress in solving such problems, an in-depth analysis reveals that the current state-of-the-art methods are not reliable. Their performances degrade substantially when the unknown optima of the function do not have a certain structure. To fill the need for a reliable algorithm for combinatorial and mixed spaces, this paper proposes Bounce that relies on a novel map of various variable types into nested embeddings of increasing dimensionality. Comprehensive experiments show that Bounce reliably achieves and often even improves upon state-of-the-art performance on a variety of high-dimensional problems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. Cross-relation Cross-bag Attention for Distantly-supervised Relation\n  Extraction\n\n*From Search Query: selective attention mechanisms*\n\n*Yujin Yuan, Liyuan Liu, Siliang Tang, Zhongfei Zhang, Yueting Zhuang, Shiliang Pu, Fei Wu, Xiang Ren*\n\n**Abstract:** Distant supervision leverages knowledge bases to automatically label\ninstances, thus allowing us to train relation extractor without human\nannotations. However, the generated training data typically contain massive\nnoise, and may result in poor performances with the vanilla supervised\nlearning. In this paper, we propose to conduct multi-instance learning with a\nnovel Cross-relation Cross-bag Selective Attention (C$^2$SA), which leads to\nnoise-robust training for distant supervised relation extractor. Specifically,\nwe employ the sentence-level selective attention to reduce the effect of noisy\nor mismatched sentences, while the correlation among relations were captured to\nimprove the quality of attention weights. Moreover, instead of treating all\nentity-pairs equally, we try to pay more attention to entity-pairs with a\nhigher quality. Similarly, we adopt the selective attention mechanism to\nachieve this goal. Experiments with two types of relation extractor demonstrate\nthe superiority of the proposed approach over the state-of-the-art, while\nfurther ablation studies verify our intuitions and demonstrate the\neffectiveness of our proposed two techniques.\n\n**Published:** 2018-12-27T03:03:51Z  (*Updated: 2018-12-27T03:03:51Z*)\n\n\n\n#### 2. Selective Attention Encoders by Syntactic Graph Convolutional Networks\n  for Document Summarization\n\n*From Search Query: selective attention mechanisms*\n\n*Haiyang Xu, Yun Wang, Kun Han, Baochang Ma, Junwen Chen, Xiangang Li*\n\n**Abstract:** Abstractive text summarization is a challenging task, and one need to design\na mechanism to effectively extract salient information from the source text and\nthen generate a summary. A parsing process of the source text contains critical\nsyntactic or semantic structures, which is useful to generate more accurate\nsummary. However, modeling a parsing tree for text summarization is not trivial\ndue to its non-linear structure and it is harder to deal with a document that\nincludes multiple sentences and their parsing trees. In this paper, we propose\nto use a graph to connect the parsing trees from the sentences in a document\nand utilize the stacked graph convolutional networks (GCNs) to learn the\nsyntactic representation for a document. The selective attention mechanism is\nused to extract salient information in semantic and structural aspect and\ngenerate an abstractive summary. We evaluate our approach on the CNN/Daily Mail\ntext summarization dataset. The experimental results show that the proposed\nGCNs based selective attention approach outperforms the baselines and achieves\nthe state-of-the-art performance on the dataset.\n\n**Published:** 2020-03-18T01:30:02Z  (*Updated: 2020-03-18T01:30:02Z*)\n\n\n\n#### 3. The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\n\n*From Search Query: hardware optimization*\n\n*Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei*\n\n**Abstract:** Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.\n\n**Published:** 2024-02-27T18:56:19Z  (*Updated: 2024-02-27T18:56:19Z*)\n\n\n\n#### 4. BrainTransformers: SNN-LLM\n\n*From Search Query: hardware optimization*\n\n*Zhengzheng Tang, Eva Zhu*\n\n**Abstract:** This study introduces BrainTransformers, an innovative Large Language Model\n(LLM) implemented using Spiking Neural Networks (SNN). Our key contributions\ninclude: (1) designing SNN-compatible Transformer components such as SNNMatmul,\nSNNSoftmax, and SNNSiLU; (2) implementing an SNN approximation of the SiLU\nactivation function; and (3) developing a Synapsis module to simulate synaptic\nplasticity. Our 3-billion parameter model, BrainTransformers-3B-Chat,\ndemonstrates competitive performance across various benchmarks, including MMLU\n(63.2), BBH (54.1), ARC-C (54.3), and GSM8K (76.3), while potentially offering\nimproved energy efficiency and biological plausibility. The model employs a\nthree-stage training approach, including SNN-specific neuronal synaptic\nplasticity training. This research opens new avenues for brain-like AI systems\nin natural language processing and neuromorphic computing. Future work will\nfocus on hardware optimization, developing specialized SNN fine-tuning tools,\nand exploring practical applications in energy-efficient computing\nenvironments.\n\n**Published:** 2024-10-03T14:17:43Z  (*Updated: 2024-10-23T03:05:37Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. AFS: An Attention-based mechanism for Supervised Feature Selection\n\n*From Search Query: selective attention mechanisms*\n\n*Danni Ge, Ning Gui, Ziyin Hu*\n\n**Abstract:** As an effective data preprocessing step, feature selection has shown its\neffectiveness to prepare high-dimensional data for many machine learning tasks.\nThe proliferation of high di-mension and huge volume big data, however, has\nbrought major challenges, e.g. computation complexity and stability on noisy\ndata, upon existing feature-selection techniques. This paper introduces a novel\nneural network-based feature selection architecture, dubbed Attention-based\nFeature Selec-tion (AFS). AFS consists of two detachable modules: an at-tention\nmodule for feature weight generation and a learning module for the problem\nmodeling. The attention module for-mulates correlation problem among features\nand supervision target into a binary classification problem, supported by a\nshallow attention net for each feature. Feature weights are generated based on\nthe distribution of respective feature se-lection patterns adjusted by\nbackpropagation during the train-ing process. The detachable structure allows\nexisting off-the-shelf models to be directly reused, which allows for much less\ntraining time, demands for the training data and requirements for expertise. A\nhybrid initialization method is also intro-duced to boost the selection\naccuracy for datasets without enough samples for feature weight generation.\nExperimental results show that AFS achieves the best accuracy and stability in\ncomparison to several state-of-art feature selection algo-rithms upon both\nMNIST, noisy MNIST and several datasets with small samples.\n\n**Published:** 2019-02-28\n\n\n\n#### 2. How Does Selective Mechanism Improve Self-Attention Networks?\n\n*From Search Query: selective attention mechanisms*\n\n*Long-Yue Wang, Zhaopeng Tu, Xing Wang, Xinwei Geng, Ting Liu, Bing Qin*\n\n**Abstract:** Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN.\n\n**Conference:** how-does-selective-mechanism-improve-self-1\n\n**Published:** 2020-05-03\n\n\n\n#### 3. UniPose: Unified Human Pose Estimation in Single Images and Videos\n\n*From Search Query: efficient multi-scale processing*\n\n*Andreas Savakis, Bruno Artacho*\n\n**Abstract:** We propose UniPose, a unified framework for human pose estimation, based on our \"Waterfall\" Atrous Spatial Pooling architecture, that achieves state-of-art-results on several pose estimation metrics. Current pose estimation methods utilizing standard CNN architectures heavily rely on statistical postprocessing or predefined anchor poses for joint localization. UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage, with high accuracy, without relying on statistical postprocessing methods. The Waterfall module in UniPose leverages the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Additionally, our method is extended to UniPose-LSTM for multi-frame processing and achieves state-of-the-art results for temporal pose estimation in Video. Our results on multiple datasets demonstrate that UniPose, with a ResNet backbone and Waterfall module, is a robust and efficient architecture for pose estimation obtaining state-of-the-art results in single person pose detection for both single images and videos.\n\n**Conference:** unipose-unified-human-pose-estimation-in-1\n\n**Published:** 2020-01-22\n\n\n\n#### 4. DETRs Beat YOLOs on Real-time Object Detection\n\n*From Search Query: efficient multi-scale processing*\n\n*Jie Chen, Yi Liu, Qingqing Dang, Wenyu Lv, Jinman Wei, Guanzhong Wang, Yian Zhao, Shangliang Xu*\n\n**Abstract:** The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: https://zhao-yian.github.io/RTDETR.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2023-04-17\n\n\n\n#### 5. Data-Driven Offline Optimization For Architecting Hardware Accelerators\n\n*From Search Query: hardware optimization*\n\n*Sergey Levine, Kevin Swersky, Milad Hashemi, Amir Yazdanbakhsh, Aviral Kumar*\n\n**Abstract:** Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a \"simulation-driven\" approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a \"data-driven\", offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators -- tailored towards both single and multiple applications -- improving performance upon state-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x.\n\n**Conference:** data-driven-offline-optimization-for\n\n**Published:** 2021-10-20\n\n\n\n#### 6. Efficient and Modular Implicit Differentiation\n\n*From Search Query: hardware optimization*\n\n*Jean-Philippe Vert, Fabian Pedregosa, Felipe Llinares-L\u00f3pez, Stephan Hoyer, Roy Frostig, Marco Cuturi, Quentin Berthet, Mathieu Blondel*\n\n**Abstract:** Automatic differentiation (autodiff) has revolutionized machine learning. It allows to express complex computations by composing elementary ones in creative ways and removes the burden of computing their derivatives by hand. More recently, differentiation of optimization problem solutions has attracted widespread attention with applications such as optimization layers, and in bi-level problems such as hyper-parameter optimization and meta-learning. However, so far, implicit differentiation remained difficult to use for practitioners, as it often required case-by-case tedious mathematical derivations and implementations. In this paper, we propose automatic implicit differentiation, an efficient and modular approach for implicit differentiation of optimization problems. In our approach, the user defines directly in Python a function $F$ capturing the optimality conditions of the problem to be differentiated. Once this is done, we leverage autodiff of $F$ and the implicit function theorem to automatically differentiate the optimization problem. Our approach thus combines the benefits of implicit differentiation and autodiff. It is efficient as it can be added on top of any state-of-the-art solver and modular as the optimality condition specification is decoupled from the implicit differentiation mechanism. We show that seemingly simple principles allow to recover many existing implicit differentiation methods and create new ones easily. We demonstrate the ease of formulating and solving bi-level optimization problems using our framework. We also showcase an application to the sensitivity analysis of molecular dynamics.\n\n**Conference:** efficient-and-modular-implicit-1\n\n**Published:** 2021-05-31\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on selective attention mechanisms, efficient multi-scale processing, and hardware optimization, here are some key insights and relevant information from the provided sources and additional context:\n\n## Selective Attention Mechanisms\n\n### Dynamic Token Selection\nThe \"Select and Pack Attention (SPA)\" mechanism introduced in the paper on \"Context-Aware Token Selection and Packing for Enhanced Vision Transformers\" is highly relevant. SPA dynamically selects informative tokens using a linear gating layer supervised by selection labels and packs these tokens into new batches. This approach ensures that only relevant tokens are processed, enhancing efficiency and performance. This concept can be adapted for autoregressive language models to selectively process tokens based on their importance, thus reducing computational overhead and improving information flow.\n\n### Multi-Scale Attention\nThe \"Multiattention Network (MANet)\" mentioned in the remote sensing image segmentation paper extends attention mechanisms across multiple scales, aggregating contextual information at different scales. This approach can be applied to language models to handle multi-scale information efficiently, ensuring that both local and global contexts are effectively utilized.\n\n## Efficient Multi-Scale Processing\n\n### Hierarchical Feature Aggregation\nThe \"Multi-Scale Aggregation Transformer Encoder\" from the unsupervised multi-scale hybrid feature extraction network (MSHFE-Net) efficiently aggregates deeper multi-scale global features. This is achieved through the integration of the Parallel Aggregation Pyramid Pooling Module (PAPPM) after the Feed-Forward Neural Network (FFN) layer, allowing for the extension and fusion of contextual information across multiple scales. This hierarchical approach can be beneficial for language models to process information at various scales efficiently.\n\n### Dynamic Feature Fusion\nThe \"Three-Branch Feature Fusion (TBSFF)\" module from the TBSFF-UNet model dynamically selects and fuses semantic information from different levels. This module uses a selective mechanism to control the flow of semantic information, which can be adapted to language models to enhance the aggregation of features at different scales.\n\n## Hardware-Efficient Implementations\n\n### Efficient Attention Computation\nThe concept of splitting attention calculation into intra-blocks and inter-blocks, as suggested in \"Various Lengths, Constant Speed,\" can be applied to optimize attention computation in language models. Using conventional attention for intra-blocks and linear attention kernel tricks for inter-blocks can significantly reduce computational complexity and improve hardware utilization.\n\n### Optimized State Tracking and Normalization\nThe \"HierarchicalRMSNorm\" and the theoretical foundations from \"Theoretical Foundations of Deep Selective State-Space Models\" highlight the importance of efficient state tracking and normalization. Implementing multi-scale adaptive normalization and using selective mechanisms to manage state across different scales can help mitigate information loss and improve computational efficiency. These concepts can be integrated into the design of autoregressive language models to optimize state management and normalization.\n\n## Theoretical Analysis of Multi-Scale Processing\n\n### Selective State-Space Models\nThe theoretical work on \"Theoretical Foundations of Deep Selective State-Space Models\" provides a solid foundation for incorporating selective mechanisms in multi-scale processing. This work shows that random linear recurrences with input-controlled transitions can capture non-linear interactions between tokens at distinct timescales, which is crucial for understanding how to design efficient and expressive multi-scale processing in neural networks.\n\nBy incorporating these insights, researchers can develop autoregressive language models that:\n\n- Use selective attention mechanisms to efficiently handle multi-scale information.\n- Implement hardware-efficient normalization and attention computations.\n- Leverage theoretical foundations to optimize state tracking and multi-scale processing.\n\nThese approaches can help achieve better performance, efficiency, and scalability in autoregressive language models.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design HierarchicalRMSNorm introduces multi-scale adaptive normalization to improve feature processing across different scales. From analyzing the seed design and references, I've identified several key areas for potential improvement:\n\n1. State Management and Tracking:\n- Current designs struggle with accurate state tracking across different scales\n- Information loss between layers affects model performance\n- Need for more efficient state retention mechanisms\n\n2. Attention and Information Flow:\n- Linear attention implementations face challenges with cumulative summation operations\n- Need for better balance between local and global attention\n- Opportunity to improve information propagation across scales\n\n3. Computational Efficiency:\n- Memory bandwidth bottlenecks in current implementations\n- Need for more efficient parallel processing\n- Opportunity to optimize hardware utilization\n\nKey References and Excerpts:\n\n1. From \"Compositional Attention\":\n\"The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing.\"\n\nThis suggests potential for improving how information is processed and composed across different scales in normalization.\n\n2. From \"Longformer\":\n\"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length.\"\n\nThe sliding window attention concept could be adapted for efficient multi-scale processing.\n\n3. From \"Theoretical Foundations of Deep Selective State-Space Models\":\n\"when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.\"\n\nThis provides theoretical backing for incorporating selective mechanisms in multi-scale processing.\n\n4. From \"Various Lengths, Constant Speed\":\n\"we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks.\"\n\nThis suggests an efficient way to handle different scales of computation.\n\nKey Insights:\n\n1. Selective Processing:\n- Incorporating selective mechanisms can improve efficiency while maintaining expressiveness\n- Need to carefully balance local and global processing\n- Opportunity to leverage theoretical insights from SSMs\n\n2. Scale Integration:\n- Different processing strategies for different scales can improve efficiency\n- Need for efficient communication between scales\n- Potential for adaptive scale selection\n\n3. Hardware Efficiency:\n- Tiling and blocking strategies can improve hardware utilization\n- Memory access patterns crucial for performance\n- Opportunity to optimize for modern hardware architectures\n\nFuture Search Plan:\n\n1. Investigate recent advances in:\n- Selective attention mechanisms\n- Efficient multi-scale processing\n- Hardware-efficient implementations\n\n2. Look for:\n- Theoretical foundations of multi-scale processing\n- Efficient state tracking mechanisms\n- Memory optimization techniques\n\nReferences:\n\n[1] Mittal, S., Raparthy, S., Rish, I., Bengio, Y., & Lajoie, G. (2021). Compositional Attention: Disentangling Search and Retrieval. International Conference on Learning Representations.\n\n[2] Beltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The Long-Document Transformer. arXiv preprint.\n\n[3] Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., & Lyons, T. (2024). Theoretical Foundations of Deep Selective State-Space Models. arXiv preprint.\n\n[4] Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention. arXiv preprint."
        },
        {
            "ready": false,
            "query": "efficient state tracking mechanisms, hardware-efficient selective attention, multi-scale feature integration",
            "detail": "Find papers discussing:\n1. Efficient state tracking mechanisms in neural networks\n2. Hardware-efficient implementations of selective attention\n3. Methods for integrating features across multiple scales",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Efficient state tracking mechanisms in neural networks\n2. Hardware-efficient implementations of selective attention\n3. Methods for integrating features across multiple scales\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Perceiver IO: A General Architecture for Structured Inputs & Outputs (Avg. Score: 0.44)\n\n*Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Andrew Brock, Evan Shelhamer, Olivier J. H'enaff, M. Botvinick, Andrew Zisserman, O. Vinyals, Jo\u00e3o Carreira*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 458  (*Influential: 46*)\n\n**TL;DR:** This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.\n\n**Abstract:** A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain&task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.\n\n##### *Relevant Chunk: No. 2/54 (Score: 0.44)*\n\n```\n4.1), estimate optical flow without relying on explicit architectural features for multiscale correspondence (Sec. 4.2, learn joint representations of video, audio, and labels without separate network trunks (Sec. 4.3), and perform image classification with no information about the 2D structure of images ( Sec . A. ## 2 RELATEd WORK\n\nNeural network research has long sought architectures that can handle large, arbitrarily structured inputs and outputs. Autoencoding (Hinton \\& Zemel, 1994) was among the first attempts to build\nrepresentation which could encode and reproduce high-dimensional inputs like images. As hardware grew more powerful, neural nets led to breakthroughs in image understanding (Krizhevsky et al., 2012, Zeiler \\& Fergus, 2014; Szegedy et al., 2015) and interest intensified: autoregressive models that could process and complete samples of handwriting were developed (Graves 2013), and new convolutional network designs led to good results in structured output spaces like semantic segmentation (Farabet et al., 2012, Long et al., 2015, Ronneberger et al., 2015), pose estimation (Toshev \\& Szegedy, 2014), detection (Sermanet et al., 2014), captioning (You et al., 2016), and optical flow (Fischer et al. 2015). At the same time, natural language applications research has made extensive progressive in capturing the structured nature of language, typically via autoregressive models (Collobert et al. 2011; Sutskever et al., 2014, Vaswani et al., 2017, Radford et al., 2019, Brown et al, 2020) or context prediction (Mikolov et al., 2013; Pennington et al., 2014, Devlin et al., 2019). Similar to our work, several groups have proposed to solve tasks in multiple domains (e.g. Kaiser et al. 2017, Alayrac et al. 2020; Akbari et al. 2021, but typically across a fixed and predefined set of modalities by means of domain-specific networks. Although single-task specialist networks remain dominant in vision, multi-task learning has become popular (Misra et al., 2016, Doersch \\& Zisserman, 2017, Kokkinos, 2017, Zamir et al., 2018) and individual models achieve generality in a restricted domain: e.g. Mask-RCNN (He et al., 2017) handles object detection, segmentation, and pose estimation. In language, training or evaluation on multiple tasks has also become common (Collobert \\& Weston, 2008, Luong et al., 2016, Devlin et al., 2019, Liu et al., 2019, Raffel et al., 2020). Several groups have demonstrated that Transformers (originally designed for language) can be used or adapted to non-language tasks (e.g. Chen et al. 2020, Lu et al. 2021), but the limited scalability of Transformers limits their usefulness as general-purpose architectures. Several groups have proposed to use attention to manipulate the size of arrays or to introduce bottlenecks in processing. Set Transformers and related work (Lee et al., 2019, Goyal et al., 2022) use a learned query (\"inducing points\") to induce local bottlenecks by mapping a set back and forth from a set with fewer elements and learned decoder queries (\"seed vectors\") to map to outputs (\"pooling by multiheaded attention'). Each layer of these networks has complexity linear in the input size, while Perceivers use a deep latent network with complexity independent of the input and output. Our work uses attention over inputs and outputs of different sizes in part to produce an efficient attention architecture, and several other efficient attention architectures have been proposed, largely for language or small-scale problems (e.g.\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.16)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.16)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 3. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.06)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.09)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n##### *Relevant Chunk: No. 13/71 (Score: 0.03)*\n\n```\nInternational Conference on Machine Learning, 2024. [8] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation 9, 1997. [9] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. International Conference on Machine Learning, 2023. [10] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischlern. Metalearned neural memory.\n```\n\n#### 4. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.05)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 14/40 (Score: 0.05)*\n\n```\non pp. 1-4, 12, 16, 19, 29, 30). [13] Songlin Yang et al. \"Gated Linear Attention Transformers with Hardware-Efficient Training\". In: arXiv preprint arXiv:2312.06635 (2023) (cit.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient state tracking mechanisms, hardware-efficient selective attention, multi-scale feature integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking\n\n*From Search Query: efficient state tracking mechanisms*\n\n*Chia-Hsuan Lee, Hao Cheng, Mari Ostendorf*\n\n**TL;DR:** A novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance in dialogue state tracking tasks and reduces the computational costs by over 50%.\n\n**Abstract:** Large language models (LLMs) have revolutionized the landscape of Natural Language Processing, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Smaller Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. In dialogue state tracking tasks, the proposed routing framework enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 2. Diable: Efficient Dialogue State Tracking as Operations on Tables\n\n*From Search Query: efficient state tracking mechanisms*\n\n*Pietro Lesci, Yoshinari Fujinuma, Momchil Hardalov, Chao Shang, L. M\u00e0rquez*\n\n**TL;DR:** Diable is proposed, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models and robust to noisy data annotations due to the table operations approach.\n\n**Abstract:** Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to noisy data annotations due to the table operations approach.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 3. Efficient Dialogue State Tracking by Selectively Overwriting Memory\n\n*From Search Query: efficient state tracking mechanisms*\n\n*Sungdong Kim, Sohee Yang, Gyuwan Kim, Sang-Woo Lee*\n\n**TL;DR:** The accuracy gaps between the current and the ground truth-given situations are analyzed and it is suggested that it is a promising direction to improve state operation prediction to boost the DST performance.\n\n**Abstract:** Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 187  (*Influential: 49*)\n\n#### 4. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware-efficient selective attention*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 5. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware-efficient selective attention*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 6. SparQ Attention: Bandwidth-Efficient LLM Inference\n\n*From Search Query: hardware-efficient selective attention*\n\n*Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr*\n\n**TL;DR:** It is shown that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.\n\n**Abstract:** The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 1*)\n\n#### 7. An Extensible Framework for Open Heterogeneous Collaborative Perception\n\n*From Search Query: multi-scale feature integration*\n\n*Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Siheng Chen, Yanfeng Wang*\n\n**TL;DR:** Heterogeneous ALliance (HEAL), a novel extensible collaborative perception framework that surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types.\n\n**Abstract:** Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types. We further implement a comprehensive codebase at: https://github.com/yifanlu0227/HEAL\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 8. A Transformer-Based Object Detector with Coarse-Fine Crossing Representations\n\n*From Search Query: multi-scale feature integration*\n\n*Zhishan Li, Ying Nie, Kai Han, Jianyuan Guo, Lei Xie, Yunhe Wang*\n\n**TL;DR:** A local-global cross fusion module to establish the connection between local \ufb01ne-grained features and global coarse-grains features and an ef\ufb01cient feature integration module is presented for fusing multi-scale representations from different stages.\n\n**Abstract:** Transformer-based object detectors have shown competitive performance recently. Compared with convolutional neural networks limited by the relatively small receptive \ufb01elds, the advantage of transformer for visual tasks is the capacity to perceive long-range dependencies among all image patches, while the de\ufb01ciency is that the local \ufb01ne-grained information is not fully excavated. In this paper, we introduce the C oarse-grained and F ine-grained crossing representations to build an ef\ufb01cient D etection T ransformer (CFDT). Speci\ufb01cally, we propose a local-global cross fusion module to establish the connection between local \ufb01ne-grained features and global coarse-grained features. Besides, we propose a coarse-\ufb01ne aware neck which enables detection tokens to interact with both coarse-grained and \ufb01ne-grained features. Furthermore, an ef\ufb01cient feature integration module is presented for fusing multi-scale representations from different stages. Experimental results on the COCO dataset demonstrate the effectiveness of the proposed method. For instance, our CFDT achieves 48.1 AP with 173G FLOPs, which possesses higher accuracy and less computation compared with the state-of-the-art transformer-based detector ViDT. Code will be available at https: //gitee.com/mindspore/models/tree/master/research/cv/CFDT .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 9. Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping\n\n*From Search Query: multi-scale feature integration*\n\n*Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Z. Guo, Xiu Li*\n\n**TL;DR:** A multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images.\n\n**Abstract:** Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, Segment Anything Model (SAM), and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 67  (*Influential: 2*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Description-Driven Task-Oriented Dialog Modeling\n\n*From Search Query: efficient state tracking mechanisms*\n\n*Yonghui Wu, Izhak Shafran, Abhinav Rastogi, Harrison Lee, Mingqiu Wang, Dian Yu, Yuan Cao, Raghav Gupta, Jeffrey Zhao*\n\n**Abstract:** Task-oriented dialogue (TOD) systems are required to identify key information from conversations for the completion of given tasks. Such information is conventionally specified in terms of intents and slots contained in task-specific ontology or schemata. Since these schemata are designed by system developers, the naming convention for slots and intents is not uniform across tasks, and may not convey their semantics effectively. This can lead to models memorizing arbitrary patterns in data, resulting in suboptimal performance and generalization. In this paper, we propose that schemata should be modified by replacing names or notations entirely with natural language descriptions. We show that a language description-driven system exhibits better understanding of task specifications, higher performance on state tracking, improved data efficiency, and effective zero-shot transfer to unseen tasks. Following this paradigm, we present a simple yet effective Description-Driven Dialog State Tracking (D3ST) model, which relies purely on schema descriptions and an \"index-picking\" mechanism. We demonstrate the superiority in quality, data efficiency and robustness of our approach as measured on the MultiWOZ (Budzianowski et al.,2018), SGD (Rastogi et al., 2020), and the recent SGD-X (Lee et al., 2021) benchmarks.\n\n**Published:** 2022-01-21\n\n\n\n#### 2. Mamba-FETrack: Frame-Event Tracking via State Space Model\n\n*From Search Query: efficient state tracking mechanisms*\n\n*Bo Jiang, Xiao Wang, Zhe Wu, Shuai Wang, Shiao Wang, Ju Huang*\n\n**Abstract:** RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream). Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets. However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism. This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking. Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams. Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network. The fused features will be fed into the tracking head for target object localization. Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker. Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking. The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}.\n\n**Published:** 2024-04-28\n\n\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: hardware-efficient selective attention*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 4. DynaBERT: Dynamic BERT with Adaptive Width and Depth\n\n*From Search Query: hardware-efficient selective attention*\n\n*Xiao Chen, Lifeng Shang, Zhiqi Huang, Qun Liu, Lu Hou, Xin Jiang*\n\n**Abstract:** The pre-trained language models like BERT, though powerful in many natural language processing tasks, are both computation and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually compress the large BERT model to a fixed smaller size. They can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can flexibly adjust the size and latency by selecting adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allowing both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT-base (or RoBERTa-base), while at smaller widths and depths consistently outperforms existing BERT compression methods. Code is available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-04-08\n\n\n\n#### 5. Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth\n\n*From Search Query: multi-scale feature integration*\n\n*Woonghyun Ka, Junmo Kim, Sehwan Chun, Donggyu Joo, Pyungwhan Ahn, Doyeon Kim*\n\n**Abstract:** Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.\n\n**Published:** 2022-01-19\n\n\n\n#### 6. Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion\n\n*From Search Query: multi-scale feature integration*\n\n*Xiang Bai, Cong Yao, Zhaoyi Wan, Zhisheng Zou, Minghui Liao*\n\n**Abstract:** Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization (DB) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed DB module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion (ASF) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed DB and ASF with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks.\n\n**Published:** 2022-02-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, focusing on efficient state tracking mechanisms, hardware-efficient selective attention, and methods for integrating features across multiple scales, here are some key points and references that can be useful:\n\n## Efficient State Tracking Mechanisms\n\n### LSTM and Variants\nLong Short-Term Memory (LSTM) networks are a well-established approach for efficient state tracking in sequence models. LSTMs mitigate the vanishing gradient problem by using cells and gates to regulate the flow of information, allowing them to remember values over arbitrary time intervals.\n\n### Modern Upgrades\nRecent upgrades like the xLSTM, which combines parallelizable and state-tracking components, offer improved efficiency and scalability. The xLSTM architecture includes blocks that are parallelizable like Transformer architectures, which can enhance computational efficiency.\n\n## Hardware-Efficient Implementations of Selective Attention\n\n### Tiling and Blocking Strategies\nThe \"Lightning Attention-2\" approach leverages tiling to separately handle intra-block and inter-block components in linear attention calculations. This method can significantly reduce computational overhead and improve hardware utilization, making it a hardware-efficient implementation of selective attention.\n\n### Memory-Efficient Attention Mechanisms\nEfficient attention mechanisms, such as those described in \"Perceiver IO,\" use flexible querying mechanisms that enable outputs of various sizes and semantics. This flexibility can help in balancing the granularity of token selection with computational cost, leading to more efficient use of hardware resources.\n\n## Methods for Integrating Features Across Multiple Scales\n\n### Hierarchical Feature Aggregation\nHierarchical processing, as suggested by the analysis, can improve efficiency by integrating local and global information. The \"FlashFFTConv\" approach, which uses matrix decomposition to compute FFTs using matrix multiply units, enables kernel fusion for long sequences and reduces I/O, providing a method for efficient multi-scale computation.\n\n### Adaptive Scale Selection\nThe \"Perceiver IO\" architecture, which augments the Perceiver with a flexible querying mechanism, allows for the integration of features across multiple scales efficiently. This architecture enables outputs of various sizes and semantics, making it adaptable to different scale requirements.\n\n### State Space Models\nState space models can be efficient for sequence processing by maintaining a compact representation of the state. While not explicitly mentioned in the provided sources, state space models are a theoretical foundation that can be explored further for efficient state tracking and multi-scale feature integration.\n\n## Additional Considerations\n\n### Hardware-Aware Neural Architecture Design\nDesigning neural architectures with hardware considerations in mind is crucial. Strategies such as tiling, blocking, and optimizing memory access patterns can significantly improve performance. The \"Lightning Attention-2\" and \"FlashFFTConv\" papers provide insights into how these strategies can be implemented effectively.\n\n### Specialized Computation Patterns\nSpecialized computation patterns, such as those using tensor cores or other specialized hardware, can enhance efficiency. For example, \"FlashFFTConv\" utilizes tensor cores to perform efficient convolutions for long sequences, which can be beneficial for autoregressive language models.\n\nBy integrating these concepts\u2014efficient state tracking mechanisms like LSTMs and their variants, hardware-efficient selective attention methods such as tiling and blocking, and hierarchical feature aggregation techniques\u2014the researcher can develop an autoregressive language model that is more efficient, scalable, and performant.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nFrom analyzing the search results, I've identified several promising directions for improving the HierarchicalRMSNorm design:\n\na) Selective Processing:\n- Current designs struggle with efficient token selection and processing\n- Need for dynamic, content-aware selection mechanisms\n- Opportunity to integrate hardware-efficient selective attention\n\nb) Multi-scale Architecture:\n- Current approaches to multi-scale processing often lack efficiency\n- Need for better integration of local and global information\n- Opportunity for hierarchical feature aggregation\n\nc) Hardware Optimization:\n- Memory bandwidth remains a bottleneck\n- Need for more efficient computation patterns\n- Opportunity for specialized hardware-aware implementations\n\n2. Useful References with Excerpts:\n\nFrom \"FlashFFTConv\":\n\"FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.\"\nThis suggests potential approaches for efficient multi-scale computation.\n\nFrom \"Lightning Attention-2\":\n\"We leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation.\"\nThis provides insights into efficient attention computation.\n\nFrom \"Perceiver IO\":\n\"Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics.\"\nThis suggests ways to handle multi-scale information efficiently.\n\nFrom \"SPECTRA\":\n\"Our approach greatly eases training and rationale regularization, generally outperforming previous work on what comes to performance and plausibility.\"\nThis offers insights into selective processing mechanisms.\n\n3. Key Insights:\n\na) Selective Processing:\n- Dynamic token selection can significantly reduce computational overhead\n- Content-aware selection mechanisms improve efficiency\n- Need to balance selection granularity with computational cost\n\nb) Multi-scale Architecture:\n- Hierarchical processing can improve efficiency\n- Need for efficient communication between scales\n- Opportunity for adaptive scale selection\n\nc) Hardware Optimization:\n- Tiling and blocking strategies improve hardware utilization\n- Memory access patterns crucial for performance\n- Need for specialized computation patterns\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Recent advances in efficient state tracking mechanisms\n2. Hardware-efficient implementations of selective attention\n3. Theoretical foundations of multi-scale feature integration\n\nSpecific areas to explore:\n- State space models for efficient sequence processing\n- Memory-efficient attention mechanisms\n- Hardware-aware neural architecture design\n\n5. References:\n\n[1] Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & Re, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[2] Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n[3] Jaegle, A., et al. (2021). Perceiver IO: A General Architecture for Structured Inputs & Outputs. International Conference on Learning Representations.\n\n[4] Guerreiro, N. M., & Martins, A. F. T. (2021). SPECTRA: Sparse Structured Text Rationalization. Conference on Empirical Methods in Natural Language Processing.\n\n[5] Chen, L., Collins, M. D., Zhu, Y., Papandreou, G., Zoph, B., Schroff, F., Adam, H., & Shlens, J. (2018). Searching for Efficient Multi-Scale Architectures for Dense Image Prediction. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "efficient state compression, adaptive feature selection, hierarchical processing optimization",
            "detail": "Find papers discussing:\n1. State compression techniques in neural networks\n2. Adaptive feature selection mechanisms\n3. Hardware-efficient hierarchical processing methods",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. State compression techniques in neural networks\n2. Adaptive feature selection mechanisms\n3. Hardware-efficient hierarchical processing methods\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.97)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.97)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.88)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.88)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.49)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.49)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.35)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.35)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 5. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.26)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.26)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n\n\n---\n## Found 15 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient state compression, adaptive feature selection, hierarchical processing optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models\n\n*From Search Query: efficient state compression*\n\n*Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan*\n\n**TL;DR:** This paper develops a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods and can be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training and lower extra storage cost than the existing works.\n\n**Abstract:** Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to $2.6\\times$ speedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the existing works.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression\n\n*From Search Query: efficient state compression*\n\n*Yufeng Zhang, Hang Yu, Jianguo Li, Weiyao Lin*\n\n**TL;DR:** A novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression that incorporates an efficient autoregressive Markov model based entropy coder and a Straight-Through Hardmax Quantization scheme to enhance the optimization of discrete latent space.\n\n**Abstract:** A BSTRACT Learned lossless data compression has garnered significant attention recently due to its superior compression ratios compared to traditional compressors. However, the computational efficiency of these models jeopardizes their practicality. This paper proposes a novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression. Our approach incorporates two essential innovations. First, we propose the Finite-State AutoRe-gressive (FSAR) entropy coder, an efficient autoregressive Markov model based entropy coder that utilizes a lookup table to expedite autoregressive entropy coding. Next, we present a Straight-Through Hardmax Quantization (STHQ) scheme to enhance the optimization of discrete latent space. Our experiments show that the proposed lossless compression method could improve the compression ratio by up to 6% compared to the baseline, with negligible extra computational time. Our work provides valuable insights into enhancing the computational efficiency of learned lossless data compression, which can have practical applications in various fields. Code is available at https://github.com/alipay/Finite_ State_Autoregressive_Entropy_Coding .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training\n\n*From Search Query: efficient state compression*\n\n*Chia-Yu Chen, Jiamin Ni, Songtao Lu, Xiaodong Cui, Pin-Yu Chen, Xiao Sun, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Wei Zhang, K. Gopalakrishnan*\n\n**TL;DR:** This work proposes a new compression technique, Scalable Sparsified Gradient Compression (ScaleCom), that leverages similarity in the gradient distribution amongst learners to provide significantly improved scalability and provides favorable convergence guarantees and is compatible with gradient all-reduce techniques.\n\n**Abstract:** Large-scale distributed training of Deep Neural Networks (DNNs) on state-of-the-art platforms is expected to be severely communication constrained. To overcome this limitation, numerous gradient compression techniques have been proposed and have demonstrated high compression ratios. However, most existing methods do not scale well to large scale distributed systems (due to gradient build-up) and/or fail to evaluate model fidelity (test accuracy) on large datasets. To mitigate these issues, we propose a new compression technique, Scalable Sparsified Gradient Compression (ScaleCom), that leverages similarity in the gradient distribution amongst learners to provide significantly improved scalability. Using theoretical analysis, we show that ScaleCom provides favorable convergence guarantees and is compatible with gradient all-reduce techniques. Furthermore, we experimentally demonstrate that ScaleCom has small overheads, directly reduces gradient traffic and provides high compression rates (65-400X) and excellent scalability (up to 64 learners and 8-12X larger batch sizes over standard training) across a wide range of applications (image, language, and speech) without significant accuracy loss.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 62  (*Influential: 3*)\n\n#### 4. Adaptive Feature Selection for No-Reference Image Quality Assessment by Mitigating Semantic Noise Sensitivity\n\n*From Search Query: adaptive feature selection*\n\n*Xudong Li, Timin Gao, Runze Hu, Yan Zhang, Shengchuan Zhang, Xiawu Zheng, Jingyuan Zheng, Yunhang Shen, Ke Li, Yutao Liu, Pingyang Dai, Rongrong Ji*\n\n**TL;DR:** A Quality-Aware Feature Matching IQA Metric (QFM-IQM) that employs an adversarial perspective to remove harmful semantic noise features from the upstream task and achieves superior performance to the state-of-the-art NR-IQA methods on eight standard IQA datasets.\n\n**Abstract:** The current state-of-the-art No-Reference Image Quality Assessment (NR-IQA) methods typically rely on feature extraction from upstream semantic backbone networks, assuming that all extracted features are relevant. However, we make a key observation that not all features are beneficial, and some may even be harmful, necessitating careful selection. Empirically, we find that many image pairs with small feature spatial distances can have vastly different quality scores, indicating that the extracted features may contain a significant amount of quality-irrelevant noise. To address this issue, we propose a Quality-Aware Feature Matching IQA Metric (QFM-IQM) that employs an adversarial perspective to remove harmful semantic noise features from the upstream task. Specifically, QFM-IQM enhances the semantic noise distinguish capabilities by matching image pairs with similar quality scores but varying semantic features as adversarial semantic noise and adaptively adjusting the upstream task's features by reducing sensitivity to adversarial noise perturbation. Furthermore, we utilize a distillation framework to expand the dataset and improve the model's generalization ability. Our approach achieves superior performance to the state-of-the-art NR-IQA methods on eight standard IQA datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under RIP\n\n*From Search Query: adaptive feature selection*\n\n*Satyen Kale, Zohar S. Karnin, Tengyuan Liang, D. P\u00e1l*\n\n**TL;DR:** This paper makes the assumption that data matrix satisfies restricted isometry property, and shows that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem.\n\n**Abstract:** Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 22  (*Influential: 3*)\n\n#### 6. A New Computationally Efficient Algorithm to solve Feature Selection for Functional Data Classification in High-dimensional Spaces\n\n*From Search Query: adaptive feature selection*\n\n*Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Alessandra Pascale, Jonathan Epperlein*\n\n**TL;DR:** A novel methodology for Feature Selection for Functional Classification, FSFC, that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and multivariate longitudinal features and can be leveraged to significantly reduce the problem's dimensionality and enhance the performances of other classification algorithms.\n\n**Abstract:** This paper introduces a novel methodology for Feature Selection for Functional Classification, FSFC, that addresses the challenge of jointly performing feature selection and classification of functional data in scenarios with categorical responses and multivariate longitudinal features. FSFC tackles a newly defined optimization problem that integrates logistic loss and functional features to identify the most crucial variables for classification. To address the minimization procedure, we employ functional principal components and develop a new adaptive version of the Dual Augmented Lagrangian algorithm. The computational efficiency of FSFC enables handling high-dimensional scenarios where the number of features may considerably exceed the number of statistical units. Simulation experiments demonstrate that FSFC outperforms other machine learning and deep learning methods in computational time and classification accuracy. Furthermore, the FSFC feature selection capability can be leveraged to significantly reduce the problem's dimensionality and enhance the performances of other classification algorithms. The efficacy of FSFC is also demonstrated through a real data application, analyzing relationships between four chronic diseases and other health and demographic factors.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. A Hierarchical Reinforcement Learning Based Optimization Framework for Large-scale Dynamic Pickup and Delivery Problems\n\n*From Search Query: hierarchical processing optimization*\n\n*Yi Ma, Xiaotian Hao, Jianye Hao, Jiawen Lu, Xingfu Liu, Xialiang Tong, M. Yuan, Zhigang Li, Jiehui Tang, Zhaopeng Meng*\n\n**TL;DR:** An upper-level agent is designed to dynamically partition the DPDP into a series of sub-problems with different scales to optimize vehicles routes towards globally better solutions and a lower-level agent is designed to solve each sub-problem by incorporating the strengths of classical operational research-based methods with reinforcement learning-based policies.\n\n**Abstract:** The Dynamic Pickup and Delivery Problem (DPDP) is an essential problem in the logistics domain, which is NP-hard. The objective is to dynamically schedule vehicles among multiple sites to serve the online generated orders such that the overall transportation cost could be minimized. The critical challenge of DPDP is the orders are not known a priori, i.e., the orders are dynamically generated in real-time. To address this problem, existing methods partition the overall DPDP into \ufb01xed-size sub-problems by caching online generated orders and solve each sub-problem, or on this basis to utilize the predicted future orders to optimize each sub-problem further. However, the solution quality and ef\ufb01ciency of these methods are unsatisfactory, especially when the problem scale is very large. In this paper, we propose a novel hierarchical optimization framework to better solve large-scale DPDPs. Speci\ufb01cally, we design an upper-level agent to dynamically partition the DPDP into a series of sub-problems with different scales to optimize vehicles routes towards globally better solutions. Besides, a lower-level agent is designed to ef\ufb01ciently solve each sub-problem by incorporating the strengths of classical operational research-based methods with reinforcement learning-based policies. To verify the effectiveness of the proposed framework, real historical data is collected from the order dispatching system of Huawei Supply Chain Business Unit and used to build a functional simulator. Extensive of\ufb02ine simulation and online testing conducted\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 54  (*Influential: 0*)\n\n#### 8. Poincar\u00e9 Embeddings for Learning Hierarchical Representations\n\n*From Search Query: hierarchical processing optimization*\n\n*Maximilian Nickel, Douwe Kiela*\n\n**TL;DR:** This work introduces a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincare ball -- and introduces an efficient algorithm to learn the embeddings based on Riemannian optimization.\n\n**Abstract:** Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincare ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincare embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 1181  (*Influential: 212*)\n\n#### 9. Joint Autoregressive and Hierarchical Priors for Learned Image Compression\n\n*From Search Query: hierarchical processing optimization*\n\n*David C. Minnen, J. Ball\u00e9, G. Toderici*\n\n**TL;DR:** It is found that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models.\n\n**Abstract:** Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate\u2013distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1088  (*Influential: 277*)\n\n### 1 related papers from ArXiv\n\n#### 1. Adaptive Feature Selection for End-to-End Speech Translation\n\n*From Search Query: adaptive feature selection*\n\n*Biao Zhang, Ivan Titov, Barry Haddow, Rico Sennrich*\n\n**Abstract:** Information in speech signals is not evenly distributed, making it an\nadditional challenge for end-to-end (E2E) speech translation (ST) to learn to\nfocus on informative features. In this paper, we propose adaptive feature\nselection (AFS) for encoder-decoder based E2E ST. We first pre-train an ASR\nencoder and apply AFS to dynamically estimate the importance of each encoded\nspeech feature to SR. A ST encoder, stacked on top of the ASR encoder, then\nreceives the filtered features from the (frozen) ASR encoder. We take L0DROP\n(Zhang et al., 2020) as the backbone for AFS, and adapt it to sparsify speech\nfeatures with respect to both temporal and feature dimensions. Results on\nLibriSpeech En-Fr and MuST-C benchmarks show that AFS facilitates learning of\nST by pruning out ~84% temporal features, yielding an average translation gain\nof ~1.3-1.6 BLEU and a decoding speedup of ~1.4x. In particular, AFS reduces\nthe performance gap compared to the cascade baseline, and outperforms it on\nLibriSpeech En-Fr with a BLEU score of 18.56 (without data augmentation)\n\n**Published:** 2020-10-16T17:21:00Z  (*Updated: 2020-10-20T13:53:39Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: efficient state compression*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 2. HPTQ: Hardware-Friendly Post Training Quantization\n\n*From Search Query: efficient state compression*\n\n*Arnon Netzer, Roy H. Jennings, Idit Diamant, Oranit Dror, Lior Dikstein, Elad Cohen, Reuven Peretz, Hai Victor Habi*\n\n**Abstract:** Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.\n\n**Published:** 2021-09-19\n\n\n\n#### 3. Learning Adaptive Discriminative Correlation Filters via Temporal Consistency Preserving Spatial Feature Selection for Robust Visual Tracking\n\n*From Search Query: adaptive feature selection*\n\n*Josef Kittler, Xiao-Jun Wu, Zhen-Hua Feng, Tianyang Xu*\n\n**Abstract:** With efficient appearance learning models, Discriminative Correlation Filter (DCF) has been proven to be very successful in recent video object tracking benchmarks and competitions. However, the existing DCF paradigm suffers from two major issues, i.e., spatial boundary effect and temporal filter degradation. To mitigate these challenges, we propose a new DCF-based tracking method. The key innovations of the proposed method include adaptive spatial feature selection and temporal consistent constraints, with which the new tracker enables joint spatial-temporal filter learning in a lower dimensional discriminative manifold. More specifically, we apply structured spatial sparsity constraints to multi-channel filers. Consequently, the process of learning spatial filters can be approximated by the lasso regularisation. To encourage temporal consistency, the filter model is restricted to lie around its historical value and updated locally to preserve the global structure in the manifold. Last, a unified optimisation framework is proposed to jointly select temporal consistency preserving spatial features and learn discriminative filters with the augmented Lagrangian method. Qualitative and quantitative evaluations have been conducted on a number of well-known benchmarking datasets such as OTB2013, OTB50, OTB100, Temple-Colour, UAV123 and VOT2018. The experimental results demonstrate the superiority of the proposed method over the state-of-the-art approaches.\n\n**Published:** 2018-07-30\n\n\n\n#### 4. Efficient Motion Planning for Automated Lane Change based on Imitation Learning and Mixed-Integer Optimization\n\n*From Search Query: hierarchical processing optimization*\n\n*Chenyang Xi, Yuankai Wu, Tianyu Shi, Lijun Sun*\n\n**Abstract:** Intelligent motion planning is one of the core components in automated vehicles, which has received extensive interests. Traditional motion planning methods suffer from several drawbacks in terms of optimality, efficiency and generalization capability. Sampling based methods cannot guarantee the optimality of the generated trajectories. Whereas the optimization-based methods are not able to perform motion planning in real-time, and limited by the simplified formalization. In this work, we propose a learning-based approach to handle those shortcomings. Mixed Integer Quadratic Problem based optimization (MIQP) is used to generate the optimal lane-change trajectories which served as the training dataset for learning-based action generation algorithms. A hierarchical supervised learning model is devised to make the fast lane-change decision. Numerous experiments have been conducted to evaluate the optimality, efficiency, and generalization capability of the proposed approach. The experimental results indicate that the proposed model outperforms several commonly used motion planning baselines.\n\n**Published:** 2019-04-18\n\n\n\n#### 5. Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization\n\n*From Search Query: hierarchical processing optimization*\n\n*Xuedong Huang, Jianfeng Gao, Michael Zeng, Wayne Xiong, Chenguang Zhu, Yu Shi, Hany Hassan Awadalla, Ruochen Xu, Yang Liu, Jie Mei, Song Wang, Liyang Lu, Baolin Peng, Pengcheng He*\n\n**Abstract:** This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.\n\n**Published:** 2022-08-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model with a focus on efficient state compression, adaptive feature selection, and hardware-efficient hierarchical processing, here are some key findings and references that align with your goals:\n\n### Efficient State Compression Techniques\n\n1. **Model Compression Methods**:\n   - The paper on \"Ultimate Compression: Joint Method of Quantization and Tensor Decomposition\" discusses advanced compression techniques that can significantly reduce the model size while maintaining competitive accuracy. This method combines quantization and tensor decomposition, achieving high compression ratios (e.g., 32.74\u00d7 for some models) without substantial accuracy loss.\n   - Another approach is parameter pruning, as discussed in the paper on \"Efficient Model Compression for Bayesian Neural Networks.\" This method uses posterior inclusion probabilities to prune parameters, leading to better generalizability and efficiency.\n\n2. **NeuZip Algorithm**:\n   - The NeuZip algorithm proposes a method to compress neural networks dynamically while maintaining their full capabilities. This involves compressing floating-point numbers, which can be particularly useful for reducing memory usage during training and inference.\n\n### Adaptive Feature Selection Mechanisms\n\n1. **Pruning and Feature Selection**:\n   - The work on \"Efficient Model Compression for Bayesian Neural Networks\" also touches on adaptive feature selection through parameter pruning. By using posterior inclusion probabilities, the model can selectively retain or discard parameters, which is a form of adaptive feature selection.\n   - The \"lottery ticket hypothesis\" and methods like SNIP and SynFlow, discussed in the guide to advances in deep learning efficiency, provide strategies for pruning weights before or during training based on their sensitivity to the loss function. These methods can be seen as adaptive feature selection mechanisms.\n\n2. **Selective State Retention**:\n   - Theoretical foundations of deep selective state-space models suggest using input-controlled transitions to selectively retain or update the hidden state. This approach can help in capturing non-linear interactions between tokens at different timescales efficiently[Analysis Note].\n\n### Hardware-Efficient Hierarchical Processing Methods\n\n1. **Hardware-Efficient Linear Attention**:\n   - The \"Short-Long Convolutions Help Hardware-Efficient Linear Attention\" paper introduces a divide-and-conquer approach to linear attention computation, which is more hardware-efficient. This method, known as CHELA, maintains global abstraction and data-dependent selection while reducing computational complexity[Analysis Note].\n   - The \"SparQ Attention\" method optimizes memory bandwidth usage within attention layers by selective fetching of cached history, leading to significant savings in attention data transfers without substantial accuracy drops[Analysis Note].\n\n2. **Tiling and Blocking Strategies**:\n   - Efficient deep learning guides emphasize the importance of tiling and blocking strategies for optimizing memory usage and computation. These strategies are crucial for hardware-efficient implementations of hierarchical processing.\n\n3. **Specialized Computation Patterns**:\n   - Optimizers like Shampoo, which use block diagonal or dense preconditioning matrices, can converge quicker and are more efficient in terms of computation and memory usage. This is relevant for optimizing the training process of large models.\n\n### Summary\n\n- **State Compression**: Techniques like joint quantization and tensor decomposition, parameter pruning using Bayesian methods, and the NeuZip algorithm can efficiently compress model states.\n- **Adaptive Feature Selection**: Methods such as SNIP, SynFlow, and posterior inclusion probability-based pruning can adaptively select features.\n- **Hardware-Efficient Hierarchical Processing**: Approaches like CHELA, SparQ Attention, and tiling/blocking strategies can optimize hierarchical processing for better hardware efficiency.\n\nThese findings and references should help in designing an autoregressive language model that is efficient in state management, adaptive in feature selection, and optimized for hardware performance.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nFrom analyzing the search results, I've identified several promising directions for improving the HierarchicalRMSNorm design:\n\na) State Tracking and Memory Management:\n- Current designs struggle with efficient state management across different scales\n- Need for selective state retention mechanisms\n- Opportunity to integrate hardware-efficient state tracking\n\nb) Attention and Feature Integration:\n- Linear attention implementations can be optimized through tiling and blocking\n- Need for efficient integration of local and global information\n- Opportunity to leverage selective mechanisms for better feature composition\n\nc) Hardware Efficiency:\n- Memory bandwidth remains a key bottleneck\n- Need for specialized computation patterns\n- Opportunity for hardware-aware implementation strategies\n\n2. Useful References with Excerpts:\n\nFrom \"Gated Linear Attention Transformers\":\n\"We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates [...] especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\"\n\nFrom \"Short-Long Convolutions Help Hardware-Efficient Linear Attention\":\n\"CHELA [...] implements linear attention in a divide-and-conquer manner [...] enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\nFrom \"SparQ Attention\":\n\"brings up to 8x savings in attention data transfers without substantial drops in accuracy [...] by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history.\"\n\nFrom \"Theoretical Foundations of Deep Selective State-Space Models\":\n\"when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection [...] capturing non-linear interactions between tokens at distinct timescales.\"\n\n3. Key Insights:\n\na) Selective Processing:\n- Data-dependent gates can improve efficiency and generalization\n- Divide-and-conquer strategies for attention computation\n- Selective state retention mechanisms for better memory utilization\n\nb) Hardware Optimization:\n- Tiling and blocking strategies crucial for performance\n- Memory bandwidth optimization through selective fetching\n- Need for specialized computation patterns\n\nc) Feature Integration:\n- Hierarchical processing can improve efficiency\n- Balance between local and global information important\n- Opportunity for adaptive scale selection\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Recent advances in efficient state compression techniques\n2. Methods for adaptive feature selection and integration\n3. Hardware-efficient implementations of hierarchical processing\n\n5. References:\n\n[1] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n[2] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n[3] Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi, C., & Orr, D. (2023). SparQ Attention: Bandwidth-Efficient LLM Inference. International Conference on Machine Learning.\n\n[4] Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., & Lyons, T. (2024). Theoretical Foundations of Deep Selective State-Space Models. arXiv preprint.\n\n[5] Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "adaptive computation mechanisms, multi-scale feature integration, hardware-efficient normalization",
            "detail": "Find papers discussing:\n1. Adaptive computation mechanisms in neural networks\n2. Multi-scale feature integration techniques\n3. Hardware-efficient normalization implementations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Adaptive computation mechanisms in neural networks\n2. Multi-scale feature integration techniques\n3. Hardware-efficient normalization implementations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.42)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.42)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n#### 2. Liquid Structural State-Space Models (Avg. Score: 0.16)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 28/54 (Score: 0.16)*\n\n```\nJ. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982. J. Jia and A. R. Benson. Neural jump stochastic differential equations. In Advances in Neural Information Processing Systems, pages 9843-9854, 2019. L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and Y. Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation, 31(4):765-783, 2019. A. Kag, Z. Zhang, and V. Saligrama. Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients? In International Conference on Learning Representations, 2019. R. KALMAN. A new approach to linear filtering and prediction problems.\n```\n\n#### 3. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.16)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.16)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 4. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.04)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 28/54 (Score: 0.04)*\n\n```\nIn Advances in Neural Information Processing Systems, pages 15544-15553, 2019. [72] Aaron R Voelker and Chris Eliasmith. Improving spiking dynamical networks: Accurate delays, higherorder synapses, and time cells. Neural computation, 30(3):569-609, 2018. [73] Aaron Russell Voelker. Dynamical systems in spiking neuromorphic hardware. PhD thesis, University of Waterloo, 2019. [74] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In The International Conference on Learning Representations $(I C L R), 2019$. [75] Jiasong Wu, Lu Wang, Guanyu Yang, Lotfi Senhadji, Limin Luo, and Huazhong Shu. Sliding conjugate symmetric sequency-ordered complex Hadamard transform: fast algorithm and applications. IEEE Transactions on Circuits and Systems I: Regular Papers, 59(6):1321-1334, 2012. [76] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S Schoenholz. A mean field theory of batch normalization.\n```\n\n#### 5. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.01)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.01)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive computation mechanisms, multi-scale feature integration, hardware-efficient normalization\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Adaptive Privacy Composition for Accuracy-first Mechanisms\n\n*From Search Query: adaptive computation mechanisms*\n\n*Ryan M. Rogers, G. Samorodnitsky, Zhiwei Steven Wu, Aaditya Ramdas*\n\n**TL;DR:** Privacy filters are developed that allow an analyst to adaptively switch between differentially private and ex-post private mechanisms subject to an overall differential privacy guarantee.\n\n**Abstract:** In many practical applications of differential privacy, practitioners seek to provide the best privacy guarantees subject to a target level of accuracy. A recent line of work by Ligett et al. '17 and Whitehouse et al. '22 has developed such accuracy-first mechanisms by leveraging the idea of noise reduction that adds correlated noise to the sufficient statistic in a private computation and produces a sequence of increasingly accurate answers. A major advantage of noise reduction mechanisms is that the analysts only pay the privacy cost of the least noisy or most accurate answer released. Despite this appealing property in isolation, there has not been a systematic study on how to use them in conjunction with other differentially private mechanisms. A fundamental challenge is that the privacy guarantee for noise reduction mechanisms is (necessarily) formulated as ex-post privacy that bounds the privacy loss as a function of the released outcome. Furthermore, there has yet to be any study on how ex-post private mechanisms compose, which allows us to track the accumulated privacy over several mechanisms. We develop privacy filters [Rogers et al. '16, Feldman and Zrnic '21, and Whitehouse et al. '22'] that allow an analyst to adaptively switch between differentially private and ex-post private mechanisms subject to an overall differential privacy guarantee.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Multi-Epoch Matrix Factorization Mechanisms for Private Machine Learning\n\n*From Search Query: adaptive computation mechanisms*\n\n*Christopher A. Choquette-Choo, H. B. McMahan, Keith Rush, Abhradeep Thakurta*\n\n**TL;DR:** This work formalizes the problem of DP mechanisms for adaptive streams with multiple participations and introduces a non-trivial extension of online matrix factorization DP mechanisms to the authors' setting, establishing the necessary theory for sensitivity calculations and efficient computation of optimal matrices.\n\n**Abstract:** We introduce new differentially private (DP) mechanisms for gradient-based machine learning (ML) with multiple passes (epochs) over a dataset, substantially improving the achievable privacy-utility-computation tradeoffs. We formalize the problem of DP mechanisms for adaptive streams with multiple participations and introduce a non-trivial extension of online matrix factorization DP mechanisms to our setting. This includes establishing the necessary theory for sensitivity calculations and efficient computation of optimal matrices. For some applications like $>\\!\\! 10,000$ SGD steps, applying these optimal techniques becomes computationally expensive. We thus design an efficient Fourier-transform-based mechanism with only a minor utility loss. Extensive empirical evaluation on both example-level DP for image classification and user-level DP for language modeling demonstrate substantial improvements over all previous methods, including the widely-used DP-SGD . Though our primary application is to ML, our main DP results are applicable to arbitrary linear queries and hence may have much broader applicability.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 3. Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion\n\n*From Search Query: adaptive computation mechanisms*\n\n*Shangyu Wu, Ying Xiong, Yufei Cui, Xue Liu, Buzhou Tang, Tei-Wei Kuo, Chun Jason Xue*\n\n**TL;DR:** A new paradigm of RA named ReFusion is proposed, a computation-efficient Retrieval representation Fusion with bi-level optimization, which directly fuses the retrieval representations into the hidden states of models.\n\n**Abstract:** Retrieval-based augmentations (RA) incorporating knowledge from an external database into language models have greatly succeeded in various knowledge-intensive (KI) tasks. However, integrating retrievals in non-knowledge-intensive (NKI) tasks is still challenging. Existing works focus on concatenating retrievals with inputs to improve model performance. Unfortunately, the use of retrieval concatenation-based augmentations causes an increase in the input length, substantially raising the computational demands of attention mechanisms. This paper proposes a new paradigm of RA named \\textbf{ReFusion}, a computation-efficient Retrieval representation Fusion with bi-level optimization. Unlike previous works, ReFusion directly fuses the retrieval representations into the hidden states of models. Specifically, ReFusion leverages an adaptive retrieval integrator to seek the optimal combination of the proposed ranking schemes across different model layers. Experimental results demonstrate that the proposed ReFusion can achieve superior and robust performance in various NKI tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. An Extensible Framework for Open Heterogeneous Collaborative Perception\n\n*From Search Query: multi-scale feature integration*\n\n*Yifan Lu, Yue Hu, Yiqi Zhong, Dequan Wang, Siheng Chen, Yanfeng Wang*\n\n**TL;DR:** Heterogeneous ALliance (HEAL), a novel extensible collaborative perception framework that surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types.\n\n**Abstract:** Collaborative perception aims to mitigate the limitations of single-agent perception, such as occlusions, by facilitating data exchange among multiple agents. However, most current works consider a homogeneous scenario where all agents use identity sensors and perception models. In reality, heterogeneous agent types may continually emerge and inevitably face a domain gap when collaborating with existing agents. In this paper, we introduce a new open heterogeneous problem: how to accommodate continually emerging new heterogeneous agent types into collaborative perception, while ensuring high perception performance and low integration cost? To address this problem, we propose HEterogeneous ALliance (HEAL), a novel extensible collaborative perception framework. HEAL first establishes a unified feature space with initial agents via a novel multi-scale foreground-aware Pyramid Fusion network. When heterogeneous new agents emerge with previously unseen modalities or models, we align them to the established unified space with an innovative backward alignment. This step only involves individual training on the new agent type, thus presenting extremely low training costs and high extensibility. To enrich agents' data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets show that HEAL surpasses SOTA methods in performance while reducing the training parameters by 91.5% when integrating 3 new agent types. We further implement a comprehensive codebase at: https://github.com/yifanlu0227/HEAL\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 5. A Transformer-Based Object Detector with Coarse-Fine Crossing Representations\n\n*From Search Query: multi-scale feature integration*\n\n*Zhishan Li, Ying Nie, Kai Han, Jianyuan Guo, Lei Xie, Yunhe Wang*\n\n**TL;DR:** A local-global cross fusion module to establish the connection between local \ufb01ne-grained features and global coarse-grains features and an ef\ufb01cient feature integration module is presented for fusing multi-scale representations from different stages.\n\n**Abstract:** Transformer-based object detectors have shown competitive performance recently. Compared with convolutional neural networks limited by the relatively small receptive \ufb01elds, the advantage of transformer for visual tasks is the capacity to perceive long-range dependencies among all image patches, while the de\ufb01ciency is that the local \ufb01ne-grained information is not fully excavated. In this paper, we introduce the C oarse-grained and F ine-grained crossing representations to build an ef\ufb01cient D etection T ransformer (CFDT). Speci\ufb01cally, we propose a local-global cross fusion module to establish the connection between local \ufb01ne-grained features and global coarse-grained features. Besides, we propose a coarse-\ufb01ne aware neck which enables detection tokens to interact with both coarse-grained and \ufb01ne-grained features. Furthermore, an ef\ufb01cient feature integration module is presented for fusing multi-scale representations from different stages. Experimental results on the COCO dataset demonstrate the effectiveness of the proposed method. For instance, our CFDT achieves 48.1 AP with 173G FLOPs, which possesses higher accuracy and less computation compared with the state-of-the-art transformer-based detector ViDT. Code will be available at https: //gitee.com/mindspore/models/tree/master/research/cv/CFDT .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping\n\n*From Search Query: multi-scale feature integration*\n\n*Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Z. Guo, Xiu Li*\n\n**TL;DR:** A multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images.\n\n**Abstract:** Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, Segment Anything Model (SAM), and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 67  (*Influential: 2*)\n\n#### 7. MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions\n\n*From Search Query: hardware-efficient normalization*\n\n*Neo Wei Ming, Zhehui Wang, Cheng Liu, R. Goh, Tao Luo*\n\n**TL;DR:** MA-BERT is proposed, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss and enables hardware-friendly processing on various computing engines, including CPUs and GPUs.\n\n**Abstract:** Due to their superior results, Transformer-based models such as BERT have become de facto standards in many Natural Language Processing (NLP) applications. However, the intensive use of complex non-linear functions within the Transformer architecture impairs its computing efficiency and complicates corresponding accelerator designs, because non-linear functions are generally computation-intensive and require special hardware support. In light of this, we propose MA-BERT, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss. Specifically, we propose four correlated techniques that include approximating softmax with a two-layer neural network, replacing GELU with ReLU, fusing normalization layers with adjacent linear layers, and leveraging knowledge transfer from baseline models. Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy. With mainly regular matrix arithmetic operations, MA-BERT enables hardware-friendly processing on various computing engines, including CPUs and GPUs. Our experimental results show that MA-BERT achieves up to 27% and 41% reduction in inference time on CPU and GPU, respectively, with comparable accuracy on many downstream tasks compared to\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 8. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware-efficient normalization*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 9. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: hardware-efficient normalization*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n### 1 related papers from ArXiv\n\n#### 1. DACT-BERT: Differentiable Adaptive Computation Time for an Efficient\n  BERT Inference\n\n*From Search Query: adaptive computation mechanisms*\n\n*Crist\u00f3bal Eyzaguirre, Felipe del R\u00edo, Vladimir Araujo, \u00c1lvaro Soto*\n\n**Abstract:** Large-scale pre-trained language models have shown remarkable results in\ndiverse NLP applications. Unfortunately, these performance gains have been\naccompanied by a significant increase in computation time and model size,\nstressing the need to develop new or complementary strategies to increase the\nefficiency of these models. In this paper we propose DACT-BERT, a\ndifferentiable adaptive computation time strategy for BERT-like models.\nDACT-BERT adds an adaptive computational mechanism to BERT's regular processing\npipeline, which controls the number of Transformer blocks that need to be\nexecuted at inference time. By doing this, the model learns to combine the most\nappropriate intermediate representations for the task at hand. Our experiments\ndemonstrate that our approach, when compared to the baselines, excels on a\nreduced computational regime and is competitive in other less restrictive ones.\n\n**Published:** 2021-09-24T04:45:55Z  (*Updated: 2021-09-24T04:45:55Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Visual Attention Network\n\n*From Search Query: adaptive computation mechanisms*\n\n*Shi-Min Hu, Ming-Ming Cheng, Zheng-Ning Liu, Cheng-Ze Lu, Meng-Hao Guo*\n\n**Abstract:** While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN surpasses similar size vision transformers(ViTs) and convolutional neural networks(CNNs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. Code is available at https://github.com/Visual-Attention-Network.\n\n**Published:** 2022-02-20\n\n\n\n#### 2. ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers\n\n*From Search Query: adaptive computation mechanisms*\n\n*Baoyuan Wang, Shusheng Yang, Xinggang Wang, Jingfeng Yao*\n\n**Abstract:** Recently, plain vision Transformers (ViTs) have shown impressive performance on various computer vision tasks, thanks to their strong modeling capacity and large-scale pretraining. However, they have not yet conquered the problem of image matting. We hypothesize that image matting could also be boosted by ViTs and present a new efficient and robust ViT-based matting system, named ViTMatte. Our method utilizes (i) a hybrid attention mechanism combined with a convolution neck to help ViTs achieve an excellent performance-computation trade-off in matting tasks. (ii) Additionally, we introduce the detail capture module, which just consists of simple lightweight convolutions to complement the detailed information required by matting. To the best of our knowledge, ViTMatte is the first work to unleash the potential of ViT on image matting with concise adaptation. It inherits many superior properties from ViT to matting, including various pretraining strategies, concise architecture design, and flexible inference strategies. We evaluate ViTMatte on Composition-1k and Distinctions-646, the most commonly used benchmark for image matting, our method achieves state-of-the-art performance and outperforms prior matting works by a large margin.\n\n**Published:** 2023-05-24\n\n\n\n#### 3. Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth\n\n*From Search Query: multi-scale feature integration*\n\n*Woonghyun Ka, Junmo Kim, Sehwan Chun, Donggyu Joo, Pyungwhan Ahn, Doyeon Kim*\n\n**Abstract:** Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.\n\n**Published:** 2022-01-19\n\n\n\n#### 4. Real-Time Scene Text Detection with Differentiable Binarization and Adaptive Scale Fusion\n\n*From Search Query: multi-scale feature integration*\n\n*Xiang Bai, Cong Yao, Zhaoyi Wan, Zhisheng Zou, Minghui Liao*\n\n**Abstract:** Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization (DB) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed DB module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion (ASF) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed DB and ASF with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks.\n\n**Published:** 2022-02-21\n\n\n\n#### 5. Training wide residual networks for deployment using a single bit for each weight\n\n*From Search Query: hardware-efficient normalization*\n\n*Mark D. McDonnell*\n\n**Abstract:** For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .\n\n**Conference:** training-wide-residual-networks-for-1\n\n**Published:** 2018-02-23\n\n\n\n#### 6. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: hardware-efficient normalization*\n\n*Xipeng Qiu, Haijun Lv, Qipeng Guo, Hang Yan, Kai Lv*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, focusing on adaptive computation mechanisms, multi-scale feature integration, and hardware-efficient normalization, here are some relevant insights and references:\n\n## Adaptive Computation Mechanisms\n\n- The paper on \"Developmental Plasticity-inspired Adaptive Pruning for Deep Networks\" introduces a generalized adaptive pruning algorithm inspired by the developmental plasticity of the brain. This method dynamically prunes inactive synapses and neurons based on their activity levels, which can be seen as an adaptive computation mechanism. This approach can help in maintaining model expressiveness while reducing redundancy, making it more efficient.\n\n- The work on \"Tiny Recurrent Neural Networks\" discusses using small RNNs to discover cognitive algorithms governing biological decision-making. These tiny RNNs, often composed of just 1-4 units, can be interpreted as adaptive computation mechanisms due to their ability to capture complex behaviors with minimal parameters. This approach could be adapted to autoregressive models to enhance their efficiency and interpretability.\n\n## Multi-scale Feature Integration\n\n- The concept of multi-scale feature integration can be inferred from the \"Developmental Plasticity-inspired Adaptive Pruning\" paper, which integrates multi-scale spatio-temporal developmental plasticity. This approach combines local and global information by considering the activity levels of neurons and synapses at different scales, which is crucial for efficient feature integration in neural networks.\n\n- The \"Dynamics of Adaptive Continuous Attractor Neural Networks\" paper discusses attractor neural networks that can represent information robustly and perform computations involving rapid state updates. These networks can integrate information at multiple scales through their adaptive mechanisms, which could be a valuable insight for designing multi-scale feature integration techniques in autoregressive models.\n\n## Hardware-Efficient Normalization\n\n- For hardware-efficient normalization, the insights from \"FlashAttention\" are relevant. This paper proposes an IO-aware exact attention algorithm that uses tiling to reduce memory reads/writes, which is crucial for optimizing memory bandwidth. Similar strategies can be applied to normalization techniques to make them more hardware-efficient. For example, using tiling and blocking strategies can optimize the computation of normalization layers in autoregressive models[Analysis Note, FlashAttention reference].\n\n- The \"COMCAT\" paper, while focused on vision models, provides insights into efficient compression and customization of attention-based models. The techniques discussed, such as reducing extra storage cost and faster training, can be adapted to optimize normalization layers in autoregressive models, making them more hardware-efficient[Analysis Note, COMCAT reference].\n\nIn summary, the key areas to focus on include:\n\n- **Adaptive Pruning and State Management**: Implementing adaptive pruning mechanisms like those inspired by developmental plasticity to reduce redundancy and maintain model expressiveness.\n- **Multi-scale Feature Integration**: Integrating local and global information using adaptive mechanisms similar to those in attractor neural networks and developmental plasticity-inspired pruning.\n- **Hardware-Efficient Normalization**: Applying IO-aware and tiling strategies to optimize memory bandwidth and computation efficiency in normalization layers, similar to those used in FlashAttention and COMCAT[Analysis Note, FlashAttention and COMCAT references].\n\nThese approaches can help in designing an autoregressive language model that is efficient, scalable, and performs well across various tasks.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nAfter analyzing the search results, I've identified several promising directions for enhancing HierarchicalRMSNorm:\n\na) State Management and Compression:\n- Need for efficient state compression while maintaining model expressiveness\n- Opportunity to leverage selective state retention mechanisms\n- Potential for hardware-aware implementation strategies\n\nb) Feature Selection and Integration:\n- Importance of adaptive feature selection across scales\n- Need for efficient integration of local and global information\n- Opportunity for data-dependent selection mechanisms\n\nc) Hardware Optimization:\n- Memory bandwidth optimization crucial for performance\n- Tiling and blocking strategies for efficient computation\n- Need for specialized computation patterns\n\n2. Useful References with Excerpts:\n\nFrom \"FlashAttention\":\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.\"\nThis suggests efficient memory management strategies for our design.\n\nFrom \"Mamba\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements [...] letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\"\nThis provides insights into selective processing mechanisms.\n\nFrom \"COMCAT\":\n\"Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution [...] with much faster training and lower extra storage cost.\"\nThis offers compression strategies we can adapt.\n\nFrom \"ScaleCom\":\n\"leverages similarity in the gradient distribution amongst learners to provide significantly improved scalability [...] provides favorable convergence guarantees.\"\nThis suggests ways to optimize information flow.\n\n3. Key Insights:\n\na) State Compression:\n- Selective state retention based on content importance\n- Hardware-aware compression strategies\n- Balance between compression and model expressiveness\n\nb) Feature Selection:\n- Data-dependent selection mechanisms\n- Multi-scale feature integration\n- Adaptive computation based on input characteristics\n\nc) Hardware Efficiency:\n- IO-aware algorithm design\n- Tiling and blocking for memory optimization\n- Specialized computation patterns for different scales\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Recent advances in adaptive computation mechanisms\n2. Efficient implementations of multi-scale feature integration\n3. Hardware-optimized normalization techniques\n\nSpecific areas to explore:\n- Adaptive computation in neural networks\n- Multi-scale feature fusion techniques\n- Hardware-efficient normalization methods\n\n5. References:\n\n[1] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[2] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[3] Xiao, J., Yin, M., Gong, Y., Zang, X., Ren, J., & Yuan, B. (2023). COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models. International Conference on Machine Learning.\n\n[4] Chen, C. Y., et al. (2021). ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training. Neural Information Processing Systems.\n\n[5] Zhang, Y., Yu, H., Li, J., & Lin, W. (2024). Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "hardware efficient neural network optimization",
            "detail": "Find papers discussing:\n1. Hardware-efficient implementation strategies for neural networks\n2. Memory optimization techniques for deep learning\n3. Hardware-aware algorithm design\n4. Performance optimization on specific hardware platforms\n\nFocus on:\n- Implementation details\n- Memory optimization strategies\n- Hardware-specific considerations\n- Performance analysis",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hardware-efficient implementation strategies for neural networks\n2. Memory optimization techniques for deep learning\n3. Hardware-aware algorithm design\n4. Performance optimization on specific hardware platforms\n\nFocus on:\n- Implementation details\n- Memory optimization strategies\n- Hardware-specific considerations\n- Performance analysis\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.87)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.87)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Scalable MatMul-free Language Modeling (Avg. Score: 0.74)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 8/27 (Score: 0.74)*\n\n```\nWe assume a constant clock rate for all implementations. We note that the FPGA implementation is done in RTL from top to bottom, and there are many optimizations that could be added. For example, we are not using any vendor-provided IPs, and we are not bursting DDR transactions, both of which would significantly accelerate operation. This approach is to achieve the most generic and cross-platform evaluation possible. ## 6 Conclusion\n\nWe have demonstrated the feasibility and effectiveness of the first scalable MatMul-free language model. Our work challenges the paradigm that MatMul operations are indispensable for building high-performing language models and paves the way for the development of more efficient and hardware-friendly architectures. We achieve performance on par with state-of-the-art Transformers while eliminating the need for MatMul operations, with an optimized implementation that significantly enhances both training and inference efficiency, reducing both memory usage and latency. As the demand for deploying language models on various platforms grows, MatMul-free LMs present a promising direction for creating models that are both effective and resource-efficient. However, one limitation of our work is that the MatMul-free LM has not been tested on extremely large-scale models (e.g., 100B+ parameters) due to computational constraints. This work serves as a call to action for institutions and organizations that have the resources to build the largest language models to invest in accelerating lightweight models. By prioritizing the development and deployment of MatMul-free architectures such as this one, the future of LLMs will only become more accessible, efficient, and sustainable. ## References\n\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [2] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [3] Hanting Chen, Yunhe Wang, Chunjing Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chao Xu, Chunfeng Xu, and Qi Tian. The addernet: Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $1468-1477,2020$. [4] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9):1659-1671, 1997. [5] Jason K Eshraghian, Max Ward, Emre O Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. Proceedings of the IEEE, 2023. [6] Rui-Jie Zhu, Qihang Zhao, Guoqi Li, and Jason K Eshraghian. SpikeGPT: Generative pretrained language model with spiking neural networks.\n```\n\n#### 3. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.55)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 20/43 (Score: 0.55)*\n\n```\nKnowledge-Based Systems, 295:111780, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems (NeurIPS), 35:30318-30332, 2022a. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022b. Payal Dhar. The carbon impact of artificial intelligence. Nature Machine Intelligence, 2:423-5, 2020. Peter U Diehl, Guido Zarrella, Andrew Cassidy, Bruno U Pedroni, and Emre Neftci. Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware. In IEEE International Conference on Rebooting Computing (ICRC), pp. 1-8, 2016. Jason K Eshraghian and Wei D Lu. The fine line between dead neurons and sparsity in binarized spiking neural networks. arXiv preprint arXiv:2201.11915, 2022. Jason K Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. arXiv preprint arXiv:2109.12894, 2021. Jason K Eshraghian, Xinxin Wang, and Wei D Lu. Memristor-based binarized spiking neural networks: Challenges and applications. IEEE Nanotechnology Magazine, 16(2):14-23, 2022. Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, Timoth\u00e9e Masquelier, Yonghong Tian, and other contributors. Spikingjelly. https://github.com/fangwei123456/spikingjelly, 2020. Accessed: 2022-05-21. Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timoth\u00e9e Masquelier, and Yonghong Tian. Deep residual learning in spiking neural networks.\n```\n\n#### 4. Linear Attention Sequence Parallelism (Avg. Score: 0.53)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 21/24 (Score: 0.53)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open pre-trained transformer language models, 2022. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch FSDP: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, B., Liu, J., Sun, W., Chen, R., Tomlin, C. J., and Yuan, Y. pbSGD: Powered stochastic gradient descent methods for accelerated non-convex optimization. In IJCAI, pp. $3258-3266,2020$. ## A. Appendix\n\n## A.1. Hardware and Software\n\nHardware. Our experimental configuration involves a maximum of 16 DGX-A100 servers, each equipped with 8 A100 GPUs, these GPUs are interconnected through NVSwitch, ensuring an inter-GPU bandwidth of 600GBps. For inter-node communication, we employ RoCE (RDMA over Converged Ethernet) technology, utilizing 8 RoCE RDMA adapters in each server.\n```\n\n#### 5. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.29)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.29)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hardware efficient neural network optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. AutoGO: Automated Computation Graph Optimization for Neural Network Evolution\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Mohammad Salameh, Keith G. Mills, Negar Hassanpour, Fred X. Han, Shuting Zhang, Wei Lu, Shangling Jui, Chunhua Zhou, Fengyu Sun, Di Niu*\n\n**TL;DR:** Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness, is presented.\n\n**Abstract:** Optimizing Deep Neural Networks (DNNs) to obtain high-quality models for efficient real-world deployment has posed multi-faceted challenges to machine learning engineers. Existing methods either search for neural architectures in heuristic design spaces or apply low-level adjustments to computation primitives to improve inference efficiency on hardware. We present Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness. Through a tokenization scheme, AutoGO performs variable-sized segment mutations, making both primitive changes and larger-grained changes to CGs. We introduce our segmentation and mutation algorithms, efficient frequent segment mining technique, as well as a pretrained context-aware predictor to estimate the impact of segment replacements. Extensive experimental results show that AutoGO can automatically evolve several typical large convolutional networks to achieve significant task performance improvement and FLOPs reduction on a range of CV tasks, ranging from Classification, Semantic Segmentation, Human Pose Estimation, to Super Resolution, yet without introducing any newer primitive operations. We also demonstrate the lightweight deployment results of AutoGO-optimized super-resolution and denoising U-Nets on a cycle simulator for a Neural Processing Unit (NPU), achieving PSNR improvement and latency/power reduction simultaneously. Code available at https://github.com/Ascend-Research/AutoGO.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. L2ight: Enabling On-Chip Learning for Optical Neural Networks via Efficient in-situ Subspace Optimization\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Jiaqi Gu, Hanqing Zhu, Chenghao Feng, Zixuan Jiang, Ray T. Chen, D. Pan*\n\n**TL;DR:** This synergistic framework L2ight is the first scalable on-chip learning solution that pushes this emerging field from intractable to scalable and further to efficient for next-generation self-learnable photonic neural chips.\n\n**Abstract:** Silicon-photonics-based optical neural network (ONN) is a promising hardware platform that could represent a paradigm shift in efficient AI with its CMOS-compatibility, flexibility, ultra-low execution latency, and high energy efficiency. In-situ training on the online programmable photonic chips is appealing but still encounters challenging issues in on-chip implementability, scalability, and efficiency. In this work, we propose a closed-loop ONN on-chip learning framework L2ight to enable scalable ONN mapping and efficient in-situ learning. L2ight adopts a three-stage learning flow that first calibrates the complicated photonic circuit states under challenging physical constraints, then performs photonic core mapping via combined analytical solving and zeroth-order optimization. A subspace learning procedure with multi-level sparsity is integrated into L2ight to enable in-situ gradient evaluation and fast adaptation, unleashing the power of optics for real on-chip intelligence. Extensive experiments demonstrate our proposed L2ight outperforms prior ONN training protocols with 3-order-of-magnitude higher scalability and over 30X better efficiency, when benchmarked on various models and learning tasks. This synergistic framework is the first scalable on-chip learning solution that pushes this emerging field from intractable to scalable and further to efficient for next-generation self-learnable photonic neural chips. From a co-design perspective, L2ight also provides essential insights for hardware-restricted unitary subspace optimization and efficient sparse training. We open-source our framework at https://github.com/JeremieMelo/L2ight.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 3. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Y. Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi, Vikas Chandra, Yingyan Lin*\n\n**TL;DR:** This work opens up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy, and proposes a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient Dnns that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real- hardware efficiency.\n\n**Abstract:** Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 4. Online Training Through Time for Spiking Neural Networks\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, D.K. He, Zhouchen Lin*\n\n**TL;DR:** This work proposes online training through time (OTTT) for SNNs, which is derived from BPTT to enable forward-in-time learning by tracking presynaptic activities and leveraging instantaneous loss and gradients, and theoretically analyze and prove that gradients of OTTT can provide a similar descent direction for optimization as gradients based on spike representations under both feedforward and recurrent conditions.\n\n**Abstract:** Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Recent progress in training methods has enabled successful deep SNNs on large-scale tasks with low latency. Particularly, backpropagation through time (BPTT) with surrogate gradients (SG) is popularly used to achieve high performance in a very small number of time steps. However, it is at the cost of large memory consumption for training, lack of theoretical clarity for optimization, and inconsistency with the online property of biological learning and rules on neuromorphic hardware. Other works connect spike representations of SNNs with equivalent artificial neural network formulation and train SNNs by gradients from equivalent mappings to ensure descent directions. But they fail to achieve low latency and are also not online. In this work, we propose online training through time (OTTT) for SNNs, which is derived from BPTT to enable forward-in-time learning by tracking presynaptic activities and leveraging instantaneous loss and gradients. Meanwhile, we theoretically analyze and prove that gradients of OTTT can provide a similar descent direction for optimization as gradients based on spike representations under both feedforward and recurrent conditions. OTTT only requires constant training memory costs agnostic to time steps, avoiding the significant memory costs of BPTT for GPU training. Furthermore, the update rule of OTTT is in the form of three-factor Hebbian learning, which could pave a path for online on-chip learning. With OTTT, it is the first time that two mainstream supervised SNN training methods, BPTT with SG and spike representation-based training, are connected, and meanwhile in a biologically plausible form. Experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS demonstrate the superior performance of our method on large-scale static and neuromorphic datasets in small time steps.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 41  (*Influential: 5*)\n\n#### 5. State Transition of Dendritic Spines Improves Learning of Sparse Spiking Neural Networks\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Yanqing Chen, Zhaofei Yu, Wei Fang, Zhengyu Ma, Tiejun Huang, Yonghong Tian*\n\n**TL;DR:** This work organizes SNN pruning techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights, which yields sparse deep networks on the large-scale dataset while maintaining state-of-the-art low performance loss.\n\n**Abstract:** Spiking Neural Networks (SNNs) are considered a promising alternative to Arti\ufb01cial Neural Networks (ANNs) for their event-driven computing paradigm when deployed on energy-ef\ufb01cient neuromorphic hardware. Recently, deep SNNs have shown breathtaking performance improvement through cutting-edge training strategy and \ufb02exi-ble structure, which also scales up the number of parameters and computational burdens in a single network. Inspired by the state transition of dendritic spines in the \ufb01lopodial model of spinogen-esis, we model different states of SNN weights, facilitating weight optimization for pruning. Furthermore, the pruning speed can be regulated by using different functions describing the growing threshold of state transition. We organize these techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights. Our approach yields sparse deep networks on the large-scale dataset (SEW ResNet18 on ImageNet) while maintaining state-of-the-art low performance loss ( \u223c 3% at 88.8% sparsity) compared to existing pruning methods on directly trained SNNs. Moreover, we \ufb01nd out pruning speed regulation while learning is crucial to avoiding disastrous performance degradation at the \ufb01nal stages of training, which may shed light on future work on SNN pruning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 32  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Multi-Component Optimization and Efficient Deployment of Neural-Networks on Resource-Constrained IoT Hardware\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Rajiv Ranjan, Albert Zomaya, Schahram Dustdar, Muhammad Intizar Ali, John G. Breslin, Pankesh Patel, Dineshkumar Sundaram, Bharath Sudharsan*\n\n**Abstract:** The majority of IoT devices like smartwatches, smart plugs, HVAC controllers, etc., are powered by hardware with a constrained specification (low memory, clock speed and processor) which is insufficient to accommodate and execute large, high-quality models. On such resource-constrained devices, manufacturers still manage to provide attractive functionalities (to boost sales) by following the traditional approach of programming IoT devices/products to collect and transmit data (image, audio, sensor readings, etc.) to their cloud-based ML analytics platforms. For decades, this online approach has been facing issues such as compromised data streams, non-real-time analytics due to latency, bandwidth constraints, costly subscriptions, recent privacy issues raised by users and the GDPR guidelines, etc. In this paper, to enable ultra-fast and accurate AI-based offline analytics on resource-constrained IoT devices, we present an end-to-end multi-component model optimization sequence and open-source its implementation. Researchers and developers can use our optimization sequence to optimize high memory, computation demanding models in multiple aspects in order to produce small size, low latency, low-power consuming models that can comfortably fit and execute on resource-constrained hardware. The experimental results show that our optimization components can produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our optimization sequence is generic and can be applied to any state-of-the-art models trained for anomaly detection, predictive maintenance, robotics, voice recognition, and machine vision.\n\n**Published:** 2022-04-20\n\n\n\n#### 2. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 3. HPTQ: Hardware-Friendly Post Training Quantization\n\n*From Search Query: hardware efficient neural network optimization*\n\n*Arnon Netzer, Roy H. Jennings, Idit Diamant, Oranit Dror, Lior Dikstein, Elad Cohen, Reuven Peretz, Hai Victor Habi*\n\n**Abstract:** Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.\n\n**Published:** 2021-09-19\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on hardware efficiency, memory optimization, and hardware-aware algorithm design, here are some key insights and strategies from the provided sources:\n\n## Hardware-Efficient Implementation Strategies\n\n- **Hardware-Aware Neural Network Design**: This approach involves co-designing neural network architectures and hardware to optimize performance. Methods like Hardware-Aware Neural Architecture Search (HA-NAS) automatically discover architectures optimized for specific hardware platforms, such as GPUs, FPGAs, and custom silicon-based accelerators. This ensures that the neural network is tailored to leverage the strengths of the underlying hardware.\n\n- **Reducing Data Bottlenecks**: Strategies to reduce inter-chip data traffic and data movement within the network are crucial. Techniques such as using extremely narrow layers and replacing time-multiplexing of signals with learnable embeddings can significantly reduce the demands on chip I/Os and improve performance.\n\n## Memory Optimization Techniques\n\n- **Network Pruning and Quantization**: While random pruning does not provide significant speed-ups due to the lack of locality in weight matrix multiplication, clever implementations using sparsity-aware kernels can be effective. Quantization techniques also reduce the memory footprint and computational cost without sacrificing much accuracy.\n\n- **Tiny-Transfer-Learning (TinyTL)**: This method freezes the weights of the network and only learns the bias modules, significantly reducing the need to store intermediate activations. It introduces a memory-efficient bias module, the lite residual module, which adds only a small memory overhead while maintaining adaptation capacity.\n\n- **MCUNet**: This framework jointly designs efficient neural architectures (TinyNAS) and lightweight inference engines (TinyEngine) for microcontrollers. It optimizes the search space to fit resource constraints and specializes the network architecture, reducing memory usage by 3.4x and accelerating inference by 1.7-3.3x compared to other solutions.\n\n## Hardware-Aware Algorithm Design\n\n- **Co-Design Methodology**: This involves designing neural network architectures in conjunction with the specific hardware they will run on. For example, Simba's multi-chip module (MCM) architecture enables the design of processor core-specific bottlenecks to reduce inter-chip communication overhead.\n\n- **Precision and Resource Adjustments**: Hardware-aware optimization adjusts parameters such as precision, memory usage, and computation requirements to leverage the strengths of the underlying hardware. This process maximizes efficiency and reduces resource consumption without sacrificing model accuracy.\n\n## Performance Optimization on Specific Hardware Platforms\n\n- **Optimization for Embedded Systems**: For resource-constrained devices, strategies like network pruning, quantization, and low-cost operations are essential. Techniques such as TinyNAS and TinyEngine are designed to optimize neural networks for microcontrollers, achieving high performance with minimal memory and energy consumption.\n\n- **GPU and Other Accelerators**: Optimizing Transformer computations on existing hardware, such as GPUs, involves addressing bottlenecks like the quadratic time and memory complexity of the attention mechanism. Using specialized cores like tensor cores for fast matrix multiplication can significantly improve performance.\n\n## Implementation Details and Performance Analysis\n\n- **Bottleneck Analysis**: A systematic analysis of the impact of bottleneck size on the performance of deep learning models, including the relationship between bottleneck size, computational resource utilization, and model accuracy, is crucial. This helps in selecting the optimal bottleneck size based on different hardware architectures.\n\n- **Memory Scheduling**: Optimizing memory scheduling according to the overall network topology rather than layer-wise optimization can reduce memory usage and accelerate inference. For example, TinyEngine adapts memory scheduling to reduce memory usage by 3.4x and accelerate inference by 1.7-3.3x.\n\nBy integrating these strategies, researchers can develop autoregressive language models that are highly efficient, scalable, and optimized for specific hardware platforms, thereby achieving better overall performance with reduced computational and memory costs.\n",
            "analysis": "Analysis Note for HierarchicalRMSNorm Proposal:\n\n1. Summary of Analysis:\nThe search results reveal important insights about adaptive normalization and multi-scale approaches. The combination of adaptive computation with hardware-efficient implementations shows promise for improving normalization efficiency. However, we need more information about hardware-specific optimizations and implementation details.\n\n2. Useful References with Excerpts:\n\nFrom GradNorm paper:\n\"...automatically balances training by dynamically tuning gradient magnitudes...\" (Chen et al., 2017)\n- Relevant for adaptive parameter tuning\n\nFrom LipsNet:\n\"...Multi-dimensional Gradient Normalization (MGN) method, to constrain the Lipschitz constant of networks...\" (Song et al., 2023)\n- Important for stability analysis\n\nFrom Batch-Instance Normalization:\n\"...learns to selectively normalize only disturbing styles while preserving useful styles...\" (Kim & Nam, 2018)\n- Valuable for adaptive feature selection\n\nFrom Layer Normalization:\n\"...computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer...\" (Ba et al., 2016)\n- Foundation for normalization techniques\n\n3. Key Insights:\n\na) Adaptive Computation:\n- Dynamic parameter adjustment improves efficiency\n- Selective normalization enhances performance\n- Gradient-based adaptation shows promise\n\nb) Stability Analysis:\n- Lipschitz constraints important for stability\n- Multi-dimensional normalization helps robustness\n- Need careful balance in adaptive mechanisms\n\nc) Memory Efficiency:\n- Hardware-aware implementation crucial\n- Need more information about optimization techniques\n- Trade-off between adaptivity and efficiency\n\n4. Future Search Plan:\n\nRound 4 Search Focus:\n- Hardware-specific optimization techniques\n- Query: \"hardware efficient neural network optimization\"\n- Detailed query: Focus on hardware-aware implementation strategies and memory optimization techniques\n\n5. References:\n\n[1] Chen, Z., Badrinarayanan, V., Lee, C. Y., & Rabinovich, A. (2017). GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks. ICML.\n\n[2] Song, X., Duan, J., Wang, W., Li, S., Chen, C., Cheng, B., ... & Wang, X. (2023). LipsNet: A Smooth and Robust Neural Network with Adaptive Lipschitz Constant. ICML.\n\n[3] Kim, H. E., & Nam, H. (2018). Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks. NeurIPS.\n\n[4] Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer Normalization. arXiv preprint arXiv:1607.06450.\n\n[5] Yu, J., Yang, L., Xu, N., Yang, J., & Huang, T. (2018). Slimmable Neural Networks. ICLR.\n\nNeed more information about hardware-specific optimizations and implementation details before proceeding with final review."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "multi-scale normalization hierarchical",
            "detail": "Find papers discussing:\n1. Multi-scale or hierarchical approaches in normalization layers\n2. Memory-efficient normalization techniques\n3. Adaptive parameter initialization in normalization\n4. Scale integration and fusion methods in deep learning\nFocus on:\n- Architectural details of multi-scale processing\n- Memory optimization techniques\n- Stability analysis\n- Convergence properties",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Multi-scale or hierarchical approaches in normalization layers\n2. Memory-efficient normalization techniques\n3. Adaptive parameter initialization in normalization\n4. Scale integration and fusion methods in deep learning\nFocus on:\n- Architectural details of multi-scale processing\n- Memory optimization techniques\n- Stability analysis\n- Convergence properties\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.17)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.17)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 2. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.15)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.15)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 3. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.14)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.14)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 4. Linearizing Large Language Models (Avg. Score: 0.14)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.14)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 5. Building Blocks for a Complex-Valued Transformer Architecture (Avg. Score: 0.11)\n\n*Florian Eilers, Xiaoyi Jiang*\n\n**Published in:** IEEE International Conference on Acoustics, Speech, and Signal Processing (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work adds to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain and shows improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n**Abstract:** Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into \u211d2. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n##### *Relevant Chunk: No. 20/22 (Score: 0.11)*\n\n```\n[32] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv:1607.06450, 2016. [33] J. Thickstun, Z. Harchaoui, and S. M. Kakade, \"Learning features of music from scratch,\" in ICLR, 2017.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: multi-scale normalization hierarchical\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Hierarchical Normalization for Robust Monocular Depth Estimation\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Chi Zhang, Wei Yin, Zhibin Wang, Gang Yu, Bin Fu, Chunhua Shen*\n\n**TL;DR:** This paper proposes a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions and presents two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain.\n\n**Abstract:** In this paper, we address monocular depth estimation with deep neural networks. To enable training of deep monocular estimation models with various sources of datasets, state-of-the-art methods adopt image-level normalization strategies to generate affine-invariant depth representations. However, learning with image-level normalization mainly emphasizes the relations of pixel representations with the global statistic in the images, such as the structure of the scene, while the fine-grained depth difference may be overlooked. In this paper, we propose a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions. Compared with previous normalization strategies applied only at the holistic image level, the proposed hierarchical normalization can effectively preserve the fine-grained details and improve accuracy. We present two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain, respectively. Our extensive experiments show that the proposed normalization strategy remarkably outperforms previous normalization methods, and we set new state-of-the-art on five zero-shot transfer benchmark datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 2. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 193  (*Influential: 28*)\n\n#### 3. Multi-News: A Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, Dragomir R. Radev*\n\n**TL;DR:** This work introduces Multi-News, the first large-scale MDS news dataset, and proposes an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets.\n\n**Abstract:** Automatic generation of summaries from multiple news articles is a valuable tool as the number of online publications grows rapidly. Single document summarization (SDS) systems have benefited from advances in neural encoder-decoder model thanks to the availability of large datasets. However, multi-document summarization (MDS) of news articles has been limited to datasets of a couple of hundred examples. In this paper, we introduce Multi-News, the first large-scale MDS news dataset. Additionally, we propose an end-to-end model which incorporates a traditional extractive summarization model with a standard SDS model and achieves competitive results on MDS datasets. We benchmark several methods on Multi-News and hope that this work will promote advances in summarization in the multi-document setting.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 505  (*Influential: 128*)\n\n#### 4. Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Amin Shabani, A. Abdi, Li Meng, Tristan Sylvain*\n\n**TL;DR:** A general multi-scale framework that can be applied to the state-of-the-art transformer-based time series forecasting models (FEDformer, Autoformer, etc.), and demonstrates the effectiveness of each of its contributions across the architecture and methodology.\n\n**Abstract:** The performance of time series forecasting has recently been greatly improved by the introduction of transformers. In this paper, we propose a general multi-scale framework that can be applied to the state-of-the-art transformer-based time series forecasting models (FEDformer, Autoformer, etc.). By iteratively refining a forecasted time series at multiple scales with shared weights, introducing architecture adaptations, and a specially-designed normalization scheme, we are able to achieve significant performance improvements, from 5.5% to 38.5% across datasets and transformer architectures, with minimal additional computational overhead. Via detailed ablation studies, we demonstrate the effectiveness of each of our contributions across the architecture and methodology. Furthermore, our experiments on various public datasets demonstrate that the proposed improvements outperform their corresponding baseline counterparts. Our code is publicly available in https://github.com/BorealisAI/scaleformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 41  (*Influential: 2*)\n\n#### 5. CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Mat\u00fas Falis, Hang Dong, Alexandra Birch, Beatrice Alex*\n\n**TL;DR:** This work proposes a set of metrics for hierarchical evaluation of the predictions of neural LMTC models using the depth-based representation of the ICD-9 ontology, and compares the evaluation scores from the proposed metrics with previously used metrics on prior art LM TC models for I CD-9 coding in MIMIC-III.\n\n**Abstract:** Large-Scale Multi-Label Text Classification (LMTC) includes tasks with hierarchical label spaces, such as automatic assignment of ICD-9 codes to discharge summaries. Performance of models in prior art is evaluated with standard precision, recall, and F1 measures without regard for the rich hierarchical structure. In this work we argue for hierarchical evaluation of the predictions of neural LMTC models. With the example of the ICD-9 ontology we describe a structural issue in the representation of the structured label space in prior art, and propose an alternative representation based on the depth of the ontology. We propose a set of metrics for hierarchical evaluation using the depth-based representation. We compare the evaluation scores from the proposed metrics with previously used metrics on prior art LMTC models for ICD-9 coding in MIMIC-III. We also propose further avenues of research involving the proposed ontological representation.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 6  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Deep Surface Normal Estimation with Hierarchical RGB-D Fusion\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Jing Chen, Yanfeng Tong, Jin Zeng, Yunmu Huang, Yongtian Wang, Wenxiu Sun, Qiong Yan*\n\n**Abstract:** The growing availability of commodity RGB-D cameras has boosted the applications in the field of scene understanding. However, as a fundamental scene understanding task, surface normal estimation from RGB-D data lacks thorough investigation. In this paper, a hierarchical fusion network with adaptive feature re-weighting is proposed for surface normal estimation from a single RGB-D image. Specifically, the features from color image and depth are successively integrated at multiple scales to ensure global surface smoothness while preserving visually salient details. Meanwhile, the depth features are re-weighted with a confidence map estimated from depth before merging into the color branch to avoid artifacts caused by input depth corruption. Additionally, a hybrid multi-scale loss function is designed to learn accurate normal estimation given noisy ground-truth dataset. Extensive experimental results validate the effectiveness of the fusion strategy and the loss design, outperforming state-of-the-art normal estimation schemes.\n\n**Conference:** deep-surface-normal-estimation-with-1\n\n**Published:** 2019-04-06\n\n\n\n#### 2. Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for Large-Scale Multi-Label Text Classification\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Jian-Xin Li, Philip S. Yu, Bo Li, Senzhang Wang, Qiran Gong, Lihong Wang, Lifang He, Hao Peng*\n\n**Abstract:** CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and are widely used in various text mining tasks such as large-scale multi-label text classification. However, most existing deep models for multi-label text classification consider either the non-consecutive and long-distance semantics or the sequential semantics, but how to consider them both coherently is less studied. In addition, most existing methods treat output labels as independent methods, but ignore the hierarchical relations among them, leading to useful semantic information loss. In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent CNNs framework for large-scale multi-label text classification. Specifically, we first propose to model each document as a word order preserved graph-of-words and normalize it as a corresponding words-matrix representation which preserves both the non-consecutive, long-distance and local sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule recurrent CNNs for more effectively learning the semantic features. To leverage the hierarchical relations among the class labels, we propose a hierarchical taxonomy embedding method to learn their representations, and define a novel weighted margin loss by incorporating the label representation similarity. Extensive evaluations on three datasets show that our model significantly improves the performance of large-scale multi-label text classification by comparing with state-of-the-art approaches.\n\n**Published:** 2019-06-09\n\n\n\n#### 3. CMG-Net: Robust Normal Estimation for Point Clouds via Chamfer Normal Distance and Multi-scale Geometry\n\n*From Search Query: multi-scale normalization hierarchical*\n\n*Dong-Ming Yan, Xiaohong Jia, Jianfeng Yang, Tianqi Yu, Weize Quan, Keqiang Li, Mingyang Zhao, Yingrui Wu*\n\n**Abstract:** This work presents an accurate and robust method for estimating normals from point clouds. In contrast to predecessor approaches that minimize the deviations between the annotated and the predicted normals directly, leading to direction inconsistency, we first propose a new metric termed Chamfer Normal Distance to address this issue. This not only mitigates the challenge but also facilitates network training and substantially enhances the network robustness against noise. Subsequently, we devise an innovative architecture that encompasses Multi-scale Local Feature Aggregation and Hierarchical Geometric Information Fusion. This design empowers the network to capture intricate geometric details more effectively and alleviate the ambiguity in scale selection. Extensive experiments demonstrate that our method achieves the state-of-the-art performance on both synthetic and real-world datasets, particularly in scenarios contaminated by noise. Our implementation is available at https://github.com/YingruiWoo/CMG-Net_Pytorch.\n\n**Published:** 2023-12-14\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on multi-scale normalization, hierarchical approaches, memory efficiency, and adaptive parameter initialization, here are some relevant findings and suggestions based on the provided sources and the detailed analysis:\n\n## Multi-scale Normalization Architecture\n\n- The concept of multi-scale processing is well-demonstrated in the EMMA model, which uses a multi-scale feature fusion (MFF) module to combine visual features from intermediate layers. This hierarchical alignment at the feature level could be adapted for normalization in language models, ensuring that features from different scales are properly integrated and normalized.\n\n- The MCH-PAN model introduces a Scale-Guided Aggregator (SGA) that dynamically aggregates feature maps at different scales. This mechanism could be a valuable reference for designing multi-scale normalization paths in language models, where features from various scales need to be normalized and integrated effectively.\n\n## Hierarchical Approaches in Normalization\n\n- Hierarchical clustering and multi-scale analysis are discussed in the context of structure-function coupling, which involves data-driven hierarchical clustering. While this is not directly applied to normalization layers, it suggests that hierarchical approaches can be effective in handling multi-scale data, which could be translated to normalization techniques in deep learning models.\n\n## Memory-Efficient Normalization Techniques\n\n- The HierarchicalRMSNorm (HiRMSNorm) proposal itself claims memory efficiency through hierarchical grouping. To verify these claims, one could compare this approach with other memory-efficient normalization techniques such as Group Normalization, which divides the channels into groups and normalizes them within each group, reducing memory and computational requirements.\n\n## Adaptive Parameter Initialization\n\n- Adaptive parameter initialization is a key aspect of the HiRMSNorm proposal. Research on existing adaptive initialization methods, such as those used in transformer models or other deep learning architectures, can provide insights into stability and convergence properties. For example, the EMMA model's use of pixel-wise alignment and multi-scale feature fusion involves adaptive adjustments that could be studied for their impact on stability and convergence.\n\n## Scale Integration and Fusion Methods\n\n- The MCH-PAN model's use of the Scale-Guided Aggregator (SGA) and the Channel-Guided Feature Integrator (CGFI) provides a robust example of scale integration and fusion. These mechanisms dynamically aggregate features at different scales, which can be a valuable reference for designing weighted combination strategies for normalized features across scales in language models.\n\n## Architectural Details and Memory Optimization\n\n- The EMMA model's architecture, which includes a pixel-wise alignment module and a multi-scale feature fusion module, demonstrates how hierarchical and multi-scale approaches can be implemented efficiently. This can serve as a model for designing memory-efficient normalization layers in autoregressive language models.\n\n## Stability Analysis and Convergence Properties\n\n- To analyze the stability and convergence properties of multi-scale normalization with adaptive initialization, one could conduct experiments similar to those in the EMMA model. This involves evaluating the model's performance across various benchmarks and analyzing the impact of adaptive initialization on training stability and convergence.\n\nIn summary, the following steps can be taken to improve the autoregressive language model design:\n\n1. **Adopt Multi-Scale Normalization**:\n   - Use hierarchical and multi-scale approaches similar to those in EMMA and MCH-PAN to normalize features across different scales.\n\n2. **Implement Adaptive Initialization**:\n   - Investigate data-dependent initialization methods and their impact on stability and convergence, drawing from examples like EMMA's pixel-wise alignment and multi-scale feature fusion.\n\n3. **Optimize Memory Efficiency**:\n   - Compare the memory efficiency of hierarchical grouping in HiRMSNorm with other techniques like Group Normalization to verify and potentially improve memory complexity claims.\n\n4. **Develop Scale Integration Mechanisms**:\n   - Study and implement scale integration methods like the SGA and CGFI from MCH-PAN to effectively combine normalized features across different scales.\n\nBy integrating these concepts, the researcher can develop a more robust, memory-efficient, and scalable autoregressive language model.\n",
            "analysis": "Initial Analysis of HierarchicalRMSNorm (HiRMSNorm) Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Multi-scale Normalization Architecture:\n- The proposal introduces multi-scale feature decomposition and scale-specific normalization paths\n- Need to investigate existing multi-scale approaches in normalization and their effectiveness\n- Research required on optimal scale selection and integration strategies\n\n2. Adaptive Parameter Initialization:\n- Proposes data-dependent initialization of scale-specific parameters\n- Need to examine existing adaptive initialization methods in normalization layers\n- Research on stability and convergence with adaptive initialization\n\n3. Memory Optimization:\n- Claims memory efficiency through hierarchical grouping\n- Need to verify memory complexity claims\n- Research on similar memory-efficient normalization approaches\n\n4. Scale Integration Mechanism:\n- Uses weighted combination of normalized features across scales\n- Need to investigate existing scale fusion approaches\n- Research on optimal weighting strategies\n\nInvestigation Plan:\n\n1. First Round:\n- Search for existing multi-scale normalization methods\n- Focus on hierarchical approaches in normalization\n- Look for memory-efficient normalization techniques\n\n2. Second Round:\n- Investigate adaptive initialization in normalization layers\n- Research scale selection and integration methods\n- Study memory optimization techniques in normalization\n\n3. Third Round:\n- Examine stability analysis of multi-scale normalization\n- Research convergence properties of adaptive initialization\n- Study scale weighting strategies\n\nInitial Findings:\n\n1. The proposal combines several interesting concepts:\n- Multi-scale processing for normalization\n- Adaptive parameter initialization\n- Memory-efficient implementation through hierarchical grouping\n\n2. Potential advantages:\n- Better feature normalization across different scales\n- Improved memory efficiency\n- More stable training through adaptive initialization\n\n3. Areas needing validation:\n- Actual memory efficiency gains\n- Stability of adaptive initialization\n- Effectiveness of scale integration\n\n4. Comparison with existing work:\n- Need to verify novelty compared to existing multi-scale approaches\n- Check for similar hierarchical normalization methods\n- Compare with other memory-efficient normalization techniques\n\nReferences found so far:\n1. Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization.\"\n2. Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). \"Layer Normalization.\"\n3. Wu, Y., & He, K. (2018). \"Group Normalization.\""
        },
        {
            "ready": false,
            "query": "memory efficient normalization language models",
            "detail": "Find papers discussing:\n1. Memory-efficient normalization techniques specifically in language models and transformers\n2. Implementation details of efficient normalization layers\n3. Comparison of different normalization approaches in terms of memory usage\n4. Optimization techniques for normalization in deep learning\n\nFocus on:\n- Concrete implementation details\n- Memory complexity analysis\n- Performance comparisons\n- Practical optimization strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Memory-efficient normalization techniques specifically in language models and transformers\n2. Implementation details of efficient normalization layers\n3. Comparison of different normalization approaches in terms of memory usage\n4. Optimization techniques for normalization in deep learning\n\nFocus on:\n- Concrete implementation details\n- Memory complexity analysis\n- Performance comparisons\n- Practical optimization strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.93)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 50/56 (Score: 0.93)*\n\n```\narXiv preprint arXiv:1609.08144, 2016. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.\n```\n\n#### 2. Normalized Attention Without Probability Cage (Avg. Score: 0.91)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.91)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 3. Transformer Quality in Linear Time (Avg. Score: 0.89)\n\n*Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le*\n\n**Published in:** International Conference on Machine Learning (2022)\t**Cited by** 152  (*Influential: 36*)\n\n**TL;DR:** This work revisit the design choices in Transformers, and proposes a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss, and a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality.\n\n**Abstract:** We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and 12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on C4 for masked language modeling.\n\n##### *Relevant Chunk: No. 17/29 (Score: 0.89)*\n\n```\nIn International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in Neural Information Processing Systems, 32:5243-5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V. Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention.\n```\n\n#### 4. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.84)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.84)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 5. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.80)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 23/30 (Score: 0.80)*\n\n```\narXiv preprint arXiv:1803.07416, 2018. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity, 2020. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38-45, 2020. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: memory efficient normalization language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Long-Short Transformer: Efficient Transformers for Language and Vision\n\n*From Search Query: memory efficient normalization language models*\n\n*Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, T. Goldstein, Anima Anandkumar, Bryan Catanzaro*\n\n**TL;DR:** This paper proposes Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks, and proposes a dual normalization strategy to account for the scale mismatch between the two attention mechanisms.\n\n**Abstract:** Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 118  (*Influential: 14*)\n\n#### 2. Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation\n\n*From Search Query: memory efficient normalization language models*\n\n*Yuchen Yang, Yingdong Shi, Cheems Wang, Xiantong Zhen, Yuxuan Shi, Jun Xu*\n\n**TL;DR:** The Approximate Backpropagation (Approx-BP) theory is proposed, which provides the theoretical feasibility of decoupling the forward and backward passes of backpropagation training, and a Memory-Sharing Backpropagation strategy, which enables the activation memory to be shared by two adjacent layers, thereby removing activation memory usage redundancy.\n\n**Abstract:** Fine-tuning pretrained large models to downstream tasks is an important problem, which however suffers from huge memory overhead due to large-scale parameters. This work strives to reduce memory overhead in fine-tuning from perspectives of activation function and layer normalization. To this end, we propose the Approximate Backpropagation (Approx-BP) theory, which provides the theoretical feasibility of decoupling the forward and backward passes. We apply our Approx-BP theory to backpropagation training and derive memory-efficient alternatives of GELU and SiLU activation functions, which use derivative functions of ReLUs in the backward pass while keeping their forward pass unchanged. In addition, we introduce a Memory-Sharing Backpropagation strategy, which enables the activation memory to be shared by two adjacent layers, thereby removing activation memory usage redundancy. Our method neither induces extra computation nor reduces training efficiency. We conduct extensive experiments with pretrained vision and language models, and the results demonstrate that our proposal can reduce up to $\\sim$$30\\%$ of the peak memory usage. Our code is released at https://github.com/yyyyychen/LowMemoryBP.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\n\n*From Search Query: memory efficient normalization language models*\n\n*Jeonghoon Kim, J. H. Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, S. Kwon, Dongsoo Lee*\n\n**TL;DR:** Parameter-Efficient and Quantization-aware Adaptation (PEQA) is presented - a simple yet effective method that combines the advantages of PEFT with quantized LLMs and significantly reduces the memory overhead associated with the optimizer state.\n\n**Abstract:** Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 66  (*Influential: 2*)\n\n#### 4. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: memory efficient normalization language models*\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 351  (*Influential: 61*)\n\n### 3 related papers from Papers with Code\n\n#### 1. I-BERT: Integer-only BERT Quantization\n\n*From Search Query: memory efficient normalization language models*\n\n*Kurt Keutzer, Michael W. Mahoney, Zhewei Yao, Amir Gholami, Sehoon Kim*\n\n**Abstract:** Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive efficient inference at the edge, and even at the data center. While quantization can be a viable solution for this, previous work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot efficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM processors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has been open-sourced.\n\n**Published:** 2021-01-05\n\n\n\n#### 2. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n\n*From Search Query: memory efficient normalization language models*\n\n*Patrick Legresley, Mostofa Patwary, Jared Casper, Mohammad Shoeybi, Raul Puri, Bryan Catanzaro*\n\n**Abstract:** Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).\n\n**Published:** 2019-09-17\n\n\n\n#### 3. QLoRA: Efficient Finetuning of Quantized LLMs\n\n*From Search Query: memory efficient normalization language models*\n\n*Luke Zettlemoyer, Ari Holtzman, Artidoro Pagnoni, Tim Dettmers*\n\n**Abstract:** We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-23\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design with a focus on memory-efficient normalization techniques, here are some key findings and suggestions from the provided sources and additional insights:\n\n## Memory-Efficient Normalization Techniques\n\n### Omission of Biases\nMemory-efficient bidirectional transformers, such as those described in the Restackio article, often omit biases in the QKV projections and LayerNorms. This adjustment can lead to a slight increase in resource utilization but optimizes the model's efficiency by reducing the number of parameters and computations required.\n\n### Normalization Techniques for Training Stability\nRecent normalization techniques, such as those proposed by Gilmer et al. (2023), have been effective in stabilizing the training process of large-scale transformer models. These techniques involve normalizing queries and keys, which can help in maintaining training stability without significant memory overhead.\n\n### Efficient Layer Normalization\nIn traditional transformers, Layer Normalization is a critical component. However, it can be optimized for memory efficiency. For instance, using a simplified version of Layer Normalization or applying normalization techniques that reduce the computational and memory requirements, such as those discussed in the context of memory-efficient bidirectional transformers, can be beneficial.\n\n## Implementation Details of Efficient Normalization Layers\n\n### Parallel Layer Execution\nMemory-efficient transformers often utilize parallel layer execution, inspired by architectures like PaLM. This method reduces training time and can be combined with efficient normalization techniques to minimize memory usage. By processing attention and MLP blocks in parallel, the overall memory footprint can be reduced.\n\n### Low-Rank Approximations\nWhile not directly focused on normalization, the concept of low-rank approximations, as seen in the GaLore algorithm, can be extended to normalization layers. By approximating the normalization parameters or the input data using low-rank structures, the memory requirements can be significantly reduced.\n\n## Comparison of Different Normalization Approaches\n\n### Theoretical and Practical Efficiency\nTheoretical models may suggest lower complexity for certain normalization techniques, but practical implementations must consider both training time and memory usage. For example, the Memory-Efficient Transformers (MTE) discussed in the Restackio article demonstrate lower time and memory costs compared to standard transformer-based approaches, especially with increased input sizes.\n\n### Performance Comparisons\nStudies have shown that memory-efficient normalization techniques can maintain performance comparable to standard approaches. For instance, the omission of biases and the use of specific normalization schemes can result in negligible performance loss while reducing memory usage.\n\n## Optimization Techniques for Normalization in Deep Learning\n\n### L-Mul Algorithm\nAlthough not directly related to normalization, the L-Mul algorithm, which approximates floating-point multiplication using integer addition, can be applied to optimize the computational resources required for normalization layers. This approach can significantly reduce the energy consumption and computational load associated with floating-point operations, which are common in normalization layers.\n\n### Adaptive Normalization\nAdaptive normalization techniques, such as those that adaptively update the normalization parameters based on the input data, can be more memory-efficient. For example, techniques like ReLoRA, which update the frozen weight matrix using previously learned low-rank adapters, can be adapted to normalization layers to reduce memory overhead.\n\n### Practical Optimization Strategies\n\n- **Hardware-Level Optimizations**: Implementing memory-efficient normalization techniques at the hardware level, such as using memristor crossbars or specialized hardware for integer addition (like L-Mul), can significantly reduce energy consumption and memory usage.\n- **Parallel Processing**: Utilizing parallel processing techniques to execute normalization layers in parallel with other model components can reduce overall training and inference time, thus optimizing memory usage.\n\nBy integrating these strategies, researchers can develop more memory-efficient normalization techniques for autoregressive language models, enhancing their performance, scalability, and efficiency.\n",
            "analysis": "Initial Analysis of HierarchicalRMSNorm Proposal:\n\n1. Summary of Analysis:\nThe initial search results reveal several important aspects about multi-scale normalization and hierarchical approaches, but more specific information is needed about normalization in language models and memory-efficient implementations.\n\n2. Useful References with Excerpts:\n\nFrom Point-M2AE paper:\n\"...modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics...\" (Zhang et al., 2022)\n- Relevant for hierarchical architecture design\n\nFrom Scaleformer:\n\"...iteratively refining a forecasted time series at multiple scales with shared weights... specially-designed normalization scheme...\" (Shabani et al., 2022)\n- Important insights on multi-scale processing with weight sharing\n\nFrom Hierarchical Normalization paper:\n\"...multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions...\" (Zhang et al., 2022)\n- Valuable concepts for hierarchical normalization strategy\n\n3. Key Insights:\n\na) Multi-scale Processing:\n- Pyramid architectures enable progressive feature processing\n- Shared weights across scales can improve efficiency\n- Need to investigate specific applications to normalization\n\nb) Memory Efficiency:\n- Current search results lack detailed information about memory-efficient normalization implementations\n- Need more specific research on memory optimization techniques\n\nc) Scale Integration:\n- Multiple approaches exist for combining multi-scale features\n- Need more information about scale integration in normalization context\n\n4. Future Search Plan:\n\nRound 2 Search Focus:\n- Memory-efficient normalization techniques in language models\n- Query: \"memory efficient normalization language models\"\n- Detailed query: Focus on implementation details of memory-efficient normalization in transformers and language models\n\nRound 3 Search Focus:\n- Adaptive parameter initialization and scale integration\n- Query: \"adaptive initialization normalization neural networks\"\n- Detailed query: Examine adaptive initialization techniques and their stability analysis\n\n5. References:\n\n[1] Zhang, R., Guo, Z., Gao, P., Fang, R., Zhao, B., Wang, D., ... & Li, H. (2022). Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training. Neural Information Processing Systems.\n\n[2] Shabani, A., Abdi, A., Meng, L., & Sylvain, T. (2022). Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting. International Conference on Learning Representations.\n\n[3] Zhang, C., Yin, W., Wang, Z., Yu, G., Fu, B., & Shen, C. (2022). Hierarchical Normalization for Robust Monocular Depth Estimation. Neural Information Processing Systems.\n\nCurrent search results provide good foundational understanding but lack specific details about memory-efficient implementations and normalization in language models. Additional searches are needed to gather more targeted information."
        },
        {
            "ready": false,
            "query": "adaptive normalization neural networks",
            "detail": "Find papers discussing:\n1. Adaptive computation techniques in normalization layers\n2. Dynamic parameter adjustment in neural networks\n3. Stability analysis of adaptive normalization\n4. Memory-efficient adaptive computation\n\nFocus on:\n- Theoretical foundations\n- Implementation details\n- Stability guarantees\n- Memory efficiency analysis",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Adaptive computation techniques in normalization layers\n2. Dynamic parameter adjustment in neural networks\n3. Stability analysis of adaptive normalization\n4. Memory-efficient adaptive computation\n\nFocus on:\n- Theoretical foundations\n- Implementation details\n- Stability guarantees\n- Memory efficiency analysis\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.59)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.59)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 2. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.41)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.41)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 3. Linearizing Large Language Models (Avg. Score: 0.37)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.37)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 4. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.30)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.30)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 5. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.26)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.26)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive normalization neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich*\n\n**TL;DR:** A gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes is presented, showing that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks.\n\n**Abstract:** Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 1089  (*Influential: 144*)\n\n#### 2. LipsNet: A Smooth and Robust Neural Network with Adaptive Lipschitz Constant for High Accuracy Optimal Control\n\n*From Search Query: adaptive normalization neural networks*\n\n*Xujie Song, Jingliang Duan, Wenxuan Wang, S. Li, Chen Chen, B. Cheng, Bo Zhang, Junqing Wei, X. Wang*\n\n**TL;DR:** This work proposes a neural network named LipsNet, which addresses the action \ufb02uctuation problem at network level rather than algorithm level, which can serve as actor networks in most RL algorithms, making it more accessible and user-friendly than previous works.\n\n**Abstract:** Deep reinforcement learning (RL) is a powerful approach for solving optimal control problems. However, RL-trained policies often suffer from the action \ufb02uctuation problem, where the consecutive actions signi\ufb01cantly differ despite only slight state variations. This problem results in mechanical components\u2019 wear and tear and poses safety hazards. The action \ufb02uctuation is caused by the high Lipschitz constant of actor networks. To address this problem, we pro-pose a neural network named LipsNet. We pro-pose the Multi-dimensional Gradient Normalization (MGN) method, to constrain the Lipschitz constant of networks with multi-dimensional input and output. Bene\ufb01ting from MGN, LipsNet achieves Lipschitz continuity, allowing smooth actions while preserving control performance by adjusting Lipschitz constant. LipsNet addresses the action \ufb02uctuation problem at network level rather than algorithm level, which can serve as actor networks in most RL algorithms, making it more \ufb02exible and user-friendly than previous works. Experiments demonstrate that LipsNet has good landscape smoothness and noise robustness, resulting in signi\ufb01cantly smoother action compared to the Multilayer Perceptron.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 2*)\n\n#### 3. Slimmable Neural Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Jiahui Yu, L. Yang, N. Xu, Jianchao Yang, Thomas S. Huang*\n\n**TL;DR:** This work presents a simple and general method to train a single neural network executable at different widths, permitting instant and adaptive accuracy-efficiency trade-offs at runtime, and demonstrates better performance of slimmable models compared with individual ones across a wide range of applications.\n\n**Abstract:** We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: this https URL\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 522  (*Influential: 94*)\n\n#### 4. Learning Semantic-aware Normalization for Generative Adversarial Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Heliang Zheng, Jianlong Fu, Yanhong Zeng, Jiebo Luo, Zhengjun Zha*\n\n**TL;DR:** This paper proposes a novel image synthesis approach by learning adversarial training, a channel grouping loss, and a mutual information loss for joint optimization, which not only enables high-\ufb01delity image synthesis but leads to superior interpretable properties.\n\n**Abstract:** The recent advances in image generation have been achieved by style-based image generators. Such approaches learn to disentangle latent factors in different image scales and encode latent factors as \u201cstyle\u201d to control image synthesis. However, existing approaches cannot further disentangle \ufb01ne-grained semantics from each other, which are often conveyed from feature channels. In this paper, we propose a novel image synthesis approach by learning S emantic-a ware r elative i mportance for feature channels in Generative Adversarial Networks (SariGAN). Such a model disentangles latent factors according to the semantic of feature channels by channel-/group-wise fusion of latent codes and feature channels. Particularly, we learn to cluster feature channels by semantics and propose an adaptive group-wise Normalization (AdaGN) to independently control the styles of different channel groups. For example, we can adjust the statistics of channel groups for a human face to control the open and close of the mouth, while keeping other facial features unchanged. We propose to use adversarial training, a channel grouping loss, and a mutual information loss for joint optimization, which not only enables high-\ufb01delity image synthesis but leads to superior interpretable properties. Extensive experiments show that our approach outperforms the SOTA style-based approaches in both unconditional image generation and conditional image inpainting tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 11  (*Influential: 2*)\n\n#### 5. Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation\n\n*From Search Query: adaptive normalization neural networks*\n\n*Yilin Lyu, Liyuan Wang, Xingxing Zhang, Zicheng Sun, Hang Su, Jun Zhu, Liping Jing*\n\n**TL;DR:** This work focuses on the most popular Batch Normalization and provides an in-depth theoretical analysis of its sub-optimality in continual learning, and proposes Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages.\n\n**Abstract:** Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Hyo-Eun Kim, Hyeonseob Nam*\n\n**Abstract:** Real-world image recognition is often challenged by the variability of visual\nstyles including object textures, lighting conditions, filter effects, etc.\nAlthough these variations have been deemed to be implicitly handled by more\ntraining data and deeper networks, recent advances in image style transfer\nsuggest that it is also possible to explicitly manipulate the style\ninformation. Extending this idea to general visual recognition problems, we\npresent Batch-Instance Normalization (BIN) to explicitly normalize unnecessary\nstyles from images. Considering certain style features play an essential role\nin discriminative tasks, BIN learns to selectively normalize only disturbing\nstyles while preserving useful styles. The proposed normalization module is\neasily incorporated into existing network architectures such as Residual\nNetworks, and surprisingly improves the recognition performance in various\nscenarios. Furthermore, experiments verify that BIN effectively adapts to\ncompletely different tasks like object classification and style transfer, by\ncontrolling the trade-off between preserving and removing style variations. BIN\ncan be implemented with only a few lines of code using popular deep learning\nframeworks.\n\n**Conference:** batch-instance-normalization-for-adaptively-1\n\n**Published:** 2018-05-21\n\n\n\n#### 2. Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Anton Hinel, Leonie Goldmann, Francesco Sanna Passino, Marcus A. K. September*\n\n**Abstract:** Data preprocessing is a crucial part of any machine learning pipeline, and it can have a significant impact on both performance and training efficiency. This is especially evident when using deep neural networks for time series prediction and classification: real-world time series data often exhibit irregularities such as multi-modality, skewness and outliers, and the model performance can degrade rapidly if these characteristics are not adequately addressed. In this work, we propose the EDAIN (Extended Deep Adaptive Input Normalization) layer, a novel adaptive neural layer that learns how to appropriately normalize irregular time series data for a given task in an end-to-end fashion, instead of using a fixed normalization scheme. This is achieved by optimizing its unknown parameters simultaneously with the deep neural network using back-propagation. Our experiments, conducted using synthetic data, a credit default prediction dataset, and a large-scale limit order book benchmark dataset, demonstrate the superior performance of the EDAIN layer when compared to conventional normalization methods and existing adaptive time series preprocessing layers.\n\n**Published:** 2023-10-23\n\n\n\n#### 3. Layer Normalization\n\n*From Search Query: adaptive normalization neural networks*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive normalization techniques, dynamic parameter adjustment, stability analysis, and memory efficiency, here are some key findings and references from the provided sources:\n\n## Adaptive Computation Techniques in Normalization Layers\n\n- **Cluster-Based Normalization (CBN) and Adaptive Context Normalization (ACN):** These methods introduce adaptive normalization techniques that can handle complex data distributions more effectively. CBN organizes data into predefined structures or contexts and normalizes based on these contexts, which can improve the performance of neural networks by adapting to different statistical properties of the data.\n- **Unsupervised Adaptive Normalization (UAN):** This method integrates clustering for normalization using a Gaussian mixture model, allowing for dynamic adjustment of normalization parameters based on the identified clusters. This approach helps in adapting to the dynamic distributions of neuron activations during the learning process.\n\n## Dynamic Parameter Adjustment in Neural Networks\n\n- **Self-Supervised Meta-Learning for All-Layer DNN-Based Adaptive Control (SSML-AC):** This framework pretrains a DNN via self-supervised meta-learning and adapts the full DNN online using composite adaptation. This approach allows for dynamic adjustment of the network parameters based on the task distribution and ensures stability guarantees through spectral normalization.\n\n## Stability Analysis of Adaptive Normalization\n\n- **SSML-AC Framework:** The stability analysis in this framework is crucial as it ensures that the online adaptation of the full DNN maintains stability. The use of spectral normalization to constrain the network\u2019s Lipschitz constant helps in enhancing the stability of the neural network during online adaptation.\n- **Cluster-Based Normalization and Adaptive Context Normalization:** These methods, while primarily focused on performance improvement, also mitigate issues related to internal covariate shift and label shift, which can contribute to stability in the training process. By normalizing based on contexts or clusters, these methods reduce the variability in the activation distributions, thereby stabilizing the learning process.\n\n## Memory-Efficient Adaptive Computation\n\n- **Integer-Only Approximation Methods:** As mentioned in the analysis note, integer-only approximation methods for nonlinear operations, such as those used in I-BERT, can significantly reduce memory usage. Applying similar techniques to normalization layers could enhance memory efficiency[Analysis Note].\n- **Quantization Techniques:** The use of quantization techniques, such as the 4-bit NormalFloat (NF4) data type, can be applied to normalization layers to reduce the memory footprint. This is highlighted in the QLoRA paper and is relevant for memory-efficient implementation[Analysis Note].\n\n### Implementation Details and Theoretical Foundations\n\n- **Batch Normalization Limitations and Alternatives:** Traditional batch normalization has limitations such as dependence on mini-batch size and uniform distribution assumptions. Alternatives like Cluster-Based Normalization, Adaptive Context Normalization, and Unsupervised Adaptive Normalization address these limitations by adapting to complex data distributions and reducing computational demands.\n- **Meta-Learning and Self-Supervised Learning:** The SSML-AC framework provides a theoretical foundation for adaptive control using meta-learning and self-supervised learning. This approach ensures that the DNN can adapt dynamically while maintaining stability guarantees.\n\n### Summary\n\nTo improve autoregressive language models, incorporating adaptive normalization techniques such as CBN, ACN, and UAN can enhance performance by adapting to complex data distributions. The SSML-AC framework offers a robust method for dynamic parameter adjustment with stability guarantees. For memory efficiency, integer-only approximation methods and quantization techniques can be applied to normalization layers. These approaches align with the need for adaptive computation, stability analysis, and memory-efficient implementations.\n",
            "analysis": "Analysis Note for HierarchicalRMSNorm Proposal:\n\n1. Summary of Analysis:\nThe search results reveal important insights about memory-efficient normalization and multi-scale approaches in language models. Key findings suggest that combining adaptive computation with hardware-efficient implementations could significantly improve normalization efficiency.\n\n2. Useful References with Excerpts:\n\nFrom I-BERT paper:\n\"...integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization...\" (Keutzer et al., 2021)\n- Relevant for efficient normalization implementation\n\nFrom Long-Short Transformer:\n\"...proposes a dual normalization strategy to account for the scale mismatch between the two attention mechanisms...\" (Zhu et al., 2021)\n- Important insights for multi-scale normalization\n\nFrom QLoRA:\n\"...4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights...\" (Dettmers et al., 2023)\n- Valuable for memory-efficient implementation\n\n3. Key Insights:\n\na) Memory Optimization:\n- Integer-only arithmetic can significantly reduce memory usage\n- Dual normalization strategies help handle multi-scale features\n- Quantization techniques can be applied to normalization layers\n\nb) Multi-scale Processing:\n- Need to carefully handle scale mismatches\n- Hierarchical approaches can improve efficiency\n- Adaptive computation can reduce memory requirements\n\nc) Implementation Efficiency:\n- Hardware-aware implementation is crucial\n- Quantization can significantly reduce memory footprint\n- Need to balance accuracy and efficiency\n\n4. Future Search Plan:\n\nRound 3 Search Focus:\n- Adaptive computation in normalization layers\n- Query: \"adaptive normalization neural networks\"\n- Detailed query: Focus on adaptive computation techniques and their stability analysis\n\nRound 4 Search Focus:\n- Hardware-efficient implementation strategies\n- Query: \"hardware efficient normalization deep learning\"\n- Detailed query: Examine hardware optimization techniques for normalization layers\n\n5. References:\n\n[1] Keutzer, K., Mahoney, M. W., Yao, Z., Gholami, A., & Kim, S. (2021). I-BERT: Integer-only BERT Quantization. arXiv preprint.\n\n[2] Zhu, C., Ping, W., Xiao, C., Shoeybi, M., Goldstein, T., Anandkumar, A., & Catanzaro, B. (2021). Long-Short Transformer: Efficient Transformers for Language and Vision. Neural Information Processing Systems.\n\n[3] Dettmers, T., Pagnoni, A., Holtzman, A., & Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. NeurIPS 2023.\n\n[4] De, S., Smith, S. L., Fernando, A., et al. (2024). Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models. arXiv preprint.\n\n[5] Yang, Y., Shi, Y., Wang, C., Zhen, X., Shi, Y., & Xu, J. (2024). Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation. ICML 2024.\n\nNeed more information about adaptive computation techniques and hardware-efficient implementations before proceeding with final review."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.972939,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.03335,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalRMSNorm",
    "proposal": "## AdaptiveGatedHRMSNorm: Enhancing Multi-scale Normalization with Adaptive Computation\n\n### 1. Motivation\n\nCurrent language models face several challenges in processing sequences efficiently:\n- Fixed computation patterns limit adaptation to input complexity\n- Inefficient integration of information across different scales\n- High memory bandwidth requirements for normalization operations\n- Suboptimal hardware utilization in multi-scale processing\n\n### 2. Problem Analysis\n\nThe original HierarchicalRMSNorm has limitations:\n1. Static scale processing regardless of input characteristics\n2. Inefficient memory access patterns in multi-scale computation\n3. Limited ability to selectively process important features\n4. High computational overhead in scale integration\n\n### 3. Core Idea and Philosophy\n\nAdaptiveGatedHRMSNorm introduces three key innovations:\n\n1. **Adaptive Scale Selection**:\n   - Content-dependent gating for scale importance\n   - Dynamic computation allocation based on feature relevance\n   - Efficient state tracking through selective retention\n\n2. **Hardware-Efficient Multi-scale Processing**:\n   - Tiled computation for improved memory access\n   - Optimized scale integration through blocking\n   - IO-aware algorithm design\n\n3. **Selective Feature Integration**:\n   - Data-dependent feature selection across scales\n   - Efficient local-global information fusion\n   - Adaptive computation paths\n\n### 4. Design Plan\n\n#### 4.1 Architecture Overview\n\nThe AdaptiveGatedHRMSNorm consists of:\n1. Adaptive Scale Gating Module\n2. Hardware-Efficient Scale Processing\n3. Selective Feature Integration\n\n#### 4.2 Mathematical Formulation\n\n1. Adaptive Scale Gating:\n```python\ndef compute_scale_importance(x, scale):\n    # Compute importance score for each scale\n    score = self.gate_net(x)\n    return torch.sigmoid(score)\n```\n\n2. Hardware-Efficient Processing:\n```python\ndef process_scale(x, scale, block_size):\n    # Process input in tiles for memory efficiency\n    tiles = split_into_tiles(x, block_size)\n    processed = []\n    for tile in tiles:\n        norm = compute_rms_norm(tile)\n        processed.append(norm)\n    return merge_tiles(processed)\n```\n\n3. Selective Feature Integration:\n```python\ndef integrate_features(features, importance):\n    # Selectively combine features based on importance\n    weighted_features = features * importance\n    return weighted_features.sum(dim=-1)\n```\n\n#### 4.3 Implementation Details\n\n```python\nclass AdaptiveGatedHRMSNorm(nn.Module):\n    def __init__(self, dim, scales=[1, 2, 4], block_size=1024):\n        super().__init__()\n        self.scales = scales\n        self.block_size = block_size\n        \n        # Scale-specific parameters\n        self.gammas = nn.ParameterDict({\n            f's{s}': nn.Parameter(torch.ones(dim))\n            for s in scales\n        })\n        \n        # Adaptive gating network\n        self.gate_net = nn.Sequential(\n            nn.Linear(dim, dim // 4),\n            nn.SiLU(),\n            nn.Linear(dim // 4, len(scales))\n        )\n        \n        # Feature selection network\n        self.select_net = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.LayerNorm(dim),\n            nn.SiLU()\n        )\n        \n    def forward(self, x):\n        B, L, D = x.shape\n        \n        # Compute scale importance\n        scale_importance = self.gate_net(x.mean(dim=1))\n        scale_importance = torch.sigmoid(scale_importance)\n        \n        # Process each scale\n        outputs = []\n        for i, scale in enumerate(self.scales):\n            # Split into tiles for memory efficiency\n            tiles = x.split(self.block_size, dim=1)\n            processed_tiles = []\n            \n            for tile in tiles:\n                # Compute RMS norm\n                rms = torch.sqrt(\n                    tile.pow(2).mean(-1, keepdim=True) + 1e-5\n                )\n                norm_tile = tile / rms * self.gammas[f's{scale}']\n                \n                # Apply feature selection\n                selected = self.select_net(norm_tile)\n                processed_tiles.append(selected)\n            \n            # Merge tiles\n            scale_output = torch.cat(processed_tiles, dim=1)\n            outputs.append(scale_output * scale_importance[:, i:i+1, None])\n        \n        # Combine scales\n        return sum(outputs)\n```\n\n### 5. Implementation Guidelines\n\n1. Memory Management:\n   - Use tiling for efficient memory access\n   - Implement cache-friendly computation patterns\n   - Optimize memory bandwidth utilization\n\n2. Feature Selection:\n   - Implement adaptive thresholds for feature importance\n   - Use efficient gating mechanisms\n   - Optimize computation paths\n\n3. Scale Integration:\n   - Implement efficient scale fusion\n   - Use hardware-aware blocking strategies\n   - Optimize parallel processing\n\n### 6. Theoretical Analysis\n\n1. Computational Complexity:\n   - Time: O(N) through tiled processing\n   - Memory: O(N/B) where B is block size\n   - Additional parameters: O(D * S) where D is dimension and S is number of scales\n\n2. Benefits:\n   - Adaptive computation based on input complexity\n   - Improved memory efficiency through tiling\n   - Better feature selection through gating\n\n3. Trade-offs:\n   - Slight increase in parameter count\n   - Additional computation for gating\n   - Memory-computation trade-off in tiling\n\n### 7. Research Summary\n\nKey findings from research:\n1. Adaptive computation improves efficiency\n2. Hardware-aware implementation crucial for performance\n3. Selective processing reduces computational overhead\n4. Multi-scale integration benefits from tiling\n\n### 8. Evolution of Design\n\nThe design evolved through several iterations:\n1. Initial focus on adaptive computation\n2. Integration of hardware-efficient processing\n3. Addition of selective feature integration\n4. Optimization of memory access patterns\n\n### 9. References\n\n[1] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training.\n\n[2] Fu, D. Y., et al. (2023). Simple Hardware-Efficient Long Convolutions for Sequence Modeling.\n\n[3] Rogers, R. M., et al. (2023). Adaptive Privacy Composition for Accuracy-first Mechanisms.\n\n[4] Wu, S., et al. (2024). Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion.\n\n[5] Lu, Y., et al. (2024). An Extensible Framework for Open Heterogeneous Collaborative Perception.\n\n### 10. Conclusion\n\nAdaptiveGatedHRMSNorm represents a significant advancement in multi-scale normalization for language models. By combining adaptive computation, hardware-efficient processing, and selective feature integration, it addresses key limitations of existing approaches while maintaining computational efficiency. The design's flexibility and scalability make it particularly suitable for modern large language models dealing with varying sequence lengths and complex feature distributions.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "AdaptiveGatedHRMSNorm enhances HierarchicalRMSNorm by incorporating adaptive gating mechanisms and hardware-efficient multi-scale processing. The design introduces content-dependent feature selection across scales, efficient state tracking through selective retention, and optimized memory access patterns for improved hardware utilization. This approach significantly reduces computational overhead while maintaining model expressiveness through careful integration of local and global information.",
    "ideation": null,
    "modelname": "adaptivegpt_1",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory scaling\n- Include concrete examples of scale selection\n- Add guidelines for parameter initialization\n- Specify hardware optimization strategies\n\n2. Theoretical Analysis:\n- Expand on stability analysis for multi-scale integration\n- Include formal proof of convergence properties\n- Provide more detailed complexity analysis\n- Add theoretical guarantees for adaptive initialization\n\n3. Integration Guidelines:\n- Add specific guidance for hardware optimization\n- Include strategies for efficient scale computation\n- Provide more detailed memory management guidelines\n- Specify requirements for different model scales\n\n4. Performance Analysis:\n- Include expected performance metrics\n- Add ablation study recommendations\n- Provide benchmarking guidelines\n- Specify memory-performance trade-offs\n\n5. Documentation:\n- Add more detailed API specifications\n- Include example configurations\n- Provide debugging guidelines\n- Add performance optimization tips",
    "user_input": ""
}