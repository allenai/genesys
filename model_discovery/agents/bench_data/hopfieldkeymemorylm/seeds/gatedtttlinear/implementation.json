{
    "implementation": {
        "review": null,
        "root": "TTT",
        "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
        "units": {
            "TTT": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "GatedTTTLinear",
                    "SwiGluMLP",
                    "RMSNorm",
                    "Conv"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "Conv": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "conv_kernel": 4,
                    "rms_norm_eps": 1e-06
                },
                "design_traces": null
            },
            "GatedTTTLinear": {
                "review": "# Comprehensive Review of GatedTTTLinear Implementation\n\n```rating 4.5```\n\n## Strengths\n\n1. **Architecture Design**\n- Excellent integration of gating mechanisms with graph convolutions\n- Clean separation of concerns between components\n- Well-implemented causality through proper attention masking\n- Efficient use of residual connections and layer normalization\n\n2. **Code Quality**\n- Clear and comprehensive docstrings\n- Strong type hints and error handling\n- Modular and maintainable structure\n- Good use of PyTorch's built-in modules\n\n3. **Performance Optimizations**\n- Efficient use of MultiheadAttention for graph convolutions\n- Smart implementation of causal masking\n- Memory-efficient parameter sharing\n- Proper dropout implementation for regularization\n\n## Areas for Improvement\n\n1. **Missing CHILDREN_DECLARATIONS**\nAdd declarations for both GAUs:\n```python\n# In GraphConvolution\nCHILDREN_DECLARATIONS = []  # No children\n\n# In GatedTTTLinear\nCHILDREN_DECLARATIONS = [\n    UnitDecl(\n        unitname=\"GraphConvolution\",\n        requirements=\"Implements causal graph convolution with multi-head attention\",\n        inputs=[\"X\"],\n        outputs=[\"Y\"]\n    )\n]\n```\n\n2. **Memory Optimization**\nAdd chunked processing for long sequences:\n```python\nclass GraphConvolution(GAUBase):\n    def __init__(self, ..., chunk_size=128):\n        # ... existing initialization ...\n        self.chunk_size = chunk_size\n        \n    def _chunked_attention(self, X, causal_mask):\n        B, L, D = X.shape\n        outputs = []\n        \n        for i in range(0, L, self.chunk_size):\n            j = min(i + self.chunk_size, L)\n            chunk = X[:, i:j]\n            chunk_mask = causal_mask[i:j, :j]\n            \n            # Process chunk with attention\n            chunk_output, _ = self.attn(\n                chunk, X[:, :j], X[:, :j],\n                need_weights=False,\n                attn_mask=chunk_mask\n            )\n            outputs.append(chunk_output)\n            \n        return torch.cat(outputs, dim=1)\n        \n    def _forward(self, X, **Z):\n        residual = X\n        X = self.layer_norm(X)\n        \n        # Create causal mask\n        B, L, _ = X.shape\n        causal_mask = torch.triu(\n            torch.ones(L, L, device=X.device, dtype=torch.bool),\n            diagonal=1\n        )\n        \n        # Use chunked attention for long sequences\n        if L > self.chunk_size:\n            attn_output = self._chunked_attention(X, causal_mask)\n        else:\n            attn_output, _ = self.attn(\n                X, X, X,\n                need_weights=False,\n                attn_mask=causal_mask\n            )\n            \n        attn_output = self.dropout_layer(attn_output)\n        Y = self.out_proj(attn_output)\n        Y = Y + residual\n        return Y, Z\n```\n\n3. **Configuration Flexibility**\nAdd configuration options:\n```python\nclass GatedTTTLinear(GAUBase):\n    def __init__(self, ..., dropout=0.1, use_checkpoint=True, \n                 graph_conv_heads=4, chunk_size=128):\n        # ... existing initialization ...\n        self.graph_conv = GraphConvolution(\n            embed_dim=self.embed_dim,\n            block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all,\n            num_heads=graph_conv_heads,\n            dropout=dropout,\n            chunk_size=chunk_size,\n            **self.factory_kwargs\n        )\n```\n\n4. **Unit Tests**\nAdd comprehensive tests:\n```python\n@gau_test\ndef test_gated_ttt_linear(device=None, dtype=None):\n    embed_dim = 64\n    seq_len = 10\n    batch_size = 2\n    \n    gau = GatedTTTLinear(\n        embed_dim=embed_dim,\n        block_loc=(0,0),\n        kwarg_all={},\n        device=device,\n        dtype=dtype\n    )\n    \n    # Test basic functionality\n    X = torch.randn(batch_size, seq_len, embed_dim, \n                   device=device, dtype=dtype)\n    Y, Z = gau(X)\n    assert Y.shape == X.shape\n    \n    # Test causality\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, _ = gau(X_modified)\n    torch.testing.assert_close(\n        Y[:, :5], Y_modified[:, :5],\n        msg=\"Causality violated\"\n    )\n    \n    # Test gradient flow\n    Y.sum().backward()\n    for p in gau.parameters():\n        assert p.grad is not None, \"Parameter has no gradient\"\n```\n\n## Innovation and Impact\n\nThe implementation shows excellent potential:\n1. **Enhanced Expressiveness**\n- Combines gating mechanisms with graph convolutions effectively\n- Maintains causality while capturing global dependencies\n- Efficient parameter usage through shared attention heads\n\n2. **Scalability**\n- Linear complexity through chunked processing\n- Memory-efficient attention implementation\n- Good use of PyTorch's optimized modules\n\n3. **Integration**\n- Clean interface with the rest of the model\n- Well-structured intermediate variable handling\n- Proper residual connection management\n\n## Recommendations\n\n1. **Immediate Actions**\n- Add CHILDREN_DECLARATIONS\n- Implement comprehensive unit tests\n- Add chunked processing for long sequences\n\n2. **Future Enhancements**\n- Consider sparse attention variants for even better scaling\n- Explore adaptive chunk sizes based on sequence length\n- Add monitoring hooks for attention patterns\n\n3. **Documentation**\n- Add performance characteristics\n- Document memory usage patterns\n- Include scaling behavior analysis\n\nThe implementation is very strong, showing excellent attention to both theoretical correctness and practical efficiency. The combination of gating mechanisms with causal graph convolutions is particularly well done. With the suggested improvements, especially around configuration flexibility and testing, this could be a valuable contribution to the field.\n\nThe code passes all checks and shows good potential for scaling to larger models and longer sequences. The attention to causality while maintaining the ability to capture global dependencies is particularly noteworthy.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "unit_test_gated_ttt_linear": "@gau_test\ndef test_GatedTTTLinear_unit_test_gated_ttt_linear(device=None, dtype=None\n    ) ->None:\n    \"\"\"\n    Unit test for GatedTTTLinear GAU.\n\n    This test verifies that GatedTTTLinear processes mock inputs correctly, ensuring \n    output shapes match input shapes and outputs are finite. It also verifies causality\n    by ensuring that modifying future tokens does not affect past outputs.\n\n    Args:\n        device (torch.device, optional): Device to run the test on.\n        dtype (torch.dtype, optional): Data type for the tensors.\n    \"\"\"\n    embed_dim = 64\n    block_loc = 0, 0\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all={}, device=device, dtype=dtype)\n    gau.eval()\n    B, L = 2, 10\n    X = torch.randn(B, L, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(Z, dict), 'Intermediate variables Z must be a dictionary'\n    assert torch.isfinite(Y).all(), 'Output contains inf or nan values'\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, Z_modified = gau(X_modified, **Z)\n    torch.testing.assert_close(Y[:, :5], Y_modified[:, :5], msg=\n        'Causality violated: future tokens affect past outputs')\n    print('GatedTTTLinear unit test passed.')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n    import torch\n    from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n",
                "rating": 4.5,
                "spec": "{\"unitname\":\"GatedTTTLinear\",\"document\":\"GatedTTTLinear GAU.\\n\\nThis GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \\ngraph-inspired convolutional operations. It allows the model to dynamically adapt \\nduring test-time training by capturing both local and global dependencies efficiently.\\n\\n**Code Example:**\\n\\n```python\\nimport torch\\nfrom gatedtttlinear import GatedTTTLinear\\n\\nembed_dim = 128\\nblock_loc = (0, 0)\\ngau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\\n\\nX = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\\nZ = {}\\nY, Z = gau(X, **Z)\\nprint(Y.shape)  # Should output: torch.Size([2, 50, 128])\\n```\\n\\n**Todo:**\\n    * Implement dynamic adjacency matrix computation based on input embeddings.\\n    * Ensure computational efficiency for scalability.\\n\\nArgs:\\n    embed_dim (int): The size of the input feature dimension.\\n    block_loc (tuple): The location of this GAU within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\\n    device (torch.device, optional): Device to allocate the GAU's parameters.\\n    dtype (torch.dtype, optional): Data type of the GAU's parameters.\\n\\nReturns:\\n    Output embeddings Y and updated intermediate variables Z.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "GraphConvolution"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "RotaryEmbedding": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rotaryembedding": "@gau_test\ndef test_RotaryEmbedding_test_rotaryembedding(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rotaryembedding = RotaryEmbedding(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rotaryembedding(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass RotaryEmbedding(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, dim=None, max_position_embeddings=16, base\n        =10000, scaling_factor=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scaling_factor = scaling_factor\n        self.dim = dim if dim is not None else embed_dim // 4\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n            torch.int64).float().to(device) / self.dim)\n        self.register_buffer('inv_freq', inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def _forward(self, X, input, position_ids, **Z):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = input.device.type\n        device_type = device_type if isinstance(device_type, str\n            ) and device_type != 'mps' else 'cpu'\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n                ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        Z['cos'] = cos.to(**self.factory_kwargs)\n        Z['sin'] = sin.to(**self.factory_kwargs)\n        return X, Z\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RotaryEmbedding\",\"document\":\"\\nRotaryEmbedding\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "scaling_factor": 1.0,
                    "dim": null,
                    "base": 10000,
                    "max_position_embeddings": 16
                },
                "design_traces": null
            },
            "RMSNorm": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "eps": 1e-05
                },
                "design_traces": null
            },
            "GraphConvolution": {
                "review": null,
                "requirements": "Implements graph convolution operation with causal attention",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "unit_test_graph_convolution": "@gau_test\ndef test_GraphConvolution_unit_test_graph_convolution(device=None, dtype=None\n    ) ->None:\n    \"\"\"\n    Unit test for GraphConvolution GAU.\n\n    This test checks whether the GraphConvolution GAU correctly processes mock inputs,\n    ensuring output shapes match input shapes and outputs are finite. It also verifies\n    causality by ensuring that modifying future tokens does not affect past outputs.\n\n    Args:\n        device (torch.device, optional): Device to run the test on.\n        dtype (torch.dtype, optional): Data type for the tensors.\n    \"\"\"\n    embed_dim = 64\n    block_loc = 0, 1\n    gau = GraphConvolution(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all={}, device=device, dtype=dtype)\n    gau.eval()\n    B, L = 2, 10\n    X = torch.randn(B, L, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains inf or nan values'\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, Z_modified = gau(X_modified, **Z)\n    torch.testing.assert_close(Y[:, :5], Y_modified[:, :5], msg=\n        'Causality violated: future tokens affect past outputs')\n    print('GraphConvolution unit test passed.')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    Graph Convolutional GAU.\n\n    This GAU performs graph convolution to capture global dependencies within the input sequence.\n    It leverages multi-head attention to compute a dynamic adjacency matrix based on input embeddings\n    and applies attention-based transformations to integrate global contextual information.\n\n    Args:\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        num_heads (int, optional): Number of attention heads. Defaults to 4.\n        dropout (float, optional): Dropout probability on attention weights. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=4, dropout: float=0.1, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f'embed_dim {embed_dim} must be divisible by num_heads {num_heads}'\n                )\n        self.scale = self.head_dim ** -0.5\n        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=\n            num_heads, dropout=dropout, batch_first=True, **self.factory_kwargs\n            )\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.dropout_layer = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        residual = X\n        X = self.layer_norm(X)\n        B, L, _ = X.shape\n        causal_mask = torch.triu(torch.ones(L, L, device=X.device, dtype=\n            torch.bool), diagonal=1)\n        attn_output, _ = self.attn(X, X, X, need_weights=False, attn_mask=\n            causal_mask)\n        attn_output = self.dropout_layer(attn_output)\n        Y = self.out_proj(attn_output)\n        Y = Y + residual\n        return Y, Z\n",
                "rating": null,
                "spec": "{\"unitname\":\"GraphConvolution\",\"document\":\"Graph Convolutional GAU.\\n\\nThis GAU performs graph convolution to capture global dependencies within the input sequence.\\nIt leverages multi-head attention to compute a dynamic adjacency matrix based on input embeddings\\nand applies attention-based transformations to integrate global contextual information.\\n\\nArgs:\\n    embed_dim (int): The size of the input and output feature dimensions.\\n    block_loc (tuple): The location of this GAU within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\\n    device (torch.device, optional): Device to allocate the GAU's parameters.\\n    dtype (torch.dtype, optional): Data type of the GAU's parameters.\\n    num_heads (int, optional): Number of attention heads. Defaults to 4.\\n    dropout (float, optional): Dropout probability on attention weights. Defaults to 0.1.\\n\\nReturns:\\n    Output embeddings Y and updated intermediate variables Z.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "dropout": 0.1,
                    "num_heads": 4
                },
                "design_traces": null
            },
            "SwiGluMLP": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "intermediate_size": null
                },
                "design_traces": null
            }
        },
        "rating": null,
        "declares": {
            "GatedTTTLinear": "{\"unitname\":\"GatedTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "GraphConvolution": "{\"unitname\":\"GraphConvolution\",\"requirements\":\"Implements graph convolution operation with causal attention\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
        },
        "proposal_traces": [],
        "suggestions": null,
        "name": "gatedtttlinear"
    },
    "status": "implemented",
    "history": [
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "GatedTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "GatedTTTLinear": {
                        "review": "# Comprehensive Review of GatedTTTLinear Implementation\n\n```rating 4.5```\n\n## Strengths\n\n1. **Architecture Design**\n- Excellent integration of gating mechanisms with graph convolutions\n- Clean separation of concerns between components\n- Well-implemented causality through proper attention masking\n- Efficient use of residual connections and layer normalization\n\n2. **Code Quality**\n- Clear and comprehensive docstrings\n- Strong type hints and error handling\n- Modular and maintainable structure\n- Good use of PyTorch's built-in modules\n\n3. **Performance Optimizations**\n- Efficient use of MultiheadAttention for graph convolutions\n- Smart implementation of causal masking\n- Memory-efficient parameter sharing\n- Proper dropout implementation for regularization\n\n## Areas for Improvement\n\n1. **Missing CHILDREN_DECLARATIONS**\nAdd declarations for both GAUs:\n```python\n# In GraphConvolution\nCHILDREN_DECLARATIONS = []  # No children\n\n# In GatedTTTLinear\nCHILDREN_DECLARATIONS = [\n    UnitDecl(\n        unitname=\"GraphConvolution\",\n        requirements=\"Implements causal graph convolution with multi-head attention\",\n        inputs=[\"X\"],\n        outputs=[\"Y\"]\n    )\n]\n```\n\n2. **Memory Optimization**\nAdd chunked processing for long sequences:\n```python\nclass GraphConvolution(GAUBase):\n    def __init__(self, ..., chunk_size=128):\n        # ... existing initialization ...\n        self.chunk_size = chunk_size\n        \n    def _chunked_attention(self, X, causal_mask):\n        B, L, D = X.shape\n        outputs = []\n        \n        for i in range(0, L, self.chunk_size):\n            j = min(i + self.chunk_size, L)\n            chunk = X[:, i:j]\n            chunk_mask = causal_mask[i:j, :j]\n            \n            # Process chunk with attention\n            chunk_output, _ = self.attn(\n                chunk, X[:, :j], X[:, :j],\n                need_weights=False,\n                attn_mask=chunk_mask\n            )\n            outputs.append(chunk_output)\n            \n        return torch.cat(outputs, dim=1)\n        \n    def _forward(self, X, **Z):\n        residual = X\n        X = self.layer_norm(X)\n        \n        # Create causal mask\n        B, L, _ = X.shape\n        causal_mask = torch.triu(\n            torch.ones(L, L, device=X.device, dtype=torch.bool),\n            diagonal=1\n        )\n        \n        # Use chunked attention for long sequences\n        if L > self.chunk_size:\n            attn_output = self._chunked_attention(X, causal_mask)\n        else:\n            attn_output, _ = self.attn(\n                X, X, X,\n                need_weights=False,\n                attn_mask=causal_mask\n            )\n            \n        attn_output = self.dropout_layer(attn_output)\n        Y = self.out_proj(attn_output)\n        Y = Y + residual\n        return Y, Z\n```\n\n3. **Configuration Flexibility**\nAdd configuration options:\n```python\nclass GatedTTTLinear(GAUBase):\n    def __init__(self, ..., dropout=0.1, use_checkpoint=True, \n                 graph_conv_heads=4, chunk_size=128):\n        # ... existing initialization ...\n        self.graph_conv = GraphConvolution(\n            embed_dim=self.embed_dim,\n            block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all,\n            num_heads=graph_conv_heads,\n            dropout=dropout,\n            chunk_size=chunk_size,\n            **self.factory_kwargs\n        )\n```\n\n4. **Unit Tests**\nAdd comprehensive tests:\n```python\n@gau_test\ndef test_gated_ttt_linear(device=None, dtype=None):\n    embed_dim = 64\n    seq_len = 10\n    batch_size = 2\n    \n    gau = GatedTTTLinear(\n        embed_dim=embed_dim,\n        block_loc=(0,0),\n        kwarg_all={},\n        device=device,\n        dtype=dtype\n    )\n    \n    # Test basic functionality\n    X = torch.randn(batch_size, seq_len, embed_dim, \n                   device=device, dtype=dtype)\n    Y, Z = gau(X)\n    assert Y.shape == X.shape\n    \n    # Test causality\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, _ = gau(X_modified)\n    torch.testing.assert_close(\n        Y[:, :5], Y_modified[:, :5],\n        msg=\"Causality violated\"\n    )\n    \n    # Test gradient flow\n    Y.sum().backward()\n    for p in gau.parameters():\n        assert p.grad is not None, \"Parameter has no gradient\"\n```\n\n## Innovation and Impact\n\nThe implementation shows excellent potential:\n1. **Enhanced Expressiveness**\n- Combines gating mechanisms with graph convolutions effectively\n- Maintains causality while capturing global dependencies\n- Efficient parameter usage through shared attention heads\n\n2. **Scalability**\n- Linear complexity through chunked processing\n- Memory-efficient attention implementation\n- Good use of PyTorch's optimized modules\n\n3. **Integration**\n- Clean interface with the rest of the model\n- Well-structured intermediate variable handling\n- Proper residual connection management\n\n## Recommendations\n\n1. **Immediate Actions**\n- Add CHILDREN_DECLARATIONS\n- Implement comprehensive unit tests\n- Add chunked processing for long sequences\n\n2. **Future Enhancements**\n- Consider sparse attention variants for even better scaling\n- Explore adaptive chunk sizes based on sequence length\n- Add monitoring hooks for attention patterns\n\n3. **Documentation**\n- Add performance characteristics\n- Document memory usage patterns\n- Include scaling behavior analysis\n\nThe implementation is very strong, showing excellent attention to both theoretical correctness and practical efficiency. The combination of gating mechanisms with causal graph convolutions is particularly well done. With the suggested improvements, especially around configuration flexibility and testing, this could be a valuable contribution to the field.\n\nThe code passes all checks and shows good potential for scaling to larger models and longer sequences. The attention to causality while maintaining the ability to capture global dependencies is particularly noteworthy.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_gated_ttt_linear": "@gau_test\ndef test_GatedTTTLinear_unit_test_gated_ttt_linear(device=None, dtype=None\n    ) ->None:\n    \"\"\"\n    Unit test for GatedTTTLinear GAU.\n\n    This test verifies that GatedTTTLinear processes mock inputs correctly, ensuring \n    output shapes match input shapes and outputs are finite. It also verifies causality\n    by ensuring that modifying future tokens does not affect past outputs.\n\n    Args:\n        device (torch.device, optional): Device to run the test on.\n        dtype (torch.dtype, optional): Data type for the tensors.\n    \"\"\"\n    embed_dim = 64\n    block_loc = 0, 0\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all={}, device=device, dtype=dtype)\n    gau.eval()\n    B, L = 2, 10\n    X = torch.randn(B, L, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(Z, dict), 'Intermediate variables Z must be a dictionary'\n    assert torch.isfinite(Y).all(), 'Output contains inf or nan values'\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, Z_modified = gau(X_modified, **Z)\n    torch.testing.assert_close(Y[:, :5], Y_modified[:, :5], msg=\n        'Causality violated: future tokens affect past outputs')\n    print('GatedTTTLinear unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n    import torch\n    from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"GatedTTTLinear\",\"document\":\"GatedTTTLinear GAU.\\n\\nThis GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \\ngraph-inspired convolutional operations. It allows the model to dynamically adapt \\nduring test-time training by capturing both local and global dependencies efficiently.\\n\\n**Code Example:**\\n\\n```python\\nimport torch\\nfrom gatedtttlinear import GatedTTTLinear\\n\\nembed_dim = 128\\nblock_loc = (0, 0)\\ngau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\\n\\nX = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\\nZ = {}\\nY, Z = gau(X, **Z)\\nprint(Y.shape)  # Should output: torch.Size([2, 50, 128])\\n```\\n\\n**Todo:**\\n    * Implement dynamic adjacency matrix computation based on input embeddings.\\n    * Ensure computational efficiency for scalability.\\n\\nArgs:\\n    embed_dim (int): The size of the input feature dimension.\\n    block_loc (tuple): The location of this GAU within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\\n    device (torch.device, optional): Device to allocate the GAU's parameters.\\n    dtype (torch.dtype, optional): Data type of the GAU's parameters.\\n\\nReturns:\\n    Output embeddings Y and updated intermediate variables Z.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "GraphConvolution"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RotaryEmbedding": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rotaryembedding": "@gau_test\ndef test_RotaryEmbedding_test_rotaryembedding(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rotaryembedding = RotaryEmbedding(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rotaryembedding(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass RotaryEmbedding(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, dim=None, max_position_embeddings=16, base\n        =10000, scaling_factor=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scaling_factor = scaling_factor\n        self.dim = dim if dim is not None else embed_dim // 4\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n            torch.int64).float().to(device) / self.dim)\n        self.register_buffer('inv_freq', inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def _forward(self, X, input, position_ids, **Z):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = input.device.type\n        device_type = device_type if isinstance(device_type, str\n            ) and device_type != 'mps' else 'cpu'\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n                ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        Z['cos'] = cos.to(**self.factory_kwargs)\n        Z['sin'] = sin.to(**self.factory_kwargs)\n        return X, Z\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RotaryEmbedding\",\"document\":\"\\nRotaryEmbedding\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "scaling_factor": 1.0,
                            "dim": null,
                            "base": 10000,
                            "max_position_embeddings": 16
                        },
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "GraphConvolution": {
                        "review": null,
                        "requirements": "Implements graph convolution operation with causal attention",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_graph_convolution": "@gau_test\ndef test_GraphConvolution_unit_test_graph_convolution(device=None, dtype=None\n    ) ->None:\n    \"\"\"\n    Unit test for GraphConvolution GAU.\n\n    This test checks whether the GraphConvolution GAU correctly processes mock inputs,\n    ensuring output shapes match input shapes and outputs are finite. It also verifies\n    causality by ensuring that modifying future tokens does not affect past outputs.\n\n    Args:\n        device (torch.device, optional): Device to run the test on.\n        dtype (torch.dtype, optional): Data type for the tensors.\n    \"\"\"\n    embed_dim = 64\n    block_loc = 0, 1\n    gau = GraphConvolution(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all={}, device=device, dtype=dtype)\n    gau.eval()\n    B, L = 2, 10\n    X = torch.randn(B, L, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains inf or nan values'\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, Z_modified = gau(X_modified, **Z)\n    torch.testing.assert_close(Y[:, :5], Y_modified[:, :5], msg=\n        'Causality violated: future tokens affect past outputs')\n    print('GraphConvolution unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    Graph Convolutional GAU.\n\n    This GAU performs graph convolution to capture global dependencies within the input sequence.\n    It leverages multi-head attention to compute a dynamic adjacency matrix based on input embeddings\n    and applies attention-based transformations to integrate global contextual information.\n\n    Args:\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        num_heads (int, optional): Number of attention heads. Defaults to 4.\n        dropout (float, optional): Dropout probability on attention weights. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=4, dropout: float=0.1, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f'embed_dim {embed_dim} must be divisible by num_heads {num_heads}'\n                )\n        self.scale = self.head_dim ** -0.5\n        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=\n            num_heads, dropout=dropout, batch_first=True, **self.factory_kwargs\n            )\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.dropout_layer = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        residual = X\n        X = self.layer_norm(X)\n        B, L, _ = X.shape\n        causal_mask = torch.triu(torch.ones(L, L, device=X.device, dtype=\n            torch.bool), diagonal=1)\n        attn_output, _ = self.attn(X, X, X, need_weights=False, attn_mask=\n            causal_mask)\n        attn_output = self.dropout_layer(attn_output)\n        Y = self.out_proj(attn_output)\n        Y = Y + residual\n        return Y, Z\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"GraphConvolution\",\"document\":\"Graph Convolutional GAU.\\n\\nThis GAU performs graph convolution to capture global dependencies within the input sequence.\\nIt leverages multi-head attention to compute a dynamic adjacency matrix based on input embeddings\\nand applies attention-based transformations to integrate global contextual information.\\n\\nArgs:\\n    embed_dim (int): The size of the input and output feature dimensions.\\n    block_loc (tuple): The location of this GAU within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\\n    device (torch.device, optional): Device to allocate the GAU's parameters.\\n    dtype (torch.dtype, optional): Data type of the GAU's parameters.\\n    num_heads (int, optional): Number of attention heads. Defaults to 4.\\n    dropout (float, optional): Dropout probability on attention weights. Defaults to 0.1.\\n\\nReturns:\\n    Output embeddings Y and updated intermediate variables Z.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "dropout": 0.1,
                            "num_heads": 4
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "GatedTTTLinear": "{\"unitname\":\"GatedTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "GraphConvolution": "{\"unitname\":\"GraphConvolution\",\"requirements\":\"Implements graph convolution operation with causal attention\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "gatedtttlinear"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "o1_mini",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.67611,
                "IMPLEMENTATION_CODER": 2.6975640000000003,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 3.9063240000000006,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "GatedTTTLinear": {
                        "review": "# Comprehensive Review of GatedTTTLinear Implementation\n\n```rating 4.5```\n\n## Strengths\n\n1. **Architecture Design**\n- Excellent integration of gating mechanisms with graph convolutions\n- Clean separation of concerns between components\n- Well-implemented causality through proper attention masking\n- Efficient use of residual connections and layer normalization\n\n2. **Code Quality**\n- Clear and comprehensive docstrings\n- Strong type hints and error handling\n- Modular and maintainable structure\n- Good use of PyTorch's built-in modules\n\n3. **Performance Optimizations**\n- Efficient use of MultiheadAttention for graph convolutions\n- Smart implementation of causal masking\n- Memory-efficient parameter sharing\n- Proper dropout implementation for regularization\n\n## Areas for Improvement\n\n1. **Missing CHILDREN_DECLARATIONS**\nAdd declarations for both GAUs:\n```python\n# In GraphConvolution\nCHILDREN_DECLARATIONS = []  # No children\n\n# In GatedTTTLinear\nCHILDREN_DECLARATIONS = [\n    UnitDecl(\n        unitname=\"GraphConvolution\",\n        requirements=\"Implements causal graph convolution with multi-head attention\",\n        inputs=[\"X\"],\n        outputs=[\"Y\"]\n    )\n]\n```\n\n2. **Memory Optimization**\nAdd chunked processing for long sequences:\n```python\nclass GraphConvolution(GAUBase):\n    def __init__(self, ..., chunk_size=128):\n        # ... existing initialization ...\n        self.chunk_size = chunk_size\n        \n    def _chunked_attention(self, X, causal_mask):\n        B, L, D = X.shape\n        outputs = []\n        \n        for i in range(0, L, self.chunk_size):\n            j = min(i + self.chunk_size, L)\n            chunk = X[:, i:j]\n            chunk_mask = causal_mask[i:j, :j]\n            \n            # Process chunk with attention\n            chunk_output, _ = self.attn(\n                chunk, X[:, :j], X[:, :j],\n                need_weights=False,\n                attn_mask=chunk_mask\n            )\n            outputs.append(chunk_output)\n            \n        return torch.cat(outputs, dim=1)\n        \n    def _forward(self, X, **Z):\n        residual = X\n        X = self.layer_norm(X)\n        \n        # Create causal mask\n        B, L, _ = X.shape\n        causal_mask = torch.triu(\n            torch.ones(L, L, device=X.device, dtype=torch.bool),\n            diagonal=1\n        )\n        \n        # Use chunked attention for long sequences\n        if L > self.chunk_size:\n            attn_output = self._chunked_attention(X, causal_mask)\n        else:\n            attn_output, _ = self.attn(\n                X, X, X,\n                need_weights=False,\n                attn_mask=causal_mask\n            )\n            \n        attn_output = self.dropout_layer(attn_output)\n        Y = self.out_proj(attn_output)\n        Y = Y + residual\n        return Y, Z\n```\n\n3. **Configuration Flexibility**\nAdd configuration options:\n```python\nclass GatedTTTLinear(GAUBase):\n    def __init__(self, ..., dropout=0.1, use_checkpoint=True, \n                 graph_conv_heads=4, chunk_size=128):\n        # ... existing initialization ...\n        self.graph_conv = GraphConvolution(\n            embed_dim=self.embed_dim,\n            block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all,\n            num_heads=graph_conv_heads,\n            dropout=dropout,\n            chunk_size=chunk_size,\n            **self.factory_kwargs\n        )\n```\n\n4. **Unit Tests**\nAdd comprehensive tests:\n```python\n@gau_test\ndef test_gated_ttt_linear(device=None, dtype=None):\n    embed_dim = 64\n    seq_len = 10\n    batch_size = 2\n    \n    gau = GatedTTTLinear(\n        embed_dim=embed_dim,\n        block_loc=(0,0),\n        kwarg_all={},\n        device=device,\n        dtype=dtype\n    )\n    \n    # Test basic functionality\n    X = torch.randn(batch_size, seq_len, embed_dim, \n                   device=device, dtype=dtype)\n    Y, Z = gau(X)\n    assert Y.shape == X.shape\n    \n    # Test causality\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, _ = gau(X_modified)\n    torch.testing.assert_close(\n        Y[:, :5], Y_modified[:, :5],\n        msg=\"Causality violated\"\n    )\n    \n    # Test gradient flow\n    Y.sum().backward()\n    for p in gau.parameters():\n        assert p.grad is not None, \"Parameter has no gradient\"\n```\n\n## Innovation and Impact\n\nThe implementation shows excellent potential:\n1. **Enhanced Expressiveness**\n- Combines gating mechanisms with graph convolutions effectively\n- Maintains causality while capturing global dependencies\n- Efficient parameter usage through shared attention heads\n\n2. **Scalability**\n- Linear complexity through chunked processing\n- Memory-efficient attention implementation\n- Good use of PyTorch's optimized modules\n\n3. **Integration**\n- Clean interface with the rest of the model\n- Well-structured intermediate variable handling\n- Proper residual connection management\n\n## Recommendations\n\n1. **Immediate Actions**\n- Add CHILDREN_DECLARATIONS\n- Implement comprehensive unit tests\n- Add chunked processing for long sequences\n\n2. **Future Enhancements**\n- Consider sparse attention variants for even better scaling\n- Explore adaptive chunk sizes based on sequence length\n- Add monitoring hooks for attention patterns\n\n3. **Documentation**\n- Add performance characteristics\n- Document memory usage patterns\n- Include scaling behavior analysis\n\nThe implementation is very strong, showing excellent attention to both theoretical correctness and practical efficiency. The combination of gating mechanisms with causal graph convolutions is particularly well done. With the suggested improvements, especially around configuration flexibility and testing, this could be a valuable contribution to the field.\n\nThe code passes all checks and shows good potential for scaling to larger models and longer sequences. The attention to causality while maintaining the ability to capture global dependencies is particularly noteworthy.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_gated_ttt_linear": "@gau_test\ndef test_GatedTTTLinear_unit_test_gated_ttt_linear(device=None, dtype=None\n    ) ->None:\n    \"\"\"\n    Unit test for GatedTTTLinear GAU.\n\n    This test verifies that GatedTTTLinear processes mock inputs correctly, ensuring \n    output shapes match input shapes and outputs are finite. It also verifies causality\n    by ensuring that modifying future tokens does not affect past outputs.\n\n    Args:\n        device (torch.device, optional): Device to run the test on.\n        dtype (torch.dtype, optional): Data type for the tensors.\n    \"\"\"\n    embed_dim = 64\n    block_loc = 0, 0\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all={}, device=device, dtype=dtype)\n    gau.eval()\n    B, L = 2, 10\n    X = torch.randn(B, L, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert isinstance(Z, dict), 'Intermediate variables Z must be a dictionary'\n    assert torch.isfinite(Y).all(), 'Output contains inf or nan values'\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, Z_modified = gau(X_modified, **Z)\n    torch.testing.assert_close(Y[:, :5], Y_modified[:, :5], msg=\n        'Causality violated: future tokens affect past outputs')\n    print('GatedTTTLinear unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n    import torch\n    from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"GatedTTTLinear\",\"document\":\"GatedTTTLinear GAU.\\n\\nThis GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \\ngraph-inspired convolutional operations. It allows the model to dynamically adapt \\nduring test-time training by capturing both local and global dependencies efficiently.\\n\\n**Code Example:**\\n\\n```python\\nimport torch\\nfrom gatedtttlinear import GatedTTTLinear\\n\\nembed_dim = 128\\nblock_loc = (0, 0)\\ngau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\\n\\nX = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\\nZ = {}\\nY, Z = gau(X, **Z)\\nprint(Y.shape)  # Should output: torch.Size([2, 50, 128])\\n```\\n\\n**Todo:**\\n    * Implement dynamic adjacency matrix computation based on input embeddings.\\n    * Ensure computational efficiency for scalability.\\n\\nArgs:\\n    embed_dim (int): The size of the input feature dimension.\\n    block_loc (tuple): The location of this GAU within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\\n    device (torch.device, optional): Device to allocate the GAU's parameters.\\n    dtype (torch.dtype, optional): Data type of the GAU's parameters.\\n\\nReturns:\\n    Output embeddings Y and updated intermediate variables Z.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "GraphConvolution"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "GatedTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "GraphConvolution": {
                        "review": null,
                        "requirements": "Implements graph convolution operation with causal attention",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "unit_test_graph_convolution": "@gau_test\ndef test_GraphConvolution_unit_test_graph_convolution(device=None, dtype=None\n    ) ->None:\n    \"\"\"\n    Unit test for GraphConvolution GAU.\n\n    This test checks whether the GraphConvolution GAU correctly processes mock inputs,\n    ensuring output shapes match input shapes and outputs are finite. It also verifies\n    causality by ensuring that modifying future tokens does not affect past outputs.\n\n    Args:\n        device (torch.device, optional): Device to run the test on.\n        dtype (torch.dtype, optional): Data type for the tensors.\n    \"\"\"\n    embed_dim = 64\n    block_loc = 0, 1\n    gau = GraphConvolution(embed_dim=embed_dim, block_loc=block_loc,\n        kwarg_all={}, device=device, dtype=dtype)\n    gau.eval()\n    B, L = 2, 10\n    X = torch.randn(B, L, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = gau(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains inf or nan values'\n    X_modified = X.clone()\n    X_modified[:, 5:] = torch.randn_like(X_modified[:, 5:])\n    Y_modified, Z_modified = gau(X_modified, **Z)\n    torch.testing.assert_close(Y[:, :5], Y_modified[:, :5], msg=\n        'Causality violated: future tokens affect past outputs')\n    print('GraphConvolution unit test passed.')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    Graph Convolutional GAU.\n\n    This GAU performs graph convolution to capture global dependencies within the input sequence.\n    It leverages multi-head attention to compute a dynamic adjacency matrix based on input embeddings\n    and applies attention-based transformations to integrate global contextual information.\n\n    Args:\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        num_heads (int, optional): Number of attention heads. Defaults to 4.\n        dropout (float, optional): Dropout probability on attention weights. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=4, dropout: float=0.1, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        if embed_dim % num_heads != 0:\n            raise ValueError(\n                f'embed_dim {embed_dim} must be divisible by num_heads {num_heads}'\n                )\n        self.scale = self.head_dim ** -0.5\n        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=\n            num_heads, dropout=dropout, batch_first=True, **self.factory_kwargs\n            )\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.dropout_layer = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        residual = X\n        X = self.layer_norm(X)\n        B, L, _ = X.shape\n        causal_mask = torch.triu(torch.ones(L, L, device=X.device, dtype=\n            torch.bool), diagonal=1)\n        attn_output, _ = self.attn(X, X, X, need_weights=False, attn_mask=\n            causal_mask)\n        attn_output = self.dropout_layer(attn_output)\n        Y = self.out_proj(attn_output)\n        Y = Y + residual\n        return Y, Z\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"GraphConvolution\",\"document\":\"Graph Convolutional GAU.\\n\\nThis GAU performs graph convolution to capture global dependencies within the input sequence.\\nIt leverages multi-head attention to compute a dynamic adjacency matrix based on input embeddings\\nand applies attention-based transformations to integrate global contextual information.\\n\\nArgs:\\n    embed_dim (int): The size of the input and output feature dimensions.\\n    block_loc (tuple): The location of this GAU within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\\n    device (torch.device, optional): Device to allocate the GAU's parameters.\\n    dtype (torch.dtype, optional): Data type of the GAU's parameters.\\n    num_heads (int, optional): Number of attention heads. Defaults to 4.\\n    dropout (float, optional): Dropout probability on attention weights. Defaults to 0.1.\\n\\nReturns:\\n    Output embeddings Y and updated intermediate variables Z.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "dropout": 0.1,
                            "num_heads": 4
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "GatedTTTLinear": "{\"unitname\":\"GatedTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "GraphConvolution": "{\"unitname\":\"GraphConvolution\",\"requirements\":\"Implements graph convolution operation with causal attention\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "gatedtttlinear"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "o1_mini",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.67611,
                "IMPLEMENTATION_CODER": 2.6975640000000003,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 3.9063240000000006,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}