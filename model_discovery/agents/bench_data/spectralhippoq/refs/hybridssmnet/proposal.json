{
    "variantname": "HybridSSMHierRMSNorm",
    "review": "The HybridSSMHierRMSNorm proposal introduces an innovative approach to enhancing hierarchical RMS normalization by integrating selective state space mechanisms and parallel scan operations. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\n1. CLARITY (4.5/5):\nThe proposal is exceptionally well-articulated with:\n- Clear motivation and problem analysis\n- Detailed mathematical formulations\n- Well-structured implementation guidelines\n- Comprehensive theoretical analysis\nThe objectives and expected benefits are clearly defined, and the implementation path is well-documented.\n\n2. INNOVATION (4.3/5):\nThe proposal demonstrates significant innovation through:\n+ Novel integration of SSMs with hierarchical normalization\n+ Unique approach to parallel scan operations in multi-scale processing\n+ Thoughtful consideration of hardware efficiency\nHowever:\n- Some concepts build on existing work in SSMs and hierarchical processing\n- Similar ideas exist in ConvSSM and DenseMamba, though with different applications\n\n3. FEASIBILITY (4.2/5):\nThe implementation appears feasible within the given framework:\n+ Detailed mathematical formulations provided\n+ Clear implementation guidelines\n+ Careful consideration of stability issues\nConcerns:\n- Additional complexity in parallel scan operations\n- Need for careful parameter tuning\n- Potential challenges in maintaining stability across scales\n\n4. SCALABILITY (4.4/5):\nThe design shows strong potential for scalability:\n+ Linear complexity maintained through selective SSM\n+ Efficient parallel processing through scan operations\n+ Hardware-aware implementation considerations\nMinor concerns:\n- Memory requirements for multi-scale state management\n- Potential overhead in very long sequences\n\n5. ACCURACY AND ROBUSTNESS (4.5/5):\nExpected improvements in performance:\n+ Enhanced ability to capture dependencies across scales\n+ Better handling of varying sequence lengths\n+ Improved feature representation through multi-scale processing\n+ More robust normalization through selective state mechanisms\n\n6. EFFICIENCY (4.3/5):\nStrong focus on computational efficiency:\n+ Linear complexity through selective SSM\n+ Efficient parallel scan operations\n+ Careful consideration of memory usage\nConcerns:\n- Overhead from parallel scan operations\n- Need for efficient implementation of multi-scale processing\n\nCOMPARATIVE ANALYSIS:\nThe proposal differs significantly from existing approaches:\n- More sophisticated than standard hierarchical normalization\n- More efficient than traditional SSMs\n- Novel combination of parallel scan operations with multi-scale processing\n- Distinct from sibling proposals in implementation approach\n\nTECHNICAL DEPTH:\nThe proposal demonstrates strong technical understanding:\n+ Comprehensive mathematical formulations\n+ Detailed stability analysis\n+ Thorough consideration of implementation aspects\n+ Clear analysis of trade-offs",
    "search_stack": [
        {
            "ready": false,
            "query": "state space model mamba convolution",
            "detail": "Analyze implementations and performance comparisons of state space models, mamba architectures, and convolutional approaches in language modeling, focusing on efficiency and scalability.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations and performance comparisons of state space models, mamba architectures, and convolutional approaches in language modeling, focusing on efficiency and scalability.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 20/86 (Score: 1.00)*\n\n```\nFull results in Table 10. | MODEL | TOKEn. | Pile <br> PPL $\\downarrow$ | LAMBADA <br> PPL $\\downarrow$ | LAMBADA <br> ACC $\\uparrow$ | HellaSwag <br> ACC $\\uparrow$ | PIQA <br> ACC $\\uparrow$ | Arc-E <br> ACC $\\uparrow$ | Arc-C <br> ACC $\\uparrow$ | WinoGrande <br> ACC $\\uparrow$ | OpenbookQA <br> ACC $\\uparrow$ | Average <br> ACC $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Pythia-1B | NeoX | 7.82 | 7.92 | 56.1 | 47.2 | 70.7 | 57.0 | 27.1 | 53.5 | 31.4 | 49.0 |\n| Mamba-790M | NeoX | $\\underline{7.33}$ | $\\underline{6.02}$ | 62.7 | 55.1 | 72.1 | 61.2 | 29.5 | 56.1 | $\\underline{34.2}$ | $\\underline{53.0}$ |\n| Mamba-2-780M | NeoX | 7.26 | 5.86 | 61.7 | 54.9 | 72.0 | 61.0 | 28.5 | 60.2 | 36.2 | 53.5 |\n| Hybrid H3-1.3B | GPT2 | - | 11.25 | 49.6 | 52.6 | 71.3 | 59.2 | 28.1 | 56.9 | 34.4 | 50.3 |\n| Pythia-1.4B | NeoX | 7.51 | 6.08 | 61.7 | 52.1 | 71.0 | 60.5 | 28.5 | 57.2 | 30.8 | 51.7 |\n| RWKV4-1.5B | NeoX | 7.70 | 7.04 | 56.4 | 52.5 | 72.4 | 60.5 | 29.4 | 54.6 | 34.0 | 51.4 |\n| Mamba-1.4B | NeoX | 6.80 | $\\underline{5.04}$ | 65.0 | 59.1 | 74.2 | 65.5 | $\\underline{32.8}$ | 61.5 | $\\underline{36.4}$ | 56.4 |\n| Mamba-2-1.3B | NeoX | 6.66 | 5.02 | 65.7 | 59.9 | 73.2 | 64.3 | 33.3 | 60.9 | 37.8 | 56.4 |\n| Hybrid H3-2.7B | GPT2 | - | 7.92 | 55.7 | 59.7 | 73.3 | 65.6 | 32.3 | 61.4 | 33.6 | 54.5 |\n| Pythia-2.8B | NeoX | 6.73 | 5.04 | 64.7 | 59.3 | 74.0 | 64.1 | 32.9 | 59.7 | 35.2 | 55.7 |\n| RWKV4-3B | NeoX | 7.00 | 5.24 | 63.9 | 59.6 | 73.7 | 67.8 | 33.1 | 59.6 | 37.0 | 56.4 |\n| Mamba-2.8B | NeoX | $\\underline{6.22}$ | $\\underline{4.23}$ | $\\underline{69.2}$ | 66.1 | $\\underline{75.2}$ | 69.7 | $\\underline{36.3}$ | 63.5 | 39.6 | 59.9 |\n| Mamba-2-2.7B | NeoX | 6.09 | 4.10 | 69.7 | 66.6 | 76.4 | 69.6 | 36.4 | 64.0 | 38.8 | 60.2 |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-29.jpg?height=401&width=1664&top_left_y=1770&top_left_x=268)\n\nFigure 10: (Efficiency Benchmarks.) (Left) Our SSD is $2-8 \\times$ faster than a Mamba fused scan for large state expansion $(N=64)$ and faster than FlashAttention-2 for sequence length 2 k and above. (Right) Sequence length 4 K : Increasing state expansion slows down the Mamba optimized scan implementation linearly. SSD can handle much larger state expansion factors without much slowdown. We compare on a challenging version of the MQAR setup from (Arora, Eyuboglu, Zhang, et al. 2024), using a harder task, longer sequences, and smaller models. Our baselines include standard multi-head softmax attention as well as the Based architecture which combines convolutions, local attention, and a linear attention variant. Results are shown in Figure 8. While Mamba-1 struggles on this task, Mamba-2 performs well across all settings. Surprisingly, it is significantly better than Mamba-1 even when the state sizes are controlled $(\\mathrm{N}=16)$. (We are not sure which aspect of the architecture is the predominant factor, which remains a question to explore in future work.) Additionally, this task validates the importance of state size: increasing from $N=16$ to $\\mathrm{N}=64$ and $\\mathrm{N}=256$ consistently improves performance on MQAR, as the larger state allows more information (key-value pairs) to be memorized. ### 9.2 Language Modeling\n\nFollowing standard protocols in LLMs, we train and evaluate the Mamba-2 architecture on standard autoregressive language modeling against other architectures. We compare both pretraining metrics (perplexity) and zero-shot evaluations. The model sizes (depth and width) follow GPT3 specifications, from 125 m to 2.7B. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training recipe described in Brown et al. (2020). This follows the same setup as reported in Mamba (Gu and Dao 2023); training details are in Appendix D. ### 9.2.1 Scaling Laws\n\nFor baselines, we compare against both Mamba and its Transformer++ recipe (Gu and Dao 2023), which is based on the PaLM and LLaMa architectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher learning rates). As Mamba has already demonstrated that it outperforms the standard Transformer architecture (GPT3 architecture) as well as recent subquadratic architectures (H3 (Dao, D.\n```\n\n#### 2. Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks (Avg. Score: 1.00)\n\n*Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 23  (*Influential: 4*)\n\n**TL;DR:** A hybrid model is introduced, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently, and suggests that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Abstract:** State-space models (SSMs), such as Mamba (Gu&Dao, 2023), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n##### *Relevant Chunk: No. 1/37 (Score: 1.00)*\n\n```\n# Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks \n\nJongho Park ${ }^{1}$, Jaeseung Park ${ }^{2 *}$ \uff0cZheyang Xiong ${ }^{3}$ \uff0cNayoung Lee ${ }^{3}$ \uff0cJaewoong Cho ${ }^{1}$ \uff0c<br>Samet Oymak ${ }^{4}$, Kangwook Lee ${ }^{1,3}$, Dimitris Papailiopoulos ${ }^{1,3}$<br>${ }^{1}$ KRAFTON, ${ }^{2}$ Seoul National University,<br>${ }^{3}$ University of Wisconsin-Madison, ${ }^{4}$ University of Michigan, Ann Arbor\n\n\n#### Abstract\n\nState-space models (SSMs), such as Mamba (Gu \\& Dao, 2023), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models. ## 1 Introduction\n\nModern large language models (LLMs) exhibit remarkable in-context learning (ICL) capabilities, enabling them to learn new tasks with a few demonstrations and without further weight fine-tuning. Although the exact emergence mechanism of these capabilities warrants further theoretical and empirical investigation (Chan et al., 2022; Wei et al., 2022; Min et al., 2022b; Schaeffer et al., 2023), experiments on larger Transformer-based models consistently demonstrate that their ICL capabilities improve as training loss reduces (Brown et al., 2020; Kaplan et al., 2020; Muennighoff et al., 2023). Meta-learning, or \"learning to learn,\" has been extensively studied (Schmidhuber et al., 1997; Ravi \\& Larochelle, 2016) and recently regained interest in the context of ICL, particularly concerning Transformer models (Vaswani et al., 2017). Garg et al. (2022), for example, proposed various ICL tasks, such as learning linear regression, and evaluated the ability of transformers to perform them when specifically trained to do so. On the other hand, Min et al. (2022a) studied fine-tuning language models to explicitly learn and perform ICL. Following these footsteps, numerous research studies have been dedicated to understanding the mechanics of Attention that enable such meta-learning capabilities, either through constructive arguments or extensive experimental investigation (Aky\u00fcrek et al., 2022; Liu et al., 2022; Bai et al., 2023; Giannou et al., 2023; Li et al., 2023a; von Oswald et al., 2023a,b; Yang et al., 2023a; Zhou et al., 2023). [^0]|  | Transformer | Mamba | MambaFormer |\n| :--- | :---: | :---: | :---: |\n| Linear regression | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |\n| Sparse linear regression | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |\n| 2NN regression | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |\n| Decision Tree | $\\checkmark$ | $\\Delta$ | $\\checkmark$ |\n| Orthogonal-outlier regression | $\\checkmark$ | $\\Delta$ | $\\checkmark$ |\n| Many-outlier regression | $\\mathbb{A}$ | $\\checkmark$ | $\\checkmark$ |\n| Sparse parity | $X$ | $\\checkmark$ | $\\checkmark$ |\n| Chain-of-Thought I/O | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |\n| Vector-valued MQAR | $\\checkmark$ | $X$ | $\\checkmark$ |\n\nTable 1: Model performances on various ICL tasks. We label the model's performance with $\\checkmark$ if the model performs on par with other baseline models, $X$ if the model struggles to learn the task, and $\\boldsymbol{\\Delta}$ if the performance improves but with a performance gap compared to other baseline models. Transformer fails in learning sparse parity, showing performance no better than random guessing, while Mamba suffers to accurately retrieve the value vector in vector-valued MQAR. Our proposed MambaFormer performs on par with other baseline models in all tasks. As Transformer language models are currently the only large models that have been reported to be capable of ICL in practice, this raises the question:\nCan attention-free models perform ICL? This question holds merit, especially considering that several recent studies have attempted to move beyond attention-based networks due to their quadratic cost (Katharopoulos et al., 2020; Zhai et al., 2021; Dao et al., 2022; Poli et al., 2023; Peng et al., 2023; Sun et al., 2023; Yang et al., 2023b). In this work, we focus specifically on state-space models (SSMs), and particularly Mamba (Gu \\& Dao, 2023). Mamba was recently demonstrated to be highly efficient while achieving near state-of-the-art performance in standard pretraining language data sets, such as the Pile (Gao et al., 2020), but at smaller model scales (e.g., up to 3 billion parameters), surpassing transformers and other attentionfree architectures across various language and non-language tasks. However, ICL capabilities usually emerge at scales beyond 3 billion parameters. As a result, the potential of these attention-free models to perform ICL remains underexplored, as testing such hypotheses usually requires scaling beyond the 7 billion parameter level. Nonetheless, we can still investigate small-scale ICL capabilities by specifically training a model to perform in-context learning, following the approach of Garg et al.\n```\n\n##### *Relevant Chunk: No. 21/37 (Score: 1.00)*\n\n```\nWe use the same models configurations as done in Aky\u00fcrek et al. (2024) and perform similar hyperparameter sweeps. See Section 3.2 for how accuracy is measured. * denotes reported accuracy in Aky\u00fcrek et al. (2024). the de facto superior model for language modeling, so it remains unclear how performance on this benchmark translates to real-world language ICL, where Transformers typically outperform LSTMs. On RegBench, which favors Transformers over attention-free models, Mamba indeed performs worse than Transformer, consistent with previous findings. Notably, hybrid architectures excel on this benchmark, converging much faster both Mamba and Transformer while achieving higher accuracy. Given prior evidence that Standard Hybrid achieves lower perplexity in language modeling ( $\\mathrm{Gu} \\&$ Dao, 2023), our new results suggest that hybrid models offer a promising direction for both language modeling and in-context learning on language tasks. We hope these results and analysis demonstrate the potential of hybrid models for language-based applications of ICL. ## 6 Discussion\n\nIn this work, we have provided a comprehensive investigation of in-context learning with state-space models (SSMs) and contrasted them with the Transformer architecture. Our study has revealed that SSMs, especially Mamba, are capable in-context learners. On the other hand, our evaluations revealed that neither SSMs nor Transformers are great at all tasks: SSMs struggle with decision tree and retrieval tasks whereas Transformers struggle with sparse parity. This has led us to the hybrid architecture MambaFormer which achieves a best-of-both-worlds performance on our ICL suite. Future research directions include exploring (1) how the performance on our ICL suite correlates with general language modeling capabilities, such as perplexity on standard NLP benchmarks, (2) developing more effective architectures by integrating elements from transformers, SSMs, and gating mechanisms, (3) identifying architectural features that contribute to effective in-context learning, and (4) assessing the impact of MambaFormer and other innovative architectures on language modeling performance. ## Impact Statement\n\nThis paper provides a comprehensive study of language modeling architectures which help identify their weaknesses, strengths, and provide recipes for new architectures. The outcomes of this work will potentially facilitate efficiency and architectural improvements for large language models. ## Acknowledgement\n\nThe work of Dimitris Papailiopoulos is supported in part by ONR Grant No. N00014-21-1-2806 and No. N00014-23-1-2848. The work of Samet Oymak is supported in part by NSF CAREER Award CCF-2046816. The work of Jaeseung Park was supported by KRAFTON AI Fellowship. The authors would like to thank Byeongju Kim and Seongjun Yang for helpful discussion and Gibbeum Lee for valuable feedback on an early draft of this paper. ## References\n\nAhn, K., Cheng, X., Daneshmand, H., and Sra, S. Transformers learn to implement preconditioned gradient descent for in-context learning. arXiv preprint arXiv:2306.00297, 2023. 3\n\nAky\u00fcrek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2022. 1, 3\n\nAky\u00fcrek, E., Wang, B., Kim, Y., and Andreas, J. In-context language learning: Architectures and algorithms. arXiv preprint arXiv:2401.12973, 2024. URL https://arxiv.org/abs/2401. 12973. $3,4,6,13,15$\n\nArora, S., Eyuboglu, S., Timalsina, A., Johnson, I., Poli, M., Zou, J., Rudra, A., and R\u00e9, C. Zoology: Measuring and improving recall in efficient language models.\n```\n\n#### 3. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 1.00)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 1.00)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 4. Jamba: A Hybrid Transformer-Mamba Language Model  (Avg. Score: 1.00)\n\n*Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, S. Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avshalom Manevich, Nir Ratner, N. Rozen, Erez Shwartz, Mor Zusman, Y. Shoham*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 44  (*Influential: 5*)\n\n**TL;DR:** Jamba is presented, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture that provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\n\n**Abstract:** We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.\n\n##### *Relevant Chunk: No. 4/24 (Score: 1.00)*\n\n```\nDespite the immense popularity of the Transformer as the predominant architecture for language models, it suffers from two main drawbacks. First, its high memory and compute requirements hinders the processing of long contexts, where the key-value (KV) cache size becomes a limiting factor. Second, its lack of a single summary state entails slow inference and low throughput, since each generated token performs a computation on the entire context. In contrast, older recurrent neural network (RNN) models, which summarize an arbitrarily long context in a single hidden state, do not suffer from these limitations. RNN models have their own shortcomings, however. They are costly to train since training cannot be parallelized across time steps. And they struggle with long distance relationships, which the hidden state captures to only a limited extent. Recent state space models (SSMs) like Mamba are more efficient to train than RNNs and are more capable at handling long distance relationships, but still lag behind the performance of comparably sized Transformer language models. Taking advantage of both model families, Jamba combines Transformer and Mamba layers, at a certain ratio. Varying the ratio of Transformer/Mamba layers allows balancing memory usage, efficient training, and long context capabilities. A few other recent attempts to combine Attention and SSM modules are worth noting. [55] mixes an S4 layer [18] with a local attention layer, followed by a sequence of local attention layers; it shows experiments with small models and simple tasks. [17] reports that interleaving Mamba and attention layers is only slightly better than pure Mamba in terms of perplexity, with models up to 1.3B parameters. [37] starts with an SSM layer followed by chunk-based Transformers, with models up to 1.3B showing improved perplexity. [13] adds an SSM layer before the self-attention in a Transformer layer, while [43] adds the SSM after the self-attention, both showing improvements on speech recognition. [36] replaces the MLP layers in the Transformer by Mamba layers, and shows benefits in simple tasks. These efforts are different from Jamba both in the particular way in which the SSM component is mixed with the attention one, and in the scale of implementation. Closest are perhaps H3 [15], a specially designed SSM that enables induction capabilities, and a generalization called Hyena [39]. The former proposed a hybrid architecture that replaces the second and middle layers with self-attention, and was implemented with up to 2.7B parameters and 400B training tokens. However, as shown in [17], its perfomance lags that of pure Mamba. Based on Hyena, StripedHyena [40] interleaves attention and SSM layers in a 7B parameter model. However, it lags behind the Attention-only Mistral-7B [23]. All of this renders Jamba the first production-grade Attention-SSM hybrid model. Scaling the hybrid Jamba architecture required overcoming several obstacles, which we dicsuss in Section 6. Jamba also includes MoE layers [14, 46], which allow increasing the model capacity (total number of available parameters) without increasing compute requirements (number of active parameters). MoE is a flexible approach that enables training extremely large models with strong performance [24]. In Jamba, MoE is applied to some of the MLP layers. The more MoE layers, and the more experts in each MoE layer, the larger the total number of model parameters. In contrast, the more experts we use at each forward pass, the larger the number of active parameters as well as the compute requirement. In our implementation of Jamba, we apply MoE at every other layer, with 16 experts and the top-2 experts used at each token (a more detailed discussion of the model architecture is provided below). We evaluated our implementation of Jamba on a wide range of benchmarks and found it performs comparably to Mixtral-8x7B [24], which has a similar number of parameters, and also to the larger Llama-2 70B [50]. In addition, our model supports a context length of 256 K tokens - the longest supported context length for production-grade publicly available models. On long-context evaluations, Jamba outperformes Mixtral on most of the evaluated datasets. At the same time, Jamba is extremely efficient; for example, its throughput is 3 x that of Mixtral-8x7B for long contexts. Moreover, our model fits in a single GPU (with 8 bit weights) even with contexts of over 128 K tokens, which is impossible with similar-size attention-only models such as Mixtral-8x7B. Somewhat unusual for a new architecture, we release Jamba (12B active parameters, 52B total available parameters) under Apache 2.0 license: https://huggingface.co/ai21labs/Jamba-v0.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space model mamba convolution\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: state space model mamba convolution*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 363  (*Influential: 58*)\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: state space model mamba convolution*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Deep Latent State Space Models for Time-Series Generation\n\n*From Search Query: state space model mamba convolution*\n\n*Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, Stefano Ermon*\n\n**TL;DR:** LS4 is proposed, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity, and sets state-of-the-art for continuous-time latent generative models, with significant improvement of mean squared error and tighter variational lower bounds on irregularly-sampled datasets.\n\n**Abstract:** Methods based on ordinary differential equations (ODEs) are widely used to build generative models of time-series. In addition to high computational overhead due to explicitly computing hidden states recurrence, existing ODE-based models fall short in learning sequence data with sharp transitions - common in many real-world systems - due to numerical challenges during optimization. In this work, we propose LS4, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity. Inspired by recent deep state space models (S4), we achieve speedups by leveraging a convolutional representation of LS4 which bypasses the explicit evaluation of hidden states. We show that LS4 significantly outperforms previous continuous-time generative models in terms of marginal distribution, classification, and prediction scores on real-world datasets in the Monash Forecasting Repository, and is capable of modeling highly stochastic data with sharp temporal transitions. LS4 sets state-of-the-art for continuous-time latent generative models, with significant improvement of mean squared error and tighter variational lower bounds on irregularly-sampled datasets, while also being x100 faster than other baselines on long sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 3*)\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space model mamba convolution*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n#### 5. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers\n\n*From Search Query: state space model mamba convolution*\n\n*Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, A. Rudra, Christopher R'e*\n\n**TL;DR:** A simple sequence model inspired by control systems that generalizes RNN heuristics, temporal convolutions, and neural differential equations while addressing their shortcomings, and introduces a trainable subset of structured matrices that endow LSSLs with long-range memory.\n\n**Abstract:** Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$ by simply simulating a linear continuous-time state-space representation $\\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 358  (*Influential: 22*)\n\n### 3 related papers from Papers with Code\n\n#### 1. MambaOut: Do We Really Need Mamba for Vision?\n\n*From Search Query: state space model mamba convolution*\n\n*Xinchao Wang, Weihao Yu*\n\n**Abstract:** Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks. The code is available at https://github.com/yuweihao/MambaOut\n\n**Published:** 2024-05-13\n\n\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space model mamba convolution*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 3. Visual Mamba: A Survey and New Outlooks\n\n*From Search Query: state space model mamba convolution*\n\n*Yu Cai, Hao Chen, Bo Du, Yihui Wang, Shu Yang, Rui Xu*\n\n**Abstract:** Mamba, a recent selective structured state space model, excels in long sequence modeling, which is vital in the large model era. Long sequence modeling poses significant challenges, including capturing long-range dependencies within the data and handling the computational demands caused by their extensive length. Mamba addresses these challenges by overcoming the local perception limitations of convolutional neural networks and the quadratic computational complexity of Transformers. Given its advantages over these mainstream foundation architectures, Mamba exhibits great potential to be a visual foundation architecture. Since January 2024, Mamba has been actively applied to diverse computer vision tasks, yielding numerous contributions. To help keep pace with the rapid advancements, this paper reviews visual Mamba approaches, analyzing over 200 papers. This paper begins by delineating the formulation of the original Mamba model. Subsequently, it delves into representative backbone networks, and applications categorized using different modalities, including image, video, point cloud, and multi-modal. Particularly, we identify scanning techniques as critical for adapting Mamba to vision tasks, and decouple these scanning techniques to clarify their functionality and enhance their flexibility across various applications. Finally, we discuss the challenges and future directions, providing insights into new outlooks in this fast evolving area. A comprehensive list of visual Mamba models reviewed in this work is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.\n\n**Published:** 2024-04-29\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using state space models, Mamba architectures, and convolutional approaches, here are some key insights and implementations from the provided sources and analysis:\n\n## State Space Models (SSMs) and Mamba Architectures\n\n### Efficiency and Scalability\n- The Mamba model, introduced by Albert Gu and Tri Dao, uses selective state spaces within a simplified neural network architecture. This approach allows for linear scaling in sequence length and achieves fast inference with 5 times higher throughput than Transformers. Mamba outperforms similarly sized Transformers and matches larger Transformers in both pretraining and downstream evaluation tasks.\n\n### Selective State Space Models\n- Mamba integrates selective state space models (SSMs) that enable the selective propagation or forgetting of information based on the current token. This mechanism addresses the weakness of traditional SSMs in content-based reasoning and improves performance on long sequences.\n\n### Hardware-Aware Parallel Algorithm\n- Mamba incorporates a hardware-aware parallel algorithm, which is crucial for efficient processing and scalability. This design ensures that the model can handle long sequences efficiently without the computational inefficiencies associated with Transformers.\n\n## Integration with Convolutional Approaches\n\n### Convolutional State Space Models\n- Convolutional State Space Models (ConvSSM) combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods. This integration demonstrates how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. This approach can enhance local feature extraction and improve overall efficiency[Analysis Summary].\n\n### Local Feature Extraction\n- The use of convolutional operations, as seen in models like MambaIR, can enhance local feature extraction. MambaIR employs convolution and channel attention to improve the capabilities of the vanilla Mamba model, particularly in image restoration tasks. This approach can be adapted for language modeling to capture local dependencies effectively.\n\n## Hierarchical Processing and Adaptive Mechanisms\n\n### Hierarchical Structure\n- Integrating hierarchical processing with state space models can maintain the hierarchical structure of the model while adding adaptivity. This is evident in models like GrootVL, which dynamically generates a tree topology based on spatial relationships and input features, breaking original sequence constraints to achieve stronger representation capabilities[Analysis Summary].\n\n### Dynamic Processing\n- Dynamic topology generation and adaptive mechanisms can improve the model's ability to capture complex dependencies. For example, the Conditional Visual State Space (C-VSS) block in SUM (Saliency Unification through Mamba) adapts dynamically to different image types, which can be a valuable insight for adapting to different linguistic contexts in language modeling.\n\n## Memory Optimization and Computational Efficiency\n\n### Selective State Updates\n- Mamba's selective state space models allow for selective propagation or forgetting of information, which can lead to memory efficiency through selective state updates. This is crucial for handling long sequences without significant memory overhead.\n\n### Parallelization Opportunities\n- The parallel scan operations in ConvSSM and Mamba's hardware-aware parallel algorithm provide opportunities for efficient parallelization, which can significantly improve computational efficiency and scalability[Analysis Summary].\n\n## Future Directions\n\n### Combining SSMs with Hierarchical Processing\n- Future work should focus on combining SSMs with hierarchical processing to leverage the strengths of both approaches. This can involve integrating SSMs into tree-based topologies or other hierarchical structures to capture both long-range and local dependencies effectively.\n\n### Integrating Convolutional Operations\n- Strategically integrating convolutional operations can enhance local feature extraction and improve the overall performance of the model. This integration should be done carefully to maintain the linear complexity and efficiency of SSMs.\n\n### Adaptive Mechanisms\n- Developing adaptive mechanisms for dynamic processing can help the model adapt to different contexts and improve its performance on various tasks. This could involve dynamic topology generation or adaptive state space models.\n\nBy focusing on these areas, researchers can develop more efficient, scalable, and adaptive autoregressive language models that leverage the strengths of state space models, Mamba architectures, and convolutional approaches.\n",
            "analysis": "### Analysis Summary\n\nBased on the initial review of the seed design and references, I've identified several key areas for potential innovation in LM block design:\n\n1. State Space Models (SSMs) are emerging as a promising alternative to attention mechanisms:\n- Mamba and ConvSSM show strong performance with linear complexity\n- SSMs can effectively capture long-range dependencies\n- Integration with convolutional approaches shows promise\n\n2. Current Limitations in the Seed Design:\n- Fixed scale processing without content adaptation\n- Limited interaction between hierarchical levels\n- Potential inefficiencies in memory utilization\n- Room for improvement in gradient propagation\n\n3. Key Research Directions:\n- Combining SSMs with hierarchical processing\n- Integrating convolutional operations for local feature extraction\n- Developing adaptive mechanisms for dynamic processing\n- Optimizing memory usage and computational efficiency\n\n### Useful References and Excerpts\n\n1. From ConvSSM paper:\n\"ConvSSM combines tensor modeling ideas of ConvLSTM with long sequence modeling approaches of state space methods... demonstrates how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation.\"\n\nKey implementation details:\n```python\ndef apply_convSSM_parallel(A, B, C, us, x0):\n    L = us.shape[0]\n    As = A * np.ones((L,)+A.shape)\n    Bus = vmap_conv(B, np.complex64(us))\n    Bus = Bus.at[0].add(np.expand_dims(A, (0, 1, 2)) * x0)\n    _, xs = lax.associative_scan(conv_binary_operator, (As, Bus))\n    ys = 2 * vmap_conv(C, xs).real\n    return xs[-1], ys\n```\n\n2. From GrootVL paper:\n\"GrootVL network first dynamically generates a tree topology based on spatial relationships and input features... feature propagation is performed based on this graph, breaking original sequence constraints to achieve stronger representation capabilities.\"\n\n3. From HiPPO paper:\n\"Provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\"\n\n### Key Insights and Analysis\n\n1. SSM Integration Opportunities:\n- SSMs provide an efficient foundation for sequence processing\n- Convolutional operations can enhance local feature extraction\n- Tree-based topologies offer flexible dependency modeling\n\n2. Architectural Innovations:\n- Parallel scan operations enable efficient processing\n- Dynamic topology generation improves adaptability\n- Hierarchical structure can be maintained while incorporating SSMs\n\n3. Performance Considerations:\n- Linear complexity is achievable through careful design\n- Memory efficiency through selective state updates\n- Parallelization opportunities in convolution operations\n\n4. Implementation Strategy:\n- Leverage existing SSM frameworks\n- Integrate convolutional operations strategically\n- Maintain hierarchical structure while adding adaptivity\n\n### Future Search Plan\n\nNext search queries should focus on:\n1. Specific implementations of SSM-based architectures\n2. Memory optimization techniques in language models\n3. Integration of hierarchical processing with state space models\n\n### References\n\n[1] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S.W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[2] Xiao, Y., Song, L., Huang, S., Wang, J., Song, S., Ge, Y., Li, X., & Shan, Y. (2024). GrootVL: Tree Topology is All You Need in State Space Model. arXiv preprint.\n\n[3] Gu, A., Johnson, I., Timalsina, A., Rudra, A., & R\u00e9, C. (2022). How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. International Conference on Learning Representations.\n\n[4] Wang, L. (2023). RRWKV: Capturing Long-range Dependencies in RWKV. arXiv preprint."
        },
        {
            "ready": false,
            "query": "SSM state management optimization techniques language models",
            "detail": "Analyze techniques and methods for efficient state management in SSMs, focusing on optimization strategies, memory efficiency, and performance improvements in language modeling applications.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze techniques and methods for efficient state management in SSMs, focusing on optimization strategies, memory efficiency, and performance improvements in language modeling applications.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 1.00)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.99)*\n\n```\nWhile state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5\\% accuracy improvement on public benchmarks. code is avalaible at: https://github.com/ WailordHe/DenseSSM. ## 1. Introduction\n\nSince the release of ChatGPT (OpenAI, 2023), large language models have entered a new epoch, showcasing outstanding abilities in language comprehension, dialogue, and logical reasoning. Over the past year, the industry has witnessed the emergence of numerous large language models, such as LLaMA (Touvron et al., 2023) and ChatGLM (Zeng et al., 2023). These large language models have given rise to a plethora of practical applications, including conversational bots, code assistants, and AI agents. The foundation of large language models lies in the Transformer network\n\n[^0]structure (Vaswani et al., 2017), primarily utilizing a multihead self-attention module for modeling relationships between tokens and a Feed-forward network for non-linear feature transformations. The scaling law (Kaplan et al., 2020) based on the Transformer structure has propelled the continuous development and expansion of large language models. In the Transformer network, multi-head self-attention (MHSA) plays a crucial role, but it comes with significant computational demands and memory requirements during inference. In terms of computational complexity, for an input sentence of length $N$, the calculation of selfattention has a complexity of $O\\left(N^{2}\\right)$ during training and inference. Regarding memory usage, previously encountered keys and values are stored, leading to a memory occupation of $O(N D)$. As a result, recent efforts on network architectures have focused on simplifying Transformer by reducing its computation and space complexity. This includes various approaches, notably convolutional language models (Poli et al., 2023), recurrent unit (Lei, 2021), long context models (Ding et al., 2023), and state space models (SSMs) (Gu et al., 2021; Gu \\& Dao, 2023). These new models have provided strong alternatives to Transformer for building efficient LLMs. SSMs propose modeling sequences by introducing an appropriate design of hidden states for handling long-range dependencies with both training parallelizability and inference efficiency. Starting from the continuous mapping system, SSMs are discretized to process discrete inputs in deep learning such as language sequence. The discretized SSMs can be computed in both linear recurrence and global convolution modes. Commonly, convolution mode is used during training to achieve parallel acceleration, while recurrence mode is used during autoregressive inference because it has lower computational complexity. The core distinction of SSMs from other neural networks, such as fully-connected neural networks, lies in the design of hidden states. Hidden states enable information to be propagated along the temporal dimension, while avoiding the computation complexity of accessing historical tokens at each step. Through state transition parameters $A$, hidden states transfer the hidden information from the previous time\nsteps to the current time step, allowing for autoregressive prediction of the next token. Hidden states play a crucial role in SSMs, but have not received sufficient investigation in the past. Weights and hidden features in different layers contain information at various levels from fine-grained to coarsegrained (Gu et al., 2021). However, in previous versions of SSMs, hidden states only flowed within the current layer and could not transmit more information to deeper layers, thus failing to capture more hierarchical information. In this paper, we propose DenseSSM to facilitate a more comprehensive flow of hidden information between layers in state space models. We first analyze the hidden state degradation in conventional SSMs which will prevent hidden information flow from low levels to high levels. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information that is useful for the final output. The proposed method is applicable to different types of SSMs, such as RetNet (Sun et al., 2023) and Mamba (Gu \\& Dao, 2023). Our approach maintains the training parallelizability and inference efficiency of SSMs, while achieving a significant improvement with only a slight increase in the number of parameters. For instance, our DenseRetNet model outperforms traditional RetNet with up to 5\\% accuracy improvement on public benchmarks.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.99)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.99)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: SSM state management optimization techniques language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. OptiMUS: Scalable Optimization Modeling with (MI)LP Solvers and Large Language Models\n\n*From Search Query: SSM state management optimization techniques language models*\n\n*Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell*\n\n**TL;DR:** OptiMUS is introduced, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions and outperforms existing state-of-the-art methods on easy datasets and on hard datasets.\n\n**Abstract:** Optimization problems are pervasive in sectors from manufacturing and distribution to healthcare. However, most such problems are still solved heuristically by hand rather than optimally by state-of-the-art solvers because the expertise required to formulate and solve these problems limits the widespread adoption of optimization tools and techniques. This paper introduces OptiMUS, a Large Language Model (LLM)-based agent designed to formulate and solve (mixed integer) linear programming problems from their natural language descriptions. OptiMUS can develop mathematical models, write and debug solver code, evaluate the generated solutions, and improve its model and code based on these evaluations. OptiMUS utilizes a modular structure to process problems, allowing it to handle problems with long descriptions and complex data without long prompts. Experiments demonstrate that OptiMUS outperforms existing state-of-the-art methods on easy datasets by more than $20\\%$ and on hard datasets (including a new dataset, NLP4LP, released with this paper that features long and complex problems) by more than $30\\%$.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 3*)\n\n#### 2. Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\n\n*From Search Query: SSM state management optimization techniques language models*\n\n*Jeonghoon Kim, J. H. Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, S. Kwon, Dongsoo Lee*\n\n**TL;DR:** Parameter-Efficient and Quantization-aware Adaptation (PEQA) is presented - a simple yet effective method that combines the advantages of PEFT with quantized LLMs and significantly reduces the memory overhead associated with the optimizer state.\n\n**Abstract:** Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 66  (*Influential: 2*)\n\n#### 3. Extreme Compression of Large Language Models via Additive Quantization\n\n*From Search Query: SSM state management optimization techniques language models*\n\n*Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan Alistarh*\n\n**TL;DR:** AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime.\n\n**Abstract:** The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of\"extreme\"LLM compression-defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter-from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 46  (*Influential: 6*)\n\n#### 4. ZipLM: Inference-Aware Structured Pruning of Language Models\n\n*From Search Query: SSM state management optimization techniques language models*\n\n*Eldar Kurtic, Elias Frantar, Dan Alistarh*\n\n**TL;DR:** ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications.\n\n**Abstract:** The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and pruning techniques, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications. In particular, ZipLM outperforms all prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and TinyBERT. Moreover, it matches the performance of the heavily optimized MobileBERT model, obtained via extensive architecture search, by simply pruning the baseline BERT-large model. When compressing GPT2, ZipLM outperforms DistilGPT2 while being 60% smaller and 30% faster. Our code is available at: https://github.com/IST-DASLab/ZipLM.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 2*)\n\n#### 5. StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization\n\n*From Search Query: SSM state management optimization techniques language models*\n\n*Shida Wang, Qianxiao Li*\n\n**TL;DR:** This paper introduces a class of reparameterization techniques for SSMs that effectively lift its memory limitations and illustrates that a principled choice of reparameterization scheme can also enhance optimization stability.\n\n**Abstract:** In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this\"curse of memory\"as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets, language models and image classifications.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using State Space Models (SSMs) and focus on efficient state management, optimization strategies, memory efficiency, and performance improvements, here are some key techniques and methods derived from the provided sources and analysis:\n\n## Efficient State Management in SSMs\n\n### Linear Complexity and Constant Memory\nSSMs, such as those used in the Mamba model, offer linear computational complexity and constant memory requirements. This is achieved by using a fixed-size state space to store history, rather than storing information for each token and performing pairwise computations as in attention-based models.\n\n### Selective State Mechanisms\nMamba models employ selective state mechanisms, where the model's dynamics can be updated based on the current input. This involves using specialized trainable linear layers to map the input to matrices that are conditioned dynamically based on the input. This approach enhances the model's ability to focus on key input components and improves content-based reasoning.\n\n### Global Selection Module\nTo address the issue of local pattern shortcuts in Mamba, a global selection module can be introduced. This module helps in handling tasks that require distributed key information, significantly improving the model's performance on complex synthetic tasks and language modeling tasks.\n\n## Hybrid Architectures\n\n### Combining SSMs with Transformers\nHybrid architectures like Jamba, which interleave blocks of Transformer and Mamba layers, can leverage the benefits of both model families. This approach provides high throughput and a small memory footprint compared to vanilla Transformers, compensating for the individual weaknesses of each architecture[Analysis Summary].\n\n### Integration of Convolutional Operations\nConvSSM models combine tensor modeling ideas of ConvLSTM with long sequence modeling approaches of state space methods. This integration allows for parallel scans to be applied to convolutional recurrences, enhancing local feature processing and improving overall efficiency[Analysis Summary].\n\n## Optimization Strategies\n\n### Hardware-Aware Implementations\nOptimizing SSMs for different hardware platforms is crucial for real-world performance. This includes leveraging CPUs, GPUs, FPGAs, ASICs, and PIM/NDPs to accelerate inference. For example, techniques like Chain-of-Thought reasoning and multi-step inference can be optimized for different hardware to improve inference efficiency.\n\n### Adaptive State Management\nAdaptive state management is key to balancing efficiency and capability. This involves updating the state representation selectively based on the input, which helps in managing memory efficiently and scaling to longer sequences.\n\n### Parallel Processing\nDesigning algorithms that enable parallel processing can significantly improve training efficiency. For instance, the Mamba architecture uses a convolutional mode for efficient parallel training, ensuring the model can learn from extensive datasets effectively.\n\n## Performance Improvements\n\n### Reducing Computational Complexity\nTechniques like the Linear Transformer and Attention-Free Transformer (AFT) can reduce the computational complexity from quadratic to linear, significantly accelerating the computation of autoregressive Transformers. These methods can be integrated into SSMs to further improve performance.\n\n### Memory Efficiency\nUsing selective state space models and adaptive state management can help in maintaining memory efficiency. This is particularly important for scaling to longer sequences and larger models without significant increases in memory requirements.\n\nBy incorporating these techniques, researchers can develop more efficient, scalable, and high-performing autoregressive language models based on SSMs, addressing the challenges of state management, memory efficiency, and performance improvements.\n",
            "analysis": "### Analysis Summary\n\nFrom analyzing the search results, several key themes and opportunities emerge for improving LM block design:\n\n1. State Space Models (SSMs) and Efficiency:\n- Mamba architectures achieve linear scaling with sequence length\n- SSMs offer efficient alternatives to attention mechanisms\n- Hardware-aware implementations crucial for performance\n\n2. Hybrid Approaches:\n- Combining SSMs with other architectures shows promise\n- Jamba and MambaFormer demonstrate benefits of hybrid designs\n- Integration of convolutional operations can enhance local feature processing\n\n3. Architectural Innovations:\n- Selective state mechanisms improve content-based reasoning\n- Parallel processing capabilities through careful algorithm design\n- Memory efficiency through adaptive state management\n\n### Useful References with Excerpts\n\n1. From \"Transformers are SSMs\":\n```\nOur SSD is 2-8\u00d7 faster than a Mamba fused scan for large state expansion (N=64) and faster than FlashAttention-2 for sequence length 2k and above.\n```\n\n2. From \"Jamba: A Hybrid Transformer-Mamba\":\n```\nJamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families... provides high throughput and small memory footprint compared to vanilla Transformers.\n```\n\n3. From \"ConvSSM\":\n```\nConvSSM combines tensor modeling ideas of ConvLSTM with long sequence modeling approaches of state space methods... demonstrates how parallel scans can be applied to convolutional recurrences.\n```\n\n4. From \"Can Mamba Learn How to Learn?\":\n```\nSSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality.\n```\n\n### Key Insights and Analysis\n\n1. Architectural Design:\n- SSMs provide efficient sequence processing but may need enhancement for certain tasks\n- Hybrid architectures can compensate for individual weaknesses\n- Memory efficiency crucial for scaling to longer sequences\n\n2. Performance Characteristics:\n- Linear complexity achievable through careful design\n- State management key to balancing efficiency and capability\n- Different architectures excel at different types of tasks\n\n3. Implementation Considerations:\n- Hardware-aware design critical for real-world performance\n- Parallel processing capabilities important for training efficiency\n- Memory management through selective state updates\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient state management in SSMs\n2. Methods for combining convolutional operations with state space models\n3. Optimization strategies for hybrid architectures\n\nProposed search queries:\n1. \"SSM state management optimization techniques language models\"\n2. \"Convolutional operations integration state space models\"\n3. \"Hybrid architecture optimization language models\"\n\n### References\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n[2] Lieber, O., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model. arXiv preprint.\n\n[3] Smith, J.T.H., et al. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[4] Park, J., et al. (2024). Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks. arXiv preprint.\n\n[5] Gu, A., et al. (2022). How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "hybrid architecture SSM compression optimization",
            "detail": "Analyze techniques and methods for combining SSMs with other architectures, focusing on compression, optimization, and parallel processing strategies.",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze techniques and methods for combining SSMs with other architectures, focusing on compression, optimization, and parallel processing strategies.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.98)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.97)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.97)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.97)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.99)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n##### *Relevant Chunk: No. 22/86 (Score: 0.94)*\n\n```\nY. Fu, et al. 2023; De et al. 2024; Glorioso et al. 2024; Lieber et al. 2024) suggests that a hybrid architecture with both SSM layers and attention layers could improve the model quality over that of a Transformer, or a pure SSM (e.g., Mamba) model, especially for in-context learning. We explore the different ways that SSD layers can be combined with attention and MLP to understand the benefits of each. Empirically we find that having around $10 \\%$ of the total number of layers being attention performs best. Combining SSD layers, attention layers, and MLP also works better than either pure Transformer++ or Mamba-2. SSD and Attention We find that SSD and attention layers are complementary: by themselves (e.g. in the Mamba-2 architecture vs. Transformer++) their performance (measured by perplexity) is nearly the same, but a mixture of SSD and attention layers outperforms the pure Mamba-2 or Transformer++ architecture. We show some results (Table 2) for the 350 M model ( 48 layers) trained to 7B tokens on the Pile with the GPT- 2 tokenizer (same number of parameters, same hyperparameters, same training and validation set). Adding in just a few attention layers already yields notable improvement and strikes the best balance between quality and efficiency. We hypothesize that the SSM layers function well as a general sequence-to-sequence mapping, and attention layers act as a retrieval mechanism to quickly refer to previous tokens in the sequence instead of forcing the model to compress all the context to its memory (SSM states). Table 2: (Combining SSD and Attention Blocks.) Perplexity of a 350 M model with 48 layers, with different number of attention layers. Having around a $10 \\%$ ratio of attention layers performs best. | Num. AtTn Blocks | 0 (Mamba-2) | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 | 11 | 15 | 24 | Transformer++ |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Perplexity $\\downarrow$ | 8.60 | 8.38 | 8.32 | 8.29 | 8.29 | 8.28 | $\\mathbf{8 . 2 6}$ | 8.27 | 8.28 | 8.30 | 8.34 | 8.50 | 8.68 |\n\nHybrid Models with SSD, MLP, and Attention We compare different ways that SSD can be combined with the (gated) MLP and attention layers, and evaluate at the 2.7B scale (64 layers), trained to 300B tokens on the Pile (same number of parameters, same hyperparameters, same training and validation set, same data order):\n\n1. Transformer++: 32 attention layers and 32 gated MLP, interleaving. 2. Mamba-2: 64 SSD layers. 3. Mamba-2-MLP: 32 SSD and 32 gated MLP layers, interleaving. 4. Mamba-2-Attention: 58 SSD layers and 6 attention layers (at indices $9,18,27,36,45,56)^{6}$. 5. Mamba-2-MLP-Attention: 28 SSD layers and 4 attention layers, interleaving with 32 gated MLP layers. We report the validation perplexity on the Pile, as well as zero-shot evaluation, in Table 3. In general, the quality of Transformer++ and Mamba-2 models are around the same. We see that adding just 6 attention layers noticeably improves over the pure Mamba-2 model (and over Transformer++). Adding MLP layers reduces model quality, but can (i) speed up training and inference due to the simplicity and hardware-efficiency of the MLP layer (ii) be easier to up-cycle to MoE models by replacing MLP layers with mixture-of-experts. Table 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare different ways SSD, MLP, and attention layers can be combined, evaluated at 2.7 B scale trained to 300 B tokens on the Pile. | MODEl | Token. | PILE <br> PPL $\\downarrow$ | LAMBADA <br> PPL $\\downarrow$ | LAMBADA <br> ACC $\\uparrow$ | HellaSwag <br> ACC $\\uparrow$ | PIQA <br> ACC $\\uparrow$ | Arc-E <br> ACC $\\uparrow$ | Arc-C <br> ACC $\\uparrow$ | WinoGrande <br> ACC $\\uparrow$ | OpenbookQA <br> ACC $\\uparrow$ | Average <br> ACC $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer++ | NeoX | 6.13 | 3.99 | $\\underline{70.3}$ | 66.4 | 75.2 | 67.7 | $\\underline{37.8}$ | 63.9 | 40.4 | 60.2 |\n| Mamba-2 | NeoX | 6.09 | 4.10 | 69.7 | 66.6 | 76.4 | 69.6 | 36.4 | 64.0 | 38.8 | 60.2 |\n| Mamba-2-MLP | NeoX | 6.13 | 4.18 | 69.3 | 65.0 | 76.4 | 68.1 | 37.0 | 63.1 | 38.2 | 59.6 |\n| Mamba-2-Attention | NeoX | 5.95 | 3.85 | 71.1 | 67.8 | $\\underline{75.8}$ | 69.9 | $\\underline{37.8}$ | 65.3 | 39.0 | 61.0 |\n| Mamba-2-MLP-Attention | NeoX | 6.00 | 3.95 | 70.0 | 66.6 | 75.4 | 70.6 | 38.6 | 64.6 | 39.2 | 60.7 |\n\n### 9.3 Speed Benchmarks\n\nWe benchmark the speed of the SSD algorithm against Mamba's scan implementation and FlashAttention-2 (Figure 10). SSD, thanks to its reformulation to use matrix multiplication as a subroutine, can exploit specialized matrix multiplication (matmul) units on GPUs, also known as tensor cores. As a result, it is 2-8\u00d7 faster than Mamba's fused associative scan, which does not leverage matmul units. Due to its linear scaling in sequence length, SSD is faster than FlashAttention-2 starting at sequence length $2 K$. However, we note that the Mamba-2 model as a whole might not be as efficient to train as Transformer at short sequence length (e.g. at $2 K$ ), since a Transformer with $L$ layers would have $\\frac{L}{2}$ MLP layers and $\\frac{L}{2}$ attention layers, while a Mamba- 2 model would have $L$ SSD layers for the same number of parameters. Generally the MLP layers are very hardware efficient since they consist of simple matrix multiplication and pointwise linearity. As shown in Section 9.2.3, one can also combine $\\frac{L}{2}$ SSD layers and $\\frac{L}{2}$ MLP layers to speed up training at short sequence length. [^4]Table 4: (Ablations: Mamba-2 block.) We ablate the major differences between the Mamba-2 and Mamba-1 neural network blocks (Figure 6, Section 7.1). Note that these components are independent of the inner sequence mixing layer; in these ablations, we use SSD for the inner SSM layer (differing from the S6 layer of Mamba-1). | Block | ABCX Projections | Extra Normalization | Parameters | Perplexity |\n| :--- | :--- | :--- | :--- | :--- |\n| Mamba-1 | Sequential | $\\boldsymbol{X}$ | 129.3 M | 11.76 |\n|  | Sequential | $\\boldsymbol{\\checkmark}$ | 129.3 M | 11.54 |\n|  | Parallel | $\\boldsymbol{X}$ | 126.5 M | 11.66 |\n| Mamba-2 | Parallel | $\\boldsymbol{\\checkmark}$ | 126.5 M | 11.49 |\n\n### 9.4 Architecture Ablations\n\n### 9.4.1 Block Design\n\nSection 7.1 introduces the Mamba-2 block, which has small modifications to the Mamba-1 block which are partly motivated by the connection to attention and also to improve the scalability of Mamba-2. Table 4 ablates these architecture changes to the block, which occur outside of the core SSM layer. The ablations validate that parallel projections to create $(A, B, C, X)$ saves parameters and performs slightly better than Mamba's sequential projections. More importantly, this modification is amenable to tensor parallelism at larger model sizes (Section 8). Additionally, the extra normalization layer also slightly improves performance. More importantly, preliminary experiments at larger scales observed that it also helps with training stability. ### 9.4.2 Head Structure\n\nSection 7.2 describes how the dimensions of the $B, C, X$ projections can be viewed as a hyperparameter analogous to notions of multi-head attention and multi-query attention. We also showed how the original Mamba architecture is analogous to multi-value attention (Proposition 7.2), which was a choice that naturally developed from the state-space model point of view and was not previously ablated. Table 5 ablates choices of the multi-head structure for the Mamba-2 architecture. Strikingly, we find a large difference between multi-value and multi-query or multi-key head patterns, despite seeming very similar. Note that this is not explained by the total state size, which is the same for all of them (equal to HPN or the product of the number of heads, head dimension, and state dimension). We also compare to multi-head patterns where the number of $C, B, X$ (analogous to $Q, K, V$ ) heads is equal. We compare against the standard multi-head pattern, as well as one with aggressive sharing where they all have only 1 head. Note that in the latter case, the model still has H different sequence mixers $M$, because each head still has a different $A$. When parameter matched, these multi-head patterns perform similarly to each other, in between the MVA and MQA/MKA patterns. ### 9.4.3 Attention Kernel Approximations\n\nSection 7.3 noted how SSD can be combined with ideas from the linear attention literature, such as various forms of kernel approximations. We ablate several variants of these suggested by previous works in Table 6. These include the cosFormer (Qin, Weixuan Sun, et al. 2022), Random Feature Attention H. Peng et al. 2021, and Positive Random Features (Performer) (Choromanski et al. 2021). We also ablate adding a normalization term, akin to the denominator of the softmax function in standard attention. We found that this introduced instabilities to most variants, but slightly improved performance for the ReLU activation function $\\psi$. Table 7 also tests more recent proposals to improve linear attention that involve expanding the feature dimension (Based (Arora, Eyuboglu, Zhang, et al. 2024) and ReBased (Aksenov et al. 2024)). These linear attention extensions aim to appropriate the $\\exp$ kernel with a quadratic approximation. ReBased also proposes to replace the QK activation function with a layer normalization; from an SSM-centric view we apply a normalization on top of $(B, C)$ before applying the SSM function. Table 5: (Ablations: Multi-head structure.) All models have state expansion factor $N=64$ and head size $P=64$ and are trained to Chinchilla scaling law token counts. The number of $A$ heads is always equal to the total heads H , i.e. each head has a separate input-dependent $A$ decay factor. (Top) 125M models, 2.5B tokens (Bottom) 360 M models, 7 B tokens\n\n| SSM Head Pattern | Attn. Analog | $A$ heads | $B$ heads | $C$ heads | $X$ heads | Layers | Params | Ppl. |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Multi-input (MIS) | Multi-value (MVA) | 24 | 1 | 1 | 24 | 24 | 126.5 M | $\\mathbf{1 1 . 6 6}$ |\n| Multi-contract (MCS) | Multi-query (MQA) | 24 | 1 | 24 | 1 | 24 | 126.5 M | 12.62 |\n| Multi-expand (MES) | Multi-key (MKA) | 24 | 24 | 1 | 1 | 24 | 126.5 M | 12.59 |\n| Multi-head (MHS) | Multi-head (MHA) | 24 | 24 | 24 | 24 | 15 | 127.6 M | 12.06 |\n| Multi-state (MSS) | - | 24 | 1 | 1 | 1 | 36 | 129.6 M | 12.00 |\n| Multi-input (MIS) | Multi-value (MVA) | 32 | 1 | 1 | 32 | 48 | 361.8 M | 8.73 |\n| Multi-contract (MCS) | Multi-query (MQA) | 32 | 1 | 32 | 1 | 48 | 361.8 M | 9.33 |\n| Multi-expand (MES) | Multi-key (MKA) | 32 | 32 | 1 | 1 | 48 | 361.8 M | 9.36 |\n| Multi-head (MHS) | Multi-head (MHA) | 32 | 1 | 1 | 1 | 70 | 361.3 M | 9.01 |\n| Multi-state (MSS) | - | 32 | 32 | 32 | 32 | 29 | 357.3 M | 9.04 |\n\nTable 6: (Ablations: Kernel approximations.) We test various proposals for the kernel activation function $\\psi$, including linear attention variants aiming to approximate the exp kernel from standard softmax attention. | Kernel activation $\\varphi$ | Perplexity |\n| :--- | :--- |\n| none | 11.58 |\n| Swish | 11.66 |\n| Exp | 11.62 |\n| ReLU | 11.73 |\n| ReLU + normalization | 11.64 |\n| cosFormer | 11.97 |\n| Random Feature Attention | 11.57 |\n| Positive Random Features (Performer) | 12.21 |\n\nTable 7: (Ablations: Kernel approximations.) We test the (Re)Based methods for linear attention approximations, which involve expanded feature maps. (Top) 130 M models. (Top) 380 M models with $N=256$. | Kernel activation $\\varphi$ | Perplexity |\n| :--- | :--- |\n| Swish | 11.67 |\n| Swish + Taylor (Based) | 12.19 |\n| LayerNorm | 11.50 |\n| LayerNorm + Square (ReBased) | 11.84 |\n| Swish | 8.58 |\n| Swish + Taylor (Based) | 8.71 |\n| LayerNorm | 8.61 |\n| LayerNorm + Square (ReBased) | 8.63 |\n\nWe note that this technique has been independently proposed as the \"QK-Norm\" for softmax attention (Team 2024) and an \"internal normalization\" for Mamba (Lieber et al. 2024). Overall, Table 6 and Table 7 found that the kernel approximation methods we tried did not seem to improve over simple pointwise non-linear activation functions for $\\psi$. Thus our default settings for Mamba-2 used $\\psi(x)=$ Swish $(x)$ to follow Mamba-1, but we suggest that removing this activation entirely may be a simpler choice that we did not extensively test. We emphasize however that SSD and vanilla linear attention differ in the inclusion of the 1-semiseparable mask $L$, while the various linear attention methods in the literature were derived to approximate softmax attention without this term; thus, our negative results may be not unexpected. ## 10 Related Work and Discussion\n\nThe state space duality framework bridges connections between SSMs, structured matrices, and attention. We discuss in more depth the relations between SSD and these concepts more broadly. Using ideas from each of the viewpoints, we also suggest some directions that the SSD framework can be extended in future work. ### 10.1 State Space Models\n\nStructured state space models can be characterized along the axes\n(i) whether it is time-invariant or time-varying.\n```\n\n\n\n---\n## Found 1 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid architecture SSM compression optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 1 related papers from Semantic Scholar\n\n#### 1. Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\n\n*From Search Query: hybrid architecture SSM compression optimization*\n\n*Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos*\n\n**TL;DR:** A hybrid model is introduced, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently, and suggests that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Abstract:** State-space models (SSMs), such as Mamba (Gu&Dao, 2023), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 44  (*Influential: 7*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by combining State Space Models (SSMs) with other architectures, focusing on compression, optimization, and parallel processing strategies, here are some key techniques and methods that can be considered:\n\n## Hybrid Architectures Combining SSMs with Other Approaches\n\n### Sparse Transformers and SSMs\n- Integrating SSMs with sparse attention mechanisms can significantly enhance efficiency. For instance, the Sparse Transformer reduces complexity from \\(O(n^2)\\) to \\(O(n\\sqrt{n})\\), which can be combined with SSMs to maintain performance while handling larger input sizes efficiently.\n\n### Convolutional State Space Models (ConvSSM)\n- ConvSSM combines tensor modeling ideas of ConvLSTM with long sequence modeling approaches of state space methods. This hybrid approach demonstrates how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation.\n\n### State Space Duality (SSD) Framework\n- The SSD framework allows designing architectures like Mamba-2, which refine selective SSMs to be 2-8X faster while remaining competitive with Transformers on language modeling. This framework can be extended to combine SSMs with other architectures like Transformers or LSTMs.\n\n## Advanced Compression and Quantization Methods\n\n### Sparse Tensors\n- Using sparse tensors can compress data by efficiently representing zero-valued elements, which is crucial for SSMs that often deal with sparse state transitions. Formats like COO, CSR, CSC, and LIL can help reduce memory footprint and enhance computational performance.\n\n### Quantization Techniques\n- Methods like LoRA (Low-Rank Adaptation) and its variants (e.g., QLoRA, LQ-LoRA) can be applied to SSMs to reduce memory requirements. These techniques involve quantizing pre-trained models and using low-rank matrices to adapt to new tasks efficiently.\n\n### Zero Redundancy Optimizer (ZeRO)\n- ZeRO and its variants (ZeRO-Offload, ZeRO-Infinity, ZeRO++) can be used to reduce redundant copies of data on GPUs, offload optimizer states and updates to CPUs, and utilize NVMe memory for offloading parameters, gradients, and activation checkpoints. This can help in training larger SSM-based models efficiently.\n\n## Parallel Processing Optimizations for SSMs\n\n### Parallel Scans in ConvSSM\n- As mentioned, ConvSSM applies parallel scans to convolutional recurrences, achieving subquadratic parallelization. This approach can be generalized to other SSM architectures to enhance parallel processing capabilities.\n\n### Hardware-Aware Implementations\n- Optimizing SSMs for specific hardware platforms (CPUs, GPUs, FPGAs, ASICs) can significantly improve performance. For example, leveraging streaming multiprocessors (SMs) in GPUs and using SIMT (single-instruction multiple threads) can enhance the efficiency of SSM computations.\n\n### Reparameterization Techniques\n- Reparameterization techniques for SSMs, as introduced in StableSSM, can improve memory efficiency and optimization stability. These techniques can be combined with parallel processing strategies to further enhance the performance of SSMs.\n\nBy integrating these techniques, researchers can develop hybrid architectures that leverage the strengths of SSMs, such as linear complexity and selective state mechanisms, while optimizing for compression, parallel processing, and overall efficiency. This approach can lead to improved autoregressive language model designs that are scalable, efficient, and robust.\n",
            "analysis": "### Analysis Summary\n\nBased on the search results, several key themes and opportunities emerge for improving LM block design:\n\n1. State Space Models (SSMs) and Efficiency:\n- SSMs offer linear complexity and constant memory usage\n- Selective state mechanisms enable content-based reasoning\n- Hardware-aware implementations crucial for performance\n\n2. Key Innovations:\n- Selective SSMs allow dynamic parameter updates based on input\n- Dense connections between layers improve information flow\n- Spectral filtering and tree-based topologies offer new approaches\n\n3. Memory Management:\n- Adaptive compression techniques reduce memory requirements\n- Selective state updates optimize resource usage\n- Efficient parallel processing through careful algorithm design\n\n### Useful References with Excerpts\n\n1. From DenseMamba paper:\n```\nHidden states enable information to be propagated along the temporal dimension, while avoiding the computation complexity of accessing historical tokens at each step. Through state transition parameters A, hidden states transfer the hidden information from the previous time steps to the current time step, allowing for autoregressive prediction of the next token.\n```\n\n2. From Transformers are SSMs:\n```\nOur state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n```\n\n3. From ConvSSM:\n```\nConvSSM combines tensor modeling ideas of ConvLSTM with long sequence modeling approaches of state space methods... demonstrates how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation.\n```\n\n4. From StableSSM:\n```\nWe introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability.\n```\n\n### Key Insights and Analysis\n\n1. State Management:\n- Selective state mechanisms crucial for performance\n- Dense connections improve information flow\n- Reparameterization can enhance stability\n\n2. Architectural Innovations:\n- Hybrid approaches combining different architectures show promise\n- Hardware-aware implementations essential\n- Parallel processing capabilities important\n\n3. Memory Efficiency:\n- Adaptive compression reduces memory requirements\n- Selective updates optimize resource usage\n- Efficient parallel processing through algorithm design\n\n4. Implementation Considerations:\n- Hardware-specific optimizations needed\n- Balance between efficiency and capability\n- Stability through careful parameter management\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for combining SSMs with other architectures\n2. Advanced compression and quantization methods\n3. Novel approaches to parallel processing in SSMs\n\nNext search queries should focus on:\n1. Hybrid architectures combining SSMs with other approaches\n2. Advanced compression techniques for state space models\n3. Parallel processing optimizations for SSMs\n\n### References\n\n[1] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n[3] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S.W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[4] Wang, S., & Li, Q. (2023). StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization. International Conference on Machine Learning.\n\n[5] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint."
        },
        {
            "ready": false,
            "query": "parallel processing hybrid architecture implementation optimization",
            "detail": "Analyze techniques and methods for efficient parallel processing in hybrid architectures, focusing on implementation strategies and hardware-aware optimizations for combined SSM-attention models.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze techniques and methods for efficient parallel processing in hybrid architectures, focusing on implementation strategies and hardware-aware optimizations for combined SSM-attention models.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 1.00)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.97)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 3/32 (Score: 0.99)*\n\n```\nIt is worth noting that their complexity is essentially quadratic, and the corresponding linear versions both suffer performance degradation (see Fig. 1 left). While it is true that the simple use of a chunk linearization strategy can rival the speed of SSMs, the drop in performance is significant. We, therefore, abandon this strategy and the softmax function to accelerate the linear attention operations of the kernel-based approach. In addition, long convolution (Fu et al., 2023b) is more efficient and easier to implement than the traditional SSMs. We believe combining these two modules could release the true power of the hybrid model with faster speed and better performance. However, the existence of such a dilemma is worth considering: (1) the linear attention in hybrid models is a speed bottleneck for long convolution, the comparison is shown in Fig. 1 right; (2) the instability of long convolution makes attention hard to optimize. Specifically, due to the intense memory access (I/O) on the GPU and cumulative summation (cumsum) operation in a casual setting,\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_542e0cd768b54533ad80g-02.jpg?height=550&width=1454&top_left_y=224&top_left_x=300)\n\nFigure 1. Demonstration of (left) comparison of various popular models on the Long Range Arena Dataset, and (right) speed benchmark on different implementations of attentions. Figure left, despite showing promising performance of SSM-Attention models, the linear version of these models degenerated. Figure right benchmarks attention speed with around 200 M parameters, showing linear attention is not linear with sequence length and is significantly slower than S 4 , which can be a speed bottleneck in these hybrid designs. the notable reduction in complexity from $\\mathcal{O}\\left(L^{2}\\right)$ to $\\mathcal{O}(L)$ in linear attention is only theoretical (Qin et al., 2024); On the other hand, the long convolution needs hand-crafted regularizations to prevent over-fitting high-frequency noise. To escape the dilemma, we have developed a novel model called CHELA, which stands for short-long Convolutional with Hardware-Efficient $\\underline{\\text { Linear }} \\underline{\\text { Attention. This model is }}$ designed to effectively and efficiently capture complex dependencies in long inputs. Inspired by FlashAttention (Dao et al., 2022; Dao, 2023), the model comprises multiple layers of real linear token mixers with hardware-friendly implementation with a gating mechanism that achieves comparable performance to full attention. We have added a stabilizer, reparameterizable (Ding et al., 2022; 2023) short-long convolutions, to the bottom layer of the long convolution to integrate multiple frequencies with inputs. As a result, the top layers of CHELA capture more refined data-dependent information using linear attention, while the convolutions add a structural bias that enhances global abstraction in multi-level frequencies. We demonstrate the efficiency and effectiveness of CHELA on various datasets and tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (LRA) benchmark (Tay et al., 2020b), which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, CHELA is not only significantly faster than the vanilla Transformer but also yields better performance. In all the settings, CHELA outperforms the baselines. Finally, we provide further analysis and ablation experiments to demonstrate the effectiveness. Our contribution can be summarized in three folds:\n\n- Efficient Implementation: We release the potential speed of linear attention in the attention-ssm hybrid model, maintaining a constant rate regardless of sequence length with fixed memory. - New Design: We improve multi-frequency learning with multi-level convolutional hierarchies, leading to better performance and stability for long convolution. - Promising Results: By connecting the above two basic modules, we propose CHELA for efficient long sequence learners are capable of both global information abstraction and fine-grained data-dependent selection.\n```\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.95)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.95)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 14/49 (Score: 0.95)*\n\n```\nFlashConv maintains nearly linear scaling, even to very long sequence lengths. Fig. 2 shows overall $2-3 \\times$ speedup over FFTConv with cuFFT using our techniques (block FFT, state-passing). Simple kernel fusion (even without block FFT) can yield speedup over cuFFT for short sequences, since memory reads/writes are the bottleneck for short sequences. For long sequences, SSMs using state passing can be dozens of times faster than even the fastest attention implementation. Table 8: Speedup on the LRA benchmark. | Models | Speedup |\n| :---: | :---: |\n| Transformer | $1 \\times$ |\n| FlashAttention [15] | $2.4 \\times$ |\n| Block-sparse FlashAttention [15] | $2.8 \\times$ |\n| S4 [28] | $2.9 \\times$ |\n| S4 with FLASHConv | $5.8 \\times$ |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_7a1798f4fbf723c76fd6g-10.jpg?height=540&width=1661&top_left_y=561&top_left_x=228)\n\nFigure 2: We compare the speed of different algorithms to perform FFT-based convolution, along with FlashAttention [15] (the fastest attention implementation we know of). We use batch size 8, hidden dimension 1024, and varying sequence length from 256 to 32 k , and measure on an A100-SMX4-40GB GPU. We see that kernel fusion gives up to $3.4 \\times$ speedup over naive FFTConv for short sequences (up to 512 ), block FFT gives up to $2 \\times$ speedup for medium length sequences $(1 \\mathrm{k}-8 \\mathrm{k})$, and state-passing allows $2.3 \\times$ faster FFTConv for long sequences ( 16 k and above). ## 7 Conclusion\n\nOur main goal is to understand and narrow the gap between attention and SSMs in language modeling in terms of modeling capabilities and hardware efficiency. Our exploration based on synthetic language tasks motivated us to design the H3 layer, which is surprisingly competitive with attention. Our BlockFFTCONV algorithm exploits matrix multiplication units and the dual recurrent-convolution view of SSMs to substantially speed up SSMs, reducing the hardware barrier between attention and SSMs. We are excited about several future directions. Our H3 layer is a simple combination of two SSMs, and more sophisticated designs could be more expressive. Our encouraging results on language models up to 1.3B parameters suggests that scaling SSMs to larger sizes is a promising avenue. Since simply adding two attention layers to H3 models already outperforms both the pure H3 model and Transformers, we are optimistic about combining the complementary strengths of SSMs and attention in the future. ## Acknowledgments\n\nWe thank Albert Gu for helpful discussion regarding the model architecture, and more importantly for sending us daily hippo videos. We thank Together Computer for providing portions of the compute used to train models in this paper. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Wu Tsai Neuroscience Stanford Interdisciplinary Graduate Fellowship, and members of the Stanford DAWN project: Facebook, Google, and VMWare.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.85)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.85)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: parallel processing hybrid architecture implementation optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Efficient Meta Neural Heuristic for Multi-Objective Combinatorial Optimization\n\n*From Search Query: parallel processing hybrid architecture implementation optimization*\n\n*Jinbiao Chen, Jiahai Wang, Zizhen Zhang, Zhiguang Cao, Te Ye, Siyuan Chen*\n\n**TL;DR:** An efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems, which is able to outperform the state-of-the-art neural heuristics in terms of solution quality and learning efficiency.\n\n**Abstract:** Recently, neural heuristics based on deep reinforcement learning have exhibited promise in solving multi-objective combinatorial optimization problems (MOCOPs). However, they are still struggling to achieve high learning efficiency and solution quality. To tackle this issue, we propose an efficient meta neural heuristic (EMNH), in which a meta-model is first trained and then fine-tuned with a few steps to solve corresponding single-objective subproblems. Specifically, for the training process, a (partial) architecture-shared multi-task model is leveraged to achieve parallel learning for the meta-model, so as to speed up the training; meanwhile, a scaled symmetric sampling method with respect to the weight vectors is designed to stabilize the training. For the fine-tuning process, an efficient hierarchical method is proposed to systematically tackle all the subproblems. Experimental results on the multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) show that, EMNH is able to outperform the state-of-the-art neural heuristics in terms of solution quality and learning efficiency, and yield competitive solutions to the strong traditional heuristics while consuming much shorter time.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 2. Betty: An Automatic Differentiation Library for Multilevel Optimization\n\n*From Search Query: parallel processing hybrid architecture implementation optimization*\n\n*Sang Keun Choe, W. Neiswanger, P. Xie, Eric P. Xing*\n\n**TL;DR:** Betty, a software library for large-scale MLO, is introduced with a novel dataflow graph that allows to develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O( d^2), and enables scaling MLO to models with hundreds of millions of parameters.\n\n**Abstract:** Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that Betty can be used to implement an array of MLO programs, while also observing up to 11% increase in test accuracy, 14% decrease in GPU memory usage, and 20% decrease in training wall time over existing implementations on multiple benchmarks. We also showcase that Betty enables scaling MLO to models with hundreds of millions of parameters. We open-source the code at https://github.com/leopard-ai/betty.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 23  (*Influential: 6*)\n\n#### 3. Breaking the Nonsmooth Barrier: A Scalable Parallel Method for Composite Optimization\n\n*From Search Query: parallel processing hybrid architecture implementation optimization*\n\n*Fabian Pedregosa, R\u00e9mi Leblond, Simon Lacoste-Julien*\n\n**TL;DR:** ProxASAGA is proposed, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm that achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term.\n\n**Abstract:** Due to their simplicity and excellent performance, parallel asynchronous variants of stochastic gradient descent have become popular methods to solve a wide range of large-scale optimization problems on multi-core architectures. Yet, despite their practical success, support for nonsmooth objectives is still lacking, making them unsuitable for many problems of interest in machine learning, such as the Lasso, group Lasso or empirical risk minimization with convex constraints. \nIn this work, we propose and analyze ProxASAGA, a fully asynchronous sparse method inspired by SAGA, a variance reduced incremental gradient algorithm. The proposed method is easy to implement and significantly outperforms the state of the art on several nonsmooth, large-scale problems. We prove that our method achieves a theoretical linear speedup with respect to the sequential version under assumptions on the sparsity of gradients and block-separability of the proximal term. Empirical benchmarks on a multi-core architecture illustrate practical speedups of up to 12x on a 20-core machine.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 35  (*Influential: 5*)\n\n#### 4. Massively Parallel and Asynchronous Tsetlin Machine Architecture Supporting Almost Constant-Time Scaling\n\n*From Search Query: parallel processing hybrid architecture implementation optimization*\n\n*Kuruge Darshana Abeyrathna, Bimal Bhattarai, Morten Goodwin, S. Gorji, Ole-Christoffer Granmo, Lei Jiao, Rupsa Saha, Rohan Kumar Yadav*\n\n**TL;DR:** It turns out that the decentralized TM learning algorithm copes well with working on outdated data, resulting in no significant loss in learning accuracy, and it is shown that the approach provides up to 50 times faster learning.\n\n**Abstract:** Using logical clauses to represent patterns, Tsetlin machines (TMs) have recently obtained competitive performance in terms of accuracy, memory footprint, energy, and learning speed on several benchmarks. A team of Tsetlin automata (TAs) composes each clause, thus driving the entire learning process. These are rewarded/penalized according to three local rules that optimize global behaviour. Each clause votes for or against a particular class, with classification resolved using a majority vote. In the parallel and asynchronous architecture that we propose here, every clause runs in its own thread for massive parallelism. For each training example, we keep track of the class votes obtained from the clauses in local voting tallies. The local voting tallies allow us to detach the processing of each clause from the rest of the clauses, supporting decentralized learning. Thus, rather than processing training examples one-by-one as in the original TM, the clauses access the training examples simultaneously, updating themselves and the local voting tallies in parallel. There is no synchronization among the clause threads, apart from atomic adds to the local voting tallies. Operating asynchronously, each team of TA will most of the time operate on partially calculated or outdated voting tallies. However, across diverse learning tasks, it turns out that our decentralized TM learning algorithm copes well with working on outdated data, resulting in no significant loss in learning accuracy. Further, we show that the approach provides up to 50 times faster learning. Finally, learning time is almost constant for reasonable clause amounts. For sufficiently large clause numbers, computation time increases approximately proportionally. Our parallel and asynchronous architecture thus allows processing of more massive datasets and operating with more clauses for higher accuracy.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 5. A Hyperparameter Optimization Toolkit for Neural Machine Translation Research\n\n*From Search Query: parallel processing hybrid architecture implementation optimization*\n\n*Xuan Zhang, Kevin Duh, Paul McNamee*\n\n**TL;DR:** A hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane and demonstrate that it is possible to discover near-optimal models under a computational budget with little effort.\n\n**Abstract:** Hyperparameter optimization is an important but often overlooked process in the research of deep learning technologies. To obtain a good model, one must carefully tune hyperparameters that determine the architecture and training algorithm. Insufficient tuning may result in poor results, while inequitable tuning may lead to exaggerated differences between models. We present a hyperparameter optimization toolkit for neural machine translation (NMT) to help researchers focus their time on the creative rather than the mundane. The toolkit is implemented as a wrapper on top of the open-source Sockeye NMT software. Using the Asynchronous Successive Halving Algorithm (ASHA), we demonstrate that it is possible to discover near-optimal models under a computational budget with little effort.Code: https://github.com/kevinduh/sockeye-recipes3Video demo: https://cs.jhu.edu/ kevinduh/j/demo.mp4\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n### 1 related papers from Papers with Code\n\n#### 1. GHOST: Building blocks for high performance sparse linear algebra on heterogeneous systems\n\n*From Search Query: parallel processing hybrid architecture implementation optimization*\n\n*Gerhard Wellein, Melven R\u00f6hrig-Z\u00f6llner, Jonas Thies, Achim Basermann, Moritz Kreutzer, Andreas Pieper, Martin Galgon, Holger Fehske, Georg Hager, Faisal Shahzad*\n\n**Abstract:** While many of the architectural details of future exascale-class high\nperformance computer systems are still a matter of intense research, there\nappears to be a general consensus that they will be strongly heterogeneous,\nfeaturing \"standard\" as well as \"accelerated\" resources. Today, such resources\nare available as multicore processors, graphics processing units (GPUs), and\nother accelerators such as the Intel Xeon Phi. Any software infrastructure that\nclaims usefulness for such environments must be able to meet their inherent\nchallenges: massive multi-level parallelism, topology, asynchronicity, and\nabstraction. The \"General, Hybrid, and Optimized Sparse Toolkit\" (GHOST) is a\ncollection of building blocks that targets algorithms dealing with sparse\nmatrix representations on current and future large-scale systems. It implements\nthe \"MPI+X\" paradigm, has a pure C interface, and provides hybrid-parallel\nnumerical kernels, intelligent resource management, and truly heterogeneous\nparallelism for multicore CPUs, Nvidia GPUs, and the Intel Xeon Phi. We\ndescribe the details of its design with respect to the challenges posed by\nmodern heterogeneous supercomputers and recent algorithmic developments.\nImplementation details which are indispensable for achieving high efficiency\nare pointed out and their necessity is justified by performance measurements or\npredictions based on performance models. The library code and several\napplications are available as open source. We also provide instructions on how\nto make use of GHOST in existing software packages, together with a case study\nwhich demonstrates the applicability and performance of GHOST as a component\nwithin a larger software stack.\n\n**Published:** 2015-07-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient parallel processing in hybrid architectures that combine State Space Models (SSMs) with attention mechanisms, here are some key techniques and methods that can be considered:\n\n## Hybrid Parallel Computing Strategies\n\n### Combining OpenMP and MPI\nFor hybrid architectures, a strategy similar to the OpenMP+MPI hybrid parallel computing approach can be beneficial. This method leverages the strengths of both OpenMP for fine-grained parallelism within shared memory machines and MPI for coarse-grained parallelism across distributed memory systems. This hybrid approach can ensure high communication efficiency and scalability, which is crucial for large-scale language models.\n\n### Dynamic Scheduling\nTo address the issue of load imbalance in parallel processing, dynamic scheduling algorithms can be employed. These algorithms adaptively adjust task allocation based on the real-time load of processors, ensuring that no single process delays the entire program. This method is particularly useful in scenarios where different tasks have varying computational requirements, such as in the case of SSMs and attention layers with different computational complexities.\n\n## Memory Optimization Strategies\n\n### Memory Hierarchy Optimization\nOptimizing the memory hierarchy is critical for efficient parallel processing. Techniques such as selective state updates and dense connections can help manage memory efficiently. For example, the Mamba model uses selective state mechanisms to reduce computational complexity and memory usage, while DenseMamba employs dense hidden connections to preserve information across layers without excessive memory access.\n\n### Hardware-Aware Implementations\nHardware-aware implementations are essential for optimizing performance. Using dialects like the \"hyper\" dialect introduced in the MLIR framework can abstract the intricacies of data communication between host processors and heterogeneous devices. This approach can optimize data management and parallel computation across diverse hardware, reducing redundant memory overhead and enhancing overall performance.\n\n## Parallel Processing Techniques\n\n### Parallel Scan Operations\nParallel scan operations are highly effective for efficient processing in SSMs. The ConvSSM model demonstrates how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization, which is particularly beneficial for long sequence modeling tasks.\n\n### Task Partitioning and Scheduling\nEfficient task partitioning and scheduling are crucial for hybrid architectures. The \"hyper.for\" operation in the MLIR-based dialect can encapsulate the logic for task partitioning and execution across various devices, ensuring optimal workload distribution and minimizing execution time. This approach can be adapted to partition tasks between SSM and attention layers based on their computational requirements and the available hardware resources.\n\n## Implementation Considerations\n\n### Balancing Compression and Capability\nThere needs to be a careful balance between model compression and capability. Techniques such as kernel fusion, parallel scan, and recomputation can help optimize the computation problem of SSMs while maintaining the model's capability. This balance is essential to ensure that the model remains efficient without compromising its performance.\n\n### Hardware-Specific Optimizations\nHardware-specific optimizations are vital for achieving optimal performance. For instance, optimizing compute, storage, and networking performance as outlined in the Google Cloud Architecture Framework can help in designing workloads that operate efficiently on cloud infrastructure. This includes autoscaling resources to match the load, which can be particularly beneficial for large-scale language models.\n\nBy integrating these techniques and strategies, researchers can develop more efficient and scalable autoregressive language models that combine the strengths of SSMs and attention mechanisms, optimized for parallel processing in hybrid architectures.\n",
            "analysis": "### Analysis Summary\n\nAfter analyzing three rounds of search results, several promising directions have emerged for improving LM block design:\n\n1. Hybrid Architecture Opportunities:\n- Combining SSMs with attention shows strong potential\n- Mamba-2 with 10% attention layers outperforms pure SSM or Transformer models\n- ConvSSM demonstrates benefits of integrating convolutional operations\n\n2. State Management Innovations:\n- Selective state mechanisms improve efficiency\n- Dense connections enhance information flow\n- Hardware-aware implementations crucial for performance\n\n3. Optimization Strategies:\n- Parallel scan operations enable efficient processing\n- Memory hierarchy optimization important for performance\n- Careful balance needed between compression and capability\n\n### Useful References with Excerpts\n\n1. From Transformers are SSMs:\n```\nSSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2), crossing over at sequence length 2K and 6\u00d7 faster at sequence length 16K.\n```\n\n2. From DenseMamba:\n```\nHidden states enable information to be propagated along the temporal dimension, while avoiding the computation complexity of accessing historical tokens at each step. Through state transition parameters A, hidden states transfer the hidden information from the previous time steps to the current time step.\n```\n\n3. From Mamba paper:\n```\nThe selection mechanism is designed to overcome the limitations of LTI models... we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation.\n```\n\n4. From ConvSSM:\n```\nConvSSM combines tensor modeling ideas of ConvLSTM with long sequence modeling approaches of state space methods... demonstrates how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization.\n```\n\n### Key Insights and Analysis\n\n1. Architecture Design:\n- Hybrid approaches combining SSMs with attention show superior performance\n- Optimal ratio appears to be ~10% attention layers\n- Dense connections between layers improve information flow\n- Convolutional operations enhance local feature processing\n\n2. State Management:\n- Selective state mechanisms crucial for efficiency\n- Dense connections help preserve information across layers\n- Hardware-aware implementation essential for performance\n- Memory hierarchy optimization key for efficiency\n\n3. Implementation Considerations:\n- Parallel scan operations enable efficient processing\n- Memory management through selective updates\n- Balance needed between compression and capability\n- Hardware-specific optimizations important\n\n4. Performance Characteristics:\n- Linear complexity achievable through careful design\n- Memory efficiency through selective state updates\n- Different architectures excel at different tasks\n- Hybrid approaches can compensate for individual weaknesses\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient parallel processing in hybrid architectures\n2. Memory optimization strategies for combined SSM-attention models\n3. Implementation details for hardware-aware designs\n\n### References\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n[2] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[4] Smith, J.T.H., et al. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[5] Park, J., et al. (2024). Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "hardware efficient hierarchical normalization implementation",
            "detail": "Search for papers discussing:\n1. Hardware-efficient implementations of hierarchical normalization in neural networks\n2. Memory optimization techniques for processing long sequences\n3. Practical implementations of SSM-based normalization\n4. Empirical studies of normalization stability in deep networks\n5. Cache-efficient algorithms for hierarchical processing",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hardware-efficient implementations of hierarchical normalization in neural networks\n2. Memory optimization techniques for processing long sequences\n3. Practical implementations of SSM-based normalization\n4. Empirical studies of normalization stability in deep networks\n5. Cache-efficient algorithms for hierarchical processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.53)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.53)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.13)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.13)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 3. Retentive network: a successor to transformer for large language models (Avg. Score: 0.07)\n\n*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 143  (*Influential: 18*)\n\n**TL;DR:** This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n##### *Relevant Chunk: No. 17/21 (Score: 0.07)*\n\n```\narXiv preprint arXiv:1909.08053, 2019. [SSI ${ }^{+}$22] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language sequences. arXiv preprint arXiv:2201.03533, 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 60006010, 2017. [WH18] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3-19, 2018. $\\left[\\mathrm{WMD}^{+}\\right.$22] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. DeepNet: Scaling Transformers to 1,000 layers. ArXiv, abs/2203.00555, 2022. [WMH ${ }^{+}$22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers. arXiv preprint arXiv:2210.06423, 2022. $\\left[\\mathrm{WPN}^{+}\\right.$19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems.\n```\n\n#### 4. Scalable MatMul-free Language Modeling (Avg. Score: 0.05)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.05)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 5. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.02)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.02)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hardware efficient hierarchical normalization implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware efficient hierarchical normalization implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\n\n*From Search Query: hardware efficient hierarchical normalization implementation*\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 1*)\n\n#### 3. Hardware-Aware Compression with Random Operation Access Specific Tile (ROAST) Hashing\n\n*From Search Query: hardware efficient hierarchical normalization implementation*\n\n*Aditya Desai, K. Zhou, Anshumali Shrivastava*\n\n**TL;DR:** With ROAST, the authors can efficiently train and deploy the model using a much smaller memory footprint in text and image classification tasks, and introduce global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest.\n\n**Abstract:** Advancements in deep learning are often associated with increasing model sizes. Training and deploying large models require sophisticated hardware and incur significantly higher costs. Thus, model compression is a widely explored approach to solving the problem. However, SOTA techniques fall short in one or more desirable aspects of compression - for instance, pruning does not reduce memory for training, quantization can only provide up to 32 \u00d7 compression, Hashed-Net is cache-inefficient, etc. This paper proposes a model-agnostic, cache-friendly, and hardware-aware model compression approach: Random Operation Access Specific Tile (ROAST) hashing. ROAST collapses the parameters by club-bing them through a lightweight mapping. While clubbing these parameters, ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern. ROAST is up to \u223c 25 \u00d7 faster to train and \u223c 50 \u00d7 faster to infer than the popular parameter sharing method HashedNet. Additionally, ROAST introduces global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest. With ROAST, we can efficiently train and deploy the model using a much smaller memory footprint ( \u223c 10 \u2212 100 \u00d7 lesser) in text and image classification tasks. ROAST-MM kernel implementation is open-source 1\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Efficient Offline Policy Optimization with a Learned Model\n\n*From Search Query: hardware efficient hierarchical normalization implementation*\n\n*Zi-Yan Liu, Siyi Li, W. Lee, Shuicheng Yan, Zhongwen Xu*\n\n**TL;DR:** This paper proposes to use a regularized one-step look-ahead approach to tackle a few hypotheses where MuZero Unplugged may not work well under the offline RL settings, including learning with limited data coverage, and learning from offline data of stochastic environments.\n\n**Abstract:** MuZero Unplugged presents a promising approach for offline policy learning from logged data. It conducts Monte-Carlo Tree Search (MCTS) with a learned model and leverages Reanalyze algorithm to learn purely from offline data. For good performance, MCTS requires accurate learned models and a large number of simulations, thus costing huge computing time. This paper investigates a few hypotheses where MuZero Unplugged may not work well under the offline RL settings, including 1) learning with limited data coverage; 2) learning from offline data of stochastic environments; 3) improperly parameterized models given the offline data; 4) with a low compute budget. We propose to use a regularized one-step look-ahead approach to tackle the above issues. Instead of planning with the expensive MCTS, we use the learned model to construct an advantage estimation based on a one-step rollout. Policy improvements are towards the direction that maximizes the estimated advantage with regularization of the dataset. We conduct extensive empirical studies with BSuite environments to verify the hypotheses and then run our algorithm on the RL Unplugged Atari benchmark. Experimental results show that our proposed approach achieves stable performance even with an inaccurate learned model. On the large-scale Atari benchmark, the proposed method outperforms MuZero Unplugged by 43%. Most significantly, it uses only 5.6% wall-clock time (i.e., 1 hour) compared to MuZero Unplugged (i.e., 17.8 hours) to achieve a 150% IQM normalized score with the same hardware and software stacks. Our implementation is open-sourced at https://github.com/sail-sg/rosmo.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 5. Persistent RNNs: Stashing Recurrent Weights On-Chip\n\n*From Search Query: hardware efficient hierarchical normalization implementation*\n\n*G. Diamos, Shubho Sengupta, Bryan Catanzaro, Mike Chrzanowski, Adam Coates, Erich Elsen, Jesse Engel, Awni Y. Hannun, S. Satheesh*\n\n**TL;DR:** This paper introduces a new technique for mapping Deep Recurrent Neural Networks efficiently onto GPUs that uses persistent computational kernels that exploit the GPU's inverted memory hierarchy to reuse network weights over multiple timesteps.\n\n**Abstract:** This paper introduces a new technique for mapping Deep Recurrent Neural Networks (RNN) efficiently onto GPUs. We show how it is possible to achieve substantially higher computational throughput at low mini-batch sizes than direct implementations of RNNs based on matrix multiplications. The key to our approach is the use of persistent computational kernels that exploit the GPU's inverted memory hierarchy to reuse network weights over multiple timesteps. Our initial implementation sustains 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU. This provides a 16\u00d7 reduction in activation memory footprint, enables model training with 12\u00d7 more parameters on the same hardware, allows us to strongly scale RNN training to 128 GPUs, and allows us to efficiently explore end-to-end speech recognition models with over 100 layers.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2016\n\n**Citations:** 92  (*Influential: 9*)\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design with a focus on hardware-efficient implementations, memory optimization, and normalization techniques, here are some key points and references that align with their goals:\n\n## Hardware-Efficient Implementations of Hierarchical Normalization\n\n- **Hardware-Aware Neural Network Design**: The co-design of neural network architectures and hardware is crucial for efficiency. Hardware-aware neural architecture search (HA-NAS) methods consider hardware constraints such as latency, memory usage, and energy consumption. This approach can be applied to optimize hierarchical normalization in neural networks for specific hardware platforms like GPUs, FPGAs, or custom silicon-based accelerators.\n\n- **Memory Optimization and Efficient Processing**: Techniques like model parallelism, mixed-precision training, and memory-efficient optimizers (e.g., DeepSpeed\u2019s ZeRO) can be adapted to optimize the memory footprint and computational efficiency of hierarchical normalization. ZeRO and its variants (ZeRO-Offload, ZeRO-Infinity, ZeRO++) help in reducing redundant data copies and offloading data to CPU or NVMe memory, which can be beneficial for large models requiring hierarchical normalization.\n\n## Memory Optimization Techniques for Processing Long Sequences\n\n- **Model Parallelism and Mixed-Precision Training**: These techniques are essential for reducing memory consumption during the training of large language models. By splitting the model across multiple devices and using lower-precision data types, the memory requirements can be significantly reduced, which is crucial for processing long sequences.\n\n- **Efficient Optimizers and Pruning**: Optimizers like DeepSpeed\u2019s ZeRO and pruning techniques such as SynFlow, SPLoRA, and QLoRA can help in reducing the memory footprint and improving computational efficiency. These methods can be particularly useful for long sequence processing by minimizing the memory usage and maintaining performance.\n\n## Practical Implementations of SSM-Based Normalization\n\n- **Convolutional State Space Models (ConvSSMs)**: These models demonstrate how parallel scans can be applied to convolutional recurrences, achieving subquadratic parallelization and fast autoregressive generation. This can be a basis for implementing SSM-based normalization in a hardware-efficient manner[Analysis Note].\n\n- **DenseMamba**: This model integrates shallow-layer hidden states into deeper layers, enhancing the training parallelizability and inference efficiency. Such dense connections can be optimized for hardware efficiency, particularly in the context of SSM-based normalization[Analysis Note].\n\n## Empirical Studies of Normalization Stability in Deep Networks\n\n- **Stability and Convergence**: Studies on residual connections and layer normalization in transformer models highlight their importance in ensuring stable distributions and faster convergence during training. These techniques can be empirically validated for their stability in deep networks, especially when integrated with SSM-based normalization.\n\n## Cache-Efficient Algorithms for Hierarchical Processing\n\n- **Bottleneck Reduction**: Techniques that reduce data movement and inter-chip communication overhead, such as those described in the context of bottleneck designs, can be applied to make hierarchical processing more cache-efficient. By using learnable embeddings and reducing data transfer volume, the algorithms can be optimized for better performance on resource-constrained environments.\n\nIn summary, the researchers can benefit from:\n- **Hardware-aware design and optimization** to ensure that hierarchical normalization is implemented efficiently on specific hardware platforms.\n- **Memory optimization techniques** such as model parallelism, mixed-precision training, and efficient optimizers to handle long sequences.\n- **Empirical studies** on the stability of SSM-based normalization integrated with techniques like residual connections and layer normalization.\n- **Cache-efficient algorithms** that minimize data movement and optimize inter-chip communication, ensuring efficient hierarchical processing.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results provide valuable insights into state space models, hierarchical processing, and normalization techniques. However, more specific information is needed about:\n- Hardware-efficient implementations of hierarchical normalization\n- Empirical validation of SSM-based normalization\n- Memory optimization techniques for long sequences\n\n2. Useful References with Excerpts:\n\nFrom ConvSSM paper:\n\"We demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs...\"\n\nFrom DenseMamba:\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom Linear Transformers paper:\n\"Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities.\"\n\n3. Key Insights:\n\na) SSM Integration:\n- Linear complexity through selective mechanisms\n- Efficient parallel scan operations\n- Hardware-aware implementations needed\n\nb) Hierarchical Processing:\n- Multi-scale feature representation\n- Dense connections for information flow\n- Memory-efficient state management\n\nc) Implementation Considerations:\n- Hardware optimization crucial\n- Memory access patterns important\n- Balance between efficiency and stability\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\na) Hardware-efficient implementations of hierarchical normalization\nb) Memory optimization techniques for long sequences\nc) Empirical studies of SSM-based normalization in practice\n\n5. References:\n\n[1] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S.W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[2] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Aksenov, Y., Balagansky, N., Vaina, S.M.L., Shaposhnikov, B., Gorbatovski, A., & Gavrilov, D. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv preprint.\n\n[4] Gu, A., Johnson, I., Timalsina, A., Rudra, A., & R\u00e9, C. (2022). How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. ICLR."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "state space model hierarchical normalization",
            "detail": "Search for papers discussing:\n1. Integration of state space models with normalization techniques\n2. Parallel scan operations in hierarchical neural networks\n3. Multi-scale feature processing with selective state mechanisms\n4. Memory-efficient implementations of hierarchical normalization\n5. Hardware optimization techniques for parallel scan operations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Integration of state space models with normalization techniques\n2. Parallel scan operations in hierarchical neural networks\n3. Multi-scale feature processing with selective state mechanisms\n4. Memory-efficient implementations of hierarchical normalization\n5. Hardware optimization techniques for parallel scan operations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.94)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 29/44 (Score: 0.94)*\n\n```\nGPU gems, 3(39):851-876, 2007. [106] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. [107] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [108] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: A new metric \\& challenges. arXiv preprint arXiv:1812.01717, 2018. [109] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4): $600-612,2004$. [110] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018. [111] Yann LeCun. The MNIST database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. [112] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-Matterport 3D dataset (HM3D): 1000 large-scale 3D environments for embodied AI. arXiv preprint arXiv:2109.08238, 2021. [113] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. [114] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9068-9079, 2018. ## Appendix for: Convolutional State Space Models for Long-range Spatiotemporal Modeling\n\nContents:\n\n- Appendix A: Propositions\n- Appendix B: ConvS5 Details: Parameterization, Initialization, Discretization\n- Appendix C: Supplementary Results\n- Appendix D: Experiment Configurations\n- Appendix E: Datasets\n\n\n## A Propositions\n\n## A. 1 Parallel Scan for Convolutional Recurrences\n\nProposition 1. Consider a convolutional recurrence as in (7) and define initial parallel scan elements $c_{k}=\\left(c_{k, a}, c_{k, b}\\right):=\\left(\\overline{\\mathcal{A}}, \\overline{\\mathcal{B}} * \\mathcal{U}_{k}\\right)$. The binary operator $\\circledast$, defined below, is associative. $$\nc_{i} \\circledast c_{j}:=\\left(c_{j, a} \\circ c_{i, a}, c_{j, a} * c_{i, b}+c_{j, b}\\right)\n$$\n\nwhere $\\circ$ denotes convolution of two kernels, $*$ denotes convolution between a kernel and input, and + is elementwise addition. Proof. Using that $\\circ$ is associative and the companion operator of $*$, i.e. $(d \\circ e) * f=d *(e * f)$ (see Blelloch [63], Section 1.4), we have:\n\n$$\n\\begin{aligned}\n\\left(c_{i} \\circledast c_{j}\\right) \\circledast c_{k} & =\\left(c_{j, a} \\circ c_{i, a}, c_{j, a} * c_{i, b}+c_{j, b}\\right) \\circledast\\left(c_{k, a}, c_{k, b}\\right) \\\\\n& =\\left(c_{k, a} \\circ\\left(c_{j, a} \\circ c_{i, a}\\right), \\quad c_{k, a} *\\left(c_{j, a} * c_{i, b}+c_{j, b}\\right)+c_{k, b}\\right) \\\\\n& =\\left(\\left(c_{k, a} \\circ c_{j, a}\\right) \\circ c_{i, a}, \\quad c_{k, a} *\\left(c_{j, a} * c_{i, b}\\right)+c_{k, a} * c_{j, b}+c_{k, b}\\right) \\\\\n& =\\left(\\left(c_{k, a} \\circ c_{j, a}\\right) \\circ c_{i, a},\\left(c_{k, a} \\circ c_{j, a}\\right) * c_{i, b}+c_{k, a} * c_{j, b}+c_{k, b}\\right) \\\\\n& =c_{i} \\circledast\\left(c_{k, a} \\circ c_{j, a}, c_{k, a} * c_{j, b}+c_{k, b}\\right) \\\\\n& =c_{i} \\circledast\\left(c_{j} \\circledast c_{k}\\right)\n\\end{aligned}\n$$\n\n## A. 2 Computational Cost of Parallel Scan for Convolutional Recurrences\n\nProposition 2. Given the effective inputs $\\overline{\\mathcal{B}} * \\mathcal{U}_{1: L} \\in \\mathbb{R}^{L \\times H \\times W \\times P}$ and a pointwise state kernel $\\mathcal{A} \\in \\mathbb{R}^{P \\times P \\times 1 \\times 1}$, the computational cost of computing the convolutional recurrence in Equation 7 with a parallel scan is $\\mathcal{O}\\left(L\\left(P^{3}+P^{2} H W\\right)\\right)$. Proof. Following Blelloch [63], given a single processor, the cost of computing the recurrence sequentially using the binary operator $\\circledast$ defined in Proposition 1 is $\\mathcal{O}\\left(L\\left(T_{\\circ}+T_{*}+T_{+}\\right)\\right)$where $T_{\\circ}$ refers to the cost of convolving two kernels, $T_{*}$ is the cost of convolution between a kernel and input and $T_{+}$is the cost of elementwise addition. The cost of elementwise addition is $T_{+}=\\mathcal{O}(P H W)$. For state kernels with resolution $k_{A}, T_{\\circ}=\\mathcal{O}\\left(P^{3} k_{A}^{4}\\right)$ and $T_{*}=\\mathcal{O}\\left(P^{2} k_{A}^{2} H W\\right)$. For pointwise convolutions this becomes $T_{\\circ}=\\mathcal{O}\\left(P^{3}\\right)$ and $T_{*}=\\mathcal{O}\\left(P^{2} H W\\right)$. Thus, the cost of computing the recurrence sequentially using $\\circledast$ is $\\mathcal{O}\\left(L\\left(P^{3}+P^{2} H W\\right)\\right)$. Since there are work-efficient algorithms for parallel scans [105], the overall cost of the parallel scan is also $\\mathcal{O}\\left(L\\left(P^{3}+P^{2} H W\\right)\\right)$. Note that ConvS5's diagonalized parameterization discussed in Section 3.4 and Appendix B leads to $T_{\\circ}=\\mathcal{O}(P)$ and $T_{*}=\\mathcal{O}(P H W)$. Therefore the cost of applying the parallel scan with ConvS5 is $\\mathcal{O}(L P H W)$.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.52)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.52)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.46)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.46)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 4. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.24)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 14/53 (Score: 0.24)*\n\n```\nAdvances in Neural Information Processing Systems, 33: $1474-1487,2020 \\mathrm{a}$. Albert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800-3809. PMLR, 2020b. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021b. Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers, 2020. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): $1735-1780,1997$. Arieh Iserles. A first course in the numerical analysis of differential equations. 44. Cambridge university press, 2009. Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pp. 87-104, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Shiva Kaul. Linear dynamical systems as a core computational primitive. Advances in Neural Information Processing Systems, 33:16808-16820, 2020. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Alex Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009. Richard Ladner and Michael Fischer. Parallel prefix computation. Journal of the ACM (JACM), 27 (4):831-838, 1980. Sivaramakrishnan Lakshmivarahan and Sudarshan Dhall. Parallel computing using the prefix problem.\n```\n\n#### 5. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.10)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.10)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n\n\n---\n## Found 5 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space model hierarchical normalization\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: state space model hierarchical normalization*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Relational State-Space Model for Stochastic Multi-Object Systems\n\n*From Search Query: state space model hierarchical normalization*\n\n*Fan Yang, Ling Chen, Fan Zhou, Yusong Gao, Wei Cao*\n\n**TL;DR:** The relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects, is introduced.\n\n**Abstract:** Real-world dynamical systems often consist of multiple stochastic subsystems that interact with each other. Modeling and forecasting the behavior of such dynamics are generally not easy, due to the inherent hardness in understanding the complicated interactions and evolutions of their constituents. This paper introduces the relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects. By letting GNNs cooperate with SSM, R-SSM provides a flexible way to incorporate relational information into the modeling of multi-object dynamics. We further suggest augmenting the model with normalizing flows instantiated for vertex-indexed random variables and propose two auxiliary contrastive objectives to facilitate the learning. The utility of R-SSM is empirically evaluated on synthetic and real time series datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 3. Hierarchical Normalization for Robust Monocular Depth Estimation\n\n*From Search Query: state space model hierarchical normalization*\n\n*Chi Zhang, Wei Yin, Zhibin Wang, Gang Yu, Bin Fu, Chunhua Shen*\n\n**TL;DR:** This paper proposes a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions and presents two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain.\n\n**Abstract:** In this paper, we address monocular depth estimation with deep neural networks. To enable training of deep monocular estimation models with various sources of datasets, state-of-the-art methods adopt image-level normalization strategies to generate affine-invariant depth representations. However, learning with image-level normalization mainly emphasizes the relations of pixel representations with the global statistic in the images, such as the structure of the scene, while the fine-grained depth difference may be overlooked. In this paper, we propose a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions. Compared with previous normalization strategies applied only at the holistic image level, the proposed hierarchical normalization can effectively preserve the fine-grained details and improve accuracy. We present two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain, respectively. Our extensive experiments show that the proposed normalization strategy remarkably outperforms previous normalization methods, and we set new state-of-the-art on five zero-shot transfer benchmark datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 4. Continuous-time identification of dynamic state-space models by deep subspace encoding\n\n*From Search Query: state space model hierarchical normalization*\n\n*G. Beintema, M. Schoukens, R. T'oth*\n\n**TL;DR:** It is proved that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and it is shown that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.\n\n**Abstract:** Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n### 1 related papers from Papers with Code\n\n#### 1. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: state space model hierarchical normalization*\n\n*Ralf Herbrich, Rainer Schlosser, Paul Mattes*\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Published:** 2023-10-08\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design by integrating state space models with hierarchical normalization, here are some key findings and insights from the provided sources:\n\n## Integration of State Space Models with Normalization Techniques\n\n- The Mamba model, a state-space sequence model, has been explored as an alternative to transformers. It inherently captures long-range dependencies and is computationally efficient. Mamba can be formulated as either a recurrence or convolution, which might be integrated with normalization techniques to enhance stability and feature representation.\n- While the sources do not explicitly discuss the integration of state space models with normalization, the Mamba architecture's ability to model long-range dependencies and its computational efficiency suggest it could be a viable candidate for such integration.\n\n## Parallel Scan Operations in Hierarchical Neural Networks\n\n- The Mamba model uses a multi-scale feature fusion (MFF) module to combine multi-scale visual features from intermediate layers, enabling hierarchical alignment at the feature level. This approach could be adapted for parallel scan operations in hierarchical neural networks, particularly in the context of language models.\n- The concept of State Space Duality in Mamba2, which simplifies matrix transformations, could be leveraged to design efficient parallel scan operations across different hierarchical levels.\n\n## Multi-scale Feature Processing with Selective State Mechanisms\n\n- Mamba introduces a selective state space mechanism that is input-dependent, which can be effective for multi-scale feature processing. This mechanism allows for the capture of long-range dependencies and is designed to be computationally efficient, making it suitable for hierarchical neural networks.\n- The Visual State Space model, which applies Mamba to the Vision Transformer architecture, uses a bi-directional SSM to better process visual features. This could be a model for how to apply similar techniques to language models, focusing on multi-scale feature processing.\n\n## Memory-Efficient Implementations of Hierarchical Normalization\n\n- Mamba models are designed to be computationally efficient and have near-linear scaling in sequence length, which implies they could be memory-efficient when integrated with hierarchical normalization techniques. The hardware-aware framework of Mamba also suggests it could be optimized for memory access patterns and cache utilization.\n\n## Hardware Optimization Techniques for Parallel Scan Operations\n\n- The EMMA model, which builds upon Mamba, shows lower latency and is nearly four times faster than transformer-based models of similar scale during inference. This is due to better cross-modal alignment and efficient processing, which could be adapted for optimizing parallel scan operations in hierarchical neural networks.\n- The Mamba architecture's formulation as either a recurrence or convolution allows for efficient hardware implementation, which is crucial for optimizing parallel scan operations and ensuring real-world performance implications are favorable.\n\n### Key Takeaways\n\n- **SSM Integration**: Mamba's state-space sequence model can be integrated with hierarchical normalization to enhance stability and feature representation, leveraging its long-range dependency modeling and computational efficiency.\n- **Parallel Scan Operations**: The multi-scale feature fusion module and State Space Duality in Mamba can be adapted for efficient parallel scan operations in hierarchical neural networks.\n- **Multi-scale Processing**: Mamba's selective state space mechanisms can effectively process multi-scale features, which is crucial for hierarchical neural networks.\n- **Memory Efficiency**: Mamba's design ensures near-linear scaling and computational efficiency, making it a promising candidate for memory-efficient implementations of hierarchical normalization.\n- **Hardware Optimization**: The hardware-aware framework of Mamba and its efficient processing capabilities suggest it can be optimized for parallel scan operations, improving real-world performance.\n\nBy leveraging these insights, the researchers can design an autoregressive language model that integrates state space models with hierarchical normalization, ensuring computational efficiency, memory efficiency, and optimal hardware performance.\n",
            "analysis": "Initial Analysis of HybridSSMHierRMSNorm Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) SSM Integration with Hierarchical Normalization:\n- How does the integration of selective state space mechanisms affect normalization stability?\n- What are the trade-offs between computational efficiency and feature representation?\n- How does parallel scan operation impact the hierarchical structure?\n\nb) Multi-scale Processing:\n- Effectiveness of parallel scan operations across different scales\n- Impact on feature representation at different hierarchical levels\n- Memory efficiency of multi-scale state management\n\nc) Hardware Efficiency:\n- Implementation feasibility of parallel scan operations\n- Memory access patterns and cache utilization\n- Real-world performance implications\n\n2. Investigation Plan:\n\nPhase 1: SSM and Hierarchical Processing\n- Research existing SSM implementations in normalization contexts\n- Analyze parallel scan operations in hierarchical structures\n- Study state management techniques in multi-scale processing\n\nPhase 2: Efficiency and Scalability\n- Investigate memory efficiency techniques in SSMs\n- Research parallel processing strategies in normalization\n- Study hardware optimization approaches\n\nPhase 3: Comparative Analysis\n- Compare with existing hierarchical normalization methods\n- Analyze similarities with state-space models\n- Evaluate novelty against prior work\n\nInitial assessment suggests this proposal combines concepts from state space models with hierarchical normalization in a potentially novel way, but requires careful investigation of implementation feasibility and performance implications."
        },
        {
            "ready": false,
            "query": "hierarchical normalization stability analysis",
            "detail": "Search for papers discussing:\n1. Stability analysis of hierarchical normalization techniques\n2. Convergence properties of multi-scale normalization\n3. Theoretical foundations of combining SSMs with normalization\n4. Impact of normalization on state space model performance\n5. Gradient flow in hierarchical neural architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Stability analysis of hierarchical normalization techniques\n2. Convergence properties of multi-scale normalization\n3. Theoretical foundations of combining SSMs with normalization\n4. Impact of normalization on state space model performance\n5. Gradient flow in hierarchical neural architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.07)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 50/56 (Score: 0.07)*\n\n```\narXiv preprint arXiv:1609.08144, 2016. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.\n```\n\n#### 2. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.06)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.06)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.05)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 12/30 (Score: 0.05)*\n\n```\nZenodo, Sept. 2021. [17] Felix A. Gers, J\u00fcrgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):2451-2471, 2000. [18] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pages 571-575, 2021. [19] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00edk, Bas R. Steunebrink, and J\u00fcrgen Schmidhuber. Lstm: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28:2222-2232, 2015. [20] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In NeurIPS, 2022. [21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [23] Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3800-3809. PMLR, 2020. [24] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 572-585, 2021. [25] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state-space layers, 2021. [26] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [27] Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns. CoRR, abs/2212.00768, 2022. [28] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. [29] Hongyu He and Marko Kabic. A unified view of long-sequence models towards modeling million-scale dependencies. CoRR, abs/2302.06218, 2023. [30] Sepp Hochreiter and Yoshua Bengio. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n```\n\n#### 4. From generalization analysis to optimization designs for state space models (Avg. Score: 0.04)\n\n*Fusheng Liu, Qianxiao Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.04)*\n\n```\nIn this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results. ## 1 Introduction\n\nSequence modeling has been a long-standing research topic in many machine learning areas, such as speech recognition (Hinton et al., 2012), time series prediction (Li et al., 2019), and natural language processing (Devlin et al., 2019). Various machine learning models have been successfully applied in sequence modeling to handle different types of sequence data, ranging from the (probabilistic) Hidden Markov model (Baum and Petrie, 1966) to deep learning models, e.g., Recurrent Neural Networks (RNNs), Long ShortTerm Memory units (Hochreiter and Schmidhuber, 1997), Gated Recurrent Unit (Chung et al., 2014), and transformers (Vaswani et al., 2017). In this paper, we focus on the state space model (SSM), which has a simple mathematical expression: $h^{\\prime}(t)=A h(t)+B x(t), y(t)=C h(t)+D x(t)$ where $h(t)$ is the hidden state, $x(t)$ is the input sequence, $y(t)$ is the output sequence and $A, B, C, D$ are trainable parameters. To simplify the analysis, we omit the skip connection by letting $D=0$. In fact, our analysis can also applied to the case when $D$ is included (see the discussions in Section 4.2). Recent studies have demonstrated the power of SSMs in deep learning. For example, it was shown in Gu et al. (2022a) that by a new parameterization and a carefully chosen initialization, the structured state space sequence (S4) model achieved strong empirical results on image and language tasks. Following the S4 model, more variants of SSMs are proposed, e.g., diagonal SSMs (Gu et al., 2022b; Gupta et al., 2022) S5 (Smith et al., 2023), H3 (Fu et al., 2023), GSS (Mehta et al., 2023), Hyena Hierarchy (Poli et al., 2023), and Mamba (Gu and Dao, 2023). Theoretical analysis and understanding of the approximation and optimization of SSMs are well studied in the literature such as (Li et al., 2021; 2022; Gu et al., 2022a; 2023). Since the SSM can be regarded as a continuous linear RNN model (Li et al., 2022), most generalization analysis of SSMs is based on the generalization theory of RNNs (Zhang et al., 2018; Chen et al., 2019; Tu et al., 2019). However, these previous works did not study the effects of the temporal dependencies in the sequence data on the SSM generalization\n(see more details on the comparison in Section 4.1). As an attempt to understand the relationship between the temporal dependencies and the generalization performance, this paper aims to provide a generalization bound that connects the memory structure of the model with the temporal structure of the data. We can, in turn, use the proposed bound to guide us in designing new algorithms to improve optimization and generalization. Specifically, we discover two roles for the proposed generalization measure: (1) generalization bound as an initialization scheme; (2) generalization bound as a regularization method. The common initialization method for the S4 model and its variants follows from the HiPPO framework (Gu et al., 2022a; 2023), which is based on the prerequisite that the training sequence data is stable. To improve the robustness of the output value scales on SSMs to different temporal patterns in the sequence data, we consider to rescale the initialization of SSMs with respect to the generalization measure. This new initialization scheme makes the SSMs more resilient on their initial output value scales to variations in the temporal patterns of the training data. Except for the initialization setup, our generalization bound can also be served as a regularizer. Regularization methods like weight decay and dropout are widely applied to training SSMs, but the hidden state matrix $A$ is not regularized because its imaginary part controls the oscillating frequencies of the basis function $e^{A t} B$ (Gu et al., 2022b). By taking into account the interaction between the SSM structure and the temporal dependencies, we introduce a new regularization method based on our bound, and it can be applied to the hidden state space to improve the generalization performance. Combining the initialization scheme and the regularization method, our method is applicable to various tasks, ranging from image classification to language processing, while only introducing a minimal computational overhead. To summarize, our contributions are as follows:\n\n- We provide a data-dependent generalization bound for SSMs by taking into account the temporal structure. Specifically, the generalization bound correlates with the memory structure of the model and the (auto)covariance process of the data. It indicates that instead of the weight or the data norm, it is the interplay between the memory structure and the temporal structure of the sequence data that influences the generalization. - Based on the proposed generalization bound, we setup an initialization scaling rule by adjusting the magnitude of the model parameters with respect to the generalization measure at initialization. This scaling rule improves the robustness of the initial output value scales on SSMs across different temporal patterns of the sequence data. - Apart from the initialization scheme, we design a new regularizer for SSMs. Unlike weight decay, our regularizer does not penalize the parameter norm but encourages the model to find a minimizer with lower generalization bound to improve the generalization performance. ## 2 Related Works\n\nSince a SSM is also a continuous linear RNN, there are three lines of related work: generalization of RNNs, temporal structure analysis on RNNs, and optimization of SSMs. Generalization of RNNs. Existing works on the generalization of RNNs focus on the generalization error bound analysis. Specifically, in the early two works of Dasgupta and Sontag (1995) and Koiran and Sontag (1998), VC dimension-based generalization bounds were provided to show the learnability of RNNs.\n```\n\n#### 5. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.02)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.02)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical normalization stability analysis\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Why Spectral Normalization Stabilizes GANs: Analysis and Improvements\n\n*From Search Query: hierarchical normalization stability analysis*\n\n*Zinan Lin, Vyas Sekar, G. Fanti*\n\n**TL;DR:** This work shows that SN controls two important failure modes of GAN training: exploding and vanishing gradients, and proposes Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization.\n\n**Abstract:** Spectral normalization (SN) is a widely-used technique for improving the stability of Generative Adversarial Networks (GANs) by forcing each layer of the discriminator to have unit spectral norm. This approach controls the Lipschitz constant of the discriminator, and is empirically known to improve sample quality in many GAN architectures. However, there is currently little understanding of why SN is so effective. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization technique, proposed over two decades ago to control gradients in the training of deep neural networks. This connection helps to explain why the most popular implementation of SN for GANs requires no hyperparameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, we show that SN tends to preserve this property throughout training. Finally, building on this theoretical understanding, we propose Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization. Theoretically, we show that BSN gives better gradient control than SN. Empirically, we demonstrate that BSN outperforms SN in sample quality on several benchmark datasets, while also exhibiting better training stability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 35  (*Influential: 3*)\n\n#### 2. Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction\n\n*From Search Query: hierarchical normalization stability analysis*\n\n*Kaifeng Lyu, Zhiyuan Li, Sanjeev Arora*\n\n**TL;DR:** For a fairly broad class of neural nets with normalization, the theory explains how GD with a finite learning rate enters the so-called Edge of Stability (EoS) regime, and characterizes the trajectory of GD in this regime via a continuous sharpness-reduction flow.\n\n**Abstract:** Normalization layers (e.g., Batch Normalization, Layer Normalization) were introduced to help with optimization difficulties in very deep nets, but they clearly also help generalization, even in not-so-deep nets. Motivated by the long-held belief that flatter minima lead to better generalization, this paper gives mathematical analysis and supporting experiments suggesting that normalization (together with accompanying weight-decay) encourages GD to reduce the sharpness of loss surface. Here\"sharpness\"is carefully defined given that the loss is scale-invariant, a known consequence of normalization. Specifically, for a fairly broad class of neural nets with normalization, our theory explains how GD with a finite learning rate enters the so-called Edge of Stability (EoS) regime, and characterizes the trajectory of GD in this regime via a continuous sharpness-reduction flow.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 60  (*Influential: 2*)\n\n#### 3. Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation\n\n*From Search Query: hierarchical normalization stability analysis*\n\n*Yilin Lyu, Liyuan Wang, Xingxing Zhang, Zicheng Sun, Hang Su, Jun Zhu, Liping Jing*\n\n**TL;DR:** This work focuses on the most popular Batch Normalization and provides an in-depth theoretical analysis of its sub-optimality in continual learning, and proposes Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages.\n\n**Abstract:** Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 4. A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent\n\n*From Search Query: hierarchical normalization stability analysis*\n\n*Yongqiang Cai, Qianxiao Li, Zuowei Shen*\n\n**TL;DR:** It is shown that unlike GD, gradient descent with BN (BNGD) converges for arbitrary learning rates for the weights, and the convergence remains linear under mild conditions.\n\n**Abstract:** Despite its empirical success and recent theoretical progress, there generally lacks a quantitative analysis of the effect of batch normalization (BN) on the convergence and stability of gradient descent. In this paper, we provide such an analysis on the simple problem of ordinary least squares (OLS). Since precise dynamical properties of gradient descent (GD) is completely known for the OLS problem, it allows us to isolate and compare the additional effects of BN. More precisely, we show that unlike GD, gradient descent with BN (BNGD) converges for arbitrary learning rates for the weights, and the convergence remains linear under mild conditions. Moreover, we quantify two different sources of acceleration of BNGD over GD -- one due to over-parameterization which improves the effective condition number and another due having a large range of learning rates giving rise to fast descent. These phenomena set BNGD apart from GD and could account for much of its robustness properties. These findings are confirmed quantitatively by numerical experiments, which further show that many of the uncovered properties of BNGD in OLS are also observed qualitatively in more complex supervised learning problems.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 36  (*Influential: 3*)\n\n#### 5. CluHTM - Semantic Hierarchical Topic Modeling based on CluWords\n\n*From Search Query: hierarchical normalization stability analysis*\n\n*Felipe Viegas, Washington Cunha, Christian Gomes, Ant\u00f4nio Pereira, L. Rocha, Marcos Andr\u00e9 Gon\u00e7alves*\n\n**TL;DR:** CluHTM is designed and evaluated, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM, and its novel contributions include the exploration of richer text representation that encapsulates both, global and local semantic information.\n\n**Abstract:** Hierarchical Topic modeling (HTM) exploits latent topics and relationships among them as a powerful tool for data analysis and exploration. Despite advantages over traditional topic modeling, HTM poses its own challenges, such as (1) topic incoherence, (2) unreasonable (hierarchical) structure, and (3) issues related to the definition of the \u201cideal\u201d number of topics and depth of the hierarchy. In this paper, we advance the state-of-the-art on HTM by means of the design and evaluation of CluHTM, a novel non-probabilistic hierarchical matrix factorization aimed at solving the specific issues of HTM. CluHTM\u2019s novel contributions include: (i) the exploration of richer text representation that encapsulates both, global (dataset level) and local semantic information \u2013 when combined, these pieces of information help to solve the topic incoherence problem as well as issues related to the unreasonable structure; (ii) the exploitation of a stability analysis metric for defining the number of topics and the \u201cshape\u201d the hierarchical structure. In our evaluation, considering twelve datasets and seven state-of-the-art baselines, CluHTM outperformed the baselines in the vast majority of the cases, with gains of around 500% over the strongest state-of-the-art baselines. We also provide qualitative and quantitative statistical analyses of why our solution works so well.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 33  (*Influential: 7*)\n\n### 1 related papers from Papers with Code\n\n#### 1. Contextual Learning in Fourier Complex Field for VHR Remote Sensing Images\n\n*From Search Query: hierarchical normalization stability analysis*\n\n*Xinbo Gao, Xiao Pu, Jiaxu Leng, Qingyan Duan, Xiyuan Gao, Yan Zhang*\n\n**Abstract:** Very high-resolution (VHR) remote sensing (RS) image classification is the fundamental task for RS image analysis and understanding. Recently, transformer-based models demonstrated outstanding potential for learning high-order contextual relationships from natural images with general resolution (224x224 pixels) and achieved remarkable results on general image classification tasks. However, the complexity of the naive transformer grows quadratically with the increase in image size, which prevents transformer-based models from VHR RS image (500x500 pixels) classification and other computationally expensive downstream tasks. To this end, we propose to decompose the expensive self-attention (SA) into real and imaginary parts via discrete Fourier transform (DFT) and therefore propose an efficient complex self-attention (CSA) mechanism. Benefiting from the conjugated symmetric property of DFT, CSA is capable to model the high-order contextual information with less than half computations of naive SA. To overcome the gradient explosion in Fourier complex field, we replace the Softmax function with the carefully designed Logmax function to normalize the attention map of CSA and stabilize the gradient propagation. By stacking various layers of CSA blocks, we propose the Fourier Complex Transformer (FCT) model to learn global contextual information from VHR aerial images following the hierarchical manners. Universal experiments conducted on commonly used RS classification data sets demonstrate the effectiveness and efficiency of FCT, especially on very high-resolution RS images.\n\n**Published:** 2022-10-28\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using hierarchical normalization techniques, here are some key points and references that align with your requirements:\n\n## Stability Analysis of Hierarchical Normalization Techniques\n\n- The use of hierarchical normalization can significantly impact the stability of training in deep neural networks. For instance, employing layer normalization across hierarchical levels can help maintain training stability. This is highlighted in the context of hierarchical memory-integrated networks, where appropriate normalization techniques are crucial for stable training.\n\n## Convergence Properties of Multi-Scale Normalization\n\n- The paper on \"Hierarchical Normalization for Robust Monocular Depth Estimation\" provides insights into multi-scale depth normalization methods. While this work is focused on computer vision, the principles of hierarchical normalization can be adapted to other domains, including language models. The method proposed involves normalizing depth representations based on spatial information and depth distributions, which could be analogous to normalizing feature representations at different scales in language models.\n\n## Theoretical Foundations of Combining SSMs with Normalization\n\n- Combining State Space Models (SSMs) with normalization techniques can be theoretically grounded in the context of maintaining stability and efficiency. For example, the ConvSSM paper discusses how convolutional recurrences can be integrated with SSMs to achieve efficient parallelization and fast autoregressive generation. This integration can be further enhanced by incorporating hierarchical normalization to stabilize the dynamics across different scales [Analysis Note].\n\n## Impact of Normalization on State Space Model Performance\n\n- Normalization techniques can significantly impact the performance of SSMs by ensuring that the feature representations are scaled appropriately across different levels. This is crucial for maintaining the stability and efficiency of the model. For instance, using task-specific batch normalization can manage parameter growth and ensure stability across tasks, which is relevant when integrating SSMs with hierarchical normalization.\n\n## Gradient Flow in Hierarchical Neural Architectures\n\n- The gradient flow in hierarchical neural architectures is critical for understanding how normalization affects the training process. Hierarchical normalization helps in stabilizing the gradient flow by ensuring that the gradients are properly scaled at each level of the hierarchy. This is essential for preventing vanishing or exploding gradients, which can hinder the convergence of the model.\n\n### Additional Insights\n\n- **Memory-Efficient Implementations**: Papers like \"FlashAttention\" provide insights into memory-efficient attention mechanisms, which can be integrated with hierarchical normalization to optimize memory usage in autoregressive models [Analysis Note].\n- **Hardware Optimization**: Efficient parallel scan operations and hardware-aware implementations are crucial for scaling up autoregressive models. Integrating hierarchical normalization with these strategies can enhance both the stability and efficiency of the model [Analysis Note].\n\nBy focusing on these aspects, researchers can develop more robust, efficient, and scalable autoregressive language models that leverage hierarchical normalization techniques effectively.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe initial search results provide valuable insights into state space models (SSMs) and their potential integration with hierarchical normalization. Key findings include:\n- Parallel scan operations for efficient sequence processing\n- Multi-scale feature processing techniques\n- Memory-efficient implementations\n- Hardware optimization strategies\n\nHowever, more specific information is needed about:\n- Hierarchical normalization techniques\n- Integration methods between SSMs and normalization\n- Stability analysis of combined approaches\n\n2. Useful References with Excerpts:\n\nFrom ConvSSM paper:\n\"First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs...\"\n\nFrom Hierarchical Normalization paper:\n\"We propose a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions.\"\n\n3. Key Insights:\n\na) SSM Architecture:\n- Linear complexity through selective mechanisms\n- Efficient parallel scan operations\n- Hardware-aware implementations\n\nb) Hierarchical Processing:\n- Multi-scale feature representation\n- Adaptive normalization strategies\n- Memory-efficient computations\n\nc) Integration Opportunities:\n- Parallel scan operations for hierarchical levels\n- Selective state mechanisms for scale-specific processing\n- Hardware-efficient implementation strategies\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\na) Query 1: Recent advances in hierarchical normalization techniques and stability analysis\nb) Query 2: Memory-efficient implementations of multi-scale processing\nc) Query 3: Hardware optimization strategies for parallel neural operations\n\n5. References:\n\n[1] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S.W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[2] Zhang, C., Yin, W., Wang, Z., Yu, G., Fu, B., & Shen, C. (2022). Hierarchical Normalization for Robust Monocular Depth Estimation. Neural Information Processing Systems.\n\n[3] Dao, T., Fu, D.Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[4] Lu, P., Wang, S., Rezagholizadeh, M., Liu, B., & Kobyzev, I. (2023). Efficient Classification of Long Documents via State-Space Models. Conference on Empirical Methods in Natural Language Processing."
        },
        {
            "ready": false,
            "query": "language model hierarchical normalization implementation",
            "detail": "Search for papers discussing:\n1. Practical implementations of hierarchical normalization in language models\n2. Empirical studies combining state space models with normalization\n3. Stability analysis methods specific to autoregressive architectures\n4. Memory-efficient normalization techniques for long sequences\n5. Hardware-optimized implementations of hierarchical processing",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Practical implementations of hierarchical normalization in language models\n2. Empirical studies combining state space models with normalization\n3. Stability analysis methods specific to autoregressive architectures\n4. Memory-efficient normalization techniques for long sequences\n5. Hardware-optimized implementations of hierarchical processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.94)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.94)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 2. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.29)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.29)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 3. Scalable MatMul-free Language Modeling (Avg. Score: 0.09)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.09)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.09)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 14/21 (Score: 0.09)*\n\n```\nAdvances in neural information processing systems, 33: 1474-1487, 2020. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Hua, W., Dai, Z., Liu, H., and Le, Q. V. Transformer quality in linear time, 2022. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020 . Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. Lei, T. When attention meets fast recurrence: Training language models with reduced compute. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7633-7648, 2021. Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., O'Horo, B., Wang, J., Zettlemoyer, L., Kozareva, Z., Diab, M. T., Stoyanov, V., and Li, X. Few-shot learning with multilingual language models. $\\operatorname{CoRR}$, abs/2112.10668, 2021. URL https: / arxiv.org/ $\\mathrm{abs} / 2112.10668$. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019. Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B. Long range language modeling via gated state spaces, 2022. Merity, S., Xiong, C., Bradbury, J., and Socher, R.\n```\n\n#### 5. Retentive network: a successor to transformer for large language models (Avg. Score: 0.07)\n\n*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 143  (*Influential: 18*)\n\n**TL;DR:** This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n##### *Relevant Chunk: No. 12/21 (Score: 0.07)*\n\n```\narXiv preprint arXiv:2101.00027, 2020. [GGR21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. $\\left[\\mathrm{HCP}^{+}\\right.$21] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021. $\\left[\\mathrm{HDW}^{+} 23\\right]$ Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023. [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv: Learning, 2016. [HS97] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735-1780, November 1997. [HSD ${ }^{+}$22a] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv, abs/2206.06336, 2022. $\\left[\\mathrm{HSD}^{+}\\right.$22b] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: language model hierarchical normalization implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition\n\n*From Search Query: language model hierarchical normalization implementation*\n\n*Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang*\n\n**TL;DR:** HD-Eval is proposed, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition via Hierarchical Criteria Decomposition and achieves a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity.\n\n**Abstract:** Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on prompting, and its independence from model parameters makes it applicable to closed-source LLMs. Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Parsel\ud83e\udd86: Algorithmic Reasoning with Language Models by Composing Decompositions\n\n*From Search Query: language model hierarchical normalization implementation*\n\n*E. Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, Nick Haber*\n\n**TL;DR:** This work introduces Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, which automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests.\n\n**Abstract:** Despite recent success in large language model (LLM) reasoning, LLMs struggle with hierarchical multi-step reasoning tasks like generating complex programs. For these tasks, humans often start with a high-level algorithmic design and implement each part gradually. We introduce Parsel, a framework enabling automatic implementation and validation of complex algorithms with code LLMs. With Parsel, we automatically decompose algorithmic tasks into hierarchical natural language function descriptions and then search over combinations of possible function implementations using tests. We show that Parsel can be used across domains requiring hierarchical reasoning, including program synthesis and robotic planning. We find that, using Parsel, LLMs solve more competition-level problems in the APPS dataset, resulting in pass rates over 75\\% higher than prior results from directly sampling AlphaCode and Codex, while often using a smaller sample budget. Moreover, with automatically generated tests, we find that Parsel can improve the state-of-the-art pass@1 performance on HumanEval from 67\\% to 85\\%. We also find that LLM-generated robotic plans using Parsel are more than twice as likely to be considered accurate than directly generated plans. Lastly, we explore how Parsel addresses LLM limitations and discuss how Parsel may be useful for human programmers. We release our code at https://github.com/ezelikman/parsel\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 3*)\n\n#### 3. CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets\n\n*From Search Query: language model hierarchical normalization implementation*\n\n*Zachary Novack, S. Garg, Julian McAuley, Zachary Chase Lipton*\n\n**TL;DR:** This work proposes Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies, which leads to improved accuracy in situations both with and without ground-truth hierarchical information.\n\n**Abstract:** Open vocabulary models (e.g. CLIP) have shown strong performance on zero-shot classification through their ability generate embeddings for each class based on their (natural language) names. Prior work has focused on improving the accuracy of these models through prompt engineering or by incorporating a small amount of labeled downstream data (via finetuning). However, there has been little focus on improving the richness of the class names themselves, which can pose issues when class labels are coarsely-defined and are uninformative. We propose Classification with Hierarchical Label Sets (or CHiLS), an alternative strategy for zero-shot classification specifically designed for datasets with implicit semantic hierarchies. CHiLS proceeds in three steps: (i) for each class, produce a set of subclasses, using either existing label hierarchies or by querying GPT-3; (ii) perform the standard zero-shot CLIP procedure as though these subclasses were the labels of interest; (iii) map the predicted subclass back to its parent to produce the final prediction. Across numerous datasets with underlying hierarchical structure, CHiLS leads to improved accuracy in situations both with and without ground-truth hierarchical information. CHiLS is simple to implement within existing zero-shot pipelines and requires no additional training cost. Code is available at: https://github.com/acmi-lab/CHILS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 62  (*Influential: 6*)\n\n#### 4. Label Augmentation for Zero-Shot Hierarchical Text Classification\n\n*From Search Query: language model hierarchical normalization implementation*\n\n*Lorenzo Paletto, Valerio Basile, Roberto Esposito*\n\n**TL;DR:** A novel approach that uses a Large 011 Language Model to augment the deepest layer 012 of the labels hierarchy in order to enhance its specificity and leverage the enriched hierarchy to perform Zero-Shot Hierarchical Classification by using the Upward score Propagation technique.\n\n**Abstract:** Hierarchical Text Classification poses the dif-001 ficult challenge of classifying documents into 002 multiple labels organized in a hierarchy. The 003 vast majority of works aimed to address this 004 problem relies on supervised methods which 005 are difficult to implement due to the scarcity of 006 labeled data in many real world applications. 007 This paper focuses on strict Zero-Shot Classi-008 fication, the setting in which the system lacks 009 both labeled instances and training data. We 010 propose a novel approach that uses a Large 011 Language Model to augment the deepest layer 012 of the labels hierarchy in order to enhance its 013 specificity. We achieve this by generating se-014 mantically relevant labels as children connected 015 to the existing branches, creating a deeper tax-016 onomy that better overlaps with the input texts. 017 We leverage the enriched hierarchy to perform 018 Zero-Shot Hierarchical Classification by using 019 the Upward score Propagation technique. We 020 test our method on four public datasets, obtain-021 ing new state-of-the art results on three of them. 022 We introduce two cosine similarity-based met-023 rics to quantify the density and granularity of 024 a label taxonomy and we show a strong corre-025 lation between the metric values and the clas-026 sification performance of our method on the 027 datasets. 028\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation\n\n*From Search Query: language model hierarchical normalization implementation*\n\n*Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Ben Swift, H. Suominen, Hongdong Li*\n\n**TL;DR:** This paper presents a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation and develops a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet.\n\n**Abstract:** Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of signvideos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT dataset. Our implementation is available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 97  (*Influential: 20*)\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models, particularly focusing on hierarchical normalization, stability analysis, and efficiency, here are some key points and references that align with the specified details:\n\n## Practical Implementations of Hierarchical Normalization in Language Models\n\n- The paper on \"A Comprehensive Overview of Large Language Models\" mentions various architectures that could be adapted for hierarchical normalization. For instance, the GPT-3 architecture, which uses dense and sparse attention in transformer layers, could be modified to incorporate hierarchical normalization techniques. This could involve layer-wise normalization or using different normalization strategies at different layers of the model.\n\n## Empirical Studies Combining State Space Models with Normalization\n\n- While the provided sources do not directly discuss combining state space models (SSMs) with normalization in language models, the concept of integrating SSMs with normalization can be inferred from papers on spatiotemporal modeling. For example, \"Convolutional State Space Models for Long-Range Spatiotemporal Modeling\" discusses how convolutional recurrences can be applied to achieve efficient autoregressive generation. This could be a starting point for exploring how SSMs can be combined with normalization techniques to improve stability and efficiency in language models.\n\n## Stability Analysis Methods Specific to Autoregressive Architectures\n\n- The stability of autoregressive models can be improved through careful normalization strategies. Papers like \"Understanding the Generalization Benefit of Normalization Layers\" and \"Overcoming Recency Bias of Normalization Statistics\" provide insights into how normalization affects gradient flow and stability. These studies suggest that normalization helps control gradient flow and maintains a balance between adaptation and stability, which is crucial for autoregressive architectures.\n\n## Memory-Efficient Normalization Techniques for Long Sequences\n\n- For memory-efficient normalization, techniques such as layer normalization or adaptive normalization can be employed. These methods are particularly useful for long sequences as they do not require storing large batches of data. The \"A Comprehensive Overview of Large Language Models\" paper mentions models like CPM-2, which use mixture-of-experts (MoE) models that can be optimized for memory efficiency.\n\n## Hardware-Optimized Implementations of Hierarchical Processing\n\n- Hardware-optimized implementations can be achieved through parallel processing and efficient use of computational resources. The concept of parallel scans in convolutional recurrences, as discussed in \"Convolutional State Space Models,\" can be applied to hierarchical processing in language models to achieve subquadratic parallelization and fast autoregressive generation. This approach can be particularly beneficial when implemented on specialized hardware like GPUs or TPUs.\n\n### Additional Search Directions\n\nTo further address the researcher's needs, additional searches could focus on:\n\n- **Hierarchical Normalization in Deep Learning**: Look for papers that specifically implement hierarchical normalization in deep learning models, such as those using transformer architectures.\n- **State Space Models in NLP**: Search for studies that integrate SSMs with language models, focusing on how normalization techniques are applied.\n- **Stability Analysis in Autoregressive Models**: Investigate papers that provide detailed stability analysis for autoregressive models, including the impact of normalization on these models.\n- **Efficient Normalization for Long Sequences**: Explore techniques like layer normalization, adaptive normalization, or other memory-efficient methods tailored for long sequences.\n- **Hardware Optimization for Hierarchical Processing**: Look into papers and implementations that optimize hierarchical processing for specific hardware, such as GPUs or TPUs, to improve efficiency.\n\nBy focusing on these areas, researchers can gather more specific and practical insights to improve the design of autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results provide valuable insights into state space models, hierarchical normalization, and stability analysis. Key findings include:\n- Theoretical foundations of SSM integration with normalization\n- Stability considerations in hierarchical architectures\n- Memory efficiency and hardware optimization strategies\n- Gradient flow control mechanisms\n\nHowever, more specific information is needed about:\n- Practical implementations of hierarchical normalization in language models\n- Empirical results of combining SSMs with normalization\n- Specific stability analysis methods for autoregressive models\n\n2. Useful References with Excerpts:\n\nFrom \"Understanding the Generalization Benefit of Normalization Layers\":\n\"Our theory explains how GD with a finite learning rate enters the so-called Edge of Stability (EoS) regime, and characterizes the trajectory of GD in this regime via a continuous sharpness-reduction flow.\"\n\nFrom \"Overcoming Recency Bias of Normalization Statistics\":\n\"Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization.\"\n\nFrom ConvSSM paper:\n\"We demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation.\"\n\n3. Key Insights:\n\na) Stability Mechanisms:\n- Normalization helps control gradient flow\n- Balance between adaptation and stability is crucial\n- Hierarchical structures require careful normalization strategies\n\nb) Integration Approaches:\n- SSMs can be effectively combined with normalization\n- Parallel processing can improve efficiency\n- Memory management is critical for scalability\n\nc) Performance Considerations:\n- Generalization benefits from proper normalization\n- Hardware-aware implementations are important\n- Balance between efficiency and stability\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\na) Practical implementations of hierarchical normalization in language models\nb) Empirical studies of SSM-normalization combinations\nc) Specific stability analysis methods for autoregressive architectures\n\n5. References:\n\n[1] Lyu, K., Li, Z., & Arora, S. (2022). Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction. Neural Information Processing Systems.\n\n[2] Lyu, Y., Wang, L., Zhang, X., Sun, Z., Su, H., Zhu, J., & Jing, L. (2023). Overcoming Recency Bias of Normalization Statistics in Continual Learning. Neural Information Processing Systems.\n\n[3] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S.W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[4] Cai, Y., Li, Q., & Shen, Z. (2018). A Quantitative Analysis of the Effect of Batch Normalization on Gradient Descent. International Conference on Machine Learning.\n\n[5] Lin, Z., Sekar, V., & Fanti, G. (2020). Why Spectral Normalization Stabilizes GANs: Analysis and Improvements. Neural Information Processing Systems."
        }
    ],
    "rating": 4.35,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.887337,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.931314,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalRMSNorm",
    "proposal": "# HybridSSMHierRMSNorm: Enhanced Hierarchical Normalization with Selective State Space Mechanisms\n\n## Motivation\n\nCurrent hierarchical normalization approaches in language models face several challenges:\n- Limited ability to capture long-range dependencies efficiently\n- High computational overhead in multi-scale processing\n- Suboptimal memory utilization across scales\n- Inefficient parallel processing capabilities\n\nTo address these limitations, we propose HybridSSMHierRMSNorm, which integrates selective state space mechanisms and parallel scan operations into hierarchical RMS normalization to enable more efficient and effective multi-scale feature processing.\n\n## Problem Analysis\n\n### Current Limitations:\n1. **Computational Efficiency**:\n   - Traditional hierarchical processing has high computational overhead\n   - Limited parallelization in multi-scale operations\n   - Inefficient memory access patterns\n\n2. **Feature Representation**:\n   - Difficulty capturing dependencies across different scales\n   - Limited interaction between hierarchical levels\n   - Suboptimal information flow between scales\n\n3. **Memory Usage**:\n   - High memory requirements for storing intermediate states\n   - Inefficient state management across scales\n   - Poor cache utilization\n\n### Opportunities for Improvement:\n1. **Integration of SSMs**:\n   - Linear complexity through selective state mechanisms\n   - Efficient parallel scan operations\n   - Hardware-aware implementations\n\n2. **Enhanced Hierarchical Processing**:\n   - Multi-scale feature representation\n   - Improved information flow between scales\n   - Adaptive state management\n\n## Core Idea and Philosophy\n\nHybridSSMHierRMSNorm enhances hierarchical normalization by:\n1. Incorporating selective state space mechanisms for efficient sequence processing\n2. Utilizing parallel scan operations for improved computational efficiency\n3. Implementing adaptive state management across scales\n4. Maintaining hardware-aware optimizations\n\n## Design Plan\n\n### Mathematical Formulation:\n\n1. Selective State Update:\n```python\ndef selective_state_update(x, scale):\n    # Compute state importance score\n    score = self.state_proj(x)  # [batch, seq_len, state_dim]\n    gamma = torch.sigmoid(score)\n    \n    # Update state selectively\n    state_new = self.ssm_proj(x)  # [batch, seq_len, state_dim]\n    state = state * (1 - gamma) + state_new * gamma\n    return state\n```\n\n2. Parallel Scan Operation:\n```python\ndef parallel_scan(x, A, B, C):\n    # Parallel scan implementation\n    L = x.shape[1]\n    As = A * torch.ones((L,) + A.shape)\n    Bx = torch.conv1d(x.transpose(1, 2), B)\n    \n    # Associative scan operation\n    _, states = lax.associative_scan(binary_operator, (As, Bx))\n    return states\n```\n\n3. Multi-scale Integration:\n```python\ndef multi_scale_process(x, scales):\n    outputs = []\n    for s in scales:\n        # Process at current scale\n        x_s = self.downsample(x, s)\n        state_s = self.selective_state_update(x_s, s)\n        y_s = self.parallel_scan(state_s, self.A[s], self.B[s], self.C[s])\n        \n        # Upsample and collect\n        y_s = self.upsample(y_s, target_len=x.shape[1])\n        outputs.append(y_s)\n    \n    # Combine scales with learned weights\n    weights = F.softmax(self.scale_weights, dim=0)\n    return sum(w * out for w, out in zip(weights, outputs))\n```\n\n### Implementation:\n\n```python\nclass HybridSSMHierRMSNorm(nn.Module):\n    def __init__(self, embed_dim, scales=[1, 2, 4], eps=1e-5):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.scales = scales\n        self.eps = eps\n        \n        # SSM parameters\n        self.A = nn.ParameterDict({\n            f's{s}': nn.Parameter(torch.randn(embed_dim))\n            for s in scales\n        })\n        self.B = nn.ParameterDict({\n            f's{s}': nn.Parameter(torch.randn(embed_dim, embed_dim))\n            for s in scales\n        })\n        self.C = nn.ParameterDict({\n            f's{s}': nn.Parameter(torch.randn(embed_dim, embed_dim))\n            for s in scales\n        })\n        \n        # State projection layers\n        self.state_proj = nn.Linear(embed_dim, embed_dim)\n        self.ssm_proj = nn.Linear(embed_dim, embed_dim)\n        \n        # Scale weights\n        self.scale_weights = nn.Parameter(torch.ones(len(scales)))\n        \n        # Compression layers\n        self.compress = nn.ModuleDict({\n            f's{s}': nn.Linear(embed_dim, embed_dim)\n            for s in scales\n        })\n\n    def forward(self, x):\n        # Normalize input\n        rms = torch.sqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n        x_norm = x / rms\n        \n        # Multi-scale processing\n        y = self.multi_scale_process(x_norm, self.scales)\n        \n        # Final normalization\n        rms_out = torch.sqrt(y.pow(2).mean(-1, keepdim=True) + self.eps)\n        return y / rms_out\n```\n\n## Implementation Guidelines\n\n1. Initialization:\n   - Initialize SSM parameters carefully for stability\n   - Set initial scale weights to equal values\n   - Use appropriate initialization for projection layers\n\n2. Training Considerations:\n   - Implement gradient checkpointing for memory efficiency\n   - Use adaptive learning rates for different components\n   - Monitor scale weights for potential collapse\n\n3. Optimization:\n   - Fuse operations where possible\n   - Implement efficient parallel processing\n   - Use hardware-aware memory management\n\n## Theoretical Analysis\n\n### Complexity Analysis:\n- Time Complexity: O(n) per scale through selective SSM\n- Memory Complexity: O(n) with efficient state management\n- Parallel Complexity: O(log n) for scan operations\n\n### Stability Analysis:\n1. Gradient Flow:\n   - Selective updates prevent vanishing gradients\n   - Multi-scale processing provides multiple paths\n   - Normalization ensures stable training\n\n2. Memory Efficiency:\n   - Selective state updates reduce memory requirements\n   - Efficient parallel processing minimizes overhead\n   - Adaptive compression optimizes memory usage\n\n## Expected Benefits\n\n1. Improved Efficiency:\n   - Linear complexity through selective SSM\n   - Efficient parallel processing\n   - Reduced memory requirements\n\n2. Enhanced Feature Representation:\n   - Better capture of long-range dependencies\n   - Improved multi-scale processing\n   - Enhanced information flow between scales\n\n3. Hardware Optimization:\n   - Efficient parallel implementation\n   - Better cache utilization\n   - Reduced memory bandwidth requirements\n\n## References\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n[2] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Smith, J.T.H., et al. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[4] Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n[5] Dao, T., et al. (2022). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. International Conference on Learning Representations.\n\n## Conclusion\n\nHybridSSMHierRMSNorm represents a significant advancement in hierarchical normalization by integrating selective state space mechanisms and parallel scan operations. The design achieves improved computational efficiency and feature representation while maintaining hardware-aware optimizations. Through careful integration of SSMs with hierarchical processing, the model provides an efficient and effective solution for modern language modeling challenges.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "HybridSSMHierRMSNorm enhances hierarchical RMS normalization by integrating selective state space mechanisms and parallel scan operations, enabling efficient multi-scale feature processing while maintaining linear complexity. The design combines the strengths of SSMs and hierarchical processing to improve both computational efficiency and model performance.",
    "ideation": null,
    "modelname": "hybridssmnet",
    "suggestions": "1. Implementation Details:\n- Provide more specific details about the parallel scan operation implementation\n- Include concrete examples of hardware optimization strategies\n- Add benchmarks for memory usage across different scales\n\n2. Stability Analysis:\n- Expand on the stability analysis for multi-scale processing\n- Include theoretical guarantees for convergence\n- Address potential failure modes and mitigation strategies\n\n3. Empirical Validation:\n- Add proposed experiments to validate the approach\n- Include ablation studies for different components\n- Provide metrics for measuring efficiency improvements\n\n4. Hardware Optimization:\n- Detail specific hardware optimization techniques\n- Include memory access pattern analysis\n- Provide cache utilization strategies\n\n5. Integration Guidelines:\n- Add more detailed integration steps with existing architectures\n- Include compatibility considerations\n- Provide debugging and monitoring recommendations",
    "user_input": ""
}