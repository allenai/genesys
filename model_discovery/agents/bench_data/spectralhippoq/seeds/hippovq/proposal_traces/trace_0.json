{
    "variantname": null,
    "review": "The HiPPOVQ proposal presents an innovative approach to improving GAU efficiency by combining event-driven processing with polynomial projections and hierarchical vector quantization. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel integration of event detection with HiPPO polynomial projections\n- Well-grounded mathematical framework for long-range modeling\n- Clear differentiation from existing approaches including eventnormer and densessmamba\n- Unique hierarchical memory management strategy\n\n2. Design Clarity:\n- Well-structured mathematical formulation\n- Clear architectural components\n- Detailed implementation guidelines\n- Thoughtful consideration of hardware efficiency\n\n3. Efficiency Improvements:\n- Potential for significant computation reduction through event-driven processing\n- Memory efficiency through hierarchical compression\n- Hardware-friendly polynomial computations\n- Adaptive resource allocation based on input complexity\n\n4. Scalability:\n- Linear complexity through efficient state space formulation\n- Hierarchical compression for long sequences\n- Adaptive computation based on input characteristics\n- Efficient memory management through vector quantization\n\nCONCERNS:\n\n1. Training Stability:\n- Complex interaction between event detection and polynomial projections\n- Multiple adaptive components may affect convergence\n- Scale selection mechanism needs careful tuning\n- Potential instability in hierarchical updates\n\n2. Implementation Complexity:\n- Integration of multiple sophisticated components\n- Hardware-specific optimizations required\n- Complex state management system\n- Potential overhead from polynomial computations\n\n3. Memory Management:\n- Trade-off between compression and representation quality\n- Overhead from maintaining multiple scales\n- Complex interaction between VQ and polynomial projections\n- State synchronization challenges\n\n4. Performance Impact:\n- Potential quality degradation from aggressive compression\n- Trade-off between adaptivity and stability\n- Impact on downstream task performance\n- Robustness to diverse inputs\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than HiPPO's basic polynomial projections\n2. More adaptive than traditional SSMs\n3. Novel integration of event detection with polynomial memory\n4. Unique hierarchical compression strategy\n\nHowever, it shares some concepts with:\n1. DenseMamba's hierarchical state integration\n2. FlashAttention's memory-efficient computation\n3. VQDNA's hierarchical vector quantization",
    "search_stack": [
        {
            "ready": false,
            "query": "scale-adaptive neural networks event-driven hierarchical processing",
            "detail": "Find papers discussing:\n1. Integration of event detection mechanisms with multi-scale or hierarchical neural architectures\n2. Training stability techniques for adaptive neural networks\n3. Memory-efficient approaches for multi-scale processing",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of event detection mechanisms with multi-scale or hierarchical neural architectures\n2. Training stability techniques for adaptive neural networks\n3. Memory-efficient approaches for multi-scale processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.05)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 21/46 (Score: 0.09)*\n\n```\nURL https://arxiv.org/abs/2204.02311. Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. CoRR, abs/1609.01704, 2016. URL/http://arxiv.org/abs/1609.01704. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dud\u00edk (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 215-223, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/coates11a.html. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\n```\n\n##### *Relevant Chunk: No. 33/46 (Score: 0.01)*\n\n```\nCoRR, abs/2103.14030, 2021b. URL/https://arxiv.org/abs/2103.14030\n\nSarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan, Guillaume Lajoie, Michael Mozer, and Yoshua Bengio. Learning to combine top-down and bottom-up signals in recurrent neural networks with attention over modules. CoRR, abs/2006.16981, 2020. URL https://arxiv.org/abs/2006.16981\n\nMichael C Mozer. Induction of multiscale temporal structure.\n```\n\n#### 2. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.01)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 18/43 (Score: 0.01)*\n\n```\nBiol. Cybern., 97(5):337-339, 2007. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Lo\u00efc Cordone, Beno\u00eet Miramond, and Philippe Thierion. Object detection with spiking neural networks on automotive event data. In International Joint Conference on Neural Networks (IJCNN), pp. 1-8, 2022. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International Conference on Machine Learning (ICML), volume 70 of Proceedings of Machine Learning Research, pp. 933-941, 2017. URL http://proceedings.mlr.press/v70/dauphin17a.html. Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Dimou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor with on-chip learning.\n```\n\n#### 3. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.01)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 50/97 (Score: 0.01)*\n\n```\nArXiv, 2404.05892, 2024. M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In Proceedings of the 40th International Conference on Machine Learning (ICML). JMLR.org, 2023. doi: 10.5555/3618408.3619572. M. Poli, A. W. Thomas, E. Nguyen, P. Ponnusamy, B. Deiseroth, K. Kersting, T. Suzuki, B. Hie, S. Ermon, C. R\u00e9, C. Zhang, and S. Massaroli. Mechanistic design and scaling of hybrid architectures. ArXiv, $2403.17844,2024$. Z. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Advances in Neural Information Processing Systems (NeurIPS), volume 37, 2023. URL https://openreview.net/forum?id=P1TCHxJwLB. Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W.\n```\n\n#### 4. Theoretical Foundations of Deep Selective State-Space Models (Avg. Score: 0.01)\n\n*Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, C. Salvi, Terry Lyons*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** Theoretical grounding is given to this recent finding that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.\n\n**Abstract:** Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.\n\n##### *Relevant Chunk: No. 25/45 (Score: 0.01)*\n\n```\nHambly, B. and Lyons, T. Uniqueness for the signature of a path of bounded variation and the reduced path group. Annals of Mathematics, pp. 109-167, 2010. Hanson, J. and Raginsky, M. Universal simulation of stable dynamical systems by recurrent neural nets. In Learning for Dynamics and Control, pp. 384-392. PMLR, 2020. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 1997. Hoglund, M., Ferrucci, E., Hernandez, C., Gonzalez, A. M., Salvi, C., Sanchez-Betancourt, L., and Zhang, Y. A neural rde approach for continuous-time non-markovian stochastic control problems. arXiv preprint arXiv:2306.14258, 2023. Issa, Z., Horvath, B., Lemercier, M., and Salvi, C. Nonadversarial training of neural sdes with signature kernel scores. Advances in Neural Information Processing Systems, 2023. Katsch, T. Gateloop: Fully data-controlled linear recurrence for sequence modeling, 2023. Kidger, P. On neural differential equations, 2022. Kidger, P., Bonnier, P., Perez Arribas, I., Salvi, C., and Lyons, T. Deep signature transforms. Advances in Neural Information Processing Systems, 32, 2019. Kidger, P., Morrill, J., Foster, J., and Lyons, T. Neural controlled differential equations for irregular time series. Advances in Neural Information Processing Systems, 33: 6696-6707, 2020. Korsky, S. A. and Berwick, R. C. On the computational power of rnns. arXiv preprint arXiv:1906.06349, 2019. LeCun, Y. A., Bottou, L., Orr, G. B., and M\u00fcller, K.-R. Efficient BackProp, pp. 9-48. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. ISBN 978-3-642-35289-8. doi: 10.1007/978-3-642-35289-8_3. URL https://doi. org/10.1007/978-3-642-35289-8_3\nLemercier, M., Salvi, C., Damoulas, T., Bonilla, E. V., and Lyons, T. Distribution regression for sequential data, 2021. Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022a. Li, Z., Han, J., Weinan, E., and Li, Q. Approximation and optimization theory for linear continuous-time recurrent neural networks.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: scale-adaptive neural networks event-driven hierarchical processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks\n\n*From Search Query: scale-adaptive neural networks event-driven hierarchical processing*\n\n*F. Paredes-Vall\u00e9s, J. Hagenaars, G. D. Croon*\n\n**TL;DR:** This article focuses on the complex task of learning to estimate optical flow from event-based camera inputs in a self-supervised manner, and modify the state-of-the-art ANN training pipeline to encode minimal temporal information in its inputs to improve its convexity.\n\n**Abstract:** The field of neuromorphic computing promises extremely low-power and low-latency sensing and processing. Challenges in transferring learning algorithms from traditional artificial neural networks (ANNs) to spiking neural networks (SNNs) have so far prevented their application to large-scale, complex regression tasks. Furthermore, realizing a truly asynchronous and fully neuromorphic pipeline that maximally attains the abovementioned benefits involves rethinking the way in which this pipeline takes in and accumulates information. In the case of perception, spikes would be passed as-is and one-by-one between an event camera and an SNN, meaning all temporal integration of information must happen inside the network. In this article, we tackle these two problems. We focus on the complex task of learning to estimate optical flow from event-based camera inputs in a self-supervised manner, and modify the state-of-the-art ANN training pipeline to encode minimal temporal information in its inputs. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 117  (*Influential: 18*)\n\n#### 2. Graph Coarsening with Neural Networks\n\n*From Search Query: scale-adaptive neural networks event-driven hierarchical processing*\n\n*Chen Cai, Dingkang Wang, Yusu Wang*\n\n**TL;DR:** This work proposes a framework for measuring the quality of coarsening algorithm and shows that depending on the goal, it need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators and parametrize the weight assignment map with graph neural networks and train it to improve the coarsens quality in an unsupervised way.\n\n**Abstract:** As large-scale graphs become increasingly more prevalent, it poses significant computational challenges to process, extract and analyze large graph data. Graph coarsening is one popular technique to reduce the size of a graph while maintaining essential properties. Despite rich graph coarsening literature, there is only limited exploration of data-driven methods in the field. In this work, we leverage the recent progress of deep learning on graphs for graph coarsening. We first propose a framework for measuring the quality of coarsening algorithm and show that depending on the goal, we need to carefully choose the Laplace operator on the coarse graph and associated projection/lift operators. Motivated by the observation that the current choice of edge weight for the coarse graph may be sub-optimal, we parametrize the weight assignment map with graph neural networks and train it to improve the coarsening quality in an unsupervised way. Through extensive experiments on both synthetic and real networks, we demonstrate that our method significantly improves common graph coarsening methods under various metrics, reduction ratios, graph sizes, and graph types. It generalizes to graphs of larger size ($25\\times$ of training graphs), is adaptive to different losses (differentiable and non-differentiable), and scales to much larger graphs than previous work.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 58  (*Influential: 6*)\n\n#### 3. Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks\n\n*From Search Query: scale-adaptive neural networks event-driven hierarchical processing*\n\n*Wenrui Zhang, Peng Li*\n\n**TL;DR:** The proposed ST-RSBP algorithm is able to train RSNNs with an accuracy surpassing that of the current state-of-art SNN BP algorithms and conventional non-spiking deep learning models.\n\n**Abstract:** Spiking neural networks (SNNs) well support spatiotemporal learning and energy-efficient event-driven hardware neuromorphic processors. As an important class of SNNs, recurrent spiking neural networks (RSNNs) possess great computational power. However, the practical application of RSNNs is severely limited by challenges in training. Biologically-inspired unsupervised learning has limited capability in boosting the performance of RSNNs. On the other hand, existing backpropagation (BP) methods suffer from high complexity of unrolling in time, vanishing and exploding gradients, and approximate differentiation of discontinuous spiking activities when applied to RSNNs. To enable supervised training of RSNNs under a well-defined loss function, we present a novel Spike-Train level RSNNs Backpropagation (ST-RSBP) algorithm for training deep RSNNs. The proposed ST-RSBP directly computes the gradient of a rated-coded loss function defined at the output layer of the network w.r.t tunable parameters. The scalability of ST-RSBP is achieved by the proposed spike-train level computation during which temporal effects of the SNN is captured in both the forward and backward pass of BP. Our ST-RSBP algorithm can be broadly applied to RSNNs with a single recurrent layer or deep RSNNs with multiple feed-forward and recurrent layers. Based upon challenging speech and image datasets including TI46, N-TIDIGITS, Fashion-MNIST and MNIST, ST-RSBP is able to train RSNNs with an accuracy surpassing that of the current state-of-art SNN BP algorithms and conventional non-spiking deep learning models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 119  (*Influential: 17*)\n\n#### 4. Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks\n\n*From Search Query: scale-adaptive neural networks event-driven hierarchical processing*\n\n*Yingyezhe Jin, Peng Li, Wenrui Zhang*\n\n**TL;DR:** The proposed HM2-BP algorithm achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams, and leads to high recognition accuracy for the 16-speaker spoken English letters of TI46 Corpus, a challenging patio-temporal speech recognition benchmark.\n\n**Abstract:** Spiking neural networks (SNNs) are positioned to enable spatio-temporal information processing and ultra-low power event-driven neuromorphic hardware. However, SNNs are yet to reach the same performances of conventional deep artificial neural networks (ANNs), a long-standing challenge due to complex dynamics and non-differentiable spike events encountered in training. The existing SNN error backpropagation (BP) methods are limited in terms of scalability, lack of proper handling of spiking discontinuities, and/or mismatch between the rate-coded loss function and computed gradient. We present a hybrid macro/micro level backpropagation (HM2-BP) algorithm for training multi-layer SNNs. The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level. The rate-coded errors are defined at the macroscopic level, computed and back-propagated across both macroscopic and microscopic levels. Different from existing BP methods, HM2-BP directly computes the gradient of the rate-coded loss function w.r.t tunable parameters. We evaluate the proposed HM2-BP algorithm by training deep fully connected and convolutional SNNs based on the static MNIST [14] and dynamic neuromorphic N-MNIST [26]. HM2-BP achieves an accuracy level of 99.49% and 98.88% for MNIST and N-MNIST, respectively, outperforming the best reported performances obtained from the existing SNN BP algorithms. Furthermore, the HM2-BP produces the highest accuracies based on SNNs for the EMNIST [3] dataset, and leads to high recognition accuracy for the 16-speaker spoken English letters of TI46 Corpus [16], a challenging patio-temporal speech recognition benchmark for which no prior success based on SNNs was reported. It also achieves competitive performances surpassing those of conventional deep learning models when dealing with asynchronous spiking streams.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 168  (*Influential: 24*)\n\n#### 5. Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting\n\n*From Search Query: scale-adaptive neural networks event-driven hierarchical processing*\n\n*Shi-Wee Deng, Yuhang Li, Shanghang Zhang, Shi Gu*\n\n**TL;DR:** This paper analyzes why the current direct training approach with surrogate gradient results in SNNs with poor generalizability, and introduces the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalIZability.\n\n**Abstract:** Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. Still, it is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we first analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, including CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained 83$\\%$ top-1 accuracy, over 10$\\%$ improvement compared to existing state of the art. Codes are available at \\url{https://github.com/Gus-Lab/temporal_efficient_training}.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 196  (*Influential: 49*)\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating event detection mechanisms with multi-scale or hierarchical neural architectures, and addressing training stability and memory efficiency, here are some key insights and relevant information from the provided sources:\n\n## Integration of Event Detection with Multi-Scale or Hierarchical Neural Architectures\n\n- The concept of event-driven processing can be inspired by Spiking Neural Networks (SNNs), which process information only when there is a significant change or 'event' in the input. This approach can be adapted to guide scale selection in hierarchical processing. For instance, the Spiking Jaccard Attention mechanism proposed in shows how event-driven processing can be integrated into attention mechanisms, which could be applied to hierarchical architectures to selectively focus on important scales or events.\n\n## Training Stability Techniques for Adaptive Neural Networks\n\n- Training stability is a critical issue when combining different adaptive mechanisms. The paper on Developmental Plasticity-inspired Adaptive Pruning for Deep Neural Networks and SNNs discusses how incorporating adaptive pruning based on temporal spiking sequences can improve training stability and efficiency. This approach could be adapted to autoregressive language models by using similar pruning criteria to eliminate redundant parameters or connections, thus stabilizing the training process.\n\n## Memory-Efficient Approaches for Multi-Scale Processing\n\n- Memory efficiency is a key aspect of both eventvq and hiergpt designs. The use of vector quantization in eventvq for efficient compression and adaptive caching can be combined with the hierarchical structure of hiergpt. The Spiking Neural Networks discussed in and also highlight memory-efficient processing through event-driven computations and sparse neuron activation. These concepts can be applied to develop scale-aware event detection and adaptive compression strategies that are memory-efficient.\n\n## Specific Technical Details\n\n- **Event-Driven Hierarchical Processing**: The idea of using event detection to guide scale selection can be explored by integrating event-driven mechanisms with hierarchical architectures. For example, the hierarchical structure can be enhanced by using event-driven updates to selectively process information at different scales, as suggested by the complementary features of eventvq and hiergpt.\n  \n- **Training Stability**: Techniques such as adaptive pruning and the use of surrogate gradient algorithms for training SNNs can be adapted to improve training stability in combined adaptive systems.\n\n- **Memory-Efficient Hierarchical Compression**: The use of vector quantization and adaptive caching in eventvq, combined with the hierarchical structure of hiergpt, can be optimized further by applying event-driven processing. This can help in selectively compressing information based on both events and scales, enhancing memory efficiency.\n\n## Relevant Concepts\n\n- **Spiking Neural Networks (SNNs)**: SNNs provide a framework for event-driven processing that can be highly efficient and biologically plausible. The concepts of spatio-temporal integrated SNNs and temporal dimension pruning can be particularly useful in designing scale-adaptive and memory-efficient autoregressive language models.\n  \n- **Attention Mechanisms**: The Spiking Jaccard Attention mechanism offers a way to compute attention efficiently in SNNs, which can be adapted to hierarchical architectures to focus on important scales or events.\n\nBy integrating these concepts, researchers can develop a unified framework that leverages both event-driven and hierarchical processing, ensuring improved training stability, memory efficiency, and overall performance of the autoregressive language model.\n",
            "analysis": "Summary of Analysis:\n\nThe two parent designs offer complementary strengths that can be combined effectively:\n\n1. eventvq:\n- Strengths:\n  * Event-driven dynamic updates for selective computation\n  * Vector quantization for efficient compression\n  * Hierarchical state management\n  * Memory efficiency through adaptive caching\n- Limitations:\n  * Complex interaction between event detection and VQ\n  * Training stability challenges\n  * Overhead from event detection\n\n2. hiergpt:\n- Strengths:\n  * Multi-scale hierarchical processing\n  * Adaptive normalization\n  * Memory-efficient computation\n  * Improved training stability\n- Limitations:\n  * Fixed computation paths\n  * Uniform processing regardless of importance\n  * Overhead from multiple scales\n\nKey Insights:\n\n1. Complementary Features:\n- eventvq's event-driven processing can guide hiergpt's multi-scale operations\n- hiergpt's hierarchical structure can enhance eventvq's state management\n- Both designs focus on efficiency but through different approaches\n\n2. Integration Opportunities:\n- Use event detection to guide scale selection in hierarchical processing\n- Apply vector quantization selectively based on scale importance\n- Combine adaptive normalization with event-driven updates\n\n3. Research Gaps:\n- Need to explore efficient integration of event detection with hierarchical processing\n- Investigate scale-specific vector quantization strategies\n- Study training stability with combined adaptive mechanisms\n\n4. Design Direction:\n- Focus on creating a unified framework that leverages both event-driven and hierarchical processing\n- Develop scale-aware event detection\n- Implement adaptive compression based on both events and scales\n\nFuture Search Plan:\n\n1. Investigate recent advances in:\n- Scale-adaptive neural networks\n- Event-driven hierarchical processing\n- Efficient multi-scale compression techniques\n\n2. Explore specific technical details of:\n- Integration of event detection with multi-scale processing\n- Training stability in adaptive systems\n- Memory-efficient hierarchical compression\n\nReferences:\n\n1. Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization.\" NeurIPS 2019.\n2. Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\" ICML 2022.\n3. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv:2312.00752.\n4. He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\" arXiv:2401.04722."
        },
        {
            "ready": false,
            "query": "scale-adaptive compression multi-scale event detection",
            "detail": "Find papers discussing:\n1. Scale-adaptive compression techniques for neural networks\n2. Multi-scale event detection and processing methods\n3. Integration of event detection with hierarchical architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Scale-adaptive compression techniques for neural networks\n2. Multi-scale event detection and processing methods\n3. Integration of event detection with hierarchical architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.38)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.38)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.27)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.27)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n#### 3. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.23)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 42/46 (Score: 0.32)*\n\n```\n[N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n## Appendix\n\n## 6 Related Work\n\nHierarchical or Multiscale Recurrent neural networks. This work takes inspiration from a wide array of work on introducing multiple scales of processing into recurrent neural networks (Chung et al. 2016; Hihi \\& Bengio, 1995; Mozer, 1991, Schmidhuber, 1991, Koutn\u00edk et al., 2014). These works divide the processing into multiple streams each operating at a different temporal granularity. While these works mainly focus on recurrent neural networks and their application is mainly on natural language tasks, we focus on introducing multiple streams of processing and a hierarchical structure into Transformers while also focusing on a broader range of domains beyond natural language. Transformers. Some of the components we describe in the proposed model have been used previously in various Transformer models. Transformer XL (Dai et al., 2019) also divides the input into segments. Each segment considers the tokens from the current segment and the previous segment for attention without passing gradients into the previous segments. A number of previous works (Zhang et al., 2021; Liu et al., 2021b, Wu et al., 2021, Yuan et al., 2021, Wang et al., 2021; Yang et al., 2021) have worked on introducing a hierarchical structure in Transformers mainly in the domain of vision. The main goal of these works has been to introduce convolution-like hierarchies into Vision Transformers (Dosovitskiy et al. 2020). While these works progressively reduce the spatial resolution of the inputs in order to introduce hierarchies, we introduce hierarchies by adding another slow stream of information processing and without reducing the spatial resolution of the inputs. We also provision for the higher level of the hierarchy (i.e. the slow stream) to provide information to the lower levels as top-down conditioning which is not possible in any of the previous works. Top-Down Conditioning. Top-down information is information propagated from higher to lower levels of the network. It represents the models beliefs of the world and provides context for interpreting perceptual information.\n```\n\n##### *Relevant Chunk: No. 21/46 (Score: 0.14)*\n\n```\nURL https://arxiv.org/abs/2204.02311. Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. CoRR, abs/1609.01704, 2016. URL/http://arxiv.org/abs/1609.01704. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dud\u00edk (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pp. 215-223, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/coates11a.html. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\n```\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.16)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.16)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: scale-adaptive compression multi-scale event detection\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. GLEN: General-Purpose Event Detection for Thousands of Types\n\n*From Search Query: scale-adaptive compression multi-scale event detection*\n\n*Qiusi Zhan, Sha Li, Kathryn Conger, Martha Palmer, Heng Ji, Jiawei Han*\n\n**TL;DR:** This work builds a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more than 20x larger in ontology than today's largest event dataset.\n\n**Abstract:** The progress of event extraction research has been hindered by the absence of wide-coverage, large-scale datasets. To make event extraction systems more accessible, we build a general-purpose event detection dataset GLEN, which covers 205K event mentions with 3,465 different types, making it more than 20x larger in ontology than today's largest event dataset. GLEN is created by utilizing the DWD Overlay, which provides a mapping between Wikidata Qnodes and PropBank rolesets. This enables us to use the abundant existing annotation for PropBank as distant supervision. In addition, we also propose a new multi-stage event detection model CEDAR specifically designed to handle the large ontology size in GLEN. We show that our model exhibits superior performance compared to a range of baselines including InstructGPT. Finally, we perform error analysis and show that label noise is still the largest challenge for improving performance for this new dataset. Our dataset, code, and models are released at \\url{https://github.com/ZQS1943/GLEN}.}\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 2*)\n\n#### 2. MUSIED: A Benchmark for Event Detection from Multi-Source Heterogeneous Informal Texts\n\n*From Search Query: scale-adaptive compression multi-scale event detection*\n\n*Xiangyu Xi, Jianwei Lv, Shuaipeng Liu, Wei Ye, Fan Yang, Guanglu Wan*\n\n**TL;DR:** A new large-scale Chinese event detection dataset based on user reviews, text conversations, and phone conversations in a leading e-commerce platform for food service is proposed, indicating that multi-domain informal event detection remains an open problem and requires further efforts.\n\n**Abstract:** Event detection (ED) identifies and classifies event triggers from unstructured texts, serving as a fundamental task for information extraction. Despite the remarkable progress achieved in the past several years, most research efforts focus on detecting events from formal texts (e.g., news articles, Wikipedia documents, financial announcements). Moreover, the texts in each dataset are either from a single source or multiple yet relatively homogeneous sources. With massive amounts of user-generated text accumulating on the Web and inside enterprises, identifying meaningful events in these informal texts, usually from multiple heterogeneous sources, has become a problem of significant practical value. As a pioneering exploration that expands event detection to the scenarios involving informal and heterogeneous texts, we propose a new large-scale Chinese event detection dataset based on user reviews, text conversations, and phone conversations in a leading e-commerce platform for food service. We carefully investigate the proposed dataset\u2019s textual informality and multi-domain heterogeneity characteristics by inspecting data samples quantitatively and qualitatively. Extensive experiments with state-of-the-art event detection methods verify the unique challenges posed by these characteristics, indicating that multi-domain informal event detection remains an open problem and requires further efforts. Our benchmark and code are released at https://github.com/myeclipse/MUSIED.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 3. PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation\n\n*From Search Query: scale-adaptive compression multi-scale event detection*\n\n*Can Qin, Haoxuan You, Lichen Wang, C.-C. Jay Kuo, Y. Fu*\n\n**TL;DR:** A novel 3D Domain Adaptation Network for point cloud data (PointDAN) is proposed, which jointly aligns the global and local features in multi-level and demonstrates the superiority of the model over the state-of-the-art general-purpose DA methods.\n\n**Abstract:** Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 181  (*Influential: 46*)\n\n#### 4. Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection\n\n*From Search Query: scale-adaptive compression multi-scale event detection*\n\n*Zihan Ma, Minnan Luo, Hao Guo, Zhi Zeng, Yiran Hao, Xiang Zhao*\n\n**TL;DR:** An Event-Radar based on multi-view learning based on multi-view learning is proposed based on multi-view learning, which integrates visual manipulation, textual emotion and mul-014 timodal inconsistency at event-level for fake news detection.\n\n**Abstract:** The swift detection of multimedia fake news 001 has emerged as a crucial task in combating ma-002 licious propaganda and safeguarding the secu-003 rity of the online environment. While existing 004 methods have achieved commendable results 005 in modeling entity-level inconsistency, address-006 ing event-level inconsistency following the in-007 herent subject-predicate logic of news and ro-008 bustly learning news representations from poor-009 quality news samples remain two challenges. In 010 this paper, we propose an Event -d R iven f A ke 011 news D etection fr A mewo R k (Event-Radar) 012 based on multi-view learning, which integrates 013 visual manipulation, textual emotion and mul-014 timodal inconsistency at event-level for fake 015 news detection. Specifically, leveraging the 016 capability of graph structures to capture inter-017 actions between events and parameters, Event-018 Radar captures event-level multimodal incon-019 sistency by constructing an event graph that 020 includes multimodal entity subject-predicate 021 logic. Additionally, to mitigate the interference 022 of poor-quality news, Event-Radar introduces 023 a multi-view fusion mechanism, learning com-024 prehensive and robust representations by com-025 puting the credibility of each view as a clue, 026 thereby detecting fake news. Extensive experi-027 ments demonstrate that Event-Radar achieves 028 outstanding performance on three large-scale 029 fake news detection benchmarks. Our studies 030 also confirm that Event-Radar exhibits strong 031 robustness, providing a paradigm for detecting 032 fake news from noisy news samples. 033\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Context-adaptive Entropy Model for End-to-end Optimized Image Compression\n\n*From Search Query: scale-adaptive compression multi-scale event detection*\n\n*Jooyoung Lee, Seunghyun Cho, Seunghwa Beack*\n\n**TL;DR:** The proposed context-adaptive entropy model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required, which allows the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an enhanced compression performance.\n\n**Abstract:** We propose a context-adaptive entropy model for use in end-to-end optimized image compression. Our model exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required. Based on these contexts, we allow the model to more accurately estimate the distribution of each latent representation with a more generalized form of the approximation models, which accordingly leads to an enhanced compression performance. Based on the experimental results, the proposed method outperforms the traditional image codecs, such as BPG and JPEG2000, as well as other previous artificial-neural-network (ANN) based approaches, in terms of the peak signal-to-noise ratio (PSNR) and multi-scale structural similarity (MS-SSIM) index.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 367  (*Influential: 56*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on scale-adaptive compression, multi-scale event detection, and the integration of event detection with hierarchical architectures, here are some relevant findings and suggestions based on the provided keywords and analysis:\n\n## Scale-Adaptive Compression Techniques\n\n- The paper \"NeuZip: Memory-Efficient Training and Inference with Dynamic Compression\" introduces an algorithm to compress neural networks while maintaining their full capabilities. This dynamic compression could be adapted for scale-adaptive compression in autoregressive language models, helping to reduce memory usage and improve efficiency.\n- Another relevant work is \"BLAST: Block-Level Adaptive Structured Matrices for Efficient Deep Learning,\" which demonstrates the use of block-level adaptive structured matrices for compressing models. This approach could be integrated into hierarchical architectures to achieve scale-adaptive compression.\n\n## Multi-Scale Event Detection and Processing Methods\n\n- The concept of event-driven processing from spiking neural networks (SNNs) can be leveraged for multi-scale event detection. SNNs efficiently process events and can guide scale selection based on temporal integration and event importance. This can be adapted for language models to detect and process events at multiple scales, enhancing their ability to capture context and nuances in text[Analysis Note: a) Event-Driven Scale Selection].\n\n## Integration of Event Detection with Hierarchical Architectures\n\n- The \"Hybrid Macro/Micro Level Backpropagation\" approach for training SNNs provides insights into capturing temporal effects at both microscopic and macroscopic levels. This can be applied to hierarchical language models by integrating event detection at different scales. For instance, using spike-train level computations to capture temporal dependencies and rate-coded errors for macroscopic adjustments can help in integrating event detection with hierarchical processing[Analysis Note: 2. Useful References with Excerpts, \"Hybrid Macro/Micro Level Backpropagation\"].\n\n## Training Stability and Memory Optimization\n\n- For training stability, gradient re-weighting techniques and hybrid training approaches can be beneficial. These methods can help balance multiple adaptive mechanisms and improve the stability of the training process in multi-scale adaptive systems[Analysis Note: b) Training Considerations].\n- Event-driven processing can reduce memory usage, and scale-specific compression based on event importance can further optimize memory efficiency. Hierarchical state management guided by events can also help in managing memory effectively[Analysis Note: c) Memory Optimization].\n\n## Future Directions\n\n- Investigate the application of NeuZip and BLAST compression techniques in the context of autoregressive language models to achieve scale-adaptive compression.\n- Adapt event-driven processing from SNNs to develop multi-scale event detection methods for language models.\n- Implement hybrid training approaches and gradient re-weighting to ensure training stability in hierarchical networks.\n- Explore hierarchical state management and scale-specific compression to optimize memory usage.\n\nBy integrating these concepts, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage scale-adaptive compression, multi-scale event detection, and hierarchical architectures.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several promising directions for combining eventvq and hiergpt:\n\na) Event-Driven Scale Selection:\n- SNNs demonstrate efficient event-driven processing that could guide scale selection\n- Temporal integration can be used to determine importance across scales\n- Event detection can be adapted for hierarchical processing\n\nb) Training Stability:\n- Gradient re-weighting techniques from SNN training can improve stability\n- Hybrid macro/micro level backpropagation shows promise for temporal learning\n- Adaptive mechanisms need careful integration to maintain stability\n\nc) Memory Efficiency:\n- Event-driven sparse processing reduces memory requirements\n- Hierarchical compression can be guided by event importance\n- Scale-specific quantization could improve efficiency\n\n2. Useful References with Excerpts:\n\nFrom \"Temporal Efficient Training of Spiking Neural Network\":\n\"We introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent [...] so that the training process can converge into flatter minima with better generalizability.\"\n- Relevant for stabilizing training of combined event-driven and hierarchical systems\n\nFrom \"Spike-Train Level Backpropagation\":\n\"The scalability is achieved by the proposed spike-train level computation during which temporal effects of the SNN is captured in both the forward and backward pass of BP.\"\n- Provides insights for handling temporal dependencies in event detection\n\nFrom \"Hybrid Macro/Micro Level Backpropagation\":\n\"The temporal effects are precisely captured by the proposed spike-train level post-synaptic potential (S-PSP) at the microscopic level. The rate-coded errors are defined at the macroscopic level\"\n- Suggests approach for integrating event detection with hierarchical processing\n\n3. Key Insights:\n\na) Event-Scale Integration:\n- Use event detection to determine which scales are most relevant\n- Apply temporal integration across scales\n- Implement adaptive scale selection based on event importance\n\nb) Training Considerations:\n- Need to balance multiple adaptive mechanisms\n- Gradient re-weighting can improve stability\n- Hybrid training approaches may be beneficial\n\nc) Memory Optimization:\n- Event-driven processing can reduce memory usage\n- Scale-specific compression based on importance\n- Hierarchical state management guided by events\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific techniques for integrating event detection with hierarchical processing\n2. Methods for efficient scale-adaptive compression\n3. Training stability in multi-scale adaptive systems\n\nNext search queries should focus on:\n1. Scale-adaptive compression techniques\n2. Multi-scale event detection methods\n3. Training stability in hierarchical networks\n\n5. References:\n\n[1] Deng, S. W., Li, Y., Zhang, S., & Gu, S. (2022). \"Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting.\" International Conference on Learning Representations.\n\n[2] Zhang, W., & Li, P. (2019). \"Spike-Train Level Backpropagation for Training Deep Recurrent Spiking Neural Networks.\" Neural Information Processing Systems.\n\n[3] Jin, Y., Li, P., & Zhang, W. (2018). \"Hybrid Macro/Micro Level Backpropagation for Training Deep Spiking Neural Networks.\" Neural Information Processing Systems.\n\n[4] Paredes-Vall\u00e9s, F., Hagenaars, J., & Croon, G. D. (2021). \"Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks.\" Neural Information Processing Systems.\n\n[5] Cai, C., Wang, D., & Wang, Y. (2021). \"Graph Coarsening with Neural Networks.\" International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "butterfly transform sequence models context-adaptive compression",
            "detail": "Find papers discussing:\n1. Applications of butterfly transforms in sequence modeling\n2. Integration of context-adaptive compression with sequence models\n3. Training strategies for multi-scale adaptive neural networks",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Applications of butterfly transforms in sequence modeling\n2. Integration of context-adaptive compression with sequence models\n3. Training strategies for multi-scale adaptive neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 8/54 (Score: 0.99)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] George B Arfken and Hans J Weber. Mathematical methods for physicists. Elsevier Academic Press, 2005 . [3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In The International Conference on Machine Learning (ICML), pages 1120-1128, 2016. [4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [5] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019. [7] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2(1):24-47, 2020. [8] John P Boyd. Chebyshev and Fourier spectral methods.\n```\n\n#### 2. Hyena Hierarchy: Towards Larger Convolutional Language Models (Avg. Score: 0.96)\n\n*Michael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, S. Baccus, Y. Bengio, Stefano Ermon, Christopher R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 163  (*Influential: 28*)\n\n**TL;DR:** This work proposes Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating, and sets a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets.\n\n**Abstract:** Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.\n\n##### *Relevant Chunk: No. 36/63 (Score: 0.96)*\n\n```\nSpringer, 2016. N. Kitaev, \u0141. Kaiser, and A. Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020 . Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. Y. Li, H. Yang, E. R. Martin, K. L. Ho, and L. Ying. Butterfly factorization. Multiscale Modeling \\& Simulation, 13(2):714-732, 2015. Y. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. Z. Li, N. Kovachki, K. Azizzadenesheli, B. Liu, K. Bhattacharya, A. Stuart, and A. Anandkumar. Fourier neural operator for parametric partial differential equations.\n```\n\n#### 3. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.83)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.92)*\n\n```\nHowever, we found that these tricks are not necessary. For example, the learned transforms in Section 4.1 typically put weight at least 0.99 on a permutation. Initialization As the BP or BPBP construction is a product of many matrices, proper initialization is crucial to avoid exponential blowup in the size of the entries or condition numbers (i.e., the exploding/vanishing gradient problem [37]). We aim to initialize each butterfly factor to be close to unitary or orthogonal, so that the magnitude of the inputs and outputs to the transform are preserved. This is easy since each of the factors $B_{N}, \\ldots, B_{2}$ has exactly two nonzeros in each row and column; for example in the real case, initializing each entry of $B_{k}$ as $\\mathcal{N}(0,1 / 2)$ guarantees $\\mathbb{E} B_{k}^{*} B_{k}=I_{N}$. Comparison to related methods Some previous works have examined similar butterfly matrices in numerical algebra or machine learning [36, 20, 32, mainly motivated by trying to parametrize cheap orthogonal matrices. Our parametrization, motivated by the goal of learning recursive transforms, differs in several ways from all previous works: 1 . We explicitly model and learn a permutation matrix $P$. 2. Our relaxation does not enforce the matrix to be orthogonal. 3. Our butterfly factors are ordered so that closer elements interact first (Figure 1), whereas some works (e.g. [32]) reverse the order. 4. Every work has a different weight-tying scheme; ours ties the blocks in each butterfly factor, leading to fewer parameters and a tighter recursive interpretation than for example [20]. Our main baseline for deep learning experiments is Thomas et al. 42, who define a special matrix class with a complicated recursive algorithm. While our BP method and theirs share some overlap (e.g., they both capture circulant matrices), they have a distinct parametrization, and the exact relation between the BP hierarchy and their LDR-SD or LDR-TD classes is unknown. From a practical standpoint, BP is significantly faster and simpler to implement than their methods. ### 3.3 Expressivity and the butterfly hierarchy\n\nThe butterfly matrix $B$ has a total of $4 N$ learnable parameters (the butterfly factors $B_{N}, B_{N / 2}, \\ldots, B_{2}$ have $2 N, N, \\ldots, 4$ entries respectively). The overall permutation $P$ has $3 \\log _{2} N$ learnable parameters; we can also tie the logits of the $\\log _{2} N$ probabilistic permutations-reflecting the fact that for some algorithms the reduction from size $N$ to $N / 2$ is self-similar to the reduction from size $N / 2^{k}$ to $N / 2^{k+1}$ reducing this to just 3 parameters. We can define a natural hierarchy of matrix classes built on the BP primitive. This hierarchy covers a spectrum ranging from extremely structured matrices with a linear number of parameters, to the entire space of square matrices. Definition 1. For any dimension $N$, let $(\\mathrm{BP})_{r}^{k}(k, r \\in \\mathbb{N})$ denote the classes of matrices that can be expressed as\n\n$$\nS\\left(\\prod_{i=1}^{k} B_{i} P_{i}\\right) S^{T}\n$$\n\nwhere each $B_{i} P_{i} \\in \\mathbb{F}^{r N \\times r N}$ is a BP module as in equation (2), and $S \\in \\mathbb{F}^{N \\times r N}=\\left[\\begin{array}{llll}I_{N} & 0 & \\ldots & 0\\end{array}\\right]$ (that is, $S$ and $S^{T}$ select the upper left $N \\times N$ entries of the BP product matrix). The subscript $r$ is understood to be 1 if omitted. Note that the BP and BPBP classes are equivalent to (BP) ${ }^{1}$ and (BP) ${ }^{2}$ respectively. We remark that $B$ and $P$ are both capable of being the identity, and thus $(\\mathrm{BP})^{k} \\subseteq(\\mathrm{BP})^{k+1}$. The BP hierarchy is expressive enough to theoretically represent many important transforms with low depth, as well as all matrices with linear depth:\n\nProposition 1. $(B P)^{1}$ captures the fast Fourier transform, the fast Hadamard transform, and their inverses exactly. $(B P)^{2}$ captures the DCT, DST, and convolution exactly. All $N \\times N$ matrices are contained in $(B P)_{2}^{4 N+10}$. Proposition 1 is shown in Appendix B. We suggest some additional conjectures about the expressiveness of the BP hierarchy in Appendix D. Even though the BP parameterization is expressive, it still retains the learnability characteristic of compressed parameterizations. In fact, neural networks comprising layers of BP and BPBP matrices still have VC dimension that is almost linear in the number of parameters (Appendix B), similar to networks with fully-connected layers [1, 17] and LDR 42, which implies a corresponding sample complexity bound. ## 4 Empirical Evaluation\n\nWe evaluate the proposed approach to verify that our butterfly parameterization can both recover fast transforms and be integrated as an effective component in ML pipelines ${ }^{3}$, In Section 4.1, we confirm that it automatically learns the fast algorithms for many discrete transforms commonly used in signal processing and machine learning. Section 4.2 further shows that it can be a useful component to increase the performance of deep learning models while ensuring fast multiplication and few parameters by design. ### 4.1 Discrete Transforms\n\nBelow we list several important classes of structured matrices. Some of them are directly captured by our parametrization and we expect that they can be recovered close to perfectly, thus providing a $O(N \\log N)$ algorithm that closely approximates the naive $O\\left(N^{2}\\right)$ matrix multiplication. Others are not perfectly captured by the BPBP class but still have recursive structure; for these, we expect that our method reconstructs them better than standard matrix compression methods (sparse, low-rank, and combinations) can. Transforms We describe the matrices we evaluate on and their applications; a standard reference is Proakis 38.\n```\n\n##### *Relevant Chunk: No. 20/35 (Score: 0.74)*\n\n```\nIn Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 244-252, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/le13.html. [25] Le Magoarou, L. and Gribonval, R. Chasing butterflies: In search of efficient dictionaries. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3287-3291, April 2015. doi: $10.1109 /$ ICASSP.2015.7178579. [26] Le Magoarou, L. and Gribonval, R. Flexible multilayer sparse approximations of matrices and applications. IEEE Journal of Selected Topics in Signal Processing, 10(4):688-700, June 2016. ISSN 1932-4553. doi: 10.1109/JSTSP.2016.2543461. [27] Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765-6816, 2017 . [28] Li, Y., Yang, H., Martin, E. R., Ho, K. L., and Ying, L. Butterfly factorization. Multiscale Modeling G8 Simulation, 13(2):714-732, 2015. [29] Mairal, J., Ponce, J., Sapiro, G., Zisserman, A., and Bach, F. R. Supervised dictionary learning. In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L. (eds.), Advances in Neural Information Processing Systems 21, pp. 1033-1040. Curran Associates, Inc., 2009. URL http://papers.nips.cc/ paper/3448-supervised-dictionary-learning.pdf\n[30] Makhoul, J. A fast cosine transform in one and two dimensions. IEEE Transactions on Acoustics, Speech, and Signal Processing, 28(1):27-34, February 1980. ISSN 0096-3518. doi: 10.1109/TASSP.1980.1163351. [31] Mena, G., Belanger, D., Linderman, S., and Snoek, J. Learning latent permutations with GumbelSinkhorn networks.\n```\n\n#### 4. BP-Transformer: Modelling Long-Range Context via Binary Partitioning (Avg. Score: 0.10)\n\n*Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, Zheng Zhang*\n\n**Published in:** arXiv.org (2019)\t**Cited by** 73  (*Influential: 4*)\n\n**TL;DR:** Ad adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), BP-Transformer (BPT for short) is proposed, which has a superior performance for long text than previous self-attention models.\n\n**Abstract:** The Transformer model is widely successful on many natural language processing tasks. However, the quadratic complexity of self-attention limit its application on long text. In this paper, adopting a fine-to-coarse attention mechanism on multi-scale spans via binary partitioning (BP), we propose BP-Transformer (BPT for short). BPT yields $O(k\\cdot n\\log (n/k))$ connections where $k$ is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity. A series of experiments on text classification, machine translation and language modeling shows BPT has a superior performance for long text than previous self-attention models. Our code, hyperparameters and CUDA kernels for sparse attention are available in PyTorch.\n\n##### *Relevant Chunk: No. 13/20 (Score: 0.10)*\n\n```\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. 2017. Hierarchical multiscale recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. Zihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional sequence to sequence learning. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 1243-1252.\n```\n\n\n\n---\n## Found 3 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: butterfly transform sequence models context-adaptive compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 3 related papers from Semantic Scholar\n\n#### 1. LoCoCo: Dropping In Convolutions for Long Context Compression\n\n*From Search Query: butterfly transform sequence models context-adaptive compression*\n\n*Ruisi Cai, Yuandong Tian, Zhangyang Wang, Beidi Chen*\n\n**Abstract:** This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward\"drop-in\"integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 2. Distilling Linguistic Context for Language Model Compression\n\n*From Search Query: butterfly transform sequence models context-adaptive compression*\n\n*Geondo Park, Gyeongman Kim, Eunho Yang*\n\n**TL;DR:** This paper presents a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation.\n\n**Abstract:** A computationally expensive and memory intensive neural network lies behind the recent success of language representation learning. Knowledge distillation, a major technique for deploying such a vast language model in resource-scarce environments, transfers the knowledge on individual word representations learned without restrictions. In this paper, inspired by the recent observations that language representations are relatively positioned and have more semantic knowledge as a whole, we present a new knowledge distillation objective for language representation learning that transfers the contextual knowledge via two types of relationships across representations: Word Relation and Layer Transforming Relation. Unlike other recent distillation techniques for the language models, our contextual distillation does not have any restrictions on architectural changes between teacher and student. We validate the effectiveness of our method on challenging benchmarks of language understanding tasks, not only in architectures of various sizes but also in combination with DynaBERT, the recently proposed adaptive size pruning method.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 30  (*Influential: 6*)\n\n#### 3. Manifold-Preserving Transformers are Effective for Short-Long Range Encoding\n\n*From Search Query: butterfly transform sequence models context-adaptive compression*\n\n*Ayan Sengupta, Md. Shad Akhtar, Tanmoy Chakraborty*\n\n**TL;DR:** This work proposes TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens and proposes a simple alternative to dot-product attention to ensure Lipschitz continuity, and highlights the shortcomings of multi-head self-attention from the statistical physics viewpoint.\n\n**Abstract:** Multi-head self-attention-based Transformers have shown promise in different learning tasks. Albeit these models exhibit significant improvement in understanding short-term and long-term contexts from sequences, encoders of Transformers and their variants fail to preserve layer-wise contextual information. Transformers usually project tokens onto sparse manifolds and fail to preserve mathematical equivalence among the token representations. In this work, we propose TransJect, an encoder model that guarantees a theoretical bound for layer-wise distance preservation between a pair of tokens. We propose a simple alternative to dot-product attention to ensure Lipschitz continuity. This allows TransJect to learn injective mappings to transform token representations to different manifolds with similar topology and preserve Euclidean distance between every pair of tokens in subsequent layers. Evaluations across multiple benchmark short- and long-sequence classification tasks show maximum improvements of 6.8% and 5.9%, respectively, over the variants of Transformers. Additionally, TransJect displays 79% better performance than Transformer on the language modeling task. We further highlight the shortcomings of multi-head self-attention from the statistical physics viewpoint. Although multi-head self-attention was incepted to learn different abstraction levels within the networks, our empirical analyses suggest that different attention heads learn randomly and unorderly. In contrast, TransJect adapts a mixture of experts for regularization; these experts are more orderly and balanced and learn different sparse representations from the input sequences. TransJect exhibits very low entropy and can be efficiently scaled to larger depths.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the keywords and details provided, here are some relevant findings and references:\n\n### Applications of Butterfly Transforms in Sequence Modeling\n\nButterfly transforms have been explored for their efficiency in learning fast algorithms for linear transformations, which can be particularly useful in sequence modeling.\n\n- The work by Dao et al., \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations,\" presents a method to learn efficient linear transformations using butterfly factorizations. This approach can be integrated into sequence models to enhance their efficiency and scalability. For instance, butterfly factorizations can be used to speed up the computation of attention mechanisms in Transformer-based models, which are commonly used in autoregressive language models.\n\n### Integration of Context-Adaptive Compression with Sequence Models\n\nContext-adaptive compression techniques can be beneficial for reducing the computational and memory requirements of sequence models.\n\n- The concept of context-adaptive entropy models, as discussed in the context of image compression, can be adapted for sequence models. These models exploit different types of contexts to allocate bits based on importance, which can help in compressing less important parts of the sequence while preserving critical information. This approach can be integrated with autoregressive models to optimize memory usage and computational efficiency[Analysis Note].\n\n### Training Strategies for Multi-Scale Adaptive Neural Networks\n\nTraining multi-scale adaptive neural networks requires careful consideration of several factors.\n\n- **Multi-Stage Training Approach**: Models like GLEN, which use a multi-stage event detection approach, can be adapted for training autoregressive models. This involves breaking down the training process into multiple stages, each focusing on different aspects of the sequence, such as local and global features[Analysis Note].\n  \n- **Hierarchical Feature Integration**: Techniques from PointDAN, which align global and local features in a multi-level framework, can be applied to autoregressive models. This involves using self-adaptive nodes to process local features and ensuring alignment between local and global features[Analysis Note].\n\n- **Handling Heterogeneous Inputs**: The challenges faced by models like MUSIED in handling heterogeneous inputs highlight the need for robust training strategies. This can involve multi-view learning, as seen in Event-Radar, to improve the robustness of the model to variant inputs[Analysis Note].\n\n### Additional Insights\n\n- **Efficient Transformations**: Butterfly factorizations can be used to optimize the computational complexity of attention mechanisms and other linear transformations within autoregressive models, making them more efficient and scalable.\n\n- **Context-Adaptive Bit Allocation**: Implementing context-adaptive bit allocation can help in optimizing the memory usage of the model by allocating more bits to important parts of the sequence and fewer bits to less important parts[Analysis Note].\n\nBy integrating these insights, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage multi-scale adaptive processing, context-adaptive compression, and efficient linear transformations.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\n\nThe search results reveal several promising directions for combining eventvq and hiergpt with insights from recent advances in:\n- Multi-scale event detection systems\n- Context-adaptive compression techniques\n- Hierarchical feature integration\n- Memory-efficient architectures\n\nKey findings:\n\na) Event Detection Integration:\n- GLEN's multi-stage detection approach shows promise for handling large-scale event processing\n- MUSIED demonstrates challenges in handling heterogeneous inputs\n- Event-Radar's multi-view learning could improve robustness\n\nb) Scale-Adaptive Processing:\n- PointDAN's multi-scale domain adaptation techniques could be adapted for sequence processing\n- Context-adaptive entropy models show benefits of bit-allocation based on importance\n- Butterfly factorizations enable efficient learned transformations\n\n2. Useful References with Excerpts:\n\nFrom \"GLEN: General-Purpose Event Detection\":\n\"...multi-stage event detection model CEDAR specifically designed to handle the large ontology size...\" \n- Relevant for designing scalable event detection mechanisms\n\nFrom \"Context-adaptive Entropy Model\":\n\"...exploits two types of contexts, bit-consuming contexts and bit-free contexts, distinguished based upon whether additional bit allocation is required...\"\n- Provides insights for adaptive compression strategies\n\nFrom \"PointDAN\":\n\"...jointly aligns the global and local features in multi-level... Self-Adaptive (SA) node module with an adjusted receptive field...\"\n- Offers framework for multi-scale feature alignment\n\n3. Key Insights:\n\na) Multi-Scale Integration:\n- Use hierarchical event detection to guide compression decisions\n- Implement context-adaptive bit allocation based on event importance\n- Leverage butterfly factorizations for efficient transformations\n\nb) Memory Efficiency:\n- Apply bit-consuming vs bit-free context distinction\n- Use self-adaptive nodes for local feature processing\n- Implement multi-view fusion for robust representations\n\nc) Training Considerations:\n- Need careful handling of heterogeneous inputs\n- Consider multi-stage training approach\n- Balance local and global feature alignment\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific techniques for efficient butterfly transformations in sequence models\n2. Methods for integrating context-adaptive compression with event detection\n3. Training strategies for multi-scale adaptive systems\n\n5. References:\n\n[1] Zhan, Q., Li, S., Conger, K., Palmer, M., Ji, H., & Han, J. (2023). \"GLEN: General-Purpose Event Detection for Thousands of Types.\" EMNLP 2023.\n\n[2] Lee, J., Cho, S., & Beack, S. (2018). \"Context-adaptive Entropy Model for End-to-end Optimized Image Compression.\" ICLR 2018.\n\n[3] Qin, C., You, H., Wang, L., Kuo, C.C.J., & Fu, Y. (2019). \"PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation.\" NeurIPS 2019.\n\n[4] Dao, T., Gu, A., Eichhorn, M., Rudra, A., & R\u00e9, C. (2019). \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations.\" ICML 2019.\n\n[5] Ma, Z., Luo, M., Guo, H., Zeng, Z., Hao, Y., & Zhao, X. (2024). \"Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection.\" ACL 2024."
        },
        {
            "ready": false,
            "query": "polynomial projections event detection memory optimization",
            "detail": "Find papers discussing:\n1. Integration of polynomial projections with event detection mechanisms\n2. Memory optimization techniques for hierarchical neural networks\n3. Training strategies for systems with multiple adaptive components",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of polynomial projections with event detection mechanisms\n2. Memory optimization techniques for hierarchical neural networks\n3. Training strategies for systems with multiple adaptive components\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.11)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 1/54 (Score: 0.11)*\n\n```\n# HiPPO: Recurrent Memory with Optimal Polynomial Projections \n\nAlbert Gu* ${ }^{* \\dagger}$, Tri Dao* ${ }^{* \\dagger}$, Stefano Ermon ${ }^{\\dagger}$, Atri Rudra ${ }^{\\ddagger}$, and Christopher R\u00e9 ${ }^{\\dagger}$<br>${ }^{\\dagger}$ Department of Computer Science, Stanford University<br>${ }^{\\ddagger}$ Department of Computer Science and Engineering, University at Buffalo, SUNY<br>\\{albertgu,trid\\}@stanford.edu, ermon@cs.stanford.edu, atri@buffalo.edu,<br>chrismre@cs.stanford.edu\n\nOctober 26, 2020\n\n\n#### Abstract\n\nA central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed.\n```\n\n#### 2. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.07)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 14/56 (Score: 0.07)*\n\n```\narXiv preprint arXiv:2312.00752, 2023. A. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487, 2020.\n```\n\n#### 3. Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions (Avg. Score: 0.05)\n\n*Stefano Massaroli, Michael Poli, Daniel Y. Fu, Hermann Kumbong, Rom N. Parnichkun, Aman Timalsina, David W. Romero, Quinn McIntyre, Beidi Chen, A. Rudra, Ce Zhang, Christopher R\u00e9, Stefano Ermon, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 10  (*Influential: 2*)\n\n**TL;DR:** This paper seeks to enable compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation, and introduces architectural improvements to convolution-based layers such as Hyena.\n\n**Abstract:** Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.\n\n##### *Relevant Chunk: No. 12/64 (Score: 0.05)*\n\n```\nIn: (2020). arXiv: 2101.00027 (cit. on pp. $2,8,35$ ). [12] Albert Gu et al. \"Hippo: Recurrent memory with optimal polynomial projections\". In: Advances in Neural Information Processing Systems 33 (2020), pp.\n```\n\n#### 4. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.04)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 14/37 (Score: 0.04)*\n\n```\n[9] P. M. Cohn. Further algebra and applications. Springer-Verlag London, Ltd., London, 2003 . [10] E Brian Davies and Mildred Hager. Perturbations of Jordan matrices. Journal of Approximation Theory, 156(1):82-94, 2009. [11] E.B. Davies. Approximate diagonalization. SIAM journal on matrix analysis and applications, 29(4):1051-1064, 2008. [12] James Demmel. The componentwise distance to the nearest singular matrix. SIAM Journal on Matrix Analysis and Applications, 13(1):10-19, 1992. [13] N Benjamin Erichson, Omri Azencot, Alejandro Queiruga, Liam Hodgkinson, and Michael W Mahoney. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021. [14] N Benjamin Erichson, Soon Hoe Lim, and Michael W Mahoney. Gated recurrent neural networks with weighted time-delay feedback. arXiv preprint arXiv:2212.00228, 2022. [15] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections.\n```\n\n#### 5. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.02)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 17/44 (Score: 0.02)*\n\n```\nIn International conference on machine learning, pages 843-852. PMLR, 2015. [55] Arieh Iserles. A first course in the numerical analysis of differential equations. 44. Cambridge university press, 2009. [56] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [57] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [58] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. On the universality of linear recurrences followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023. [59] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. [60] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2017. [61] Tao Lei, Yu Zhang, Sida Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4470-4481, 2018. [62] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33:1474-1487, 2020. [63] Guy Blelloch. Prefix sums and their applications.\n```\n\n\n\n---\n## Found 0 related papers from 0 external sources\n\n\n\nYour 1 raw search queries input to the search frame: polynomial projections event detection memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on the integration of polynomial projections with event detection mechanisms, memory optimization techniques, and training strategies for systems with multiple adaptive components, here are some key points and relevant information:\n\n## Integration of Polynomial Projections with Event Detection Mechanisms\n\n- The HiPPO framework, mentioned in the analysis, is particularly relevant. It introduces a formal framework for recurrent memory using optimal polynomial projections, which can be integrated with event-driven processing. This approach allows for robust handling of varying timescales and can be combined with event detection to guide context compression and adaptive processing.\n\n## Memory Optimization Techniques for Hierarchical Neural Networks\n\n- **Rematerialization and Checkpointing**: Techniques like rematerialization or checkpointing can significantly reduce memory usage during training. This involves storing only a subset of intermediate activations and recomputing the necessary ones during the backward pass, which is crucial for training larger models with limited hardware memory.\n\n- **Adaptive Fusion and Context Compression**: The LoCoCo method, which leverages a data-driven adaptive fusion technique, can be applied to blend previous key-value pairs with incoming tokens. This approach minimizes the loss of contextual information and reduces memory requirements, making it suitable for hierarchical neural networks[Analysis Note].\n\n- **Polynomial Projections**: The HiPPO framework's use of polynomial projections for memory updates can scale through time to remember all history without priors on the timescale. This is beneficial for handling long-term dependencies and optimizing memory usage.\n\n## Training Strategies for Systems with Multiple Adaptive Components\n\n- **Butterfly Factorizations**: Using butterfly factorizations for efficient linear transformations can help in optimizing computational efficiency. This method can be integrated with event detection to guide context compression and reduce computational complexity to O(N log N)[Analysis Note].\n\n- **Selective Processing Based on Event Importance**: Implementing hierarchical memory updates based on the importance of events can help in selective processing. This approach ensures that only relevant information is processed in detail, reducing computational and memory overhead[Analysis Note].\n\n- **Multi-Scale Feature Integration**: Combining polynomial projections with multi-scale feature integration can help in handling variable-length dependencies and improving the robustness of the model. This approach allows the model to capture diverse relationships and maintain performance across different timescales[Analysis Note].\n\n## Additional Considerations\n\n- **Prompt Tuning and Continuous Prompts**: For fine-tuning and adapting the model to specific tasks, techniques like prompt tuning can be efficient. This involves optimizing a continuous vector within the embedding space, which can be particularly useful when only the continuous prompts are tuned, keeping the rest of the model's parameters unchanged.\n\nBy integrating these strategies, researchers can develop an autoregressive language model that is more efficient in terms of memory usage, computational power, and scalability, while also improving its performance in handling variable-length dependencies and adaptive processing.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\n\nThe search results reveal promising directions for combining eventvq and hiergpt through:\n\na) Efficient Linear Transformations:\n- Butterfly factorizations provide efficient algorithms for linear transforms\n- Can be integrated with event detection for adaptive processing\n- Enables O(N log N) complexity for many operations\n\nb) Context-Adaptive Processing:\n- LoCoCo demonstrates effective context compression through adaptive fusion\n- Dynamic integration of previous and incoming information\n- Maintains performance while reducing memory requirements\n\nc) Multi-Scale Integration:\n- HiPPO framework shows benefits of polynomial projections for memory\n- Enables robust handling of varying timescales\n- Can be combined with event-driven processing\n\n2. Useful References with Excerpts:\n\nFrom \"Learning Fast Algorithms for Linear Transforms\":\n\"...introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms...\"\n- Relevant for optimizing computational efficiency\n\nFrom \"LoCoCo\":\n\"...leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information...\"\n- Provides insights for memory-efficient processing\n\nFrom \"HiPPO\":\n\"...formal framework yields a new memory update mechanism that scales through time to remember all history, avoiding priors on the timescale...\"\n- Useful for handling variable-length dependencies\n\n3. Key Insights:\n\na) Integration Strategy:\n- Use butterfly factorizations for efficient linear transformations\n- Apply event detection to guide context compression\n- Implement hierarchical memory updates based on importance\n\nb) Memory Optimization:\n- Adaptive fusion of context information\n- Scale-specific compression based on events\n- Polynomial projections for long-term memory\n\nc) Computational Efficiency:\n- O(N log N) complexity through butterfly transforms\n- Selective processing based on event importance\n- Multi-scale feature integration\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific techniques for combining event detection with polynomial projections\n2. Methods for adaptive context compression in hierarchical systems\n3. Training stability with multiple adaptive mechanisms\n\n5. References:\n\n[1] Dao, T., Gu, A., Eichhorn, M., Rudra, A., & R\u00e9, C. (2019). \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations.\" International Conference on Machine Learning.\n\n[2] Cai, R., Tian, Y., Wang, Z., & Chen, B. (2024). \"LoCoCo: Dropping In Convolutions for Long Context Compression.\" International Conference on Machine Learning.\n\n[3] Gu, A., Dao, T., Ermon, S., Rudra, A., & R\u00e9, C. (2020). \"HiPPO: Recurrent Memory with Optimal Polynomial Projections.\" Neural Information Processing Systems.\n\n[4] Poli, M., et al. (2023). \"Hyena Hierarchy: Towards Larger Convolutional Language Models.\" International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "training stability multiple adaptive components, empirical results hierarchical architectures, hardware optimization memory compression",
            "detail": "Find papers discussing:\n1. Training stability techniques for models with multiple adaptive components, focusing on empirical results and practical guidelines\n2. Hardware optimization strategies for polynomial computations in neural networks\n3. Memory compression techniques in hierarchical architectures with empirical evaluations\n4. Integration methods for complex adaptive systems with multiple components\n5. Practical implementations of event-driven processing in neural networks",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Training stability techniques for models with multiple adaptive components, focusing on empirical results and practical guidelines\n2. Hardware optimization strategies for polynomial computations in neural networks\n3. Memory compression techniques in hierarchical architectures with empirical evaluations\n4. Integration methods for complex adaptive systems with multiple components\n5. Practical implementations of event-driven processing in neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.08)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.08)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.05)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.05)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n#### 3. Liquid Structural State-Space Models (Avg. Score: 0.02)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 28/54 (Score: 0.02)*\n\n```\nJ. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982. J. Jia and A. R. Benson. Neural jump stochastic differential equations. In Advances in Neural Information Processing Systems, pages 9843-9854, 2019. L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and Y. Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation, 31(4):765-783, 2019. A. Kag, Z. Zhang, and V. Saligrama. Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients? In International Conference on Learning Representations, 2019. R. KALMAN. A new approach to linear filtering and prediction problems.\n```\n\n#### 4. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.02)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 6/40 (Score: 0.02)*\n\n```\non pp. 1, 2, 9, 16). [3] Colin White et al. \"Neural architecture search: Insights from 1000 papers\". In: arXiv preprint arXiv:2301.08727 (2023) (cit.\n```\n\n#### 5. Scalable MatMul-free Language Modeling (Avg. Score: 0.01)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 10/27 (Score: 0.01)*\n\n```\narXiv preprint arXiv:1602.02830, 2016. [8] Sreyes Venkatesh, Razvan Marinescu, and Jason K Eshraghian. Squat: Stateful quantizationaware training in recurrent spiking neural networks. arXiv preprint arXiv:2404.19668, 2024. [9] Jason K Eshraghian, Xinxin Wang, and Wei D Lu. Memristor-based binarized spiking neural networks: Challenges and applications. IEEE Nanotechnology Magazine, 16(2):14-23, 2022. [10] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. [11] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: training stability multiple adaptive components, empirical results hierarchical architectures, hardware optimization memory compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Enhancing Idiomatic Representation in Multiple Languages via an Adaptive Contrastive Triplet Loss\n\n*From Search Query: training stability multiple adaptive components*\n\n*Wei He, M. Idiart, Carolina Scarton, Aline Villavicencio*\n\n**TL;DR:** This paper proposes an approach to model idiomaticity effectively using a triplet loss that incorporates the asymmetric contribution of components words to an idiomatic meaning for training language models by using adaptive contrastive learning and resampling miners to build an idiomatic-aware learning objective.\n\n**Abstract:** Accurately modeling idiomatic or non-compositional language has been a longstanding challenge in Natural Language Processing (NLP). This is partly because these expressions do not derive their meanings solely from their constituent words, but also due to the scarcity of relevant data resources, and their impact on the performance of downstream tasks such as machine translation and simplification. In this paper we propose an approach to model idiomaticity effectively using a triplet loss that incorporates the asymmetric contribution of components words to an idiomatic meaning for training language models by using adaptive contrastive learning and resampling miners to build an idiomatic-aware learning objective. Our proposed method is evaluated on a SemEval challenge and outperforms previous alternatives significantly in many metrics.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. An Adaptive Kernel Approach to Federated Learning of Heterogeneous Causal Effects\n\n*From Search Query: training stability multiple adaptive components*\n\n*Thanh Vinh Vo, Arnab Bhattacharyya, Young Lee, T. Leong*\n\n**TL;DR:** An adaptive transfer algorithm that learns the similarities among the data sources by utilizing Random Fourier Features to disentangle the loss function into multiple components, each of which is associated with a data source.\n\n**Abstract:** We propose a new causal inference framework to learn causal effects from multiple, decentralized data sources in a federated setting. We introduce an adaptive transfer algorithm that learns the similarities among the data sources by utilizing Random Fourier Features to disentangle the loss function into multiple components, each of which is associated with a data source. The data sources may have different distributions; the causal effects are independently and systematically incorporated. The proposed method estimates the similarities among the sources through transfer coefficients, and hence requiring no prior information about the similarity measures. The heterogeneous causal effects can be estimated with no sharing of the raw training data among the sources, thus minimizing the risk of privacy leak. We also provide minimax lower bounds to assess the quality of the parameters learned from the disparate sources. The proposed method is empirically shown to outperform the baselines on decentralized data sources with dissimilar distributions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 3. SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization\n\n*From Search Query: training stability multiple adaptive components*\n\n*Amir Hertz, Or Perel, R. Giryes, O. Sorkine-Hornung, D. Cohen-Or*\n\n**TL;DR:** A spatially adaptive progressive encoding scheme for input signals of MLP networks is presented, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing.\n\n**Abstract:** Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 60  (*Influential: 6*)\n\n#### 4. Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons\n\n*From Search Query: empirical results hierarchical architectures*\n\n*Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, Zhaopeng Tu*\n\n**TL;DR:** This work proposes to further enhance the strength of hybrid models with an advanced variant of RNNs \u2013 Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition.\n\n**Abstract:** Recent studies have shown that a hybrid of self-attention networks (SANs) and recurrent neural networks RNNs outperforms both individual architectures, while not much is known about why the hybrid models work. With the belief that modeling hierarchical structure is an essential complementary between SANs and RNNs, we propose to further enhance the strength of hybrid models with an advanced variant of RNNs \u2013 Ordered Neurons LSTM (ON-LSTM), which introduces a syntax-oriented inductive bias to perform tree-like composition. Experimental results on the benchmark machine translation task show that the proposed approach outperforms both individual architectures and a standard hybrid model. Further analyses on targeted linguistic evaluation and logical inference tasks demonstrate that the proposed approach indeed benefits from a better modeling of hierarchical structure.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 5. Learning Cut Selection for Mixed-Integer Linear Programming via Hierarchical Sequence Model\n\n*From Search Query: empirical results hierarchical architectures*\n\n*Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, M. Yuan, Jianguo Zeng, Yongdong Zhang, Feng Wu*\n\n**TL;DR:** A novel hierarchical sequence model (HEM) is proposed to learn cut selection policies via reinforcement learning that significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs, including MIPLIB 2017.\n\n**Abstract:** Cutting planes (cuts) are important for solving mixed-integer linear programs (MILPs), which formulate a wide range of important real-world applications. Cut selection -- which aims to select a proper subset of the candidate cuts to improve the efficiency of solving MILPs -- heavily depends on (P1) which cuts should be preferred, and (P2) how many cuts should be selected. Although many modern MILP solvers tackle (P1)-(P2) by manually designed heuristics, machine learning offers a promising approach to learn more effective heuristics from MILPs collected from specific applications. However, many existing learning-based methods focus on learning which cuts should be preferred, neglecting the importance of learning the number of cuts that should be selected. Moreover, we observe from extensive empirical results that (P3) what order of selected cuts should be preferred has a significant impact on the efficiency of solving MILPs as well. To address this challenge, we propose a novel hierarchical sequence model (HEM) to learn cut selection policies via reinforcement learning. Specifically, HEM consists of a two-level model: (1) a higher-level model to learn the number of cuts that should be selected, (2) and a lower-level model -- that formulates the cut selection task as a sequence to sequence learning problem -- to learn policies selecting an ordered subset with the size determined by the higher-level model. To the best of our knowledge, HEM is the first method that can tackle (P1)-(P3) in cut selection simultaneously from a data-driven perspective. Experiments show that HEM significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs, including MIPLIB 2017. Moreover, experiments demonstrate that HEM well generalizes to MILPs that are significantly larger than those seen during training.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 29  (*Influential: 6*)\n\n#### 6. Rethinking Decision Transformer via Hierarchical Reinforcement Learning\n\n*From Search Query: empirical results hierarchical architectures*\n\n*Yi Ma, Chenjun Xiao, Hebin Liang, Jianye Hao*\n\n**TL;DR:** This work introduces a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL, and shows DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discusses the potential failure of these choices.\n\n**Abstract:** Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the transformer architecture in reinforcement learning (RL). However, a notable limitation of DT is its reliance on recalling trajectories from datasets, losing the capability to seamlessly stitch sub-optimal trajectories together. In this work we introduce a general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL. At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level policy subsequently generates an action conditioned on the given prompt. We show DT emerges as a special case of this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these choices. Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to enable the stitching ability, which further leads to the development of new offline RL algorithms. Our empirical results clearly show that the proposed algorithms significantly surpass DT on several control and navigation benchmarks. We hope our contributions can inspire the integration of transformer architectures within the field of RL.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 7. Hardware-Aware Compression with Random Operation Access Specific Tile (ROAST) Hashing\n\n*From Search Query: hardware optimization memory compression*\n\n*Aditya Desai, K. Zhou, Anshumali Shrivastava*\n\n**TL;DR:** With ROAST, the authors can efficiently train and deploy the model using a much smaller memory footprint in text and image classification tasks, and introduce global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest.\n\n**Abstract:** Advancements in deep learning are often associated with increasing model sizes. Training and deploying large models require sophisticated hardware and incur significantly higher costs. Thus, model compression is a widely explored approach to solving the problem. However, SOTA techniques fall short in one or more desirable aspects of compression - for instance, pruning does not reduce memory for training, quantization can only provide up to 32 \u00d7 compression, Hashed-Net is cache-inefficient, etc. This paper proposes a model-agnostic, cache-friendly, and hardware-aware model compression approach: Random Operation Access Specific Tile (ROAST) hashing. ROAST collapses the parameters by club-bing them through a lightweight mapping. While clubbing these parameters, ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern. ROAST is up to \u223c 25 \u00d7 faster to train and \u223c 50 \u00d7 faster to infer than the popular parameter sharing method HashedNet. Additionally, ROAST introduces global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest. With ROAST, we can efficiently train and deploy the model using a much smaller memory footprint ( \u223c 10 \u2212 100 \u00d7 lesser) in text and image classification tasks. ROAST-MM kernel implementation is open-source 1\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks\n\n*From Search Query: hardware optimization memory compression*\n\n*Y. Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi, Vikas Chandra, Yingyan Lin*\n\n**TL;DR:** This work opens up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy, and proposes a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient Dnns that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real- hardware efficiency.\n\n**Abstract:** Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 9. Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings\n\n*From Search Query: hardware optimization memory compression*\n\n*Edouard Yvinec, Arnaud Dapogny, K\u00e9vin Bailly*\n\n**TL;DR:** The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.\n\n**Abstract:** The massive interest in deep neural networks (DNNs) for both computer vision and natural language processing has been sparked by the growth in computational power. However, this led to an increase in the memory footprint, to a point where it can be challenging to simply load a model on commodity devices such as mobile phones. To address this limitation, quantization is a favored solution as it maps high precision tensors to a low precision, memory efficient format. In terms of memory footprint reduction, its most effective variants are based on codebooks. These methods, however, suffer from two limitations. First, they either define a single codebook for each tensor, or use a memory-expensive mapping to multiple codebooks. Second, gradient descent optimization of the mapping favors jumps toward extreme values, hence not defining a proximal search. In this work, we propose to address these two limitations. First, we initially group similarly distributed neurons and leverage the re-ordered structure to either apply different scale factors to the different groups, or map weights that fall in these groups to several codebooks, without any mapping overhead. Second, stemming from this initialization, we propose a joint learning of the codebook and weight mappings that bears similarities with recent gradient-based post-training quantization techniques. Third, drawing estimation from straight-through estimation techniques, we introduce a novel gradient update definition to enable a proximal search of the codebooks and their mappings. The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. An End-To-End-Trainable Iterative Network Architecture for Accelerated Radial Multi-Coil 2D Cine MR Image Reconstruction\n\n*From Search Query: training stability multiple adaptive components*\n\n*Christoph Kolbitsch, Tobias Schaeffter, Markus Haltmeier, Andreas Kofler*\n\n**Abstract:** Purpose: Iterative Convolutional Neural Networks (CNNs) which resemble unrolled learned iterative schemes have shown to consistently deliver state-of-the-art results for image reconstruction problems across different imaging modalities. However, because these methodes include the forward model in the architecture, their applicability is often restricted to either relatively small reconstruction problems or to problems with operators which are computationally cheap to compute. As a consequence, they have so far not been applied to dynamic non-Cartesian multi-coil reconstruction problems. Methods: In this work, we propose a CNN-architecture for image reconstruction of accelerated 2D radial cine MRI with multiple receiver coils. The network is based on a computationally light CNN-component and a subsequent conjugate gradient (CG) method which can be jointly trained end-to-end using an efficient training strategy. We investigate the proposed training-strategy and compare our method to other well-known reconstruction techniques with learned and non-learned regularization methods. Results: Our proposed method outperforms all other methods based on non-learned regularization. Further, it performs similar or better than a CNN-based method employing a 3D U-Net and a method using adaptive dictionary learning. In addition, we empirically demonstrate that even by training the network with only iteration, it is possible to increase the length of the network at test time and further improve the results. Conclusions: End-to-end training allows to highly reduce the number of trainable parameters of and stabilize the reconstruction network. Further, because it is possible to change the length of the network at test time, the need to find a compromise between the complexity of the CNN-block and the number of iterations in each CG-block becomes irrelevant.\n\n**Published:** 2021-02-01\n\n\n\n#### 2. Un-Mixing Test-Time Normalization Statistics: Combatting Label Temporal Correlation\n\n*From Search Query: training stability multiple adaptive components*\n\n*Behzad Bozorgtabar, Jean-Philippe Thiran, Guillaume Vray, Devavrat Tomar*\n\n**Abstract:** Recent test-time adaptation methods heavily rely on nuanced adjustments of batch normalization (BN) parameters. However, one critical assumption often goes overlooked: that of independently and identically distributed (i.i.d.) test batches with respect to unknown labels. This oversight leads to skewed BN statistics and undermines the reliability of the model under non-i.i.d. scenarios. To tackle this challenge, this paper presents a novel method termed 'Un-Mixing Test-Time Normalization Statistics' (UnMix-TNS). Our method re-calibrates the statistics for each instance within a test batch by mixing it with multiple distinct statistics components, thus inherently simulating the i.i.d. scenario. The core of this method hinges on a distinctive online unmixing procedure that continuously updates these statistics components by incorporating the most similar instances from new test batches. Remarkably generic in its design, UnMix-TNS seamlessly integrates with a wide range of leading test-time adaptation methods and pre-trained architectures equipped with BN layers. Empirical evaluations corroborate the robustness of UnMix-TNS under varied scenarios-ranging from single to continual and mixed domain shifts, particularly excelling with temporally correlated test data and corrupted non-i.i.d. real-world streams. This adaptability is maintained even with very small batch sizes or single instances. Our results highlight UnMix-TNS's capacity to markedly enhance stability and performance across various benchmarks. Our code is publicly available at https://github.com/devavratTomar/unmixtns.\n\n**Published:** 2024-01-16\n\n\n\n#### 3. VAE with a VampPrior\n\n*From Search Query: empirical results hierarchical architectures*\n\n*Max Welling, Jakub M. Tomczak*\n\n**Abstract:** Many different methods to train deep generative models have been introduced\nin the past. In this paper, we propose to extend the variational auto-encoder\n(VAE) framework with a new type of prior which we call \"Variational Mixture of\nPosteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture\ndistribution (e.g., a mixture of Gaussians) with components given by\nvariational posteriors conditioned on learnable pseudo-inputs. We further\nextend this prior to a two layer hierarchical model and show that this\narchitecture with a coupled prior and posterior, learns significantly better\nmodels. The model also avoids the usual local optima issues related to useless\nlatent dimensions that plague VAEs. We provide empirical studies on six\ndatasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,\nFrey Faces and Histopathology patches, and show that applying the hierarchical\nVampPrior delivers state-of-the-art results on all datasets in the unsupervised\npermutation invariant setting and the best results or comparable to SOTA\nmethods for the approach with convolutional networks.\n\n**Published:** 2017-05-19\n\n\n\n#### 4. OTOv3: Automatic Architecture-Agnostic Neural Network Training and Compression from Structured Pruning to Erasing Operators\n\n*From Search Query: empirical results hierarchical architectures*\n\n*Luming Liang, Ilya Zharkov, HsiangTao Wu, Zeyu Chen, Zhihui Zhu, Tianyu Ding, Tianyi Chen*\n\n**Abstract:** Compressing a predefined deep neural network (DNN) into a compact sub-network with competitive performance is crucial in the efficient machine learning realm. This topic spans various techniques, from structured pruning to neural architecture search, encompassing both pruning and erasing operators perspectives. Despite advancements, existing methods suffers from complex, multi-stage processes that demand substantial engineering and domain knowledge, limiting their broader applications. We introduce the third-generation Only-Train-Once (OTOv3), which first automatically trains and compresses a general DNN through pruning and erasing operations, creating a compact and competitive sub-network without the need of fine-tuning. OTOv3 simplifies and automates the training and compression process, minimizes the engineering efforts required from users. It offers key technological advancements: (i) automatic search space construction for general DNNs based on dependency graph analysis; (ii) Dual Half-Space Projected Gradient (DHSPG) and its enhanced version with hierarchical search (H2SPG) to reliably solve (hierarchical) structured sparsity problems and ensure sub-network validity; and (iii) automated sub-network construction using solutions from DHSPG/H2SPG and dependency graphs. Our empirical results demonstrate the efficacy of OTOv3 across various benchmarks in structured pruning and neural architecture search. OTOv3 produces sub-networks that match or exceed the state-of-the-arts. The source code will be available at https://github.com/tianyic/only_train_once.\n\n**Published:** 2023-12-15\n\n\n\n#### 5. Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases\n\n*From Search Query: hardware optimization memory compression*\n\n*Yuxiong He, Zhewei Yao, Reza Yazdani Aminabadi, Cheng Li, Xiaoxia Wu*\n\n**Abstract:** Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to $3\\times$ for throughput-oriented scenarios compared to the inference of FP16, and improves the SOTA BERT INT8 performance from FasterTransformer by up to $1.7\\times$. We provide insights into the failure cases when applying W4A4 to decoder-only models, and further explore the compatibility of INT4 quantization with other compression methods, like pruning and layer reduction.\n\n**Published:** 2023-01-27\n\n\n\n#### 6. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers\n\n*From Search Query: hardware optimization memory compression*\n\n*Yuxiong He, Conglong Li, Xiaoxia Wu, Minjia Zhang, Reza Yazdani Aminabadi, Zhewei Yao*\n\n**Abstract:** How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency.\n\n**Published:** 2022-06-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on training stability, hardware optimization, memory compression, and integration of complex adaptive systems, here are some relevant insights and references:\n\n## Training Stability Techniques for Models with Multiple Adaptive Components\n\n- The \"AdaBelief Optimizer\" is a relevant reference here. It adapts the stepsize according to the 'belief' in the current gradient direction, which helps in achieving fast convergence, good generalization, and training stability. This is particularly useful for models with multiple adaptive components.\n\n- The concept of using multiple discriminators during training, as seen in \"Multi-adversarial autoencoders,\" can also enhance stability and accelerate the convergence rate. This approach could be adapted for autoregressive models to improve training stability.\n\n## Hardware Optimization Strategies for Polynomial Computations in Neural Networks\n\n- For hardware optimization, the \"FlashAttention\" algorithm provides insights into efficient memory management. It uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, which can be applied to optimize polynomial computations in neural networks.\n\n- The \"Composite Learning Unit (CLU)\" framework, while not specifically focused on polynomial computations, discusses the importance of efficient use of computational resources. This could be extended to optimize hardware for polynomial computations by leveraging distributed learning and resource optimization techniques.\n\n## Memory Compression Techniques in Hierarchical Architectures with Empirical Evaluations\n\n- The \"FlashAttention\" algorithm also addresses memory efficiency by reducing the number of memory reads/writes. This IO-aware exact attention algorithm can be a model for developing memory compression techniques in hierarchical architectures.\n\n- The \"DenseMamba\" model, which integrates shallow-layer hidden states into deeper layers, provides insights into retaining fine-grained information while managing memory efficiently. This selective integration can be a strategy for memory compression in hierarchical architectures.\n\n## Integration Methods for Complex Adaptive Systems with Multiple Components\n\n- The \"Composite Learning Unit (CLU)\" and \"Composite Learning System (CLS)\" frameworks offer a cooperative setup where multiple CLUs work together to tackle complex tasks. This approach can be adapted to integrate multiple adaptive components in an autoregressive language model, leveraging the collective strengths of individual learning units.\n\n- The hierarchical learning framework proposed in the locomotor adaptation model combines stabilizing control, reinforcement learning, and performance-based memory updates. This hierarchical approach can be a model for integrating complex adaptive systems in language models, ensuring stability and adaptability.\n\n## Practical Implementations of Event-Driven Processing in Neural Networks\n\n- The HiPPOVQ proposal itself integrates event-driven processing with HiPPO polynomial projections and hierarchical vector quantization. This integration is crucial for guiding polynomial projection selection and optimizing hierarchical processing. The theoretical foundations and practical guidelines from this proposal can be directly applied to improve the autoregressive language model[Analysis Note].\n\nIn summary:\n\n- **Training Stability**: Use optimizers like AdaBelief and multi-adversarial training approaches to enhance stability.\n- **Hardware Optimization**: Apply efficient memory management techniques like FlashAttention and leverage distributed learning for resource optimization.\n- **Memory Compression**: Implement IO-aware algorithms and selective integration of hidden states to manage memory efficiently.\n- **Integration of Complex Adaptive Systems**: Utilize cooperative frameworks like CLU and CLS to integrate multiple adaptive components.\n- **Event-Driven Processing**: Integrate event detection with polynomial projections and hierarchical processing as outlined in the HiPPOVQ proposal[Analysis Note].\n",
            "analysis": "Analysis Note for HiPPOVQ Proposal - Round 3\n\n1. Summary of Analysis:\nAfter three rounds of search, we have gathered substantial information about the key components and innovations of HiPPOVQ. The proposal appears to be novel in its integration of event-driven processing with HiPPO polynomial projections and hierarchical vector quantization. The search results have revealed both promising aspects and potential challenges.\n\n2. Useful References with Key Excerpts:\n\nFrom \"How to Train Your HiPPO\" (Gu et al., 2022):\n\"We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials\"\n- Critical for understanding theoretical foundations and stability\n\nFrom \"FlashAttention\" (Dao et al., 2022):\n\"IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\"\n- Important for efficient implementation of memory management\n\nFrom \"AdaBelief Optimizer\" (Zhuang et al., 2020):\n\"Adapt the stepsize according to the 'belief' in the current gradient direction [...] simultaneously achieve fast convergence, good generalization, and training stability\"\n- Relevant for handling multiple adaptive components\n\nFrom \"DenseMamba\" (He et al., 2024):\n\"Selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output\"\n- Insights for hierarchical integration\n\n3. Key Insights:\n\na) Theoretical Foundation:\n- HiPPO framework provides mathematically principled approach to long-range modeling\n- Event detection can guide polynomial projection selection\n- Hierarchical processing can be optimized through careful parameterization\n\nb) Implementation Considerations:\n- Memory efficiency requires balancing compression and representation quality\n- Training stability needs special attention with multiple adaptive components\n- Hardware optimization opportunities through structured computations\n\nc) Integration Challenges:\n- Novel algorithms needed for combining event detection with polynomial projections\n- Hierarchical memory management requires efficient compression strategies\n- Scale selection mechanism must balance adaptivity and stability\n\n4. Future Search Plan:\n\nNeed one final round focusing on:\n1. Specific techniques for training stability with multiple adaptive components\n2. Empirical results from similar hierarchical architectures\n3. Hardware optimization strategies for polynomial computations\n4. Memory compression techniques in practice\n5. Integration methods for complex adaptive systems\n\n5. References:\n\n[1] Gu, A., Johnson, I., Timalsina, A., Rudra, A., & R\u00e9, C. (2022). How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. International Conference on Learning Representations.\n\n[2] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[3] Zhuang, J., Tang, T., Ding, Y., Tatikonda, S.C., Dvornek, N., Papademetris, X., & Duncan, J. (2020). AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. Neural Information Processing Systems.\n\n[4] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection. arXiv preprint.\n\n[5] Yu, A., Mahoney, M. W., & Erichson, N. B. (2024). There is HOPE to Avoid HiPPOs for Long-memory State Space Models. arXiv preprint.\n\n[6] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "polynomial projections event detection language models, hierarchical memory compression neural networks, vector quantization adaptive computation",
            "detail": "Find papers discussing:\n1. Integration of polynomial projections with event-based processing in neural networks\n2. Hierarchical memory architectures using vector quantization or similar compression techniques\n3. Adaptive scale selection mechanisms in deep learning models\n4. Efficient implementation of polynomial computations in neural networks\n5. Memory compression techniques for language models\nFocus on theoretical analysis, implementation details, and performance implications.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of polynomial projections with event-based processing in neural networks\n2. Hierarchical memory architectures using vector quantization or similar compression techniques\n3. Adaptive scale selection mechanisms in deep learning models\n4. Efficient implementation of polynomial computations in neural networks\n5. Memory compression techniques for language models\nFocus on theoretical analysis, implementation details, and performance implications.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.42)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.42)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.39)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 17/44 (Score: 0.39)*\n\n```\nIn International conference on machine learning, pages 843-852. PMLR, 2015. [55] Arieh Iserles. A first course in the numerical analysis of differential equations. 44. Cambridge university press, 2009. [56] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [57] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [58] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L Smith. On the universality of linear recurrences followed by nonlinear projections. arXiv preprint arXiv:2307.11888, 2023. [59] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In International Conference on Learning Representations, 2018. [60] James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2017. [61] Tao Lei, Yu Zhang, Sida Wang, Hui Dai, and Yoav Artzi. Simple recurrent units for highly parallelizable recurrence. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4470-4481, 2018. [62] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33:1474-1487, 2020. [63] Guy Blelloch. Prefix sums and their applications.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.13)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.13)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 4. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.11)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 21/38 (Score: 0.11)*\n\n```\narXiv preprint arXiv:2001.04451, 2020. [33] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Haotong Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. Proceedings of the 29th Symposium on Operating Systems Principles, 2023. [34] Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. Cats: Contextually-aware thresholding for sparsity in large language models. arXiv preprint arXiv:2404.08763, 2024. [35] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. [36] Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization for efficient llm deployment. NeurIPS Workshop, 2024. [37] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024. [38] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [39] Yuhong Li, Tianle Cai, Yi Zhang, De huai Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? ArXiv, abs/2210.09298, 2022. [40] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration.\n```\n\n#### 5. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.09)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 32/43 (Score: 0.09)*\n\n```\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. $1631-1642,2013$. Pao-Sheng Vincent Sun, Alexander Titterton, Anjlee Gopiani, Tim Santos, Arindam Basu, Wei D Lu, and Jason K Eshraghian. Intelligence processing units accelerate neuromorphic learning. arXiv preprint arXiv:2211.10725, 2022. Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. arXiv preprint arXiv:1503.00075, 2015. Y Tay, D Bahri, D Metzler, D Juan, Z Zhao, and C Zheng. Synthesizer: rethinking self-attention in transformer models. arXiv preprint arXiv:2005.00743, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems (NeurIPS), pp. 5998-6008, 2017. Sreyes Venkatesh, Razvan Marinescu, and Jason K Eshraghian. Squat: Stateful quantization-aware training in recurrent spiking neural networks. Neuro-Inspired Computational Elements (NICE), 2024. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for training high-performance spiking neural networks. Frontiers in neuroscience, 12:331, 2018. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: polynomial projections event detection language models, hierarchical memory compression neural networks, vector quantization adaptive computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Cross-Lingual Event Detection via Optimized Adversarial Training\n\n*From Search Query: polynomial projections event detection language models*\n\n*Luis Guzman-Nateras, Minh Le Nguyen, Thien Huu Nguyen*\n\n**TL;DR:** This work employs Adversarial Language Adaptation to train a Language Discriminator to discern between the source and target languages using unlabeled data and optimize the adversarial training process by only presenting the discriminator with the most informative samples.\n\n**Abstract:** In this work, we focus on Cross-Lingual Event Detection where a model is trained on data from a \\textit{source} language but its performance is evaluated on data from a second, \\textit{target}, language. Most recent works in this area have harnessed the language-invariant qualities displayed by pre-trained Multi-lingual Language Models. Their performance, however, reveals there is room for improvement as the cross-lingual setting entails particular challenges. We employ Adversarial Language Adaptation to train a Language Discriminator to discern between the source and target languages using unlabeled data. The discriminator is trained in an adversarial manner so that the encoder learns to produce refined, language-invariant representations that lead to improved performance. More importantly, we optimize the adversarial training process by only presenting the discriminator with the most informative samples. We base our intuition about what makes a sample informative on two disparate metrics: sample similarity and event presence. Thus, we propose leveraging Optimal Transport as a solution to naturally combine these two distinct information sources into the selection process. Extensive experiments on 8 different language pairs, using 4 languages from unrelated families, show the flexibility and effectiveness of our model that achieves state-of-the-art results.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 23  (*Influential: 0*)\n\n#### 2. BAT: Learning to Reason about Spatial Sounds with Large Language Models\n\n*From Search Query: polynomial projections event detection language models*\n\n*Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol Choi, David Harwath*\n\n**TL;DR:** BAT is presented, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability of spatial sound reasoning.\n\n**Abstract:** Spatial sound reasoning is a fundamental human skill, enabling us to navigate and interpret our surroundings based on sound. In this paper we present BAT, which combines the spatial sound perception ability of a binaural acoustic scene analysis model with the natural language reasoning capabilities of a large language model (LLM) to replicate this innate ability. To address the lack of existing datasets of in-the-wild spatial sounds, we synthesized a binaural audio dataset using AudioSet and SoundSpaces 2.0. Next, we developed SpatialSoundQA, a spatial sound-based question-answering dataset, offering a range of QA tasks that train BAT in various aspects of spatial sound perception and reasoning. The acoustic front end encoder of BAT is a novel spatial audio encoder named Spatial Audio Spectrogram Transformer, or Spatial-AST, which by itself achieves strong performance across sound event detection, spatial localization, and distance estimation. By integrating Spatial-AST with LLaMA-2 7B model, BAT transcends standard Sound Event Localization and Detection (SELD) tasks, enabling the model to reason about the relationships between the sounds in its environment. Our experiments demonstrate BAT's superior performance on both spatial sound perception and reasoning, showcasing the immense potential of LLMs in navigating and interpreting complex spatial audio environments.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 6  (*Influential: 2*)\n\n#### 3. Unleash GPT-2 Power for Event Detection\n\n*From Search Query: polynomial projections event detection language models*\n\n*Amir Pouran Ben Veyseh, Viet Dac Lai, Franck Dernoncourt, Thien Huu Nguyen*\n\n**TL;DR:** This work proposes to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data and the student is trained on combination of the original and GPT-generated data while being led by the anchorknowledge from the teacher.\n\n**Abstract:** Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text. Recently, several ED datasets in various domains have been proposed. However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models. To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED. To prevent the noises inevitable in automatically generated data from hampering training process, we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data. The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher. Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks. We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 51  (*Influential: 4*)\n\n#### 4. EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression\n\n*From Search Query: hierarchical memory compression neural networks*\n\n*Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu*\n\n**TL;DR:** An optimized GPU implementation which supports training GNNs with compressed activations for Pytorch Geometric and Pytorch, and a memory-ef\ufb01cient framework called EXACT, which for the first time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations.\n\n**Abstract:** Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously dif\ufb01cult to implement due to the lack of necessary tools in common graph learning packages. To un-leash the potential of this direction, we provide an optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-ef\ufb01cient framework called \u201cEXACT\u201d, which for the \ufb01rst time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to 32 \u00d7 with 0 . 2 - 0 . 5% accuracy drop and 10 - 25% time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on ogbn-products from a 48GB GPU to a 12GB GPU. The code is available at https://github.com/warai-0toko\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 55  (*Influential: 7*)\n\n#### 5. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: hierarchical memory compression neural networks*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 2*)\n\n#### 6. A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks\n\n*From Search Query: hierarchical memory compression neural networks*\n\n*Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Liang Chen, Zibin Zheng, Baokun Wang, Changhua Meng*\n\n**TL;DR:** This work proposes SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance, and provides theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts.\n\n**Abstract:** While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of higher task accuracy requires a larger hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This work explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compression, SpikeGCL is either comparable to or outperforms many fancy state-of-the-art supervised and self-supervised methods across several graph benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 7. Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution\n\n*From Search Query: vector quantization adaptive computation*\n\n*Zhaoyang Zhang, Wenqi Shao, Jinwei Gu, Xiaogang Wang, Luo Ping*\n\n**TL;DR:** Differentiable Dynamic Quantization (DDQ) is able to quantize challenging lightweight architectures like MobileNets, where different layers prefer different quantization parameters, and can be easily implemented using low-precision matrix-vector multiplication.\n\n**Abstract:** Model quantization is challenging due to many tedious hyper-parameters such as precision (bitwidth), dynamic range (minimum and maximum discrete values) and stepsize (interval between discrete values). Unlike prior arts that carefully tune these values, we present a fully differentiable approach to learn all of them, named Differentiable Dynamic Quantization (DDQ), which has several benefits. (1) DDQ is able to quantize challenging lightweight architectures like MobileNets, where different layers prefer different quantization parameters. (2) DDQ is hardware-friendly and can be easily implemented using low-precision matrix-vector multiplication, making it capable in many hardware such as ARM. (3) Extensive experiments show that DDQ outperforms prior arts on many networks and benchmarks, especially when models are already efficient and compact. e.g., DDQ is the first approach that achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 27  (*Influential: 3*)\n\n#### 8. VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling\n\n*From Search Query: vector quantization adaptive computation*\n\n*Siyuan Li, Zedong Wang, Zicheng Liu, Di Wu, Cheng Tan, Jiangbin Zheng, Yufei Huang, Stan Z. Li*\n\n**TL;DR:** VQDNA is introduced, a general-purpose framework that renovates genome tokenization from the perspective of genome vocabulary learning and proposes Hierarchical Residual Quantization (HRQ), where varying scales of codebooks are designed in a hierarchy to enrich the genome vocabulary in a coarse-to-fine manner.\n\n**Abstract:** Similar to natural language models, pre-trained genome language models are proposed to capture the underlying intricacies within genomes with unsupervised sequence modeling. They have become essential tools for researchers and practitioners in biology. However, the hand-crafted tokenization policies used in these models may not encode the most discriminative patterns from the limited vocabulary of genomic data. In this paper, we introduce VQDNA, a general-purpose framework that renovates genome tokenization from the perspective of genome vocabulary learning. By leveraging vector-quantized codebooks as learnable vocabulary, VQDNA can adaptively tokenize genomes into pattern-aware embeddings in an end-to-end manner. To further push its limits, we propose Hierarchical Residual Quantization (HRQ), where varying scales of codebooks are designed in a hierarchy to enrich the genome vocabulary in a coarse-to-fine manner. Extensive experiments on 32 genome datasets demonstrate VQDNA's superiority and favorable parameter efficiency compared to existing genome language models. Notably, empirical analysis of SARS-CoV-2 mutations reveals the fine-grained pattern awareness and biological significance of learned HRQ vocabulary, highlighting its untapped potential for broader applications in genomics.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 9. Hierarchical Vector Quantized Transformer for Multi-class Unsupervised Anomaly Detection\n\n*From Search Query: vector quantization adaptive computation*\n\n*Ruiying Lu, Yujie Wu, Long Tian, Dongsheng Wang, Bo Chen, Xiyang Liu, Ruimin Hu*\n\n**TL;DR:** This paper proposes a hierarchical vector quantized prototype-oriented Transformer under a probabilistic framework, and investigates an exquisite hierarchical framework to relieve the codebook collapse issue and replenish frail normal patterns.\n\n**Abstract:** Unsupervised image Anomaly Detection (UAD) aims to learn robust and discriminative representations of normal samples. While separate solutions per class endow expensive computation and limited generalizability, this paper focuses on building a unified framework for multiple classes. Under such a challenging setting, popular reconstruction-based networks with continuous latent representation assumption always suffer from the\"identical shortcut\"issue, where both normal and abnormal samples can be well recovered and difficult to distinguish. To address this pivotal issue, we propose a hierarchical vector quantized prototype-oriented Transformer under a probabilistic framework. First, instead of learning the continuous representations, we preserve the typical normal patterns as discrete iconic prototypes, and confirm the importance of Vector Quantization in preventing the model from falling into the shortcut. The vector quantized iconic prototype is integrated into the Transformer for reconstruction, such that the abnormal data point is flipped to a normal data point.Second, we investigate an exquisite hierarchical framework to relieve the codebook collapse issue and replenish frail normal patterns. Third, a prototype-oriented optimal transport method is proposed to better regulate the prototypes and hierarchically evaluate the abnormal score. By evaluating on MVTec-AD and VisA datasets, our model surpasses the state-of-the-art alternatives and possesses good interpretability. The code is available at https://github.com/RuiyingLu/HVQ-Trans.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 5*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Residual Dense Network for Image Restoration\n\n*From Search Query: hierarchical memory compression neural networks*\n\n*Yu Kong, Yulun Zhang, Yapeng Tian, Bineng Zhong, Yun Fu*\n\n**Abstract:** Convolutional neural network has recently achieved great success for image restoration (IR) and also offered hierarchical features. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in IR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.\n\n**Published:** 2018-12-25\n\n\n\n#### 2. High-Efficiency Neural Video Compression via Hierarchical Predictive Learning\n\n*From Search Query: hierarchical memory compression neural networks*\n\n*Zhan Ma, Fengqing Zhu, Dandan Ding, Wuyang Cong, Zhihao Duan, Ming Lu*\n\n**Abstract:** The enhanced Deep Hierarchical Video Compression-DHVC 2.0-has been introduced. This single-model neural video codec operates across a broad range of bitrates, delivering not only superior compression performance to representative methods but also impressive complexity efficiency, enabling real-time processing with a significantly smaller memory footprint on standard GPUs. These remarkable advancements stem from the use of hierarchical predictive coding. Each video frame is uniformly transformed into multiscale representations through hierarchical variational autoencoders. For a specific scale's feature representation of a frame, its corresponding latent residual variables are generated by referencing lower-scale spatial features from the same frame and then conditionally entropy-encoded using a probabilistic model whose parameters are predicted using same-scale temporal reference from previous frames and lower-scale spatial reference of the current frame. This feature-space processing operates from the lowest to the highest scale of each frame, completely eliminating the need for the complexity-intensive motion estimation and compensation techniques that have been standard in video codecs for decades. The hierarchical approach facilitates parallel processing, accelerating both encoding and decoding, and supports transmission-friendly progressive decoding, making it particularly advantageous for networked video applications in the presence of packet loss. Source codes will be made available.\n\n**Published:** 2024-10-03\n\n\n\n#### 3. TorchSparse: Efficient Point Cloud Inference Engine\n\n*From Search Query: vector quantization adaptive computation*\n\n*Song Han, Yujun Lin, Xiuyu Li, Zhijian Liu, Haotian Tang*\n\n**Abstract:** Deep learning on point clouds has received increased attention thanks to its wide applications in AR/VR and autonomous driving. These applications require low latency and high accuracy to provide real-time user experience and ensure user safety. Unlike conventional dense workloads, the sparse and irregular nature of point clouds poses severe challenges to running sparse CNNs efficiently on the general-purpose hardware. Furthermore, existing sparse acceleration techniques for 2D images do not translate to 3D point clouds. In this paper, we introduce TorchSparse, a high-performance point cloud inference engine that accelerates the sparse convolution computation on GPUs. TorchSparse directly optimizes the two bottlenecks of sparse convolution: irregular computation and data movement. It applies adaptive matrix multiplication grouping to trade computation for better regularity, achieving 1.4-1.5x speedup for matrix multiplication. It also optimizes the data movement by adopting vectorized, quantized and fused locality-aware memory access, reducing the memory movement cost by 2.7x. Evaluated on seven representative models across three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv, respectively.\n\n**Published:** 2022-04-21\n\n\n\n#### 4. AdANNS: A Framework for Adaptive Semantic Search\n\n*From Search Query: vector quantization adaptive computation*\n\n*Ali Farhadi, Prateek Jain, Sham Kakade, Qingqing Cao, Alan Fan, Sharan Ranjit S, Aditya Kusupati, Aniket Rege*\n\n**Abstract:** Web-scale search systems learn an encoder to embed a given query which is then hooked into an approximate nearest neighbor search (ANNS) pipeline to retrieve similar data points. To accurately capture tail queries and data points, learned representations typically are rigid, high-dimensional vectors that are generally used as-is in the entire ANNS pipeline and can lead to computationally expensive retrieval. In this paper, we argue that instead of rigid representations, different stages of ANNS can leverage adaptive representations of varying capacities to achieve significantly better accuracy-compute trade-offs, i.e., stages of ANNS that can get away with more approximate computation should use a lower-capacity representation of the same data point. To this end, we introduce AdANNS, a novel ANNS design framework that explicitly leverages the flexibility of Matryoshka Representations. We demonstrate state-of-the-art accuracy-compute trade-offs using novel AdANNS-based key ANNS building blocks like search data structures (AdANNS-IVF) and quantization (AdANNS-OPQ). For example on ImageNet retrieval, AdANNS-IVF is up to 1.5% more accurate than the rigid representations-based IVF at the same compute budget; and matches accuracy while being up to 90x faster in wall-clock time. For Natural Questions, 32-byte AdANNS-OPQ matches the accuracy of the 64-byte OPQ baseline constructed using rigid representations -- same accuracy at half the cost! We further show that the gains from AdANNS translate to modern-day composite ANNS indices that combine search structures and quantization. Finally, we demonstrate that AdANNS can enable inference-time adaptivity for compute-aware search on ANNS indices built non-adaptively on matryoshka representations. Code is open-sourced at https://github.com/RAIVNLab/AdANNS.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-30\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, particularly focusing on the integration of polynomial projections, event-driven processing, hierarchical memory management, adaptive scale selection, and efficient implementation, here are some key points and references that can guide the investigation:\n\n## Integration of Polynomial Projections with Event-Based Processing\n\n- The concept of combining polynomial projections with event-driven processing is novel and not directly addressed in the provided sources. However, understanding how polynomial projections work in recurrent memory models can be a starting point. For example, the work by Gu et al. on \"HiPPO: Recurrent Memory with Optimal Polynomial Projections\" provides a foundation for polynomial projections in language models.\n- To integrate event detection, one might look into how event-driven processing is handled in other domains, such as in Epidemic Intelligence Systems (EIS) which use event-based data for early detection and forecasting. While not directly applicable, the principles of event detection could be adapted to language models.\n\n## Hierarchical Memory Architectures Using Vector Quantization\n\n- Hierarchical memory management using vector quantization is a key area of investigation. Vector quantization is a technique used for compressing data, and its application in hierarchical settings can be found in various neural network architectures.\n  - For instance, the use of vector quantization in neural networks is discussed in the context of memory efficiency and representation quality. However, specific hierarchical memory architectures using vector quantization are not detailed in the provided sources. Researchers may need to look into broader literature on neural network compression techniques.\n  - Theoretical foundations on how vector quantization affects memory efficiency and representation quality can be explored through studies on neural network-based prediction intervals and compression techniques, although these might not be specifically hierarchical.\n\n## Adaptive Scale Selection Mechanisms\n\n- Adaptive scale selection mechanisms are crucial for dynamic processing in neural networks. While the provided sources do not directly address adaptive scale selection in the context of language models, the concept of dynamic processing can be inferred from other domains.\n  - For example, the use of Bayesian optimization and dynamic tuning in other models (like the groundwater level forecasting model) suggests that adaptive methods can be effective in optimizing performance metrics. This could be a starting point for developing adaptive scale selection mechanisms in language models.\n\n## Efficient Implementation of Polynomial Computations\n\n- Efficient computation of polynomial projections in neural networks is essential. The work by Gu et al. on HiPPO provides insights into how polynomial projections can be efficiently computed in recurrent memory models. This can serve as a basis for optimizing polynomial computations in an event-driven setting.\n  - Additionally, studying the computational implications of combining polynomial projections with event-based processing will require analyzing the computational complexity and potential optimizations, such as parallel processing or specialized hardware.\n\n## Memory Compression Techniques for Language Models\n\n- Memory compression techniques are vital for efficient language model implementation. Vector quantization is one such technique that can be used to compress memory in hierarchical settings.\n  - Theoretical analysis on the trade-offs between compression and representation quality can be found in studies on neural network-based prediction intervals and compression techniques. These studies can provide insights into how to balance memory efficiency with model performance.\n\n### Additional Considerations\n\n- **Theoretical Foundations**: Mathematical properties of combining polynomial projections with event detection, stability analysis of hierarchical memory updates, and convergence guarantees with adaptive scale selection are critical. These aspects may require delving into mathematical and computational theory beyond the provided sources.\n- **Implementation Considerations**: Efficient computation methods, memory management strategies for hierarchical quantized states, and hardware optimization techniques will be essential. Studies on efficient neural network architectures and hardware optimizations can provide valuable insights.\n- **Performance Implications**: The impact on long-range dependency modeling, efficiency gains from adaptive computation, and scalability with sequence length need to be evaluated through benchmarks and performance analysis. This may involve comparing the proposed model with existing state-of-the-art models in terms of these metrics.\n\n### Next Steps\n\n1. **Conduct Broad Search**:\n   - Search for works on polynomial projections in language models, focusing on how these can be integrated with event-driven processing.\n   - Investigate hierarchical memory architectures and their application in neural networks.\n\n2. **Investigate Hierarchical Memory Architectures**:\n   - Study vector quantization and other compression techniques in the context of hierarchical memory management.\n   - Analyze the trade-offs between compression and representation quality.\n\n3. **Study Adaptive Scale Selection Mechanisms**:\n   - Research adaptive methods used in other domains and adapt them to the context of language models.\n   - Evaluate the stability and convergence implications of dynamic scale selection.\n\n4. **Efficient Implementation**:\n   - Research efficient polynomial computation methods and their optimization in event-driven settings.\n   - Examine hardware optimization techniques for combined operations.\n\n5. **Performance Analysis**:\n   - Look for benchmarks on long-range dependency modeling and efficiency metrics for adaptive computation.\n   - Analyze scalability studies of similar architectures to understand how the proposed model scales with sequence length and model size.\n",
            "analysis": "Initial Analysis of HiPPOVQ Proposal:\n\n1. Key Components Requiring Investigation:\n\na) Integration of Event-Driven Processing with HiPPO:\n- How does event detection interact with polynomial projections?\n- What are the computational implications of combining these approaches?\n- Are there existing works combining event-based processing with polynomial memory?\n\nb) Hierarchical Memory Management:\n- How does the combination of vector quantization and polynomial projections affect memory efficiency?\n- What are the trade-offs between compression and representation quality?\n- Are there existing hierarchical memory architectures using similar techniques?\n\nc) Scale Selection Mechanism:\n- How does event-driven scale selection compare to fixed hierarchical processing?\n- What are the stability implications of dynamic scale selection?\n- Are there similar adaptive scale selection mechanisms in literature?\n\n2. Comparison with Previous Designs:\n\nThe proposal appears distinct from previous siblings and similar designs:\n- eventnormer focused on event-conditioned normalization\n- sparseretnet used sparse computation in RetNet\n- hierarchical_gated_linear_attent combined hierarchical processing with linear attention\n- densessmamba focused on dense state connections in SSMs\n\nHiPPOVQ introduces novel elements:\n- Integration of polynomial projections with event-driven processing\n- Hierarchical memory management with vector quantization\n- Event-guided scale selection mechanism\n\n3. Research Questions:\n\na) Theoretical Foundations:\n- Mathematical properties of combining polynomial projections with event detection\n- Stability analysis of hierarchical memory updates\n- Convergence guarantees with adaptive scale selection\n\nb) Implementation Considerations:\n- Efficient computation of polynomial projections in event-driven setting\n- Memory management strategies for hierarchical quantized states\n- Hardware optimization for combined operations\n\nc) Performance Implications:\n- Impact on long-range dependency modeling\n- Efficiency gains from adaptive computation\n- Scalability with sequence length\n\n4. Investigation Plan:\n\nPhase 1: Theoretical Foundation\n- Search for works on polynomial projections in language models\n- Investigate event detection in hierarchical architectures\n- Study memory compression techniques in neural networks\n\nPhase 2: Implementation Approaches\n- Research efficient polynomial computation methods\n- Examine vector quantization in hierarchical settings\n- Study hardware optimization techniques\n\nPhase 3: Performance Analysis\n- Look for benchmarks on long-range dependency modeling\n- Investigate efficiency metrics for adaptive computation\n- Analyze scalability studies of similar architectures\n\nReferences from Initial Analysis:\n\n[1] Gu, A., et al. (2020). \"HiPPO: Recurrent Memory with Optimal Polynomial Projections.\"\n[2] De, S., et al. (2024). \"Griffin: Mixing Gated Linear Recurrences with Local Attention.\"\n[3] Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention.\"\n[4] Yu, A., et al. (2023). \"Robustifying State-space Models for Long Sequences.\"\n\nNext Steps:\n1. Conduct broad search on polynomial projections and event detection\n2. Investigate hierarchical memory architectures\n3. Study vector quantization in language models"
        },
        {
            "ready": false,
            "query": "HiPPO polynomial projections language models, event detection neural networks memory efficiency, state space models long range dependencies",
            "detail": "Find papers discussing:\n1. HiPPO theoretical foundations and implementations in language models\n2. Integration of polynomial projections with state space models\n3. Memory-efficient event detection mechanisms\n4. Long-range dependency modeling in state space architectures\n5. Training stability for hierarchical models\nFocus on mathematical formulations, implementation details, and theoretical analysis.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. HiPPO theoretical foundations and implementations in language models\n2. Integration of polynomial projections with state space models\n3. Memory-efficient event detection mechanisms\n4. Long-range dependency modeling in state space architectures\n5. Training stability for hierarchical models\nFocus on mathematical formulations, implementation details, and theoretical analysis.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.99)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.99)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 2. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.99)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.99)*\n\n```\n# There is HOPE to Avoid HiPPOs for Long-memory State Space Models \n\nAnnan Yu, ${ }^{1 *}$ Michael W. Mahoney, ${ }^{2,3,4}$ N. Benjamin Erichson ${ }^{2,3}$<br>${ }^{1}$ Center for Applied Mathematics, Cornell University<br>${ }^{2}$ Lawrence Berkeley National Laboratory<br>${ }^{3}$ International Computer Science Institute<br>${ }^{4}$ Department of Statistics, University of California at Berkeley\n\n\n#### Abstract\n\nState-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity. ## 1 Introduction\n\nState-space models (SSMs) [14] have gained popularity and success in sequence modeling. Known for its excellent efficiency and capability of handling long sequences, an SSM leverages the continuous-time linear, time-invariant (LTI) systems. These systems are often defined by four matrices $\\Gamma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ as\n\n$$\n\\mathbf{x}^{\\prime}(t)=\\mathbf{A} \\mathbf{x}(t)+\\mathbf{B u}(t), \\quad \\mathbf{y}(t)=\\mathbf{C x}(t)+\\mathbf{D u}(t)\n$$\n\nand they can be used to model the mappings from input time-series $\\mathbf{u}(\\cdot)$ to the output timesseries $\\mathbf{y}(\\cdot)$, where $\\mathbf{u}(t) \\in \\mathbb{R}^{m}$ and $\\mathbf{y}(t) \\in \\mathbb{R}^{p}$ for every $t$. The (hidden) states, which capture the latent dynamics, are denoted as $\\mathbf{x}=\\mathbf{x}(t) \\in \\mathbb{R}^{n}$. The system matrices are of dimensions $\\mathbf{A} \\in \\mathbb{C}^{n \\times n}, \\mathbf{B} \\in \\mathbb{C}^{n \\times m}, \\mathbf{C} \\in \\mathbb{C}^{p \\times n}$, and $\\mathbf{D} \\in \\mathbb{C}^{p \\times m}$. Often, the size $n$ of the state vector $\\mathbf{x}$ is much larger than $m$ and $p$, which allows us to memorize information about the past inputs $\\left.\\mathbf{u}\\right|_{(-\\infty, t]}$ in the state vector $\\mathbf{x}(t)$ and retrieve it later to compute $\\mathbf{y}$ via $\\mathbf{C}$. [^0]The so-called S4 [14] and S4D [13] models both set $m=p=1$, and they differ in the structural requirement of $\\mathbf{A}$. This framework was later generalized to the case where $m, p>1$ by the S 5 model [27] via the parallel scans. Another line of research involves making the state transition rule $\\mathbf{A}$ depend on the input $\\mathbf{u}$, along which the two most notable models are Liquid-S4 [17] and Mamba [11], where the latter model achieves the state-of-the-art performance on large-scale real-world datasets. However, SSMs typically need to be initialized and trained (very) carefully. A randomly initialized SSM has suboptimal performance, but the so-called high-order polynomial projection operators (HiPPO) $[29,12,15]$ can be used to empirically improve it. Subsequent work has considered stably transforming HiPPO matrices into simplified structures [13, 32]. The empirical success of the HiPPO framework is traditionally ascribed to interpreting the state vector $\\mathbf{x}(t)$ as the projection of the inputs $\\left.\\mathbf{u}\\right|_{(-\\infty, t]}$ onto some orthogonal polynomial basis. However, such a claim is not completely satisfiable - indeed, the most popular HiPPO-LegS framework is not the Legendre-basis projection operator, as it misses the scaling factor $1 / t$ (see [15, Thm.\n```\n\n#### 3. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.98)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/37 (Score: 0.98)*\n\n```\n# How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections \n\n\\author{\nAlbert $\\mathrm{Gu}^{* \\dagger}$, Isys Johnson ${ }^{* \\ddagger}$, Aman Timalsina ${ }^{\\ddagger}$, Atri Rudra ${ }^{\\ddagger}$, and Christopher R\u00e9 ${ }^{\\dagger}$ <br> ${ }^{\\dagger}$ Department of Computer Science, Stanford University <br> ${ }^{\\dagger}$ albertgu@stanford.edu, chrismre@cs.stanford.edu <br> ${ }^{\\ddagger}$ Department of Computer Science and Engineering, University at Buffalo <br> $\\ddagger$ isysjohn, amantima, atri\\}@buffalo.edu\n\n}\n\n\n#### Abstract\n\nLinear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4).\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.97)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.97)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 5. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.97)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.97)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n\n\n---\n## Found 12 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: HiPPO polynomial projections language models, event detection neural networks memory efficiency, state space models long range dependencies\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections\n\n*From Search Query: HiPPO polynomial projections language models*\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 68  (*Influential: 5*)\n\n#### 2. Linearly Mapping from Image to Text Space\n\n*From Search Query: HiPPO polynomial projections language models*\n\n*Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick*\n\n**TL;DR:** It is shown that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection, indicating that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images.\n\n**Abstract:** The extent to which text-only language models (LMs) learn to represent features of the non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to caption images when a vision model's parameters are optimized to encode images in the language space. We test a stronger hypothesis: that the conceptual representations learned by frozen text-only models and vision-only models are similar enough that this can be achieved with a linear map. We show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs. elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images. Code is available here: https://github.com/jmerullo/limber\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 94  (*Influential: 11*)\n\n#### 3. Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions\n\n*From Search Query: HiPPO polynomial projections language models*\n\n*Majid Yazdani, Meghdad Farahmand, James Henderson*\n\n**TL;DR:** It is shown that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex.\n\n**Abstract:** Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation, machine translation and word sense disambiguation. We present methods of non-compositionality detection for English noun compounds using the unsupervised learning of a semantic composition function. Compounds which are not well modeled by the learned semantic composition function are considered noncompositional. We explore a range of distributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further. We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex. We show that enforcing sparsity is a useful regularizer in learning complex composition functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2015\n\n**Citations:** 42  (*Influential: 5*)\n\n#### 4. On the Scalability and Memory Efficiency of Semidefinite Programs for Lipschitz Constant Estimation of Neural Networks\n\n*From Search Query: event detection neural networks memory efficiency*\n\n*Zi Wang, Bin Hu, Aaron J. Havens, Alexandre Araujo, Yang Zheng, Yudong Chen, Somesh Jha*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 5. Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks\n\n*From Search Query: event detection neural networks memory efficiency*\n\n*Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen*\n\n**TL;DR:** This study proposes a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate and introduces novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.\n\n**Abstract:** Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 89  (*Influential: 9*)\n\n#### 6. Training Spiking Neural Networks with Event-driven Backpropagation\n\n*From Search Query: event detection neural networks memory efficiency*\n\n*Yaoyu Zhu, Zhaofei Yu, Wei Fang, Xiaodong Xie, Tiejun Huang, T. Masquelier*\n\n**TL;DR:** This paper analyzes the commonly used temporal backpropagation training approach and proves that the sum of gradients remains unchanged between fully-connected and convolutional layers, and proposes a backward kernel that can solve the reverse gradient problem for time-based gradients and keep the property of the invariable sum ofGradients.\n\n**Abstract:** Spiking Neural networks (SNNs) represent and transmit information by spatiotemporal spike patterns, which bring two major advantages: biological plausibility and suitability for ultralow-power neuromorphic implementation. Despite this, the binary firing characteristic makes training SNNs more challenging. To learn the parameters of deep SNNs in an event-driven fashion as in inference of SNNs, back-propagation with respect to spike timing is proposed. Although this event-driven learning has the advantages of lower computational cost and memory occupation, the accuracy is far below the recurrent neural network-like learning approaches. In this paper, we first analyze the commonly used temporal backpropagation training approach and prove that the sum of gradients remains unchanged between fully-connected and convolutional layers. Secondly, we show that the max pooling layer meets the above invariance rule, while the average pooling layer does not, which will suffer the gradient vanishing problem but can be revised to meet the requirement. Thirdly, we point out the reverse gradient problem for time-based gradients and propose a backward kernel that can solve this problem and keep the property of the invariable sum of gradients. The experimental results show that the proposed approach achieves state-of-the-art performance on CIFAR10 among time-based training methods. Also, this is the first time that the time-based backpropagation approach successfully trains SNN on the CIFAR100 dataset. Our code is available at https://github.com/zhuyaoyu/SNN-event-driven-learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 30  (*Influential: 2*)\n\n#### 7. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: state space models long range dependencies*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 8. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: state space models long range dependencies*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 216  (*Influential: 38*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Learning Memory-guided Normality for Anomaly Detection\n\n*From Search Query: event detection neural networks memory efficiency*\n\n*Bumsub Ham, Jongyoun Noh, Hyunjong Park*\n\n**Abstract:** We address the problem of anomaly detection, that is, detecting anomalous events in a video sequence. Anomaly detection methods based on convolutional neural networks (CNNs) typically leverage proxy tasks, such as reconstructing input video frames, to learn models describing normality without seeing anomalous samples at training time, and quantify the extent of abnormalities using the reconstruction error at test time. The main drawbacks of these approaches are that they do not consider the diversity of normal patterns explicitly, and the powerful representation capacity of CNNs allows to reconstruct abnormal video frames. To address this problem, we present an unsupervised learning approach to anomaly detection that considers the diversity of normal patterns explicitly, while lessening the representation capacity of CNNs. To this end, we propose to use a memory module with a new update scheme where items in the memory record prototypical patterns of normal data. We also present novel feature compactness and separateness losses to train the memory, boosting the discriminative power of both memory items and deeply learned features from normal data. Experimental results on standard benchmarks demonstrate the effectiveness and efficiency of our approach, which outperforms the state of the art.\n\n**Conference:** learning-memory-guided-normality-for-anomaly-1\n\n**Published:** 2020-03-30\n\n\n\n#### 2. SELD-TCN: Sound Event Localization & Detection via Temporal Convolutional Networks\n\n*From Search Query: event detection neural networks memory efficiency*\n\n*Sherif Abdulatif, Karim Guirguis, Christoph Schorn, Andre Guntoro, Bin Yang*\n\n**Abstract:** The understanding of the surrounding environment plays a critical role in autonomous robotic systems, such as self-driving cars. Extensive research has been carried out concerning visual perception. Yet, to obtain a more complete perception of the environment, autonomous systems of the future should also take acoustic information into account. Recent sound event localization and detection (SELD) frameworks utilize convolutional recurrent neural networks (CRNNs). However, considering the recurrent nature of CRNNs, it becomes challenging to implement them efficiently on embedded hardware. Not only are their computations strenuous to parallelize, but they also require high memory bandwidth and large memory buffers. In this work, we develop a more robust and hardware-friendly novel architecture based on a temporal convolutional network(TCN). The proposed framework (SELD-TCN) outperforms the state-of-the-art SELDnet performance on four different datasets. Moreover, SELD-TCN achieves 4x faster training time per epoch and 40x faster inference time on an ordinary graphics processing unit (GPU).\n\n**Published:** 2020-03-03\n\n\n\n#### 3. fMRI-S4: learning short- and long-range dynamic fMRI dependencies using 1D Convolutions and State Space Models\n\n*From Search Query: state space models long range dependencies*\n\n*Guido van Wingen, Rajat Mani Thomas, Ahmed El-Gazzar*\n\n**Abstract:** Single-subject mapping of resting-state brain functional activity to non-imaging phenotypes is a major goal of neuroimaging. The large majority of learning approaches applied today rely either on static representations or on short-term temporal correlations. This is at odds with the nature of brain activity which is dynamic and exhibit both short- and long-range dependencies. Further, new sophisticated deep learning approaches have been developed and validated on single tasks/datasets. The application of these models for the study of a different targets typically require exhaustive hyperparameter search, model engineering and trial and error to obtain competitive results with simpler linear models. This in turn limit their adoption and hinder fair benchmarking in a rapidly developing area of research. To this end, we propose fMRI-S4; a versatile deep learning model for the classification of phenotypes and psychiatric disorders from the timecourses of resting-state functional magnetic resonance imaging scans. fMRI-S4 capture short- and long- range temporal dependencies in the signal using 1D convolutions and the recently introduced state-space models S4. The proposed architecture is lightweight, sample-efficient and robust across tasks/datasets. We validate fMRI-S4 on the tasks of diagnosing major depressive disorder (MDD), autism spectrum disorder (ASD) and sex classifcation on three multi-site rs-fMRI datasets. We show that fMRI-S4 can outperform existing methods on all three tasks and can be trained as a plug&play model without special hyperpararameter tuning for each setting\n\n**Published:** 2022-08-08\n\n\n\n#### 4. Mega: Moving Average Equipped Gated Attention\n\n*From Search Query: state space models long range dependencies*\n\n*Luke Zettlemoyer, Jonathan May, Graham Neubig, Liangke Gui, Junxian He, Xiang Kong, Chunting Zhou, Xuezhe Ma*\n\n**Abstract:** The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.\n\n**Published:** 2022-09-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on the integration of HiPPO polynomial projections, state space models, memory-efficient event detection, long-range dependency modeling, and training stability, here are some key findings and references:\n\n## HiPPO Theoretical Foundations and Implementations in Language Models\n\n- The HiPPO (High-dimensional Polynomial Projection) technique is crucial for retaining long histories of states by transforming input data into a higher-dimensional space using polynomial functions. This is essential for capturing complex relationships and improving learning efficiency in long-range dependencies.\n- HiPPO is integrated into models like Mamba, where it enables the model to handle long sequences effectively. The HiPPO-KAN model, for instance, combines HiPPO with the Kolmogorov-Arnold network (KAN) framework for efficient time series analysis, which can be adapted for language models.\n\n## Integration of Polynomial Projections with State Space Models\n\n- The BiMamba-S model, which uses bidirectional Mamba with shared projection layers, integrates polynomial projections within a state space framework. This model processes input sequences in both forward and reverse directions, leveraging shared linear projection layers to enhance efficiency and depth without significantly increasing parameter counts.\n- State space models like Mamba utilize polynomial projections to capture long-range dependencies and context-aware predictions, which is particularly useful in tasks requiring the analysis of extended dependencies in multivariate sequences.\n\n## Memory-Efficient Event Detection Mechanisms\n\n- The Selective Scan Algorithm in sparse sequence-to-sequence models like Mamba enables the model to focus on relevant data by filtering out irrelevant information. This selective attention mechanism enhances the model's ability to process long sequences efficiently and is suitable for tasks such as event detection.\n- Techniques like FlashAttention's IO-aware algorithms and EXACT's activation compression strategies can be adapted to reduce memory footprint and improve efficiency in event detection within language models[Initial Analysis of HiPPOVQ Proposal].\n\n## Long-Range Dependency Modeling in State Space Architectures\n\n- Convolutional State Space Models (ConvSSM) establish an equivalence between the dynamics of ConvSSMs and traditional SSMs, which provides insights into parameterization and initialization strategies for modeling long-range dependencies. This can be applied to improve the long-range dependency modeling in state space architectures within language models[Initial Analysis of HiPPOVQ Proposal].\n- The BiMamba-S model, with its bidirectional processing and shared projection layers, is designed to handle long-range dependencies effectively by capturing complex contextual relationships.\n\n## Training Stability for Hierarchical Models\n\n- Hierarchical Residual Quantization (HRQ) in models like VQDNA demonstrates how hierarchical vector quantization can be designed to enrich the vocabulary in a coarse-to-fine manner, which can help in stabilizing the training process for hierarchical models[Initial Analysis of HiPPOVQ Proposal].\n- Techniques such as residual connections and layer normalization, commonly used in transformer architectures, help in training deeper networks by ensuring stable gradients and input distributions. These techniques can be applied to hierarchical models to improve training stability.\n\nIn summary, the integration of HiPPO with state space models like BiMamba-S, the use of selective attention mechanisms for memory efficiency, and the application of hierarchical quantization and IO-aware algorithms can significantly enhance the performance and efficiency of autoregressive language models. Additionally, leveraging insights from ConvSSM and transformer architectures can help in modeling long-range dependencies and ensuring training stability.\n",
            "analysis": "Initial Analysis of HiPPOVQ Proposal\n\n1. Summary of Analysis:\nThe search results reveal several key insights about the components of HiPPOVQ, though more investigation is needed in specific areas:\n\na) Novel Integration Aspects:\n- The combination of event-driven processing with polynomial projections appears novel\n- No existing work directly addresses the integration of HiPPO with event detection\n- The hierarchical approach to memory management using VQ is unique\n\nb) Technical Feasibility:\n- FlashAttention's IO-aware techniques could inform efficient implementation\n- ConvSSM's approach to long-range dependencies offers relevant insights\n- EXACT's activation compression strategies could be adapted\n\n2. Useful References with Key Excerpts:\n\nFrom FlashAttention (Dao et al., 2022):\n\"IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\"\n- Relevant for efficient implementation of memory management\n\nFrom ConvSSM (Smith et al., 2023):\n\"establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies\"\n- Important for theoretical foundations of state space modeling\n\nFrom EXACT (Liu et al., 2022):\n\"reduce the memory footprint of activations by up to 32\u00d7 with 0.2-0.5% accuracy drop\"\n- Demonstrates feasibility of aggressive compression\n\nFrom VQDNA (Li et al., 2024):\n\"Hierarchical Residual Quantization (HRQ), where varying scales of codebooks are designed in a hierarchy to enrich the vocabulary in a coarse-to-fine manner\"\n- Relevant for hierarchical VQ design\n\n3. Key Insights for Design:\n\na) Memory Management:\n- IO-aware algorithms are crucial for efficiency\n- Hierarchical compression can achieve significant memory reduction\n- Trade-off between compression and accuracy is manageable\n\nb) Theoretical Framework:\n- Need deeper investigation of HiPPO-event detection interaction\n- SSM equivalence could provide initialization insights\n- Hierarchical VQ requires careful codebook design\n\nc) Implementation Considerations:\n- Tiling strategies for memory efficiency\n- Activation compression techniques\n- Hardware-aware optimizations\n\n4. Future Search Plan:\n\nRound 2 Focus Areas:\n1. HiPPO theoretical foundations and implementations\n2. Event detection mechanisms in language models\n3. Memory-efficient state space models\n\nRound 3 Focus Areas:\n1. Vector quantization in hierarchical settings\n2. Efficient polynomial computation methods\n3. Training stability techniques\n\n5. References:\n\n[1] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[2] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[3] Liu, Z., Zhou, K., Yang, F., Li, L., Chen, R., & Hu, X. (2022). EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression. International Conference on Learning Representations.\n\n[4] Li, S., Wang, Z., Liu, Z., Wu, D., Tan, C., Zheng, J., Huang, Y., & Li, S. Z. (2024). VQDNA: Unleashing the Power of Vector Quantization for Multi-Species Genomic Sequence Modeling. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "training stability adaptive models neural networks, memory compression hierarchical architectures, hardware optimization polynomial computation",
            "detail": "Find papers discussing:\n1. Training stability techniques for models with multiple adaptive components\n2. Memory compression strategies in hierarchical neural architectures\n3. Hardware optimization methods for polynomial computations\n4. Efficient event detection in long sequences\n5. Integration techniques for combining multiple adaptive mechanisms\nFocus on implementation details, empirical results, and practical considerations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Training stability techniques for models with multiple adaptive components\n2. Memory compression strategies in hierarchical neural architectures\n3. Hardware optimization methods for polynomial computations\n4. Efficient event detection in long sequences\n5. Integration techniques for combining multiple adaptive mechanisms\nFocus on implementation details, empirical results, and practical considerations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.64)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.64)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.01)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.01)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 3. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization  (Avg. Score: 0.01)\n\n*R. Csord\u00e1s, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 42  (*Influential: 7*)\n\n**TL;DR:** This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.\n\n**Abstract:** Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.\n\n##### *Relevant Chunk: No. 28/47 (Score: 0.01)*\n\n```\nIn Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. In Proc. Association for Computational Linguistics (ACL), pp. 403-413, Virtual only, July 2020. Jerry Fodor and Brian P McLaughlin. Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work. Cognition, 35(2):183-204, 1990. Jerry A Fodor, Zenon W Pylyshyn, et al. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch\u00e4rli. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. Preprint arXiv:2007.08970, 2020. Alex Graves. Adaptive computation time for recurrent neural networks. In Int. Conf. on Learning Representations (ICLR) Workshop Track, Vancouver, Canada, April 2016. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John P. Agapiou, Adri\u00e0 Puigdom\u00e8nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): $471-476,2016$. Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. Preprint arXiv:2012.05208, 2020. Stephen Jos\u00e9 Hanson. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena, 42 $(1-3): 265-272,1990$. Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proc. North American Chapter of the Association for Computational Linguistics on Human Language Technologies (NAACL-HLT), pp. 1118-1128, Minneapolis, USA, June 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, Las Vegas, NV, USA, June 2016. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, pp. $1735-1780,1997$. Dieuwke Hupkes, Anand Singh, Kris Korrel, German Kruszewski, and Elia Bruni. Learning compositionally through attentive guidance.\n```\n\n#### 4. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.01)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.01)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n#### 5. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.01)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 36/51 (Score: 0.01)*\n\n```\narXiv preprint arXiv:1606.08415, 2016. [32] Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pages 177-186, 1987. [33] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. [34] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. [35] John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982. [36] Kazuki Irie, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: training stability adaptive models neural networks, memory compression hierarchical architectures, hardware optimization polynomial computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model\n\n*From Search Query: training stability adaptive models neural networks*\n\n*Amine Ouasfi, A. Boukhayma*\n\n**TL;DR:** A efficient mechanism to remedy generalization issues in forward generalizable models for implicit shape reconstruction from unoriented point cloud by combining the inter-shape data prior of the network with an intra-shape regularization prior of a Nystr\\\"om Kernel Ridge Regression.\n\n**Abstract:** Feedforward generalizable models for implicit shape reconstruction from unoriented point cloud present multiple advantages, including high performance and inference speed. However, they still suffer from generalization issues, ranging from underfitting the input point cloud, to misrepresenting samples outside of the training data distribution, or with toplogies unseen at training. We propose here an efficient mechanism to remedy some of these limitations at test time. We combine the inter-shape data prior of the network with an intra-shape regularization prior of a Nystr\\\"om Kernel Ridge Regression, that we further adapt by fitting its hyperprameters to the current shape. The resulting shape function defined in a shape specific Reproducing Kernel Hilbert Space benefits from desirable stability and efficiency properties and grants a shape adaptive expressiveness-robustness trade-off. We demonstrate the improvement obtained through our method with respect to baselines and the state-of-the-art using synthetic and real data.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 2. GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks\n\n*From Search Query: training stability adaptive models neural networks*\n\n*Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang*\n\n**TL;DR:** This work introduces a spatio-temporal pre- training framework that seamlessly integrates with downstream baselines and enhances their performance, and introduces an adaptive mask strategy as part of the pre-training mechanism.\n\n**Abstract:** In recent years, there has been a rapid development of spatio-temporal prediction techniques in response to the increasing demands of traffic management and travel planning. While advanced end-to-end models have achieved notable success in improving predictive performance, their integration and expansion pose significant challenges. This work aims to address these challenges by introducing a spatio-temporal pre-training framework that seamlessly integrates with downstream baselines and enhances their performance. The framework is built upon two key designs: (i) We propose a spatio-temporal mask autoencoder as a pre-training model for learning spatio-temporal dependencies. The model incorporates customized parameter learners and hierarchical spatial pattern encoding networks. These modules are specifically designed to capture spatio-temporal customized representations and intra- and inter-cluster region semantic relationships, which have often been neglected in existing approaches. (ii) We introduce an adaptive mask strategy as part of the pre-training mechanism. This strategy guides the mask autoencoder in learning robust spatio-temporal representations and facilitates the modeling of different relationships, ranging from intra-cluster to inter-cluster, in an easy-to-hard training manner. Extensive experiments conducted on representative benchmarks demonstrate the effectiveness of our proposed method. We have made our model implementation publicly available at https://github.com/HKUDS/GPT-ST.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 3. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: memory compression hierarchical architectures*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 2*)\n\n#### 4. Towards mental time travel: a hierarchical memory for reinforcement learning agents\n\n*From Search Query: memory compression hierarchical architectures*\n\n*Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, Felix Hill*\n\n**TL;DR:** Hierarchical Chunk Attention Memory improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures), and is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Abstract:** Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore\"mentally time-travel\"-- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 42  (*Influential: 5*)\n\n#### 5. History Compression via Language Models in Reinforcement Learning\n\n*From Search Query: memory compression hierarchical architectures*\n\n*Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, S. Lehner, Hamid Eghbalzadeh, Sepp Hochreiter*\n\n**TL;DR:** This work proposes to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency, and introduces FrozenHopfield, which automatically associates observations with pretrained token embeddings.\n\n**Abstract:** In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with pretrained token embeddings. To form these associations, a modern Hopfield network stores these token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 2*)\n\n#### 6. AutoGO: Automated Computation Graph Optimization for Neural Network Evolution\n\n*From Search Query: hardware optimization polynomial computation*\n\n*Mohammad Salameh, Keith G. Mills, Negar Hassanpour, Fred X. Han, Shuting Zhang, Wei Lu, Shangling Jui, Chunhua Zhou, Fengyu Sun, Di Niu*\n\n**TL;DR:** Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness, is presented.\n\n**Abstract:** Optimizing Deep Neural Networks (DNNs) to obtain high-quality models for efficient real-world deployment has posed multi-faceted challenges to machine learning engineers. Existing methods either search for neural architectures in heuristic design spaces or apply low-level adjustments to computation primitives to improve inference efficiency on hardware. We present Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness. Through a tokenization scheme, AutoGO performs variable-sized segment mutations, making both primitive changes and larger-grained changes to CGs. We introduce our segmentation and mutation algorithms, efficient frequent segment mining technique, as well as a pretrained context-aware predictor to estimate the impact of segment replacements. Extensive experimental results show that AutoGO can automatically evolve several typical large convolutional networks to achieve significant task performance improvement and FLOPs reduction on a range of CV tasks, ranging from Classification, Semantic Segmentation, Human Pose Estimation, to Super Resolution, yet without introducing any newer primitive operations. We also demonstrate the lightweight deployment results of AutoGO-optimized super-resolution and denoising U-Nets on a cycle simulator for a Neural Processing Unit (NPU), achieving PSNR improvement and latency/power reduction simultaneously. Code available at https://github.com/Ascend-Research/AutoGO.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 7. Lipschitz constant estimation of Neural Networks via sparse polynomial optimization\n\n*From Search Query: hardware optimization polynomial computation*\n\n*Fabian Latorre G\u00f3mez, Paul Rolland, V. Cevher*\n\n**TL;DR:** This work introduces LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks, and shows how to use the sparse connectivity of a network, to significantly reduce the complexity of computation.\n\n**Abstract:** We introduce LiPopt, a polynomial optimization framework for computing increasingly tighter upper bounds on the Lipschitz constant of neural networks. The underlying optimization problems boil down to either linear (LP) or semidefinite (SDP) programming. We show how to use the sparse connectivity of a network, to significantly reduce the complexity of computation. This is specially useful for convolutional as well as pruned neural networks. We conduct experiments on networks with random weights as well as networks trained on MNIST, showing that in the particular case of the $\\ell_\\infty$-Lipschitz constant, our approach yields superior estimates, compared to baselines available in the literature.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 124  (*Influential: 6*)\n\n#### 8. On Robustness to Adversarial Examples and Polynomial Optimization\n\n*From Search Query: hardware optimization polynomial computation*\n\n*Pranjal Awasthi, Abhratanu Dutta, Aravindan Vijayaraghavan*\n\n**TL;DR:** The main contribution of this work is to exhibit a strong connection between achieving robustness to adversarial examples, and a rich class of polynomial optimization problems, thereby making progress on the above questions.\n\n**Abstract:** We study the design of computationally efficient algorithms with provable guarantees, that are robust to adversarial (test time) perturbations. While there has been an explosion of recent work on this topic due to its connections to test time robustness of deep networks, there is limited theoretical understanding of several basic questions like (i) when and how can one design provably robust learning algorithms? (ii) what is the price of achieving robustness to adversarial examples in a computationally efficient manner? The main contribution of this work is to exhibit a strong connection between achieving robustness to adversarial examples, and a rich class of polynomial optimization problems, thereby making progress on the above questions. In particular, we leverage this connection to (a) design computationally efficient robust algorithms with provable guarantees for a large class of hypothesis, namely linear classifiers and degree-2 polynomial threshold functions~(PTFs), (b) give a precise characterization of the price of achieving robustness in a computationally efficient manner for these classes, (c) design efficient algorithms to certify robustness and generate adversarial attacks in a principled manner for 2-layer neural networks. We empirically demonstrate the effectiveness of these attacks on real data.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 31  (*Influential: 2*)\n\n### 6 related papers from Papers with Code\n\n#### 1. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients\n\n*From Search Query: training stability adaptive models neural networks*\n\n*James S. Duncan, Xenophon Papademetris, Yifan Ding, Nicha Dvornek, Sekhar Tatikonda, Tommy Tang, Juntang Zhuang*\n\n**Abstract:** Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the \"belief\" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-15\n\n\n\n#### 2. Residual Dense Network for Image Super-Resolution\n\n*From Search Query: training stability adaptive models neural networks*\n\n*Yulun Zhang, Yu Kong, Yapeng Tian, Bineng Zhong, Yun Fu*\n\n**Abstract:** A very deep convolutional neural network (CNN) has recently achieved great\nsuccess for image super-resolution (SR) and offered hierarchical features as\nwell. However, most deep CNN based SR models do not make full use of the\nhierarchical features from the original low-resolution (LR) images, thereby\nachieving relatively-low performance. In this paper, we propose a novel\nresidual dense network (RDN) to address this problem in image SR. We fully\nexploit the hierarchical features from all the convolutional layers.\nSpecifically, we propose residual dense block (RDB) to extract abundant local\nfeatures via dense connected convolutional layers. RDB further allows direct\nconnections from the state of preceding RDB to all the layers of current RDB,\nleading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is\nthen used to adaptively learn more effective features from preceding and\ncurrent local features and stabilizes the training of wider network. After\nfully obtaining dense local features, we use global feature fusion to jointly\nand adaptively learn global hierarchical features in a holistic way. Extensive\nexperiments on benchmark datasets with different degradation models show that\nour RDN achieves favorable performance against state-of-the-art methods.\n\n**Conference:** residual-dense-network-for-image-super-1\n\n**Published:** 2018-02-24\n\n\n\n#### 3. Self-Supervised Variational Auto-Encoders\n\n*From Search Query: memory compression hierarchical architectures*\n\n*Jakub M. Tomczak, Ioannis Gatopoulos*\n\n**Abstract:** Density estimation, compression and data generation are crucial tasks in artificial intelligence. Variational Auto-Encoders (VAEs) constitute a single framework to achieve these goals. Here, we present a novel class of generative models, called self-supervised Variational Auto-Encoder (selfVAE), that utilizes deterministic and discrete variational posteriors. This class of models allows to perform both conditional and unconditional sampling, while simplifying the objective function. First, we use a single self-supervised transformation as a latent variable, where a transformation is either downscaling or edge detection. Next, we consider a hierarchical architecture, i.e., multiple transformations, and we show its benefits compared to the VAE. The flexibility of selfVAE in data reconstruction finds a particularly interesting use case in data compression tasks, where we can trade-off memory for better data quality, and vice-versa. We present performance of our approach on three benchmark image data (Cifar10, Imagenette64, and CelebA).\n\n**Conference:** self-supervised-variational-auto-encoders\n\n**Published:** 2020-10-05\n\n\n\n#### 4. Geometry-Oblivious FMM for Compressing Dense SPD Matrices\n\n*From Search Query: memory compression hierarchical architectures*\n\n*Anonymous*\n\n**Abstract:** We present GOFMM (geometry-oblivious FMM), a novel method that creates a hierarchical low-rank approximation, \"compression,\" of an arbitrary dense symmetric positive definite (SPD) matrix. For many applications, GOFMM enables an approximate matrix-vector multiplication in $N \\log N$ or even $N$ time, where $N$ is the matrix size. Compression requires $N \\log N$ storage and work. In general, our scheme belongs to the family of hierarchical matrix approximation methods. In particular, it generalizes the fast multipole method (FMM) to a purely algebraic setting by only requiring the ability to sample matrix entries. Neither geometric information (i.e., point coordinates) nor knowledge of how the matrix entries have been generated is required, thus the term \"geometry-oblivious.\" Also, we introduce a shared-memory parallel scheme for hierarchical matrix computations that reduces synchronization barriers. We present results on the Intel Knights Landing and Haswell architectures, and on the NVIDIA Pascal architecture for a variety of matrices.\n\n**Published:** 2017-07-01\n\n\n\n#### 5. Depth-Optimal Synthesis of Clifford Circuits with SAT Solvers\n\n*From Search Query: hardware optimization polynomial computation*\n\n*Anonymous*\n\n**Abstract:** Circuit synthesis is the task of decomposing a given logical functionality into a sequence of elementary gates. It is (depth-)optimal if it is impossible to achieve the desired functionality with even shorter circuits. Optimal synthesis is a central problem in both quantum and classical hardware design, but also plagued by complexity-theoretic obstacles. Motivated by fault-tolerant quantum computation, we consider the special case of synthesizing blocks of Clifford unitaries. Leveraging entangling input stimuli and the stabilizer formalism allows us to reduce the Clifford synthesis problem to a family of poly-size satisfiability (SAT) problems -- one for each target circuit depth. On a conceptual level, our result showcases that the Clifford synthesis problem is contained in the first level of the polynomial hierarchy ($\\mathsf{NP}$), while the classical synthesis problem for logical circuits is known to be complete for the second level of the polynomial hierarchy ($\\Sigma_2^{\\mathsf{P}}$). Based on this theoretical reduction, we formulate a SAT encoding for depth-optimal Clifford synthesis. We then employ SAT solvers to determine a satisfying assignment or to prove that no such assignment exists. From that, the shortest depth for which synthesis is still possible (optimality) as well as the actual circuit (synthesis) can be obtained. Empirical evaluations show that the optimal synthesis approach yields a substantial depth improvement for random Clifford circuits and Clifford+T circuits for Grover search.\n\n**Published:** 2023-05-02\n\n\n\n#### 6. Efficient Large-Scale Multi-Drone Delivery Using Transit Networks\n\n*From Search Query: hardware optimization polynomial computation*\n\n*Shushman Choudhury, Marco Pavone, Mykel J. Kochenderfer, Kiril Solovey*\n\n**Abstract:** We consider the problem of controlling a large fleet of drones to deliver packages simultaneously across broad urban areas. To conserve energy, drones hop between public transit vehicles (e.g., buses and trams). We design a comprehensive algorithmic framework that strives to minimize the maximum time to complete any delivery. We address the multifaceted complexity of the problem through a two-layer approach. First, the upper layer assigns drones to package delivery sequences with a near-optimal polynomial-time task allocation algorithm. Then, the lower layer executes the allocation by periodically routing the fleet over the transit network while employing efficient bounded-suboptimal multi-agent pathfinding techniques tailored to our setting. Experiments demonstrate the efficiency of our approach on settings with up to $200$ drones, $5000$ packages, and transit networks with up to $8000$ stops in San Francisco and Washington DC. Our results show that the framework computes solutions typically within a few seconds on commodity hardware, and that drones travel up to $360 \\%$ of their flight range with public transit.\n\n**Published:** 2019-09-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, especially in the context of the HiPPOVQ proposal, here are some key points and references that address the specified areas:\n\n## Training Stability Techniques for Models with Multiple Adaptive Components\n\n- **Adaptive Learning Rates**: Techniques such as AdaGrad, RMSprop, and Adam are crucial for maintaining training stability in models with multiple adaptive components. These methods dynamically adjust the learning rate based on the gradient information, which helps in avoiding overshooting and ensuring stable convergence.\n- **Fine-Tuning and Adaptive Ratios**: The paper on \"The Dominant Role of Fine-Tuning and Adaptive Ratios in Neural Network Robustness\" highlights the importance of fine-tuning and adaptive pruning ratios to maintain both accuracy and robustness. This approach can be adapted to ensure stability in models with multiple adaptive components.\n\n## Memory Compression Strategies in Hierarchical Neural Architectures\n\n- **Vector Quantization**: The HiPPOVQ proposal itself suggests using vector quantization for memory compression in hierarchical architectures. This method can efficiently manage memory by compressing the state space representations, which is crucial for long-range modeling[Analysis Note].\n- **Mixed-Precision Training**: The study on \"Systematic Analysis of Low-Precision Training in Deep Neural Networks\" discusses strategies for setting training bit-widths based on various factors such as accumulation, frequency, and depth. This can help in optimizing memory usage and maintaining performance in hierarchical models.\n\n## Hardware Optimization Methods for Polynomial Computations\n\n- **Structured Computations**: The HiPPO framework and related works like \"Convolutional State Space Models\" emphasize the use of structured representations and state space models. These models can be optimized for hardware through efficient polynomial computations, leveraging the mathematical properties of HiPPO projections[Analysis Note].\n- **Efficient Parameterization**: Papers like \"There is HOPE to Avoid HiPPOs\" and \"Convolutional State Space Models\" provide insights into parameterization schemes that can optimize the computational efficiency of polynomial projections, which is essential for hardware optimization.\n\n## Efficient Event Detection in Long Sequences\n\n- **Event-Driven Processing**: The integration of event-driven processing with HiPPO polynomial projections is a novel aspect of the HiPPOVQ proposal. This approach can optimize event detection by using the hierarchical and adaptive nature of the model to selectively process significant events in long sequences[Analysis Note].\n- **Hierarchical Processing**: The \"DenseMamba\" paper discusses selectively integrating shallow-layer hidden states into deeper layers, which can help in retaining fine-grained information crucial for event detection in long sequences.\n\n## Integration Techniques for Combining Multiple Adaptive Mechanisms\n\n- **Hierarchical Memory Management**: The HiPPOVQ proposal suggests a hierarchical approach to memory management using vector quantization with polynomial projections. This integration is unique and requires careful balancing of compression and representation to maintain both efficiency and performance[Analysis Note].\n- **Combining Event Detection and Polynomial Projections**: There is a need for novel algorithms that integrate event detection mechanisms with polynomial projections. This integration can be achieved by leveraging the theoretical foundations of HiPPO and the practical considerations of hierarchical processing and memory management[Analysis Note].\n\nIn summary, the key to improving the autoregressive language model design lies in:\n\n- Utilizing adaptive learning rates and fine-tuning strategies to ensure training stability.\n- Implementing memory compression strategies such as vector quantization and mixed-precision training.\n- Optimizing hardware through structured computations and efficient parameterization.\n- Developing efficient event detection mechanisms integrated with hierarchical processing.\n- Carefully integrating multiple adaptive mechanisms to balance adaptivity and stability.\n\nThese approaches, grounded in the theoretical and practical insights from the referenced works, can help in achieving the desired improvements in the HiPPOVQ model.\n",
            "analysis": "Analysis Note for HiPPOVQ Proposal\n\n1. Summary of Analysis:\nThe search results reveal several important aspects about the HiPPOVQ proposal's key components and their integration:\n\na) Novel Integration:\n- The combination of event-driven processing with HiPPO polynomial projections appears novel\n- No existing work directly addresses the integration of event detection with polynomial projections for adaptive scale selection\n- The hierarchical approach to memory management using vector quantization with polynomial projections is unique\n\nb) Technical Foundations:\n- HiPPO theoretical framework provides strong mathematical basis for long-range modeling\n- State space models offer efficient computation through structured representations\n- Event detection mechanisms can be optimized for memory efficiency\n\n2. Useful References with Key Excerpts:\n\nFrom \"How to Train Your HiPPO\" (Gu et al., 2022):\n\"We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials\"\n- Critical for understanding theoretical foundations of polynomial projections\n\nFrom \"There is HOPE to Avoid HiPPOs\" (Yu et al., 2024):\n\"develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators\"\n- Relevant for initialization and training stability\n\nFrom \"Convolutional State Space Models\" (Smith et al., 2023):\n\"establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies\"\n- Important for efficient implementation\n\nFrom \"DenseMamba\" (He et al., 2024):\n\"selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output\"\n- Relevant for hierarchical integration\n\n3. Key Insights:\n\na) Theoretical Framework:\n- HiPPO provides mathematically principled approach to long-range modeling\n- Event detection can guide polynomial projection selection\n- Hierarchical processing can be optimized through careful parameterization\n\nb) Implementation Considerations:\n- Memory efficiency requires careful balance of compression and representation\n- Training stability needs special attention with multiple adaptive components\n- Hardware optimization opportunities through structured computations\n\nc) Integration Challenges:\n- Combining event detection with polynomial projections requires novel algorithms\n- Hierarchical memory management needs efficient compression strategies\n- Scale selection mechanism must balance adaptivity and stability\n\n4. Future Search Plan:\n\nRound 3 Focus Areas:\n1. Training stability techniques for adaptive models\n2. Memory compression strategies in hierarchical architectures\n3. Hardware optimization for polynomial computations\n4. Event detection efficiency in long sequences\n5. Integration methods for multiple adaptive components\n\n5. References:\n\n[1] Gu, A., Johnson, I., Timalsina, A., Rudra, A., & R\u00e9, C. (2022). How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. International Conference on Learning Representations.\n\n[2] Yu, A., Mahoney, M. W., & Erichson, N. B. (2024). There is HOPE to Avoid HiPPOs for Long-memory State Space Models. arXiv preprint.\n\n[3] Smith, J.T.H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[4] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection. arXiv preprint.\n\n[5] Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.575556,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.737331,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": null,
    "proposal": "# HiPPOVQ: Event-Driven Hierarchical Processing with Polynomial Projections\n\n## 1. Motivation\n\nCurrent language models face several key challenges:\n1. Inefficient uniform processing of sequences\n2. Memory bottlenecks in handling long contexts\n3. Limited adaptivity to varying computational needs\n4. Suboptimal compression of states and attention weights\n\nHiPPOVQ addresses these challenges by combining:\n- Event-driven processing from eventvq\n- Hierarchical normalization from hiergpt\n- Polynomial projections from HiPPO framework\n\n## 2. Problem Analysis\n\n### Parent Design Analysis\n\neventvq:\nStrengths:\n- Event-driven dynamic updates\n- Vector quantization for compression\n- Adaptive computation\nLimitations:\n- Complex training dynamics\n- Memory overhead from event detection\n- Fixed compression ratios\n\nhiergpt:\nStrengths:\n- Multi-scale hierarchical processing\n- Efficient memory usage\n- Improved training stability\nLimitations:\n- Uniform processing regardless of importance\n- Fixed computation paths\n- Overhead from multiple scales\n\n### Integration Opportunities\n\n1. Event-Guided Scale Selection:\n- Use event detection to determine relevant scales\n- Apply polynomial projections for memory updates\n- Enable adaptive scale-specific processing\n\n2. Hierarchical Memory Management:\n- Combine polynomial projections with vector quantization\n- Scale-specific compression based on importance\n- Efficient long-term dependency modeling\n\n3. Adaptive Computation:\n- Event-driven selection of processing scales\n- Scale-specific vector quantization\n- Dynamic resource allocation\n\n## 3. Core Idea and Philosophy\n\nHiPPOVQ introduces three key innovations:\n\n1. Event-Driven Scale Selection (EDSS):\n   - Uses event detection to guide scale selection\n   - Applies polynomial projections for memory updates\n   - Enables efficient multi-scale processing\n\n2. Hierarchical Polynomial Memory (HPM):\n   - Combines HiPPO with vector quantization\n   - Scale-specific memory compression\n   - Efficient long-term dependency modeling\n\n3. Adaptive Scale Integration (ASI):\n   - Dynamic integration of multiple scales\n   - Event-guided resource allocation\n   - Efficient feature fusion\n\n## 4. Design Plan\n\n### 4.1 Architecture Overview\n\nThe HiPPOVQ block consists of three main components:\n\n1. Event Detection and Scale Selection:\n```python\nclass EDSSModule(nn.Module):\n    def __init__(self, dim, num_scales=3):\n        super().__init__()\n        self.event_detector = EventDetector(dim)\n        self.scale_selector = ScaleSelector(dim, num_scales)\n        self.hippo = HiPPOProjector(dim)\n        \n    def forward(self, x):\n        # Detect events and importance\n        events, importance = self.event_detector(x)\n        \n        # Select scales based on events\n        scales = self.scale_selector(importance)\n        \n        # Apply polynomial projections\n        memory = self.hippo(x, scales)\n        return memory, scales\n```\n\n2. Hierarchical Memory Manager:\n```python\nclass HPMemory(nn.Module):\n    def __init__(self, dim, num_scales=3):\n        super().__init__()\n        self.scales = num_scales\n        self.memory_projectors = nn.ModuleList([\n            HiPPOProjector(dim // (2**i))\n            for i in range(num_scales)\n        ])\n        self.quantizers = nn.ModuleList([\n            VectorQuantizer(dim // (2**i))\n            for i in range(num_scales)\n        ])\n        \n    def forward(self, x, scales):\n        memories = []\n        for i, scale in enumerate(scales):\n            # Project to polynomial basis\n            mem = self.memory_projectors[i](x)\n            \n            # Quantize if not important\n            if not scale:\n                mem = self.quantizers[i](mem)\n            memories.append(mem)\n        return memories\n```\n\n3. Scale Integration:\n```python\nclass ASIntegrator(nn.Module):\n    def __init__(self, dim, num_scales=3):\n        super().__init__()\n        self.scale_weights = nn.Parameter(\n            torch.ones(num_scales)\n        )\n        self.output_proj = nn.Linear(dim, dim)\n        \n    def forward(self, memories, scales):\n        # Weight different scales\n        weights = F.softmax(\n            self.scale_weights * scales, dim=0\n        )\n        \n        # Integrate memories\n        output = sum(m * w for m, w in zip(memories, weights))\n        return self.output_proj(output)\n```\n\n### 4.2 Mathematical Formulation\n\n1. Event-Driven Scale Selection:\n   \\[\n   e(x) = \\sigma(W_e x + b_e)\n   \\]\n   \\[\n   s(e) = \\text{softmax}(W_s e + b_s)\n   \\]\n   \\[\n   m(x) = \\text{HiPPO}(x, s(e(x)))\n   \\]\n\n2. Hierarchical Polynomial Memory:\n   \\[\n   p_i(x) = \\text{Project}(x, \\text{basis}_i)\n   \\]\n   \\[\n   m_i(x) = s_i ? p_i(x) : \\text{VQ}(p_i(x))\n   \\]\n\n3. Adaptive Scale Integration:\n   \\[\n   w = \\text{softmax}(v \\odot s)\n   \\]\n   \\[\n   y = \\sum_i w_i m_i(x)\n   \\]\n\n### 4.3 Implementation Guidelines\n\n1. Event Detection:\n```python\ndef detect_events(x):\n    # Compute importance scores\n    importance = compute_importance(x)\n    \n    # Select relevant scales\n    scales = select_scales(importance)\n    return scales, importance\n```\n\n2. Memory Management:\n```python\ndef manage_memory(x, scales):\n    # Project to polynomial basis\n    memory = project_polynomial(x)\n    \n    # Apply scale-specific compression\n    memory = compress_memory(memory, scales)\n    return memory\n```\n\n3. Scale Integration:\n```python\ndef integrate_scales(memories, scales):\n    # Compute scale weights\n    weights = compute_weights(scales)\n    \n    # Combine memories\n    output = combine_memories(memories, weights)\n    return output\n```\n\n## 5. Theoretical Analysis\n\n1. Computational Complexity:\n- Best case: O(N) for single scale\n- Average case: O(N log N) with multiple scales\n- Worst case: O(N log N) due to polynomial projections\n\n2. Memory Usage:\n- Reduced by factor of (1/k) where k is compression ratio\n- Additional overhead from polynomial projections: O(d)\n- Scale-specific storage: O(N log d)\n\n3. Expected Benefits:\n- Improved efficiency through selective processing\n- Better memory utilization via adaptive compression\n- Enhanced long-term dependency modeling\n\n## 6. Research Summary\n\nKey findings from research:\n1. Polynomial projections enable efficient memory updates\n2. Event-driven processing improves computational efficiency\n3. Hierarchical compression maintains performance\n4. Scale-specific processing enhances adaptivity\n\n## 7. Evolution of Design\n\nThe design evolved through several stages:\n1. Initial integration of event detection with HiPPO\n2. Addition of scale-specific compression\n3. Integration of polynomial projections\n4. Optimization of memory management\n\n## 8. Implementation Details\n\nKey components:\n1. Event detection module\n2. Polynomial projection system\n3. Scale-specific memory manager\n4. Adaptive integration mechanism\n\n## 9. Advantages and Trade-offs\n\nAdvantages:\n- Efficient memory updates through polynomial projections\n- Adaptive computation based on events\n- Improved long-term dependency modeling\n- Hardware-friendly implementation\n\nTrade-offs:\n- Increased implementation complexity\n- Training stability considerations\n- Additional memory overhead from projections\n\n## 10. References\n\n[1] Gu, A., et al. (2020). \"HiPPO: Recurrent Memory with Optimal Polynomial Projections.\" Neural Information Processing Systems.\n\n[2] De, S., et al. (2024). \"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models.\" arXiv preprint.\n\n[3] Massaroli, S., et al. (2023). \"Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions.\" Neural Information Processing Systems.\n\n[4] Yu, A., et al. (2023). \"Robustifying State-space Models for Long Sequences via Approximate Diagonalization.\" arXiv preprint.\n\n[5] Smith, J.T.H., et al. (2023). \"Convolutional State Space Models for Long-Range Spatiotemporal Modeling.\" Neural Information Processing Systems.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "HiPPOVQ combines the event-driven vector quantization from eventvq with the hierarchical processing from hiergpt, enhanced by polynomial projections from HiPPO. The design uses event detection to guide multi-scale processing and compression, while polynomial projections enable efficient memory updates and long-term dependency modeling. This integration aims to achieve better efficiency, scalability, and performance through adaptive computation and memory management.",
    "ideation": null,
    "modelname": "HiPPOVQ",
    "suggestions": "1. Training Stability:\n- Add detailed analysis of interaction between event detection and polynomial projections\n- Include gradient flow analysis through multiple adaptive components\n- Provide specific initialization strategies\n- Consider progressive training approach\n\n2. Implementation Details:\n- Add concrete hardware optimization guidelines\n- Include ablation study design\n- Specify fallback mechanisms for edge cases\n- Provide more detailed memory management strategies\n\n3. Memory Management:\n- Add analysis of compression ratio selection\n- Include state synchronization protocols\n- Specify memory hierarchy optimization\n- Provide complexity analysis for different sequence lengths\n\n4. Performance Guarantees:\n- Add theoretical bounds on compression error\n- Include analysis of worst-case scenarios\n- Specify minimum performance guarantees\n- Add robustness analysis for different input distributions\n\n5. Hardware Considerations:\n- Add specific hardware optimization guidelines\n- Include memory bandwidth analysis\n- Provide cache optimization strategies\n- Specify hardware-specific implementation details",
    "user_input": ""
}