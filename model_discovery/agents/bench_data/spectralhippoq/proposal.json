{
    "variantname": "SpectralHiPPOProjector",
    "review": "The SpectralHiPPOQ proposal presents an innovative approach to enhancing GAU efficiency by integrating spectral filtering with hierarchical polynomial memory and adaptive scale integration. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel integration of spectral filtering with HiPPO polynomial projections\n- Well-grounded mathematical framework for memory management\n- Clear differentiation from existing approaches including metahippoq and dynamichypermlp\n- Unique combination of spectral methods with hierarchical processing\n\n2. Design Clarity:\n- Well-structured mathematical formulation\n- Clear architectural components\n- Detailed implementation guidelines\n- Thoughtful consideration of hardware efficiency\n\n3. Memory Efficiency:\n- Potential for significant memory savings through spectral compression\n- Hierarchical processing enables efficient state management\n- Scale-specific compression strategies\n- Adaptive memory management based on input complexity\n\n4. Scalability:\n- Linear complexity through efficient spectral operations\n- Hierarchical compression for long sequences\n- Adaptive computation based on input characteristics\n- Efficient memory management through spectral filtering\n\nCONCERNS:\n\n1. Hardware Efficiency:\n- FFT operations have poor hardware utilization (as noted in FlashFFTConv research)\n- Memory access patterns need optimization\n- Potential overhead from spectral operations\n- Complex memory hierarchy I/O requirements\n\n2. Implementation Complexity:\n- Integration of multiple sophisticated components\n- Complex interaction between spectral filtering and polynomial projections\n- Potential numerical stability issues\n- Need for careful initialization and normalization\n\n3. Training Stability:\n- Complex gradient flows through spectral operations\n- Multiple adaptive components may affect convergence\n- Scale selection mechanism needs careful tuning\n- Potential instability in hierarchical updates\n\n4. Performance Trade-offs:\n- Compression vs. representation quality balance\n- Computational overhead from FFT operations\n- Impact on downstream task performance\n- Hardware-specific optimization requirements\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than basic HiPPO polynomial projections\n2. Novel integration of spectral filtering with hierarchical memory\n3. Unique approach to adaptive scale integration\n4. Different from existing spectral state space models\n\nHowever, it shares some concepts with:\n1. Spectral State Space Models (Agarwal et al., 2023)\n2. FlashFFTConv (Fu et al., 2023)\n3. HiPPO framework (Gu et al., 2020)",
    "search_stack": [
        {
            "ready": false,
            "query": "adaptive state management, spectral methods",
            "detail": "integration of spectral methods in LM blocks, hierarchical processing efficiency",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nintegration of spectral methods in LM blocks, hierarchical processing efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.00)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 5/35 (Score: 0.00)*\n\n```\nIt is widely employed in spectral methods to solve PDEs. 4. Convolution: widely used in statistics, image processing, computer vision, and natural language processing. 5. Hadamard transform: commonly used in quantum information processing algorithms, and in ML as a fast random projection or kernel approximation method. 6. Discrete Hartley transform: similar to the DFT, but it transforms real inputs to real outputs. It was designed as a more efficient option than the DFT for real data. Methods We assume that the transform $\\mathcal{T}$ is fully-specified, e.g., from $N$ linearly independent input-output pairs from which the matrix representation $T_{N} \\in \\mathbb{F}^{N \\times N}$ can be computed. To recover a fast algorithm of the transform, we wish to approximate $T_{N}$ with the product of one or more blocks of butterfly and permutation products, by minimizing the Frobenius norm of the difference:\n\n$$\n\\operatorname{minimize} \\frac{1}{N^{2}}\\left\\|T_{N}-B^{(N)} P^{(N)}\\right\\|_{F}^{2}\n$$\n\nBy design, this factorization yields a fast $O(N \\log N)$ algorithm for the transform.\n```\n\n#### 2. Spectral State Space Models (Avg. Score: 0.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 16/31 (Score: 0.00)*\n\n```\n+ Phase Init. | 89.0 | 60.2 | 95.1 | 94.2 |\n\n## B Computational complexity and comparison to other methods. Using the STU method to make a sequence of $L$ predictions, the features $U^{+}, U^{-} \\in \\mathbb{R}^{L \\times d_{\\text {in }} \\times K}$ can be computed in time $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\log (L)\\right)$ using the Discrete Fast Fourier Transform, where $K$ is the number of filters and $L$ is the context length. The linear prediction part (i.e. spectral component) takes $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$ time, and the autoregressive part can be implemented in total time $O\\left(L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$. Therefore the overall runtime is $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot\\left(\\log (L)+d_{\\text {out }}\\right)\\right) \\cdot{ }^{4}$\nFor comparison, consider LRU and transformers. The same computation carried out by LRU w. diagonal system matrices is dominated by the hidden dimension, i.e. $O\\left(L \\cdot d_{\\text {hidden }} \\cdot\\left(d_{\\text {in }}+d_{\\text {out }}\\right)\\right)$. Thus, the number of filters is replaced by $d_{\\text {hiden }}$, which is usually an order of magnitude larger, although STU has another $O(\\log L)$ overhead. A transformer model with full attention runs in time $O\\left(L^{2} d_{\\text {in }} d_{\\text {out }}\\right)$, which is significantly more costly than both LRU and STU. This is consistent with the motivation of SSM as more efficient models for sequences. ## C Proof of Theorem 3.1\n\nWe begin by observing that without loss of generality we can assume that $A$ is a real-diagonal matrix. This can be ensured by performing a spectral decomposition of $A=U \\Sigma U^{\\top}$ and absorbing the $U, U^{\\top}$ by redefining the system. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define the following vector for any $\\alpha \\in \\mathbb{R}, \\mu(\\alpha) \\in \\mathbb{R}^{L}$, with $\\mu(\\alpha)(i)=(\\alpha-1) \\alpha^{i-1}$. Further define the Hankel matrix $H$ as\n\n$$\nZ \\triangleq \\int_{0}^{1} \\mu(\\alpha) \\mu(\\alpha)^{\\top} d \\alpha\n$$\n\nAs the following lemma shows the Hankel matrix $Z$ above is the same Hankel matrix defined in the definition of STU (3). Lemma C.1. $Z$ is a Hankel matrix with entries given as\n\n$$\nZ(i, j)=\\frac{2}{(i+j)^{3}-(i+j)}\n$$\n\n[^3]Lemma C.2. We have that the following statements hold regarding $\\mu(\\alpha)$ for any $\\alpha \\in[0,1]$,\n\n- $|\\mu(\\alpha)|^{2} \\leq 1$\n- For any $\\alpha \\in[0,1]$ and any unit vector $v$ we have that\n\n$$\n\\left(\\mu(\\alpha)^{\\top} v\\right)^{2} \\leq 12\\left(v^{\\top} H v\\right)\n$$\n\nLemma C.3. For any $\\alpha \\in[0,1]$, let $\\tilde{\\mu}(\\alpha)$ be the projection of $\\mu(\\alpha)$ on the subspace spanned by top $k$ eigenvectors of $Z$, then we have that\n\n$$\n\\|\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\|^{2} \\leq 12 \\sum_{i=k+1}^{L} \\sigma_{i}\n$$\n\nFinally the following lemma from HSZ17] shows that the spectrum of the matrix $Z$ decays exponentially.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 3. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.00)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 4/34 (Score: 0.00)*\n\n```\n(13) turns out to be pervasive in many if not all physics phenomena. Much of the theoretical analysis by (Greengard and Rokhlin, 1987; Hackbusch, 1999) is concerned with quantifying such aspects. The key insight into these Multilevel Methods can be summarized as follows: perform no approximation for near interactions, and apply progressively lower-precision approximation for progressively longer distance interactions. The simple case shown in Eq. (9)-(13) is a good example. To satisfy the tolerance of $10^{-3}$, we need full rank (no approximation) for the diagonal blocks (near interactions), higher precision approximation (rank-2 vs full-rank of 4 ) for the $4 \\times 4$ off-diagonal blocks at level-0 (mid-distance) and lower precision approximation (rank-2 vs full-rank of 8 ) for the $8 \\times 8$ off-diagonal blocks at level-1 (long-distance). In this section, we present some intuition to answer two important questions: 1) Does the hierarchical low-rank structure hold for the attention matrix $A$ in Eq. (3)? 2) What is the algorithm to efficiently compute the hierarchical low-rank structure? We only give an informal exposition of the hierarchical attention. The formal mathematical derivation is deferred to the Appendix. ### 5.1 Hierarchical Structure As Inductive Bias\n\nThe error analysis in (Greengard and Rokhlin, 1987; Hackbusch, 1999) offers little direct insight since the attention matrix $A$ in Eq.\n```\n\n##### *Relevant Chunk: No. 9/34 (Score: 0.00)*\n\n```\n(29) to Appendix Section A. 5 and A.6. ## 7 Algorithm And Computational Complexity\n\nTo facilitate the description and the complexity analysis of the algorithm, we define a few more hierarchy-related notations. In addition to sequence length $L$, number of hierarchy levels $M$ and embedding or feature size $d$ in Eq. (1), the new notations include: 1) $N_{r}$ : numerical rank of the off-diagonal blocks (for instance, 2 in Eq. (20)). This is also the diagonal block size at level-0;2) $N_{b}^{(l)}$ : number of blocks at level- $l$. Note that $L$ and $d$ are usually data-dependent hyper-parameters, while $N_{r}$ is the only model hyper-parameter responsible for our method's inductive bias. In turn, $N_{b}^{(l)}$ and $M$ are derived parameters, computed as:\n\n$$\n\\begin{aligned}\nN_{b}^{(0)} & =\\frac{L}{N_{r}}, \\quad N_{b}^{(l+1)}=\\frac{N_{b}^{(l)}}{2} \\\\\nM & =\\log _{2}\\left(N_{b}^{(0)}\\right)\n\\end{aligned}\n$$\n\nIt is easy to verify that\n\n$$\n\\sum_{l=0}^{M-1} N_{b}^{(l)}=\\sum_{l=0}^{M-1} \\frac{N_{b}^{(0)}}{2^{l}} \\approx 2 N_{b}^{(0)}\n$$\n\nIt is important to note that only the diagonal blocks at level-0 and the super-diagonal and subdiagonal blocks at level- $l$ are needed in applying the hierarchical attention matrix. This is clearly shown in Eq. (21)- (23). This means that only $N_{b}^{(l)}-1$ super-diagonal and sub-diagonal blocks are computed at level- $l$. This is crucial to the overall linear complexity in run time and memory. We should also note that all matrix blocks in coarse attention matrix $\\tilde{A}^{(l)}$ have the same size $N_{r} \\times N_{r}$. This is due to the rank map in Eq.\n```\n\n\n\n---\n## Found 12 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: adaptive state management, spectral methods\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Adaptive Experimental Design with Temporal Interference: A Maximum Likelihood Approach\n\n*From Search Query: adaptive state management*\n\n*P. Glynn, Ramesh Johari, M. Rasouli*\n\n**TL;DR:** Using a novel application of classical martingale analysis of Markov chains via Poisson's equation, efficient designs are characterized via a succinct convex optimization problem to propose a consistent, efficient online experimental design that adaptively samples the two Markov Chains.\n\n**Abstract:** Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different inventory management algorithms in an online retail site. Standard randomized controlled trials are typically not feasible, since the goal is to estimate policy performance on the entire system. Instead, the typical current practice involves dynamically alternating between the two policies for fixed lengths of time, and comparing the average performance of each over the intervals in which they were run as an estimate of the treatment effect. However, this approach suffers from *temporal interference*: one algorithm alters the state of the system as seen by the second algorithm, biasing estimates of the treatment effect. Further, the simple non-adaptive nature of such designs implies they are not sample efficient. \nWe develop a benchmark theoretical model in which to study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies). We assume estimation of the steady state reward for each chain proceeds via nonparametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efficient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martingale analysis of Markov chains via Poisson's equation, we characterize efficient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efficient online experimental design that adaptively samples the two Markov chains.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 26  (*Influential: 0*)\n\n#### 2. Towards Relational POMDPs for Adaptive Dialogue Management\n\n*From Search Query: adaptive state management*\n\n*Pierre Lison*\n\n**TL;DR:** This paper represents the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models and encoded in a small set of general rules expressed as a Markov Logic network.\n\n**Abstract:** Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions of the state and action spaces.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2010\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 3. Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning\n\n*From Search Query: adaptive state management*\n\n*Shenzhi Wang, Qisen Yang, Jiawei Gao, M. Lin, Hao Chen, Liwei Wu, Ning Jia, Shiji Song, Gao Huang*\n\n**TL;DR:** Theoretically, it is proved that state-adaptive balances are necessary for achieving a higher policy performance upper bound, and extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark.\n\n**Abstract:** Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a balance model to select a suitable policy for each state. Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 4. Neural Spectral Methods: Self-supervised learning in the spectral domain\n\n*From Search Query: spectral methods*\n\n*Yiheng Du, N. Chalapathi, Aditi Krishnapriyan*\n\n**Abstract:** We present Neural Spectral Methods, a technique to solve parametric Partial Differential Equations (PDEs), grounded in classical spectral methods. Our method uses orthogonal bases to learn PDE solutions as mappings between spectral coefficients. In contrast to current machine learning approaches which enforce PDE constraints by minimizing the numerical quadrature of the residuals in the spatiotemporal domain, we leverage Parseval's identity and introduce a new training strategy through a \\textit{spectral loss}. Our spectral loss enables more efficient differentiation through the neural network, and substantially reduces training complexity. At inference time, the computational cost of our method remains constant, regardless of the spatiotemporal resolution of the domain. Our experimental results demonstrate that our method significantly outperforms previous machine learning approaches in terms of speed and accuracy by one to two orders of magnitude on multiple different problems. When compared to numerical solvers of the same accuracy, our method demonstrates a $10\\times$ increase in performance speed.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 5. How a student becomes a teacher: learning and forgetting through spectral methods\n\n*From Search Query: spectral methods*\n\n*Lorenzo Giambagli, L. Buffoni, Lorenzo Chicchi, Duccio Fanelli*\n\n**TL;DR:** A radically different optimization scheme which builds on a spectral representation of the linear transfer of information between layers, which could isolate a stable student substructure that mirrors the true complexity of the teacher in terms of computing neurons, path distribution and topological attributes.\n\n**Abstract:** In theoretical machine learning, the teacher\u2013student paradigm is often employed as an effective metaphor for real-life tuition. A student network is trained on data generated by a fixed teacher network until it matches the instructor\u2019s ability to cope with the assigned task. The above scheme proves particularly relevant when the student network is overparameterized (namely, when larger layer sizes are employed) as compared to the underlying teacher network. Under these operating conditions, it is tempting to speculate that the student ability to handle the given task could be eventually stored in a sub-portion of the whole network. This latter should be to some extent reminiscent of the frozen teacher structure, according to suitable metrics, while being approximately invariant across different architectures of the student candidate network. Unfortunately, state-of-the-art conventional learning techniques could not help in identifying the existence of such an invariant subnetwork, due to the inherent degree of non-convexity that characterizes the examined problem. In this work, we take a decisive leap forward by proposing a radically different optimization scheme which builds on a spectral representation of the linear transfer of information between layers. The gradient is hence calculated with respect to both eigenvalues and eigenvectors with negligible increase in terms of computational and complexity load, as compared to standard training algorithms. Working in this framework, we could isolate a stable student substructure, that mirrors the true complexity of the teacher in terms of computing neurons, path distribution and topological attributes. When pruning unimportant nodes of the trained student, as follows a ranking that reflects the optimized eigenvalues, no degradation in the recorded performance is seen above a threshold that corresponds to the effective teacher size. The observed behavior can be pictured as a genuine second-order phase transition that bears universality traits. Code is available at: https://github.com/Jamba15/Spectral-regularization-teacher-student/tree/master.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Contrastive and Non-Contrastive Self-Supervised Learning Recover Global and Local Spectral Embedding Methods\n\n*From Search Query: spectral methods*\n\n*Randall Balestriero, Yann LeCun*\n\n**TL;DR:** A unifying framework under the helm of spectral manifold learning to address limitations of self-Supervised Learning and build the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively.\n\n**Abstract:** Self-Supervised Learning (SSL) surmises that inputs and pairwise positive relationships are enough to learn meaningful representations. Although SSL has recently reached a milestone: outperforming supervised methods in many modalities\\dots the theoretical foundations are limited, method-specific, and fail to provide principled design guidelines to practitioners. In this paper, we propose a unifying framework under the helm of spectral manifold learning to address those limitations. Through the course of this study, we will rigorously demonstrate that VICReg, SimCLR, BarlowTwins et al. correspond to eponymous spectral methods such as Laplacian Eigenmaps, Multidimensional Scaling et al. This unification will then allow us to obtain (i) the closed-form optimal representation for each method, (ii) the closed-form optimal network parameters in the linear regime for each method, (iii) the impact of the pairwise relations used during training on each of those quantities and on downstream task performances, and most importantly, (iv) the first theoretical bridge between contrastive and non-contrastive methods towards global and local spectral embedding methods respectively, hinting at the benefits and limitations of each. For example, (i) if the pairwise relation is aligned with the downstream task, any SSL method can be employed successfully and will recover the supervised method, but in the low data regime, VICReg's invariance hyper-parameter should be high; (ii) if the pairwise relation is misaligned with the downstream task, VICReg with small invariance hyper-parameter should be preferred over SimCLR or BarlowTwins.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 117  (*Influential: 7*)\n\n### 2 related papers from ArXiv\n\n#### 1. Two Step CCA: A new spectral method for estimating vector models of\n  words\n\n*From Search Query: spectral methods*\n\n*Paramveer Dhillon, Jordan Rodu, Dean Foster, Lyle Ungar*\n\n**Abstract:** Unlabeled data is often used to learn representations which can be used to\nsupplement baseline features in a supervised learner. For example, for text\napplications where the words lie in a very high dimensional space (the size of\nthe vocabulary), one can learn a low rank \"dictionary\" by an\neigen-decomposition of the word co-occurrence matrix (e.g. using PCA or CCA).\nIn this paper, we present a new spectral method based on CCA to learn an\neigenword dictionary. Our improved procedure computes two set of CCAs, the\nfirst one between the left and right contexts of the given word and the second\none between the projections resulting from this CCA and the word itself. We\nprove theoretically that this two-step procedure has lower sample complexity\nthan the simple single step procedure and also illustrate the empirical\nefficacy of our approach and the richness of representations learned by our Two\nStep CCA (TSCCA) procedure on the tasks of POS tagging and sentiment\nclassification.\n\n**Published:** 2012-06-27T19:59:59Z  (*Updated: 2012-06-27T19:59:59Z*)\n\n\n\n#### 2. Spectral Learning for Supervised Topic Models\n\n*From Search Query: spectral methods*\n\n*Yong Ren, Yining Wang, Jun Zhu*\n\n**Abstract:** Supervised topic models simultaneously model the latent topic structure of\nlarge collections of documents and a response variable associated with each\ndocument. Existing inference methods are based on variational approximation or\nMonte Carlo sampling, which often suffers from the local minimum defect.\nSpectral methods have been applied to learn unsupervised topic models, such as\nlatent Dirichlet allocation (LDA), with provable guarantees. This paper\ninvestigates the possibility of applying spectral methods to recover the\nparameters of supervised LDA (sLDA). We first present a two-stage spectral\nmethod, which recovers the parameters of LDA followed by a power update method\nto recover the regression model parameters. Then, we further present a\nsingle-phase spectral algorithm to jointly recover the topic distribution\nmatrix as well as the regression weights. Our spectral algorithms are provably\ncorrect and computationally efficient. We prove a sample complexity bound for\neach algorithm and subsequently derive a sufficient condition for the\nidentifiability of sLDA. Thorough experiments on synthetic and real-world\ndatasets verify the theory and demonstrate the practical effectiveness of the\nspectral algorithms. In fact, our results on a large-scale review rating\ndataset demonstrate that our single-phase spectral algorithm alone gets\ncomparable or even better performance than state-of-the-art methods, while\nprevious work on spectral methods has rarely reported such promising\nperformance.\n\n**Published:** 2016-02-19T02:07:20Z  (*Updated: 2016-02-19T02:07:20Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Predicting future state for adaptive clinical pathway management\n\n*From Search Query: adaptive state management*\n\n*Anonymous*\n\n**Abstract:** Clinical decision support systems are assisting physicians in providing care\nto patients. However, in the context of clinical pathway management such\nsystems are rather limited as they only take the current state of the patient\ninto account and ignore the possible evolvement of that state in the future. In\nthe past decade, the availability of big data in the healthcare domain did open\na new era for clinical decision support. Machine learning technologies are now\nwidely used in the clinical domain, nevertheless, mostly as a tool for disease\nprediction. A tool that not only predicts future states, but also enables\nadaptive clinical pathway management based on these predictions is still in\nneed. This paper introduces weighted state transition logic, a logic to model\nstate changes based on actions planned in clinical pathways. Weighted state\ntransition logic extends linear logic by taking weights -- numerical values\nindicating the quality of an action or an entire clinical pathway -- into\naccount. It allows us to predict the future states of a patient and it enables\nadaptive clinical pathway management based on these predictions. We provide an\nimplementation of weighted state transition logic using semantic web\ntechnologies, which makes it easy to integrate semantic data and rules as\nbackground knowledge. Executed by a semantic reasoner, it is possible to\ngenerate a clinical pathway towards a target state, as well as to detect\npotential conflicts in the future when multiple pathways are coexisting. The\ntransitions from the current state to the predicted future state are traceable,\nwhich builds trust from human users on the generated pathway.\n\n**Published:** 2020-10-20\n\n\n\n#### 2. Data Interpreter: An LLM Agent For Data Science\n\n*From Search Query: adaptive state management*\n\n*Zhibin Gou, Taicheng Guo, Lingyao Zhang, Jiaqi Chen, Danyang Li, Chenxing Wei, Ceyao Zhang, Wei Tao, Binhao Wu, Bang Liu, Xiawu Zheng, Min Yang, Li Zhang, Chenglin Wu, Zongze Xu, Yuheng Cheng, Yaying Fei, Xinbing Liang, Xiangtao Lu, Xiangru Tang, Tuo Zhou, Mingchen Zhuge, Jinlin Wang, Jiayi Zhang, Bangbang Liu, Yizhang Lin, Sirui Hong*\n\n**Abstract:** Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.\n\n**Published:** 2024-02-28\n\n\n\n#### 3. On symmetrizing the ultraspherical spectral method for self-adjoint problems\n\n*From Search Query: spectral methods*\n\n*Anonymous*\n\n**Abstract:** A mechanism is described to symmetrize the ultraspherical spectral method for\nself-adjoint problems. The resulting discretizations are symmetric and banded.\nAn algorithm is presented for an adaptive spectral decomposition of\nself-adjoint operators. Several applications are explored to demonstrate the\nproperties of the symmetrizer and the adaptive spectral decomposition.\n\n**Published:** 2019-03-20\n\n\n\n#### 4. Learning to correct spectral methods for simulating turbulent flows\n\n*From Search Query: spectral methods*\n\n*Stephan Hoyer, Michael P. Brenner, Jamie A. Smith, Leonardo Zepeda-N\u00fa\u00f1ez, Peter Norgaard, Dmitrii Kochkov, Gideon Dresdner*\n\n**Abstract:** Despite their ubiquity throughout science and engineering, only a handful of partial differential equations (PDEs) have analytical, or closed-form solutions. This motivates a vast amount of classical work on numerical simulation of PDEs and more recently, a whirlwind of research into data-driven techniques leveraging machine learning (ML). A recent line of work indicates that a hybrid of classical numerical techniques and machine learning can offer significant improvements over either approach alone. In this work, we show that the choice of the numerical scheme is crucial when incorporating physics-based priors. We build upon Fourier-based spectral methods, which are known to be more efficient than other numerical schemes for simulating PDEs with smooth and periodic solutions. Specifically, we develop ML-augmented spectral solvers for three common PDEs of fluid dynamics. Our models are more accurate (2-4x) than standard spectral solvers at the same resolution but have longer overall runtimes (~2x), due to the additional runtime cost of the neural network component. We also demonstrate a handful of key design principles for combining machine learning and numerical methods for solving PDEs.\n\n**Published:** 2022-07-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models (LMs) by integrating spectral methods and enhancing adaptive state management, here are some key points and references that can guide the researchers:\n\n## Integration of Spectral Methods\n\n### Spectral Filtering and State Space Models\n- The concept of adaptive state dimensionality and spectral filtering, as mentioned in **Reference 0** (AdaptiveSpectralSSD), can be applied to language models. This involves using spectral methods to compress and manage the state space efficiently, which can improve memory efficiency and computational adaptability in LMs like HiPPOVQ.\n\n### Hierarchical Processing and Spectral Element Methods\n- Although the spectral element method (SEM) discussed in is primarily used for solving nonlinear and nonlocal PDEs, the idea of using high-order basis functions (like Chebyshev or Lagrange polynomials) and enforcing continuity at element intersections can inspire hierarchical processing in LMs. This could involve breaking down the input sequence into segments and applying spectral methods to each segment, ensuring continuity and smoothness across segments.\n\n## Adaptive State Management\n\n### Selective State Mechanisms and Hierarchical Normalization\n- The HybridSSMHierRMSNorm approach, which integrates selective state space mechanisms with hierarchical normalization, can be adapted for LMs. This method emphasizes efficient parallel processing and adaptive state management, which are crucial for improving the efficiency of hierarchical processing in models like HiPPOVQ.\n\n### Event-Driven Processing and Adaptive Computation\n- The HiPPOVQ design already incorporates event-driven processing and hierarchical normalization. Enhancing this with adaptive state dimensionality and spectral filtering can further improve memory management and adaptive computation. This aligns with the idea of using event detection for scale selection and combining HiPPO projections with vector quantization for memory efficiency.\n\n## Efficient Memory Manipulation and Long-Range Memory\n\n### Linearized Attention Mechanisms\n- Insights from linearized attention mechanisms, as discussed in **Reference 2** (Linear Transformers Are Secretly Fast Weight Programmers), can inform the development of efficient memory manipulation techniques within HiPPOVQ. This equivalence with fast weight controllers can help in optimizing memory access and update operations.\n\n### Integration of RNNs, Convolutions, and Continuous-Time Models\n- Combining recurrent, convolutional, and continuous-time models with linear state-space layers, as explored in **Reference 3** (Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers), can enhance long-range memory capabilities in HiPPOVQ. This integration can leverage the strengths of different model types to improve overall performance and efficiency.\n\n## Future Search Directions\n\n### Advanced Spectral Methods in LMs\n- Investigate recent advancements in spectral methods, such as pseudospectral methods and spectral element methods, and how they can be adapted for language models to improve hierarchical processing and memory efficiency.\n\n### Adaptive Control and Meta-Learning\n- While not directly related to LMs, the concept of self-supervised meta-learning for adaptive control, as seen in, can inspire new approaches to adaptive state management in LMs. This involves pretraining models on offline data and adapting them online, which could be translated to pretraining LMs on large datasets and fine-tuning them adaptively based on new inputs.\n\nBy integrating these concepts, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage the strengths of spectral methods and adaptive state management.\n",
            "analysis": "1. **Summary of Analysis**:\n   - The seed design, HiPPOVQ, integrates event-driven processing with hierarchical normalization and polynomial projections to address inefficiencies in current language models. It aims to improve memory management, adaptive computation, and scale-specific processing.\n   - The proposal leverages event detection for scale selection and combines HiPPO projections with vector quantization for memory efficiency.\n   - The design is innovative but faces challenges in training stability, implementation complexity, and memory management.\n\n2. **Useful References**:\n   - **Reference 0**: AdaptiveSpectralSSD introduces adaptive state dimensionality and spectral filtering to improve state space models, highlighting the importance of adaptive compression and spectral methods.\n   - **Reference 1**: HybridSSMHierRMSNorm integrates selective state space mechanisms with hierarchical normalization, emphasizing efficient parallel processing and adaptive state management.\n   - **Reference 2**: Linear Transformers Are Secretly Fast Weight Programmers discusses the equivalence of linearized attention mechanisms with fast weight controllers, offering insights into efficient memory manipulation.\n   - **Reference 3**: Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers explores the integration of RNNs, convolutions, and neural differential equations, providing a framework for long-range memory.\n\n3. **Key Insights and Analysis**:\n   - Adaptive state dimensionality and spectral filtering (Reference 0) can be integrated into HiPPOVQ to enhance memory efficiency and computational adaptability.\n   - The use of selective state mechanisms and parallel scan operations (Reference 1) can improve the efficiency of hierarchical processing in HiPPOVQ.\n   - Insights from linearized attention mechanisms (Reference 2) can inform the development of efficient memory manipulation techniques within HiPPOVQ.\n   - The integration of RNNs, convolutions, and continuous-time models (Reference 3) suggests potential for enhancing long-range memory capabilities in HiPPOVQ.\n\n4. **Future Search Plan**:\n   - Investigate more on the integration of spectral methods and adaptive state management in LM blocks.\n   - Explore recent advancements in hierarchical processing and memory efficiency in language models.\n\n5. **References**:\n   - Gu, A., et al. (2020). \"HiPPO: Recurrent Memory with Optimal Polynomial Projections.\" Neural Information Processing Systems.\n   - Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" arXiv preprint.\n   - Schlag, I., Irie, K., & Schmidhuber, J. (2021). \"Linear Transformers Are Secretly Fast Weight Programmers.\" International Conference on Machine Learning.\n   - Gu, A., et al. (2021). \"Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers.\" Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "spectral filtering, hierarchical processing",
            "detail": "spectral methods in language models, adaptive state management",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nspectral methods in language models, adaptive state management\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.22)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.27)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.18)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.04)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 7/20 (Score: 0.04)*\n\n```\nSiivola and A. Honkela. A state-space method for language modeling. In 2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721), pages 548-553, St Thomas, VI, USA, 2003. IEEE. ISBN 978-0-7803-7980-0. doi: 10.1109/ ASRU.2003.1318499. [2] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections.\n```\n\n#### 3. Memorizing Transformers (Avg. Score: 0.00)\n\n*Yuhuai Wu, M. Rabe, DeLesley S. Hutchins, Christian Szegedy*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 138  (*Influential: 15*)\n\n**TL;DR:** It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).\n\n**Abstract:** Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.\n\n##### *Relevant Chunk: No. 21/26 (Score: 0.00)*\n\n```\nRonald J. Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural Computation, 1990. Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric language models. $A C L, 9: 362-373,2021$. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 30/74 (Score: 0.00)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS). 2022. [21] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. \"Hungry Hungry Hippos: Towards Language Modeling with State Space Models\". In: The International Conference on Learning Representations (ICLR). 2023. [22] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. \"Language Modeling with Gated Convolutional Networks\". In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933-941. [23] DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017. [24] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. \"LongNet: Scaling Transformers to 1,000,000,000 Tokens\".\n```\n\n\n\n---\n## Found 13 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: spectral filtering, hierarchical processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Spectral Filtering for General Linear Dynamical Systems\n\n*From Search Query: spectral filtering*\n\n*Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang*\n\n**TL;DR:** The recently introduced technique of spectral filtering is extended, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Abstract:** We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 92  (*Influential: 4*)\n\n#### 2. Learning Linear Dynamical Systems via Spectral Filtering\n\n*From Search Query: spectral filtering*\n\n*Elad Hazan, Karan Singh, Cyril Zhang*\n\n**TL;DR:** A polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning, based on a novel filtering technique, which may be of independent interest.\n\n**Abstract:** We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 93  (*Influential: 3*)\n\n#### 3. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\n\n*From Search Query: hierarchical processing*\n\n*C. Qi, L. Yi, Hao Su, L. Guibas*\n\n**TL;DR:** A hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set and proposes novel set learning layers to adaptively combine features from multiple scales to learn deep point set features efficiently and robustly.\n\n**Abstract:** Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 9612  (*Influential: 1902*)\n\n#### 4. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: hierarchical processing*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 194  (*Influential: 28*)\n\n#### 5. Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality\n\n*From Search Query: hierarchical processing*\n\n*Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu*\n\n**TL;DR:** Hierarchical Decomposition (HiDe-)Prompt is proposed, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy.\n\n**Abstract:** Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at \\url{https://github.com/thu-ml/HiDe-Prompt}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 43  (*Influential: 9*)\n\n### 4 related papers from ArXiv\n\n#### 1. Language Through a Prism: A Spectral Approach for Multiscale Language\n  Representations\n\n*From Search Query: spectral filtering*\n\n*Alex Tamkin, Dan Jurafsky, Noah Goodman*\n\n**Abstract:** Language exhibits structure at different scales, ranging from subwords to\nwords, sentences, paragraphs, and documents. To what extent do deep models\ncapture information at these scales, and can we force them to better capture\nstructure across this hierarchy? We approach this question by focusing on\nindividual neurons, analyzing the behavior of their activations at different\ntimescales. We show that signal processing provides a natural framework for\nseparating structure across scales, enabling us to 1) disentangle\nscale-specific information in existing embeddings and 2) train models to learn\nmore about particular scales. Concretely, we apply spectral filters to the\nactivations of a neuron across an input, producing filtered embeddings that\nperform well on part of speech tagging (word-level), dialog speech acts\nclassification (utterance-level), or topic classification (document-level),\nwhile performing poorly on the other tasks. We also present a prism layer for\ntraining models, which uses spectral filters to constrain different neurons to\nmodel structure at different scales. Our proposed BERT + Prism model can better\npredict masked tokens using long-range context and produces multiscale\nrepresentations that perform better at utterance- and document-level tasks. Our\nmethods are general and readily applicable to other domains besides language,\nsuch as images, audio, and video.\n\n**Published:** 2020-11-09T23:17:43Z  (*Updated: 2020-11-09T23:17:43Z*)\n\n\n\n#### 2. Provable Length Generalization in Sequence Prediction via Spectral\n  Filtering\n\n*From Search Query: spectral filtering*\n\n*Annie Marsden, Evan Dogariu, Naman Agarwal, Xinyi Chen, Daniel Suo, Elad Hazan*\n\n**Abstract:** We consider the problem of length generalization in sequence prediction. We\ndefine a new metric of performance in this setting -- the Asymmetric-Regret --\nwhich measures regret against a benchmark predictor with longer context length\nthan available to the learner. We continue by studying this concept through the\nlens of the spectral filtering algorithm. We present a gradient-based learning\nalgorithm that provably achieves length generalization for linear dynamical\nsystems. We conclude with proof-of-concept experiments which are consistent\nwith our theory.\n\n**Published:** 2024-11-01T21:11:40Z  (*Updated: 2024-11-01T21:11:40Z*)\n\n\n\n#### 3. R2D2: Recursive Transformer based on Differentiable Tree for\n  Interpretable Hierarchical Language Modeling\n\n*From Search Query: hierarchical processing*\n\n*Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, Gerard de Melo*\n\n**Abstract:** Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.\n\n**Published:** 2021-07-02T11:00:46Z  (*Updated: 2022-03-03T05:22:59Z*)\n\n\n\n#### 4. Injecting Hierarchy with U-Net Transformers\n\n*From Search Query: hierarchical processing*\n\n*David Donahue, Vladislav Lialin, Anna Rumshisky*\n\n**Abstract:** The Transformer architecture has become increasingly popular over the past\ntwo years, owing to its impressive performance on a number of natural language\nprocessing (NLP) tasks. However, all Transformer computations occur at the\nlevel of word representations and therefore, it may be argued that Transformer\nmodels do not explicitly attempt to learn hierarchical structure which is\nwidely assumed to be integral to language. In the present work, we introduce\nhierarchical processing into the Transformer model, taking inspiration from the\nU-Net architecture, popular in computer vision for its hierarchical view of\nnatural images. We empirically demonstrate that the proposed architecture\noutperforms both the vanilla Transformer and some strong baselines in the\ndomain of chit-chat dialogue.\n\n**Published:** 2019-10-16T15:48:46Z  (*Updated: 2021-04-01T19:41:09Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: spectral filtering*\n\n*Micha\u00ebl Defferrard, Pierre Vandergheynst, Xavier Bresson*\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n\n**Conference:** convolutional-neural-networks-on-graphs-with-1\n\n**Published:** 2016-06-30\n\n\n\n#### 2. Spectral Collaborative Filtering\n\n*From Search Query: spectral filtering*\n\n*Philip S. Yu, Jiawei Zhang, Fei Jiang, Chun-Ta Lu, Lei Zheng*\n\n**Abstract:** Despite the popularity of Collaborative Filtering (CF), CF-based methods are haunted by the \\textit{cold-start} problem, which has a significantly negative impact on users' experiences with Recommender Systems (RS). In this paper, to overcome the aforementioned drawback, we first formulate the relationships between users and items as a bipartite graph. Then, we propose a new spectral convolution operation directly performing in the \\textit{spectral domain}, where not only the proximity information of a graph but also the connectivity information hidden in the graph are revealed. With the proposed spectral convolution operation, we build a deep recommendation model called Spectral Collaborative Filtering (SpectralCF). Benefiting from the rich information of connectivity existing in the \\textit{spectral domain}, SpectralCF is capable of discovering deep connections between users and items and therefore, alleviates the \\textit{cold-start} problem for CF. To the best of our knowledge, SpectralCF is the first CF-based method directly learning from the \\textit{spectral domains} of user-item bipartite graphs. We apply our method on several standard datasets. It is shown that SpectralCF significantly outperforms state-of-the-art models. Code and data are available at \\url{https://github.com/lzheng21/SpectralCF}.\n\n**Published:** 2018-08-30\n\n\n\n#### 3. SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes\n\n*From Search Query: hierarchical processing*\n\n*Joaquim R. R. A. Martins, Joseph Morlier, John T. Hwang, Thierry Lefebvre, Jasper Bussemaker, Youssef Diouane, Nathalie Bartoli, Remi Lafage, Paul Saves*\n\n**Abstract:** The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.\n\n**Published:** 2023-05-23\n\n\n\n#### 4. Hierarchical Neural Memory Network for Low Latency Event Processing\n\n*From Search Query: hierarchical processing*\n\n*Ken Sakurada, Masaki Onishi, Yasutaka Furukawa, Ryuhei Hamaguchi*\n\n**Abstract:** This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/\n\n**Conference:** hierarchical-neural-memory-network-for-low\n\n**Published:** 2023-05-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using spectral methods and adaptive state management, here are some key points and references that can guide the researchers:\n\n## Spectral Methods in Language Models\n\n### Spectral Filtering and State Space Management\n- The concept of spectral state space models can be highly relevant. These models use spectral filtering to manage the state space efficiently, which can enhance memory efficiency and computational adaptability in autoregressive models like HiPPOVQ. This approach can compress and manage the state space effectively, reducing the complexity of long-range memory tasks.\n\n### Hierarchical Processing\n- Hierarchical processing can be improved by applying spectral methods to input segments. The H-Transformer-1D model, for example, uses a hierarchical attention mechanism to compute attention with linear complexity. This method ensures continuity and smoothness across segments, which can be beneficial for handling long sequences in autoregressive models.\n\n## Adaptive State Management\n\n### Selective State Mechanisms and Hierarchical Normalization\n- Adaptive state management can be enhanced by integrating selective state mechanisms and hierarchical normalization. This approach can improve parallel processing and state management by dynamically adjusting the state dimensions based on the input context. Hierarchical normalization can help in maintaining the stability and efficiency of the model during training and inference.\n\n## Recent Advancements and Applications\n\n### Neural Spectral Methods\n- Neural spectral methods introduce the concept of spectral loss for efficient differentiation, which can significantly reduce training complexity and improve performance. This approach can be applied to autoregressive models to enhance their training efficiency and overall performance.\n\n### Meta-Learning and Adaptive Control\n- Investigating meta-learning and adaptive control techniques can provide insights into how to adaptively manage the state of the model. Meta-learning can help the model learn how to learn and adapt to different contexts, while adaptive control can optimize the state management dynamically based on the input data.\n\n## Key Insights and Analysis\n\n### Continuity and Smoothness\n- Spectral element methods ensure continuity and smoothness across segments, which is crucial for maintaining the coherence and context in autoregressive models. This can be particularly useful when dealing with long-range dependencies and complex sequences.\n\n### Computational Efficiency\n- The integration of spectral methods and adaptive state management can significantly improve the computational efficiency of autoregressive models. By reducing the complexity of state space management and enhancing parallel processing, these models can be trained and deployed more efficiently.\n\nWhile the provided sources do not directly discuss the application of spectral methods in language models, the insights from the analysis and the references to spectral state space models, H-Transformer-1D, and neural spectral methods provide a solid foundation for improving autoregressive language model design.\n\n### Future Search Plan\n\n- **Explore Recent Papers**: Look for recent papers and preprints on arXiv, ACL, and ICLR that discuss the application of spectral methods in NLP and language models.\n- **Adaptive Control and Meta-Learning**: Investigate papers and research on adaptive control and meta-learning techniques that can be applied to adaptive state management in language models.\n- **Hierarchical Processing**: Study hierarchical attention mechanisms and their applications in transformer-based models to improve the efficiency and scalability of autoregressive models.\n\nBy focusing on these areas, researchers can develop more efficient, scalable, and robust autoregressive language models.\n",
            "analysis": "1. **Summary of Analysis**:\n   - The integration of spectral methods and adaptive state management can significantly enhance the efficiency and scalability of language models like HiPPOVQ. Spectral filtering and adaptive state dimensionality offer robust solutions for memory management and computational adaptability.\n   - Hierarchical processing using spectral methods can be inspired by spectral element methods, which ensure continuity and smoothness across segments.\n   - Adaptive state management can benefit from selective state mechanisms and hierarchical normalization, improving parallel processing and state management.\n\n2. **Useful References**:\n   - **Spectral State Space Models**: Proposes a novel sequence prediction architecture using spectral filtering, highlighting robustness and efficiency for long-range memory tasks.\n   - **H-Transformer-1D**: Describes an efficient hierarchical method to compute attention with linear complexity, offering insights into hierarchical processing.\n   - **Neural Spectral Methods**: Introduces spectral loss for efficient differentiation, significantly reducing training complexity and improving performance.\n\n3. **Key Insights and Analysis**:\n   - Spectral filtering can be used to enhance memory efficiency and computational adaptability in HiPPOVQ by compressing and managing the state space efficiently.\n   - Hierarchical processing can be improved by applying spectral methods to input segments, ensuring continuity and smoothness across segments.\n   - Adaptive state management can be enhanced by integrating selective state mechanisms and hierarchical normalization, improving parallel processing and state management.\n\n4. **Future Search Plan**:\n   - Explore recent advancements in spectral methods and their applications in language models.\n   - Investigate adaptive control and meta-learning techniques for potential applications in adaptive state management.\n\n5. **References**:\n   - Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). \"Spectral State Space Models.\" arXiv preprint.\n   - Zhu, Z., & Soricut, R. (2021). \"H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences.\" Annual Meeting of the Association for Computational Linguistics.\n   - Du, Y., Chalapathi, N., & Krishnapriyan, A. (2023). \"Neural Spectral Methods: Self-supervised learning in the spectral domain.\" International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "spectral filtering, adaptive state management",
            "detail": "spectral methods in language models, hierarchical processing",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nspectral methods in language models, hierarchical processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation (Avg. Score: 0.11)\n\n*Xiang Hu, Haitao Mi, Liang Li, Gerard de Melo*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2022)\t**Cited by** 10  (*Influential: 0*)\n\n**TL;DR:** This paper uses a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference and significantly improves the grammar induction quality and achieves competitive results in downstream tasks.\n\n**Abstract:** Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n\u00b3) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.However, its rule-based pruning process suffers from local optima and slow inference. In this paper, we propose a unified R2D2 method that overcomes these issues. We use a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference. Our parser casts parsing as a split point scoring task by first scoring all split points for a given sentence and then using the highest-scoring one to recursively split a span into two parts. The reverse order of the splits is considered as the order of pruning in the encoder. We optimize the unsupervised parser by minimizing the Kullback\u2013Leibler distance between tree probabilities from the parser and the R2D2 model.Our experiments show that our Fast-R2D2 significantly improves the grammar induction quality and achieves competitive results in downstream tasks.\n\n##### *Relevant Chunk: No. 18/26 (Score: 0.11)*\n\n```\nACL. Joshua Goodman. 1998. Parsing inside-out. CoRR, cmp-lg/9805007. Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics. Phu Mon Htut, Kyunghyun Cho, and Samuel Bowman. 2018. Grammar induction with neural language models: An unusual replication. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4998-5003, Brussels, Belgium. Association for Computational Linguistics. Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, and Gerard de Melo. 2021. R2D2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4897-4908, Online. Association for Computational Linguistics. Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization with gumbel-softmax. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\n```\n\n#### 2. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.03)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.03)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n#### 3. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.01)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 25/46 (Score: 0.01)*\n\n```\nFrederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model.\n```\n\n#### 4. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.00)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 7/20 (Score: 0.00)*\n\n```\nSiivola and A. Honkela. A state-space method for language modeling. In 2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721), pages 548-553, St Thomas, VI, USA, 2003. IEEE. ISBN 978-0-7803-7980-0. doi: 10.1109/ ASRU.2003.1318499. [2] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 30/74 (Score: 0.00)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS). 2022. [21] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. \"Hungry Hungry Hippos: Towards Language Modeling with State Space Models\". In: The International Conference on Learning Representations (ICLR). 2023. [22] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. \"Language Modeling with Gated Convolutional Networks\". In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933-941. [23] DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017. [24] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. \"LongNet: Scaling Transformers to 1,000,000,000 Tokens\".\n```\n\n\n\n---\n## Found 11 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: spectral filtering, adaptive state management\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Spectral Filtering for General Linear Dynamical Systems\n\n*From Search Query: spectral filtering*\n\n*Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang*\n\n**TL;DR:** The recently introduced technique of spectral filtering is extended, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Abstract:** We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 92  (*Influential: 4*)\n\n#### 2. Learning Linear Dynamical Systems via Spectral Filtering\n\n*From Search Query: spectral filtering*\n\n*Elad Hazan, Karan Singh, Cyril Zhang*\n\n**TL;DR:** A polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning, based on a novel filtering technique, which may be of independent interest.\n\n**Abstract:** We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 93  (*Influential: 3*)\n\n#### 3. Adaptive Experimental Design with Temporal Interference: A Maximum Likelihood Approach\n\n*From Search Query: adaptive state management*\n\n*P. Glynn, Ramesh Johari, M. Rasouli*\n\n**TL;DR:** Using a novel application of classical martingale analysis of Markov chains via Poisson's equation, efficient designs are characterized via a succinct convex optimization problem to propose a consistent, efficient online experimental design that adaptively samples the two Markov Chains.\n\n**Abstract:** Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different inventory management algorithms in an online retail site. Standard randomized controlled trials are typically not feasible, since the goal is to estimate policy performance on the entire system. Instead, the typical current practice involves dynamically alternating between the two policies for fixed lengths of time, and comparing the average performance of each over the intervals in which they were run as an estimate of the treatment effect. However, this approach suffers from *temporal interference*: one algorithm alters the state of the system as seen by the second algorithm, biasing estimates of the treatment effect. Further, the simple non-adaptive nature of such designs implies they are not sample efficient. \nWe develop a benchmark theoretical model in which to study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies). We assume estimation of the steady state reward for each chain proceeds via nonparametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efficient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martingale analysis of Markov chains via Poisson's equation, we characterize efficient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efficient online experimental design that adaptively samples the two Markov chains.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 26  (*Influential: 0*)\n\n#### 4. Towards Relational POMDPs for Adaptive Dialogue Management\n\n*From Search Query: adaptive state management*\n\n*Pierre Lison*\n\n**TL;DR:** This paper represents the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models and encoded in a small set of general rules expressed as a Markov Logic network.\n\n**Abstract:** Open-ended spoken interactions are typically characterised by both structural complexity and high levels of uncertainty, making dialogue management in such settings a particularly challenging problem. Traditional approaches have focused on providing theoretical accounts for either the uncertainty or the complexity of spoken dialogue, but rarely considered the two issues simultaneously. This paper describes ongoing work on a new approach to dialogue management which attempts to fill this gap. We represent the interaction as a Partially Observable Markov Decision Process (POMDP) over a rich state space incorporating both dialogue, user, and environment models. The tractability of the resulting POMDP can be preserved using a mechanism for dynamically constraining the action space based on prior knowledge over locally relevant dialogue structures. These constraints are encoded in a small set of general rules expressed as a Markov Logic network. The first-order expressivity of Markov Logic enables us to leverage the rich relational structure of the problem and efficiently abstract over large regions of the state and action spaces.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2010\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 5. Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning\n\n*From Search Query: adaptive state management*\n\n*Shenzhi Wang, Qisen Yang, Jiawei Gao, M. Lin, Hao Chen, Liwei Wu, Ning Jia, Shiji Song, Gao Huang*\n\n**TL;DR:** Theoretically, it is proved that state-adaptive balances are necessary for achieving a higher policy performance upper bound, and extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark.\n\n**Abstract:** Offline-to-online reinforcement learning (RL) is a training paradigm that combines pre-training on a pre-collected dataset with fine-tuning in an online environment. However, the incorporation of online fine-tuning can intensify the well-known distributional shift problem. Existing solutions tackle this problem by imposing a policy constraint on the policy improvement objective in both offline and online learning. They typically advocate a single balance between policy improvement and constraints across diverse data collections. This one-size-fits-all manner may not optimally leverage each collected sample due to the significant variation in data quality across different states. To this end, we introduce Family Offline-to-Online RL (FamO2O), a simple yet effective framework that empowers existing algorithms to determine state-adaptive improvement-constraint balances. FamO2O utilizes a universal model to train a family of policies with different improvement/constraint intensities, and a balance model to select a suitable policy for each state. Theoretically, we prove that state-adaptive balances are necessary for achieving a higher policy performance upper bound. Empirically, extensive experiments show that FamO2O offers a statistically significant improvement over various existing methods, achieving state-of-the-art performance on the D4RL benchmark. Codes are available at https://github.com/LeapLabTHU/FamO2O.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n### 2 related papers from ArXiv\n\n#### 1. Language Through a Prism: A Spectral Approach for Multiscale Language\n  Representations\n\n*From Search Query: spectral filtering*\n\n*Alex Tamkin, Dan Jurafsky, Noah Goodman*\n\n**Abstract:** Language exhibits structure at different scales, ranging from subwords to\nwords, sentences, paragraphs, and documents. To what extent do deep models\ncapture information at these scales, and can we force them to better capture\nstructure across this hierarchy? We approach this question by focusing on\nindividual neurons, analyzing the behavior of their activations at different\ntimescales. We show that signal processing provides a natural framework for\nseparating structure across scales, enabling us to 1) disentangle\nscale-specific information in existing embeddings and 2) train models to learn\nmore about particular scales. Concretely, we apply spectral filters to the\nactivations of a neuron across an input, producing filtered embeddings that\nperform well on part of speech tagging (word-level), dialog speech acts\nclassification (utterance-level), or topic classification (document-level),\nwhile performing poorly on the other tasks. We also present a prism layer for\ntraining models, which uses spectral filters to constrain different neurons to\nmodel structure at different scales. Our proposed BERT + Prism model can better\npredict masked tokens using long-range context and produces multiscale\nrepresentations that perform better at utterance- and document-level tasks. Our\nmethods are general and readily applicable to other domains besides language,\nsuch as images, audio, and video.\n\n**Published:** 2020-11-09T23:17:43Z  (*Updated: 2020-11-09T23:17:43Z*)\n\n\n\n#### 2. Provable Length Generalization in Sequence Prediction via Spectral\n  Filtering\n\n*From Search Query: spectral filtering*\n\n*Annie Marsden, Evan Dogariu, Naman Agarwal, Xinyi Chen, Daniel Suo, Elad Hazan*\n\n**Abstract:** We consider the problem of length generalization in sequence prediction. We\ndefine a new metric of performance in this setting -- the Asymmetric-Regret --\nwhich measures regret against a benchmark predictor with longer context length\nthan available to the learner. We continue by studying this concept through the\nlens of the spectral filtering algorithm. We present a gradient-based learning\nalgorithm that provably achieves length generalization for linear dynamical\nsystems. We conclude with proof-of-concept experiments which are consistent\nwith our theory.\n\n**Published:** 2024-11-01T21:11:40Z  (*Updated: 2024-11-01T21:11:40Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: spectral filtering*\n\n*Micha\u00ebl Defferrard, Pierre Vandergheynst, Xavier Bresson*\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n\n**Conference:** convolutional-neural-networks-on-graphs-with-1\n\n**Published:** 2016-06-30\n\n\n\n#### 2. Spectral Collaborative Filtering\n\n*From Search Query: spectral filtering*\n\n*Philip S. Yu, Jiawei Zhang, Fei Jiang, Chun-Ta Lu, Lei Zheng*\n\n**Abstract:** Despite the popularity of Collaborative Filtering (CF), CF-based methods are haunted by the \\textit{cold-start} problem, which has a significantly negative impact on users' experiences with Recommender Systems (RS). In this paper, to overcome the aforementioned drawback, we first formulate the relationships between users and items as a bipartite graph. Then, we propose a new spectral convolution operation directly performing in the \\textit{spectral domain}, where not only the proximity information of a graph but also the connectivity information hidden in the graph are revealed. With the proposed spectral convolution operation, we build a deep recommendation model called Spectral Collaborative Filtering (SpectralCF). Benefiting from the rich information of connectivity existing in the \\textit{spectral domain}, SpectralCF is capable of discovering deep connections between users and items and therefore, alleviates the \\textit{cold-start} problem for CF. To the best of our knowledge, SpectralCF is the first CF-based method directly learning from the \\textit{spectral domains} of user-item bipartite graphs. We apply our method on several standard datasets. It is shown that SpectralCF significantly outperforms state-of-the-art models. Code and data are available at \\url{https://github.com/lzheng21/SpectralCF}.\n\n**Published:** 2018-08-30\n\n\n\n#### 3. Predicting future state for adaptive clinical pathway management\n\n*From Search Query: adaptive state management*\n\n*Anonymous*\n\n**Abstract:** Clinical decision support systems are assisting physicians in providing care\nto patients. However, in the context of clinical pathway management such\nsystems are rather limited as they only take the current state of the patient\ninto account and ignore the possible evolvement of that state in the future. In\nthe past decade, the availability of big data in the healthcare domain did open\na new era for clinical decision support. Machine learning technologies are now\nwidely used in the clinical domain, nevertheless, mostly as a tool for disease\nprediction. A tool that not only predicts future states, but also enables\nadaptive clinical pathway management based on these predictions is still in\nneed. This paper introduces weighted state transition logic, a logic to model\nstate changes based on actions planned in clinical pathways. Weighted state\ntransition logic extends linear logic by taking weights -- numerical values\nindicating the quality of an action or an entire clinical pathway -- into\naccount. It allows us to predict the future states of a patient and it enables\nadaptive clinical pathway management based on these predictions. We provide an\nimplementation of weighted state transition logic using semantic web\ntechnologies, which makes it easy to integrate semantic data and rules as\nbackground knowledge. Executed by a semantic reasoner, it is possible to\ngenerate a clinical pathway towards a target state, as well as to detect\npotential conflicts in the future when multiple pathways are coexisting. The\ntransitions from the current state to the predicted future state are traceable,\nwhich builds trust from human users on the generated pathway.\n\n**Published:** 2020-10-20\n\n\n\n#### 4. Data Interpreter: An LLM Agent For Data Science\n\n*From Search Query: adaptive state management*\n\n*Zhibin Gou, Taicheng Guo, Lingyao Zhang, Jiaqi Chen, Danyang Li, Chenxing Wei, Ceyao Zhang, Wei Tao, Binhao Wu, Bang Liu, Xiawu Zheng, Min Yang, Li Zhang, Chenglin Wu, Zongze Xu, Yuheng Cheng, Yaying Fei, Xinbing Liang, Xiangtao Lu, Xiangru Tang, Tuo Zhou, Mingchen Zhuge, Jinlin Wang, Jiayi Zhang, Bangbang Liu, Yizhang Lin, Sirui Hong*\n\n**Abstract:** Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.\n\n**Published:** 2024-02-28\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using spectral methods and adaptive state management, here are some key points and references that can be particularly useful:\n\n## Spectral Methods in Language Models\n\n- **Spectral State Space Models**: While the provided sources do not directly mention this specific term, the concept of spectral filtering and its application in state space models can be inferred from other domains. For instance, the idea of spectral filtering in non-line-of-sight imaging can be analogously applied to language models to handle long-range dependencies and improve model stability. Researchers can explore how spectral methods can be adapted to compress and manage the state space efficiently in language models.\n\n## Hierarchical Processing\n\n- **H-Transformer-1D**: This model uses hierarchical attention mechanisms to compute attention with linear complexity, which is a valuable insight for hierarchical processing in language models. By applying similar hierarchical methods, researchers can ensure continuity and smoothness across segments in autoregressive models.\n\n## Adaptive State Management\n\n- **SpaLLM (Sketched Parameter Adaptation of LLMs)**: Although not directly related to spectral methods, SpaLLM introduces a novel approach to adaptive state management by using sketched parameter adaptation. This method involves transforming pre-trained LLM weights into lookup tables and fine-tuning these values, which simplifies the adaptation process and supports the dynamic management of multiple adapters. This approach can be integrated with spectral methods to enhance adaptive state management and improve parallel processing and state management efficiency.\n\n## Integration of Spectral and Adaptive Methods\n\n- **Selective State Mechanisms and Hierarchical Normalization**: The concept of selective state mechanisms and hierarchical normalization, as hinted at in the analysis, can be crucial. By integrating spectral filtering with these mechanisms, researchers can improve the robustness and efficiency of autoregressive models. This involves applying spectral methods to input segments to ensure continuity and smoothness, while using selective state mechanisms to enhance parallel processing and state management.\n\n## Future Search Plan\n\n- **Recent Advancements in Spectral Methods**: Investigate recent papers and preprints that apply spectral methods in sequence modeling or other related domains. For example, looking into how spectral state space models or neural spectral methods are applied in other contexts can provide valuable insights for adaptation in language models.\n- **Adaptive Control and Meta-Learning**: Explore adaptive control and meta-learning techniques that can be applied to improve adaptive state management in autoregressive models. This could involve meta-learning algorithms that adapt to different tasks or contexts, enhancing the model's ability to handle variant inputs and improve robustness.\n\nBy combining these approaches, researchers can develop an autoregressive language model that leverages spectral methods for efficient state management, hierarchical processing for handling long-range dependencies, and adaptive state management for improved robustness and efficiency.\n",
            "analysis": "1. **Summary of Analysis**:\n   - The integration of spectral methods and adaptive state management in language models can significantly enhance memory efficiency and computational adaptability. Spectral filtering, as demonstrated in spectral state space models, offers robust solutions for handling long-range dependencies and improving model stability.\n   - Hierarchical processing can benefit from spectral methods, as seen in models like H-Transformer-1D, which use hierarchical attention mechanisms to ensure continuity and smoothness across segments.\n   - Adaptive state management can be enhanced by integrating selective state mechanisms and hierarchical normalization, improving parallel processing and state management.\n\n2. **Useful References**:\n   - **Spectral State Space Models**: Proposes a novel sequence prediction architecture using spectral filtering, highlighting robustness and efficiency for long-range memory tasks.\n   - **H-Transformer-1D**: Describes an efficient hierarchical method to compute attention with linear complexity, offering insights into hierarchical processing.\n   - **Neural Spectral Methods**: Introduces spectral loss for efficient differentiation, significantly reducing training complexity and improving performance.\n\n3. **Key Insights and Analysis**:\n   - Spectral filtering can enhance memory efficiency and computational adaptability by compressing and managing the state space efficiently.\n   - Hierarchical processing can be improved by applying spectral methods to input segments, ensuring continuity and smoothness across segments.\n   - Adaptive state management can benefit from selective state mechanisms and hierarchical normalization, improving parallel processing and state management.\n\n4. **Future Search Plan**:\n   - Explore recent advancements in spectral methods and their applications in language models.\n   - Investigate adaptive control and meta-learning techniques for potential applications in adaptive state management.\n\n5. **References**:\n   - Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). \"Spectral State Space Models.\" arXiv preprint.\n   - Zhu, Z., & Soricut, R. (2021). \"H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences.\" Annual Meeting of the Association for Computational Linguistics.\n   - Du, Y., Chalapathi, N., & Krishnapriyan, A. (2023). \"Neural Spectral Methods: Self-supervised learning in the spectral domain.\" International Conference on Learning Representations."
        },
        {
            "ready": true,
            "query": "spectral filtering, hierarchical processing",
            "detail": "spectral methods in language models, adaptive state management",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nspectral methods in language models, adaptive state management\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.22)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.27)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.18)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.04)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 7/20 (Score: 0.04)*\n\n```\nSiivola and A. Honkela. A state-space method for language modeling. In 2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721), pages 548-553, St Thomas, VI, USA, 2003. IEEE. ISBN 978-0-7803-7980-0. doi: 10.1109/ ASRU.2003.1318499. [2] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections.\n```\n\n#### 3. Memorizing Transformers (Avg. Score: 0.00)\n\n*Yuhuai Wu, M. Rabe, DeLesley S. Hutchins, Christian Szegedy*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 138  (*Influential: 15*)\n\n**TL;DR:** It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).\n\n**Abstract:** Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.\n\n##### *Relevant Chunk: No. 21/26 (Score: 0.00)*\n\n```\nRonald J. Williams and Jing Peng. An efficient gradient-based algorithm for on-line training of recurrent network trajectories. Neural Computation, 1990. Dani Yogatama, Cyprien de Masson d'Autume, and Lingpeng Kong. Adaptive semiparametric language models. $A C L, 9: 362-373,2021$. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 30/74 (Score: 0.00)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS). 2022. [21] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. \"Hungry Hungry Hippos: Towards Language Modeling with State Space Models\". In: The International Conference on Learning Representations (ICLR). 2023. [22] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. \"Language Modeling with Gated Convolutional Networks\". In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933-941. [23] DeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017. [24] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. \"LongNet: Scaling Transformers to 1,000,000,000 Tokens\".\n```\n\n\n\n---\n## Found 13 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: spectral filtering, hierarchical processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Spectral Filtering for General Linear Dynamical Systems\n\n*From Search Query: spectral filtering*\n\n*Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang*\n\n**TL;DR:** The recently introduced technique of spectral filtering is extended, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Abstract:** We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 92  (*Influential: 4*)\n\n#### 2. Learning Linear Dynamical Systems via Spectral Filtering\n\n*From Search Query: spectral filtering*\n\n*Elad Hazan, Karan Singh, Cyril Zhang*\n\n**TL;DR:** A polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning, based on a novel filtering technique, which may be of independent interest.\n\n**Abstract:** We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 93  (*Influential: 3*)\n\n#### 3. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\n\n*From Search Query: hierarchical processing*\n\n*C. Qi, L. Yi, Hao Su, L. Guibas*\n\n**TL;DR:** A hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set and proposes novel set learning layers to adaptively combine features from multiple scales to learn deep point set features efficiently and robustly.\n\n**Abstract:** Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 9612  (*Influential: 1902*)\n\n#### 4. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: hierarchical processing*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 194  (*Influential: 28*)\n\n#### 5. Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality\n\n*From Search Query: hierarchical processing*\n\n*Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu*\n\n**TL;DR:** Hierarchical Decomposition (HiDe-)Prompt is proposed, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy.\n\n**Abstract:** Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at \\url{https://github.com/thu-ml/HiDe-Prompt}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 43  (*Influential: 9*)\n\n### 4 related papers from ArXiv\n\n#### 1. Language Through a Prism: A Spectral Approach for Multiscale Language\n  Representations\n\n*From Search Query: spectral filtering*\n\n*Alex Tamkin, Dan Jurafsky, Noah Goodman*\n\n**Abstract:** Language exhibits structure at different scales, ranging from subwords to\nwords, sentences, paragraphs, and documents. To what extent do deep models\ncapture information at these scales, and can we force them to better capture\nstructure across this hierarchy? We approach this question by focusing on\nindividual neurons, analyzing the behavior of their activations at different\ntimescales. We show that signal processing provides a natural framework for\nseparating structure across scales, enabling us to 1) disentangle\nscale-specific information in existing embeddings and 2) train models to learn\nmore about particular scales. Concretely, we apply spectral filters to the\nactivations of a neuron across an input, producing filtered embeddings that\nperform well on part of speech tagging (word-level), dialog speech acts\nclassification (utterance-level), or topic classification (document-level),\nwhile performing poorly on the other tasks. We also present a prism layer for\ntraining models, which uses spectral filters to constrain different neurons to\nmodel structure at different scales. Our proposed BERT + Prism model can better\npredict masked tokens using long-range context and produces multiscale\nrepresentations that perform better at utterance- and document-level tasks. Our\nmethods are general and readily applicable to other domains besides language,\nsuch as images, audio, and video.\n\n**Published:** 2020-11-09T23:17:43Z  (*Updated: 2020-11-09T23:17:43Z*)\n\n\n\n#### 2. Provable Length Generalization in Sequence Prediction via Spectral\n  Filtering\n\n*From Search Query: spectral filtering*\n\n*Annie Marsden, Evan Dogariu, Naman Agarwal, Xinyi Chen, Daniel Suo, Elad Hazan*\n\n**Abstract:** We consider the problem of length generalization in sequence prediction. We\ndefine a new metric of performance in this setting -- the Asymmetric-Regret --\nwhich measures regret against a benchmark predictor with longer context length\nthan available to the learner. We continue by studying this concept through the\nlens of the spectral filtering algorithm. We present a gradient-based learning\nalgorithm that provably achieves length generalization for linear dynamical\nsystems. We conclude with proof-of-concept experiments which are consistent\nwith our theory.\n\n**Published:** 2024-11-01T21:11:40Z  (*Updated: 2024-11-01T21:11:40Z*)\n\n\n\n#### 3. R2D2: Recursive Transformer based on Differentiable Tree for\n  Interpretable Hierarchical Language Modeling\n\n*From Search Query: hierarchical processing*\n\n*Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, Gerard de Melo*\n\n**Abstract:** Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.\n\n**Published:** 2021-07-02T11:00:46Z  (*Updated: 2022-03-03T05:22:59Z*)\n\n\n\n#### 4. Injecting Hierarchy with U-Net Transformers\n\n*From Search Query: hierarchical processing*\n\n*David Donahue, Vladislav Lialin, Anna Rumshisky*\n\n**Abstract:** The Transformer architecture has become increasingly popular over the past\ntwo years, owing to its impressive performance on a number of natural language\nprocessing (NLP) tasks. However, all Transformer computations occur at the\nlevel of word representations and therefore, it may be argued that Transformer\nmodels do not explicitly attempt to learn hierarchical structure which is\nwidely assumed to be integral to language. In the present work, we introduce\nhierarchical processing into the Transformer model, taking inspiration from the\nU-Net architecture, popular in computer vision for its hierarchical view of\nnatural images. We empirically demonstrate that the proposed architecture\noutperforms both the vanilla Transformer and some strong baselines in the\ndomain of chit-chat dialogue.\n\n**Published:** 2019-10-16T15:48:46Z  (*Updated: 2021-04-01T19:41:09Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: spectral filtering*\n\n*Micha\u00ebl Defferrard, Pierre Vandergheynst, Xavier Bresson*\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n\n**Conference:** convolutional-neural-networks-on-graphs-with-1\n\n**Published:** 2016-06-30\n\n\n\n#### 2. Spectral Collaborative Filtering\n\n*From Search Query: spectral filtering*\n\n*Philip S. Yu, Jiawei Zhang, Fei Jiang, Chun-Ta Lu, Lei Zheng*\n\n**Abstract:** Despite the popularity of Collaborative Filtering (CF), CF-based methods are haunted by the \\textit{cold-start} problem, which has a significantly negative impact on users' experiences with Recommender Systems (RS). In this paper, to overcome the aforementioned drawback, we first formulate the relationships between users and items as a bipartite graph. Then, we propose a new spectral convolution operation directly performing in the \\textit{spectral domain}, where not only the proximity information of a graph but also the connectivity information hidden in the graph are revealed. With the proposed spectral convolution operation, we build a deep recommendation model called Spectral Collaborative Filtering (SpectralCF). Benefiting from the rich information of connectivity existing in the \\textit{spectral domain}, SpectralCF is capable of discovering deep connections between users and items and therefore, alleviates the \\textit{cold-start} problem for CF. To the best of our knowledge, SpectralCF is the first CF-based method directly learning from the \\textit{spectral domains} of user-item bipartite graphs. We apply our method on several standard datasets. It is shown that SpectralCF significantly outperforms state-of-the-art models. Code and data are available at \\url{https://github.com/lzheng21/SpectralCF}.\n\n**Published:** 2018-08-30\n\n\n\n#### 3. SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes\n\n*From Search Query: hierarchical processing*\n\n*Joaquim R. R. A. Martins, Joseph Morlier, John T. Hwang, Thierry Lefebvre, Jasper Bussemaker, Youssef Diouane, Nathalie Bartoli, Remi Lafage, Paul Saves*\n\n**Abstract:** The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.\n\n**Published:** 2023-05-23\n\n\n\n#### 4. Hierarchical Neural Memory Network for Low Latency Event Processing\n\n*From Search Query: hierarchical processing*\n\n*Ken Sakurada, Masaki Onishi, Yasutaka Furukawa, Ryuhei Hamaguchi*\n\n**Abstract:** This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/\n\n**Conference:** hierarchical-neural-memory-network-for-low\n\n**Published:** 2023-05-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using spectral methods and adaptive state management, here are some key points and references that can guide the research:\n\n## Spectral Methods in Language Models\n\n### Spectral Filtering\nSpectral filtering techniques can be adapted to enhance language models by efficiently managing state spaces and handling long-range dependencies. Here are some insights:\n\n- **Efficient State Space Management**: Spectral filtering can be used to compress and manage the state space efficiently, which is crucial for improving memory efficiency in large language models. This approach is discussed in the context of general linear dynamical systems, but it can be adapted for language models.\n\n- **Multiscale Language Representations**: Using spectral filters to disentangle scale-specific information in embeddings can provide insights into multiscale processing in language models. This approach helps in capturing different scales of information within the input data, ensuring that the model can process and generate text with varying levels of detail and context.\n\n### Hierarchical Processing\n\n- **Segment Continuity and Smoothness**: Integrating spectral methods into hierarchical processing frameworks can ensure continuity and smoothness across segments. This is particularly important in autoregressive models where the generation of text is sequential and depends on the context from previous segments.\n\n## Adaptive State Management\n\n### Selective State Mechanisms and Hierarchical Normalization\n\n- **Parallel Processing and State Management**: Adaptive state management can be improved by leveraging selective state mechanisms and hierarchical normalization. These techniques enhance parallel processing and state management, allowing the model to handle complex and dynamic state transitions more efficiently.\n\n### Adaptive Control and Meta-Learning\n\n- **Adaptive Dialogue Management**: The use of Partially Observable Markov Decision Processes (POMDPs) for adaptive dialogue management highlights the importance of adaptive state management in complex systems. This framework can be extended to autoregressive language models to improve their ability to adapt to different contexts and tasks.\n\n## Relevant Architectures and Techniques\n\n### PanGu-\u03b1 Model\nThe PanGu-\u03b1 model, mentioned in the context of autoregressive models, includes a query layer at the end of standard transformer layers to predict the next token. This architecture, while not explicitly using spectral methods, demonstrates an innovative approach to token prediction that could be enhanced with spectral filtering and adaptive state management techniques.\n\n### Multilingual Models\nModels like mT5, which are trained on multilingual datasets, can benefit from spectral methods to handle the complexity of multiple languages and improve the model's ability to generate correct outputs across different languages. The use of larger vocabulary sizes and data sampling procedures can also be optimized with spectral filtering and adaptive state management.\n\n## Future Research Directions\n\n- **Investigate Recent Advancements in Spectral Methods**: Look into recent papers and preprints that discuss the application of spectral methods in machine learning and natural language processing. This could include works on spectral filters for multiscale representations and their application in language models.\n\n- **Explore Adaptive Control and Meta-Learning**: Investigate how adaptive control and meta-learning techniques can be applied to improve the adaptive state management in autoregressive language models. This could involve integrating POMDPs or other adaptive frameworks into the model architecture.\n\nBy combining these insights, researchers can develop novel autoregressive language model blocks that leverage spectral methods for efficient state management and hierarchical processing, while also incorporating adaptive state management techniques to enhance the model's performance, robustness, and scalability.\n",
            "analysis": "1. **Summary of Analysis**:\n   - Spectral methods, particularly spectral filtering, offer promising approaches for enhancing language models by improving memory efficiency and handling long-range dependencies. These methods can be integrated into hierarchical processing frameworks to ensure continuity and smoothness across segments.\n   - Adaptive state management can be improved by leveraging selective state mechanisms and hierarchical normalization, which enhance parallel processing and state management.\n\n2. **Useful References**:\n   - **Spectral Filtering for General Linear Dynamical Systems**: Discusses spectral filtering techniques for efficient phase identification, which can be adapted for language models to manage state spaces efficiently.\n   - **Language Through a Prism: A Spectral Approach for Multiscale Language Representations**: Explores spectral filters for disentangling scale-specific information in embeddings, offering insights into multiscale processing in language models.\n   - **Towards Relational POMDPs for Adaptive Dialogue Management**: Introduces a POMDP framework for adaptive dialogue management, highlighting the importance of adaptive state management in complex systems.\n\n3. **Key Insights and Analysis**:\n   - Spectral filtering can be used to enhance memory efficiency and computational adaptability by compressing and managing the state space efficiently.\n   - Hierarchical processing can be improved by applying spectral methods to input segments, ensuring continuity and smoothness across segments.\n   - Adaptive state management can benefit from selective state mechanisms and hierarchical normalization, improving parallel processing and state management.\n\n4. **Future Search Plan**:\n   - Investigate recent advancements in spectral methods and their applications in language models.\n   - Explore adaptive control and meta-learning techniques for potential applications in adaptive state management.\n\n5. **References**:\n   - Hazan, E., Lee, H., Singh, K., Zhang, C., & Zhang, Y. (2018). \"Spectral Filtering for General Linear Dynamical Systems.\" Neural Information Processing Systems.\n   - Tamkin, A., Jurafsky, D., & Goodman, N. (2020). \"Language Through a Prism: A Spectral Approach for Multiscale Language Representations.\" arXiv preprint.\n   - Lison, P. (2010). \"Towards Relational POMDPs for Adaptive Dialogue Management.\" Annual Meeting of the Association for Computational Linguistics."
        },
        {
            "ready": false,
            "query": "spectral methods HiPPO integration memory compression",
            "detail": "Find papers discussing:\n1. Integration of spectral methods with HiPPO and polynomial projections\n2. Memory compression techniques specifically for state space models\n3. Hardware-efficient alternatives to FFT operations\n4. Empirical comparisons of spectral vs polynomial architectures\n5. Memory access optimization for spectral operations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of spectral methods with HiPPO and polynomial projections\n2. Memory compression techniques specifically for state space models\n3. Hardware-efficient alternatives to FFT operations\n4. Empirical comparisons of spectral vs polynomial architectures\n5. Memory access optimization for spectral operations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.42)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.42)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.15)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.15)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 3. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers (Avg. Score: 0.14)\n\n*Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 244  (*Influential: 14*)\n\n**TL;DR:** A simple sequence model inspired by control systems that generalizes RNN heuristics, temporal convolutions, and neural differential equations while addressing their shortcomings, and introduces a trainable subset of structured matrices that endow LSSLs with long-range memory.\n\n**Abstract:** Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$ by simply simulating a linear continuous-time state-space representation $\\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.\n\n##### *Relevant Chunk: No. 31/59 (Score: 0.14)*\n\n```\nData Mining and Knowledge Discovery, pages 1-29, 2021. doi: https://doi.org/10.1007/ s10618-021-00745-9. [56] Trieu H Trinh, Andrew M Dai, Minh-Thang Luong, and Quoc V Le. Learning longer-term dependencies in RNNs with auxiliary losses. In The International Conference on Machine Learning (ICML), 2018. [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [58] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. In Advances in Neural Information Processing Systems, pages 15544-15553, 2019 . [59] Robert L Williams, Douglas A Lawrence, et al. Linear state-space control systems. Wiley Online Library, 2007 . [60] Max A Woodbury. Inverting modified matrices. Memorandum report, 42:106, 1950. [61] Guofeng Zhang, Tongwen Chen, and Xiang Chen. Performance recovery in digital implementation of analogue systems. SIAM journal on control and optimization, 45(6):2207-2223, 2007. [62] Huaguang Zhang, Zhanshan Wang, and Derong Liu. A comprehensive review of stability analysis of continuous-time recurrent neural networks. IEEE Transactions on Neural Networks and Learning Systems, 25(7):1229-1262, 2014. ## A Related Work\n\nWe provide an extended related work comparing the LSSL to previous recurrent, convolutional, and continuoustime models. HiPPO The LSSL is most closely related to the HiPPO framework for continuous-time memory 24] and its predecessor, the Legendre Memory Unit (LMU) 58. The HiPPO-RNN and the LMU define dynamics of the form of equation (1), and incorporate it into an RNN architecture. A successor to the LMU, the LMU-FFT [12] keeps the original linear dynamics, allowing the LMU to be computed with a cached convolution kernel. These methods all suffer from two main limitations. First, the state matrix $A$ and discretization timescale $\\Delta t$ cannot be trained due to both limitations in theoretical understanding of which $A$ matrices are effective, as well as computational limitations. Second, (1) is a 1-D to $N$-D map, requiring states to be projected back down to 1-D. This creates an overall 1-D bottleneck in the state, limiting the expressivity of the model. Compared to these, the LSSL does not use a conventional RNN architecture, instead keeping the linear recurrence (4) and downprojecting it with the second part of the state space representation (5).\n```\n\n#### 4. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.10)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/37 (Score: 0.10)*\n\n```\n# How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections \n\n\\author{\nAlbert $\\mathrm{Gu}^{* \\dagger}$, Isys Johnson ${ }^{* \\ddagger}$, Aman Timalsina ${ }^{\\ddagger}$, Atri Rudra ${ }^{\\ddagger}$, and Christopher R\u00e9 ${ }^{\\dagger}$ <br> ${ }^{\\dagger}$ Department of Computer Science, Stanford University <br> ${ }^{\\dagger}$ albertgu@stanford.edu, chrismre@cs.stanford.edu <br> ${ }^{\\ddagger}$ Department of Computer Science and Engineering, University at Buffalo <br> $\\ddagger$ isysjohn, amantima, atri\\}@buffalo.edu\n\n}\n\n\n#### Abstract\n\nLinear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4).\n```\n\n#### 5. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.08)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.08)*\n\n```\n# There is HOPE to Avoid HiPPOs for Long-memory State Space Models \n\nAnnan Yu, ${ }^{1 *}$ Michael W. Mahoney, ${ }^{2,3,4}$ N. Benjamin Erichson ${ }^{2,3}$<br>${ }^{1}$ Center for Applied Mathematics, Cornell University<br>${ }^{2}$ Lawrence Berkeley National Laboratory<br>${ }^{3}$ International Computer Science Institute<br>${ }^{4}$ Department of Statistics, University of California at Berkeley\n\n\n#### Abstract\n\nState-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity. ## 1 Introduction\n\nState-space models (SSMs) [14] have gained popularity and success in sequence modeling. Known for its excellent efficiency and capability of handling long sequences, an SSM leverages the continuous-time linear, time-invariant (LTI) systems. These systems are often defined by four matrices $\\Gamma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$ as\n\n$$\n\\mathbf{x}^{\\prime}(t)=\\mathbf{A} \\mathbf{x}(t)+\\mathbf{B u}(t), \\quad \\mathbf{y}(t)=\\mathbf{C x}(t)+\\mathbf{D u}(t)\n$$\n\nand they can be used to model the mappings from input time-series $\\mathbf{u}(\\cdot)$ to the output timesseries $\\mathbf{y}(\\cdot)$, where $\\mathbf{u}(t) \\in \\mathbb{R}^{m}$ and $\\mathbf{y}(t) \\in \\mathbb{R}^{p}$ for every $t$. The (hidden) states, which capture the latent dynamics, are denoted as $\\mathbf{x}=\\mathbf{x}(t) \\in \\mathbb{R}^{n}$. The system matrices are of dimensions $\\mathbf{A} \\in \\mathbb{C}^{n \\times n}, \\mathbf{B} \\in \\mathbb{C}^{n \\times m}, \\mathbf{C} \\in \\mathbb{C}^{p \\times n}$, and $\\mathbf{D} \\in \\mathbb{C}^{p \\times m}$. Often, the size $n$ of the state vector $\\mathbf{x}$ is much larger than $m$ and $p$, which allows us to memorize information about the past inputs $\\left.\\mathbf{u}\\right|_{(-\\infty, t]}$ in the state vector $\\mathbf{x}(t)$ and retrieve it later to compute $\\mathbf{y}$ via $\\mathbf{C}$. [^0]The so-called S4 [14] and S4D [13] models both set $m=p=1$, and they differ in the structural requirement of $\\mathbf{A}$. This framework was later generalized to the case where $m, p>1$ by the S 5 model [27] via the parallel scans. Another line of research involves making the state transition rule $\\mathbf{A}$ depend on the input $\\mathbf{u}$, along which the two most notable models are Liquid-S4 [17] and Mamba [11], where the latter model achieves the state-of-the-art performance on large-scale real-world datasets. However, SSMs typically need to be initialized and trained (very) carefully. A randomly initialized SSM has suboptimal performance, but the so-called high-order polynomial projection operators (HiPPO) $[29,12,15]$ can be used to empirically improve it. Subsequent work has considered stably transforming HiPPO matrices into simplified structures [13, 32]. The empirical success of the HiPPO framework is traditionally ascribed to interpreting the state vector $\\mathbf{x}(t)$ as the projection of the inputs $\\left.\\mathbf{u}\\right|_{(-\\infty, t]}$ onto some orthogonal polynomial basis. However, such a claim is not completely satisfiable - indeed, the most popular HiPPO-LegS framework is not the Legendre-basis projection operator, as it misses the scaling factor $1 / t$ (see [15, Thm.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: spectral methods HiPPO integration memory compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference\n\n*From Search Query: spectral methods HiPPO integration memory compression*\n\n*Harry Dong, Xinyu Yang, Zhenyu (Allen) Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen*\n\n**TL;DR:** LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps, shows merit on a variety of tasks where it can help reduce the performance gap from caching everything.\n\n**Abstract:** Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 4*)\n\n#### 2. LoCoCo: Dropping In Convolutions for Long Context Compression\n\n*From Search Query: spectral methods HiPPO integration memory compression*\n\n*Ruisi Cai, Yuandong Tian, Zhangyang Wang, Beidi Chen*\n\n**Abstract:** This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward\"drop-in\"integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 3. Compressed Context Memory For Online Language Model Interaction\n\n*From Search Query: spectral methods HiPPO integration memory compression*\n\n*Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh Song*\n\n**TL;DR:** This paper proposes a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments.\n\n**Abstract:** This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 4. Free Lunch for Efficient Textual Commonsense Integration in Language Models\n\n*From Search Query: spectral methods HiPPO integration memory compression*\n\n*Wanyun Cui, Xingran Chen*\n\n**TL;DR:** The idea is to group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples, and proposes a spectral clustering-based algorithm to solve this problem.\n\n**Abstract:** Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its efficiency without modifying the model. Our idea is to group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples. We theoretically investigate this problem and demonstrate that its upper bound can be reduced to the classic graph k-cut problem. Consequently, we propose a spectral clustering-based algorithm to solve this problem. Extensive experiments illustrate that the proposed batch partitioning approach effectively reduces the computational cost while preserving performance. The efficiency improvement is more pronounced on larger datasets and on devices with more memory capacity, attesting to its practical utility for large-scale applications.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. Initialization and Regularization of Factorized Neural Layers\n\n*From Search Query: spectral methods HiPPO integration memory compression*\n\n*M. Khodak, Neil A. Tenenholtz, Lester W. Mackey, Nicol\u00f3 Fusi*\n\n**TL;DR:** Factorized layers--operations parameterized by products of two or more matrices--occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures, and deep nets containing such layers are studied.\n\n**Abstract:** Factorized layers--operations parameterized by products of two or more matrices--occur in a variety of deep learning contexts, including compressed model training, certain types of knowledge distillation, and multi-head self-attention architectures. We study how to initialize and regularize deep nets containing such layers, examining two simple, understudied schemes, spectral initialization and Frobenius decay, for improving their performance. The guiding insight is to design optimization routines for these networks that are as close as possible to that of their well-tuned, non-decomposed counterparts; we back this intuition with an analysis of how the initialization and regularization schemes impact training with gradient descent, drawing on modern attempts to understand the interplay of weight-decay and batch-normalization. Empirically, we highlight the benefits of spectral initialization and Frobenius decay across a variety of settings. In model compression, we show that they enable low-rank methods to significantly outperform both unstructured sparsity and tensor methods on the task of training low-memory residual networks; analogs of the schemes also improve the performance of tensor decomposition techniques. For knowledge distillation, Frobenius decay enables a simple, overcomplete baseline that yields a compact model from over-parameterized training without requiring retraining with or pruning a teacher network. Finally, we show how both schemes applied to multi-head attention lead to improved performance on both translation and unsupervised pre-training.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 50  (*Influential: 12*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on the integration of spectral methods with HiPPO, memory compression techniques, and hardware-efficient alternatives, here are some key findings and references that align with your goals:\n\n## Integration of Spectral Methods with HiPPO and Polynomial Projections\n- The HiPPO framework, introduced by Gu et al. in 2020, uses polynomial projections to achieve efficient and scalable memory updates. This framework has been generalized to other orthogonal functions and has inspired various works, including those that integrate with deep state-space models. For example, the HiPPO formalism has been extended to use other common orthogonal functions, which can be seen as a form of spectral method integration.\n\n## Memory Compression Techniques for State Space Models\n- State-space models (SSMs) like HiPPO and its variants often rely on efficient memory management to scale. The HiPPO paper itself introduces a memory update mechanism that avoids priors on the timescale, effectively compressing the memory needed to remember the history of the sequence.\n- The work on S4 and H3 models, which build upon the HiPPO framework, also focuses on efficient state-space modeling. These models use techniques such as depthwise structures and specific polynomial projections to reduce computational and memory requirements.\n\n## Hardware-Efficient Alternatives to FFT Operations\n- FFT operations are known for their poor hardware utilization, particularly in terms of using specialized matrix multiply units and incurring expensive I/O between layers of the memory hierarchy. Alternatives such as using Monarch matrices or FlashFFTConv have been proposed to replace FFT operations with more hardware-efficient methods. For instance, the Monarch Mixer paper shows that replacing FFT operations with Monarch matrices can improve hardware efficiency.\n\n## Empirical Comparisons of Spectral vs Polynomial Architectures\n- Empirical comparisons between spectral and polynomial architectures are crucial for understanding their trade-offs. While the HiPPO paper and its follow-ups provide insights into the performance of polynomial projections, comparing these directly to spectral methods would require specific studies. For example, the Centaurus model discussed in the context of deep SSM models explores different block configurations, including those that could be compared to spectral methods in terms of performance and efficiency.\n\n## Memory Access Optimization for Spectral Operations\n- Memory access patterns are critical for the performance of spectral operations. The FlashFFTConv paper highlights the importance of optimizing memory access and using tensor cores efficiently for convolutions, which can be extended to spectral operations. Efficient parallel processing strategies and hardware-specific optimizations are also essential for optimizing memory access in these contexts.\n\nIn summary, the following resources provide valuable insights:\n\n- **HiPPO and its extensions**: These papers discuss the integration of polynomial projections and spectral methods, as well as memory compression techniques.\n- **Monarch Mixer and FlashFFTConv**: These works offer hardware-efficient alternatives to FFT operations and highlight the importance of memory access optimization.\n- **Deep SSM models**: These models, such as S4, H3, and Centaurus, provide insights into efficient state-space modeling and the comparison of different architectures.\n\nBy leveraging these resources, researchers can develop more efficient and scalable autoregressive language models that integrate spectral methods with HiPPO and optimize memory and hardware usage.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal important insights about hardware efficiency, spectral operations, and state space models that are relevant to evaluating the SpectralHiPPOQ proposal.\n\nKey Findings:\n\n1. Hardware Efficiency Challenges:\nFrom Monarch Mixer paper:\n\"FFT operations rely on the convolution theorem... replacing FFT operations with Monarch matrices shows better hardware efficiency.\"\n\n2. Memory Management:\nFrom FlashFFTConv:\n\"Two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy.\"\n\n3. Polynomial Projections:\nFrom HiPPO paper:\n\"The framework yields a new memory update mechanism that scales through time to remember all history, avoiding priors on the timescale.\"\n\nKey Insights:\n\n1. Hardware Optimization:\n- FFT operations have poor hardware utilization\n- Need for specialized matrix multiply units\n- Memory hierarchy I/O is a significant bottleneck\n\n2. Spectral Methods:\n- Can be replaced with more hardware-efficient alternatives\n- Trade-off between computational efficiency and accuracy\n- Integration with polynomial projections needs careful consideration\n\n3. Implementation Considerations:\n- Memory access patterns crucial for performance\n- Need for efficient parallel processing strategies\n- Hardware-specific optimizations required\n\nFuture Search Plan:\nNeed additional information on:\n1. Integration of spectral methods with HiPPO framework\n2. Hardware-efficient alternatives to FFT operations\n3. Memory compression techniques for polynomial projections\n4. Empirical comparisons of different spectral architectures\n\nReferences:\n[1] Fu, D.Y., et al. (2023). \"Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture.\" NeurIPS.\n[2] Fu, D.Y., et al. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores.\" arXiv preprint.\n[3] Gu, A., et al. (2020). \"HiPPO: Recurrent Memory with Optimal Polynomial Projections.\" NeurIPS.\n[4] Zhu, R.J., et al. (2024). \"Scalable MatMul-free Language Modeling.\" arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "spectral state space models polynomial projections",
            "detail": "Find papers discussing:\n1. Integration of spectral filtering with polynomial projections or state space models\n2. Hierarchical memory management using spectral methods\n3. Applications of FFT or spectral methods in transformer architectures\n4. Memory compression techniques using spectral decomposition\n5. Adaptive scale integration with spectral filtering",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of spectral filtering with polynomial projections or state space models\n2. Hierarchical memory management using spectral methods\n3. Applications of FFT or spectral methods in transformer architectures\n4. Memory compression techniques using spectral decomposition\n5. Adaptive scale integration with spectral filtering\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.99)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.99)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.54)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.54)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.21)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.21)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.17)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 13/54 (Score: 0.17)*\n\n```\nIn Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2019. [19] Tri Dao, Christopher M De Sa, and Christopher R\u00e9. Gaussian quadrature for kernel features. In Advances in Neural Information Processing Systems (NeurIPS), pages 6107-6117, 2017. [20] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In The International Conference on Machine Learning (ICML), 2019. [21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In The International Conference on Learning Representations (ICLR), 2020. [22] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1060-1079. SIAM, 2018. [23] Raymond A DeCarlo. Linear systems: A state variable approach with numerical implementation. Prentice-Hall, Inc., 1989. [24] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems (NeurIPS), pages $3844-3852,2016$. [25] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci. edu $/ \\mathrm{ml}$. [26] Krzysztof Duda. Accurate, guaranteed stable, sliding discrete Fourier transform [DSP tips \\& tricks]. IEEE Signal Processing Magazine, 27(6):124-127, 2010. [27] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural ODEs. In Advances in Neural Information Processing Systems, pages 3134-3144, 2019. [28] Behrouz Farhang-Boroujeny and Saeed Gazor. Generalized sliding FFT and its application to implementation of block LMS adaptive filters.\n```\n\n#### 5. The Hidden Attention of Mamba Models  (Avg. Score: 0.09)\n\n*Ameen Ali, Itamar Zimerman, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 19  (*Influential: 3*)\n\n**TL;DR:** A third view is added and it is shown that selective SSMs can be viewed as attention-driven models and empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers.\n\n**Abstract:** The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains, including NLP, long-range sequence processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/33 (Score: 0.09)*\n\n```\nvol. 34, pp. 2501-2508 (2020) 4, 12\n54. Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., R\u00e9, C.: S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in neural information processing systems 35, 2846-2861 (2022) 3\n55. Nguyen, T., Nguyen, T., Baraniuk, R.: Mitigating over-smoothing in transformers via regularized nonlocal functionals. Advances in Neural Information Processing Systems 36 (2024) 20\n56. Park, J., Park, J., Xiong, Z., Lee, N., Cho, J., Oymak, S., Lee, K., Papailiopoulos, D.: Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248 (2024) 14\n57. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K.K., et al.: Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 (2023) 14\n58. Pi\u00f3ro, M., Ciebiera, K., Kr\u00f3l, K., Ludziejewski, J., Jaszczur, S.: Moe-mamba: Efficient selective state space models with mixture of experts. arXiv preprint arXiv:2401.04081 (2024) 1,3\n59. Poli, M., Massaroli, S., Nguyen, E., Fu, D.Y., Dao, T., Baccus, S., Bengio, Y., Ermon, S., R\u00e9, C.: Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866 (2023) 5, 13, 14, 27\n60. Romero, D.W., Kuzina, A., Bekkers, E.J., Tomczak, J.M., Hoogendoorn, M.: Ckconv: Continuous kernel convolution for sequential data. arXiv preprint arXiv:2102.02611 (2021) 27\n61. Ru, L., Zheng, H., Zhan, Y., Du, B.: Token contrast for weakly-supervised semantic segmentation.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: spectral state space models polynomial projections\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections\n\n*From Search Query: spectral state space models polynomial projections*\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 68  (*Influential: 5*)\n\n#### 2. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: spectral state space models polynomial projections*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 3. Solving High-Dimensional PDEs with Latent Spectral Models\n\n*From Search Query: spectral state space models polynomial projections*\n\n*Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, Mingsheng Long*\n\n**TL;DR:** Inspired by classical spectral methods in numerical analysis, this work designs a neural spectral block to solve PDEs in the latent space that approximates complex input-output mappings via learning multiple basis operators, enjoying nice theoretical guarantees for convergence and approximation.\n\n**Abstract:** Deep models have achieved impressive progress in solving partial differential equations (PDEs). A burgeoning paradigm is learning neural operators to approximate the input-output mappings of PDEs. While previous deep models have explored the multiscale architectures and various operator designs, they are limited to learning the operators as a whole in the coordinate space. In real physical science problems, PDEs are complex coupled equations with numerical solvers relying on discretization into high-dimensional coordinate space, which cannot be precisely approximated by a single operator nor efficiently learned due to the curse of dimensionality. We present Latent Spectral Models (LSM) toward an efficient and precise solver for high-dimensional PDEs. Going beyond the coordinate space, LSM enables an attention-based hierarchical projection network to reduce the high-dimensional data into a compact latent space in linear time. Inspired by classical spectral methods in numerical analysis, we design a neural spectral block to solve PDEs in the latent space that approximates complex input-output mappings via learning multiple basis operators, enjoying nice theoretical guarantees for convergence and approximation. Experimentally, LSM achieves consistent state-of-the-art and yields a relative gain of 11.5% averaged on seven benchmarks covering both solid and fluid physics. Code is available at https://github.com/thuml/Latent-Spectral-Models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 4*)\n\n#### 4. Graph Neural Networks with Learnable and Optimal Polynomial Bases\n\n*From Search Query: spectral state space models polynomial projections*\n\n*Y. Guo, Zhewei Wei*\n\n**TL;DR:** This paper proposes two spectral GNN models, inspired by Favard's Theorem, which learns a polynomial basis from the space of all possible orthonormal bases and proposes a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal.\n\n**Abstract:** Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features? In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from Wang&Zhang (2022) and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conducted to demonstrate the effectiveness of our proposed models. Our code is available at https://github.com/yuziGuo/FarOptBasis.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 5*)\n\n#### 5. Spectral Graph Matching and Regularized Quadratic Relaxations: Algorithm and Theory\n\n*From Search Query: spectral state space models polynomial projections*\n\n*Z. Fan, Cheng Mao, Yihong Wu, Jiaming Xu*\n\n**TL;DR:** A spectral method, GRAph Matching by Pairwise eigen-Alignments (GRAMPA), which constructs a similarity matrix as a weighted sum of outer products between all pairs of eigenvectors of the two graphs, and outputs a matching by a simple rounding procedure.\n\n**Abstract:** Graph matching, also known as network alignment, aims at recovering the latent vertex correspondence between two unlabeled, edge-correlated weighted graphs. To tackle this task, we propose a spectral method, GRAph Matching by Pairwise eigen-Alignments (GRAMPA), which \ufb01rst constructs a similarity matrix as a weighted sum of outer products between all pairs of eigenvectors of the two graphs, and then outputs a matching by a simple rounding procedure. For a universality class of correlated Wigner models, GRAMPA achieves exact recovery of the latent matching between two graphs with edge correlation 1 \u2212 1 / polylog( n ) and average degree at least polylog( n ) . This matches the state-of-the-art guarantees for polynomial-time algorithms established for correlated Erd\u02ddos-R\u00b4enyi graphs, and signi\ufb01cantly improves over existing spectral meth-ods. The superiority of GRAMPA is also demonstrated on a variety of synthetic and real datasets, in terms of both statistical accuracy and computational ef\ufb01ciency.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 43  (*Influential: 7*)\n\n\n\n---\n## Web search results\n\n To help the researchers improve autoregressive language model design using the specified keywords and details, here are some relevant findings and directions:\n\n### Integration of Spectral Filtering with Polynomial Projections or State Space Models\n\n- The proposal to integrate spectral filtering with HiPPO polynomial projections is novel and warrants investigation. While the provided sources do not directly address this integration, the concept can be inferred from papers that discuss advanced time series forecasting models. For example, the \"SAMformer\" and \"PETformer\" models mentioned in the time series forecasting repository use advanced transformer architectures that could be adapted to incorporate spectral filtering. Specifically, the \"PETformer\" uses placeholder-enhanced transformers, which might be a starting point for integrating spectral methods.\n\n### Hierarchical Memory Management Using Spectral Methods\n\n- Hierarchical memory management is crucial for efficient and scalable models. The \"SageFormer\" and \"PRformer\" models use series-aware graph-enhanced transformers and pyramidal recurrent transformers, respectively, which involve hierarchical structures. While these models do not explicitly use spectral methods, they demonstrate the feasibility of hierarchical approaches. To integrate spectral methods, one could explore how spectral decomposition can be used for scale-specific compression, similar to the hierarchical polynomial memory approaches mentioned in the initial analysis.\n\n### Applications of FFT or Spectral Methods in Transformer Architectures\n\n- There is limited direct literature on the application of FFT or spectral methods within transformer architectures for language models. However, the concept of using spectral methods in time series forecasting models, such as the \"W-Transformers\" which use a wavelet-based transformer framework, can be a valuable reference. Adapting these ideas to language models could involve using spectral methods for feature extraction or preprocessing, potentially enhancing the model's ability to capture long-range dependencies.\n\n### Memory Compression Techniques Using Spectral Decomposition\n\n- Spectral decomposition can be a powerful tool for memory compression. While the sources provided do not directly address this in the context of language models, the idea of using spectral methods for compression can be drawn from signal processing literature. For example, the discussion on filtering and spectral methods in the MNE documentation and the Savitzky-Golay filter notes provide insights into how spectral techniques can be used to reduce data complexity.\n\n### Adaptive Scale Integration with Spectral Filtering\n\n- Adaptive scale integration is a key aspect of the proposal. The \"TiVaT\" model, which uses joint-axis attention for time series forecasting with lead-lag dynamics, demonstrates adaptive scale processing. Integrating spectral filtering into such models could enhance their ability to handle multi-scale data efficiently. The \"PDETime\" model, which rethinks long-term multivariate time series forecasting from the perspective of partial differential equations, also offers a framework where spectral methods could be incorporated for adaptive scale integration.\n\n### Investigation Plan and Initial Assessment\n\n- To verify the uniqueness and effectiveness of the proposed SpectralHiPPOQ model, researchers should:\n  - Research existing state space models and memory systems that use spectral filtering, such as those mentioned in the time series forecasting repository.\n  - Examine hierarchical polynomial memory approaches and how they can be enhanced with spectral methods.\n  - Study adaptive scale integration techniques in existing models and explore how spectral filtering can improve these methods.\n  - Compare the proposed model with existing HiPPO variants and other spectral methods to ensure it offers a unique and efficient solution.\n\nBy exploring these directions and integrating insights from time series forecasting and signal processing, the researchers can develop a novel autoregressive language model that leverages spectral methods to improve efficiency, memory usage, and overall performance.\n",
            "analysis": "Initial Analysis of SpectralHiPPOQ Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Spectral Filtering Integration\n- Novel combination of spectral filtering with HiPPO polynomial projections\n- Potential impact on computational efficiency and memory usage\n- Stability considerations for spectral operations\n\n2. Hierarchical Memory Management\n- Effectiveness of scale-specific compression\n- Trade-offs between compression and representation quality\n- Comparison with existing hierarchical approaches\n\n3. Adaptive Scale Integration\n- Novel aspects of spectral-based scale integration\n- Efficiency of multi-scale processing\n- Comparison with existing adaptive approaches\n\nInvestigation Plan:\n1. Research spectral filtering in state space models and memory systems\n2. Examine hierarchical polynomial memory approaches\n3. Study adaptive scale integration techniques\n4. Compare with existing HiPPO variants and spectral methods\n\nInitial Assessment:\n- The proposal combines spectral filtering with hierarchical polynomial memory in a novel way\n- Integration of spectral methods could improve efficiency\n- Need to verify uniqueness compared to existing approaches\n- Potential concerns about computational overhead from FFT operations\n\nCompared to Previous Designs:\n- More focused on spectral methods than metahippoq\n- Different approach to memory management than dynamichypermlp\n- Novel integration of spectral filtering compared to heragate\n- Distinct from hgss_gau's state space approach\n\nThe proposal appears to offer a unique combination of techniques, but requires verification through literature search."
        },
        {
            "ready": false,
            "query": "hierarchical compression state space models spectral",
            "detail": "Find papers discussing:\n1. Hierarchical compression techniques in state space models\n2. Memory-efficient implementations of spectral methods\n3. Adaptive scaling in polynomial projections\n4. Hardware optimization for spectral operations in neural networks\n5. Integration of spectral methods with hierarchical structures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical compression techniques in state space models\n2. Memory-efficient implementations of spectral methods\n3. Adaptive scaling in polynomial projections\n4. Hardware optimization for spectral operations in neural networks\n5. Integration of spectral methods with hierarchical structures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.91)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.91)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors (Avg. Score: 0.81)\n\n*Ido Amos, Jonathan Berant, Ankit Gupta*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 10  (*Influential: 0*)\n\n**TL;DR:** This work finds vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and improves the best reported results of SSMs on the PathX-256 task by 20 absolute points.\n\n**Abstract:** Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.\n\n##### *Relevant Chunk: No. 16/31 (Score: 0.81)*\n\n```\narXiv.org, abs/2112.10740, 12 2021. ISSN 2331-8422. URL/https://arxiv.org/abs/2112.10740\n\nDaniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling. In Andreas Krause 0001, Emma Brunskill, KyungHyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 2329 July 2023, Honolulu, Hawaii, USA, volume abs/2302.06646, pp. 10373-10391. PMLR, 2 2023. doi: 10.48550/arxiv.2302.06646. URLhttps://proceedings.mlr.press/v202/ fu23a.html. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling. volume abs/2101.00027, 12 2020. URL https://arxiv.org/abs/2101.00027. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, volume abs/2008.07669, 8 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https: / / openreview. net/ forum?id=uYLFoz1vlAC\n\nAlbert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.08)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.08)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.06)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.06)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 5. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.05)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.05)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical compression state space models spectral\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: hierarchical compression state space models spectral*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 2. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: hierarchical compression state space models spectral*\n\n*Paul Mattes, Rainer Schlosser, R. Herbrich*\n\n**TL;DR:** Hieros is a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space that allows for more efficient training than RNN- based world models and more efficient imagination than Transformer-based world models.\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: hierarchical compression state space models spectral*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Solving High-Dimensional PDEs with Latent Spectral Models\n\n*From Search Query: hierarchical compression state space models spectral*\n\n*Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, Mingsheng Long*\n\n**TL;DR:** Inspired by classical spectral methods in numerical analysis, this work designs a neural spectral block to solve PDEs in the latent space that approximates complex input-output mappings via learning multiple basis operators, enjoying nice theoretical guarantees for convergence and approximation.\n\n**Abstract:** Deep models have achieved impressive progress in solving partial differential equations (PDEs). A burgeoning paradigm is learning neural operators to approximate the input-output mappings of PDEs. While previous deep models have explored the multiscale architectures and various operator designs, they are limited to learning the operators as a whole in the coordinate space. In real physical science problems, PDEs are complex coupled equations with numerical solvers relying on discretization into high-dimensional coordinate space, which cannot be precisely approximated by a single operator nor efficiently learned due to the curse of dimensionality. We present Latent Spectral Models (LSM) toward an efficient and precise solver for high-dimensional PDEs. Going beyond the coordinate space, LSM enables an attention-based hierarchical projection network to reduce the high-dimensional data into a compact latent space in linear time. Inspired by classical spectral methods in numerical analysis, we design a neural spectral block to solve PDEs in the latent space that approximates complex input-output mappings via learning multiple basis operators, enjoying nice theoretical guarantees for convergence and approximation. Experimentally, LSM achieves consistent state-of-the-art and yields a relative gain of 11.5% averaged on seven benchmarks covering both solid and fluid physics. Code is available at https://github.com/thuml/Latent-Spectral-Models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 4*)\n\n#### 5. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: hierarchical compression state space models spectral*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 2*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the specified keywords and details, here are some key points and references that can be helpful:\n\n## Hierarchical Compression Techniques in State Space Models\n- The concept of hierarchical compression can be applied to state space models by utilizing techniques that reduce the dimensionality of the state space while preserving essential information. For instance, the use of Principal Component Analysis (PCA) or other dimensionality reduction methods can be adapted from other domains. Although the provided sources do not directly discuss hierarchical compression in state space models, the idea of reducing dimensionality using PCA, as seen in the context of feature extraction, can be a starting point.\n\n## Memory-Efficient Implementations of Spectral Methods\n- Spectral methods, such as those used in Spectral State Space Models (SSSMs), can be optimized for memory efficiency. The \"FlashFFTConv\" paper highlights the inefficiencies of FFT operations in terms of hardware utilization and memory hierarchy I/O. This suggests the need for alternative convolutional methods that better utilize hardware resources, such as specialized matrix multiply units.\n\n## Adaptive Scaling in Polynomial Projections\n- The HiPPO framework, which involves polynomial projections in sequence modeling, can be extended to include adaptive scaling. This involves adjusting the polynomial bases dynamically based on the input data to improve the model's ability to capture long-range dependencies efficiently. The theoretical foundations of HiPPO and its variants, such as those using the Fourier basis, provide a rich framework for exploring adaptive scaling.\n\n## Hardware Optimization for Spectral Operations in Neural Networks\n- Optimizing spectral operations for hardware involves ensuring that the algorithms are compatible with the architecture of the hardware. For example, the \"FlashFFTConv\" paper discusses the limitations of FFT in utilizing specialized matrix multiply units and suggests alternative convolutional methods that are more hardware-friendly. This optimization is crucial for efficient training and inference in neural networks.\n\n## Integration of Spectral Methods with Hierarchical Structures\n- Integrating spectral methods with hierarchical structures can be achieved by combining spectral state space models with hierarchical representations. The work on \"Spectral State Space Models\" and \"HiPPO\" provides a foundation for this integration. By using hierarchical forms, such as those described in the context of renormalizing generative models (RGMs), one can learn compositionality over space and time, which is beneficial for sequence modeling and autoregressive language models.\n\n### Additional Considerations\n\n- **Hierarchical Universal Value Function Approximators (H-UVFAs):** While not directly related to language models, the hierarchical structure and embedding techniques used in H-UVFAs can provide insights into how to construct hierarchical models that efficiently capture complex dependencies. The use of higher-order tensor decompositions (like PARAFAC or Tucker decomposition) to create embeddings can be adapted to sequence modeling.\n\n- **Renormalizing Generative Models (RGMs):** These models, which use hierarchical forms and renormalization group techniques, can be applied to learn compositionality over long sequences. This approach can be integrated with spectral methods to enhance the ability of autoregressive models to handle long-range dependencies and complex structures.\n\nBy combining these insights, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage hierarchical compression, adaptive scaling, and optimized spectral operations.\n",
            "analysis": "Summary of Analysis:\nThe initial search results provide valuable insights into spectral state space models and polynomial projections, but more information is needed on specific aspects of the proposal.\n\nKey Findings:\n\n1. Spectral State Space Models:\n- Novel formulation combining spectral filtering with state space models [1]\n- Theoretical advantages in handling long-range dependencies without spectrum dependency\n- Efficient computation through fixed convolutional filters\n\n2. HiPPO Framework:\n- Mathematical foundations for polynomial projections in sequence modeling [2]\n- Connection between SSMs and polynomial bases\n- Importance of initialization and normalization\n\n3. Implementation Considerations:\n- FFT operations have poor hardware utilization [3]\n- Need for efficient memory management in long sequences\n- Trade-offs between compression and representation quality\n\nUseful References with Excerpts:\n\n[1] From \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n\n[2] From \"How to Train Your HiPPO\":\n\"Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis.\"\n\n[3] From \"FlashFFTConv\":\n\"The FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy.\"\n\nFuture Search Plan:\nNeed additional searches on:\n1. Hierarchical compression techniques in state space models\n2. Integration of spectral methods with adaptive scaling\n3. Memory efficiency in polynomial projections\n4. Hardware optimization for spectral operations\n\nReferences:\n[1] Agarwal, N., et al. (2023). \"Spectral State Space Models.\" arXiv preprint.\n[2] Gu, A., et al. (2022). \"How to Train Your HiPPO.\" ICLR.\n[3] Fu, D.Y., et al. (2023). \"FlashFFTConv.\" arXiv preprint.\n[4] Gu, A., et al. (2020). \"HiPPO: Recurrent Memory with Optimal Polynomial Projections.\" NeurIPS.\n[5] Ali, A., et al. (2024). \"The Hidden Attention of Mamba Models.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "spectral filtering neural networks memory compression",
            "detail": "Find papers discussing:\n1. Spectral filtering techniques in neural networks and their applications to sequence modeling\n2. Memory compression methods specifically for state space models and transformers\n3. Hardware-efficient implementations of spectral operations and polynomial projections\n4. Integration of spectral methods with adaptive scaling in neural architectures\n5. Empirical comparisons of different spectral filtering approaches",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Spectral filtering techniques in neural networks and their applications to sequence modeling\n2. Memory compression methods specifically for state space models and transformers\n3. Hardware-efficient implementations of spectral operations and polynomial projections\n4. Integration of spectral methods with adaptive scaling in neural architectures\n5. Empirical comparisons of different spectral filtering approaches\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.87)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.87)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 3. Block-State Transformers (Avg. Score: 0.67)\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n##### *Relevant Chunk: No. 14/28 (Score: 0.67)*\n\n```\n[11] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling, 2023. [12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections, 2020. [13] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [14] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022. [15] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models, 2022. [16] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [17] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. [18] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(2):107-116, 1998. [19] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. [20] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9099-9117. PMLR, 17-23 Jul 2022. [21] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Blockrecurrent transformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.64)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.64)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 5. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.38)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.38)*\n\n```\nIn International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: spectral filtering neural networks memory compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: spectral filtering neural networks memory compression*\n\n*M. Defferrard, X. Bresson, P. Vandergheynst*\n\n**TL;DR:** This work presents a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs.\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 7140  (*Influential: 768*)\n\n#### 2. EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression\n\n*From Search Query: spectral filtering neural networks memory compression*\n\n*Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu*\n\n**TL;DR:** An optimized GPU implementation which supports training GNNs with compressed activations for Pytorch Geometric and Pytorch, and a memory-ef\ufb01cient framework called EXACT, which for the first time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations.\n\n**Abstract:** Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously dif\ufb01cult to implement due to the lack of necessary tools in common graph learning packages. To un-leash the potential of this direction, we provide an optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-ef\ufb01cient framework called \u201cEXACT\u201d, which for the \ufb01rst time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to 32 \u00d7 with 0 . 2 - 0 . 5% accuracy drop and 10 - 25% time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on ogbn-products from a 48GB GPU to a 12GB GPU. The code is available at https://github.com/warai-0toko\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 55  (*Influential: 7*)\n\n#### 3. Specformer: Spectral Graph Neural Networks Meet Transformers\n\n*From Search Query: spectral filtering neural networks memory compression*\n\n*Deyu Bo, Chuan Shi, Lele Wang, Renjie Liao*\n\n**TL;DR:** This work introduces Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter and design a decoder with learnable bases to enable non-local graph convolution.\n\n**Abstract:** Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 57  (*Influential: 10*)\n\n#### 4. A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks\n\n*From Search Query: spectral filtering neural networks memory compression*\n\n*Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Liang Chen, Zibin Zheng, Baokun Wang, Changhua Meng*\n\n**TL;DR:** This work proposes SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance, and provides theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts.\n\n**Abstract:** While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of higher task accuracy requires a larger hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This work explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compression, SpikeGCL is either comparable to or outperforms many fancy state-of-the-art supervised and self-supervised methods across several graph benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 5. \"Lossless\" Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach\n\n*From Search Query: spectral filtering neural networks memory compression*\n\n*Lingyu Gu, Yongqiang Du, Yuan Zhang, Di Xie, Shiliang Pu, R. Qiu, Zhenyu Liao*\n\n**TL;DR:** This paper demonstrates that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there existsasymptotic spectral equivalence between the NTK matrices for a large family of DNN models.\n\n**Abstract:** Modern deep neural networks (DNNs) are extremely powerful; however, this comes at the price of increased depth and having more parameters per layer, making their training and inference more computationally challenging. In an attempt to address this key limitation, efforts have been devoted to the compression (e.g., sparsification and/or quantization) of these large-scale machine learning models, so that they can be deployed on low-power IoT devices. In this paper, building upon recent advances in neural tangent kernel (NTK) and random matrix theory (RMT), we provide a novel compression approach to wide and fully-connected \\emph{deep} neural nets. Specifically, we demonstrate that in the high-dimensional regime where the number of data points $n$ and their dimension $p$ are both large, and under a Gaussian mixture model for the data, there exists \\emph{asymptotic spectral equivalence} between the NTK matrices for a large family of DNN models. This theoretical result enables\"lossless\"compression of a given DNN to be performed, in the sense that the compressed network yields asymptotically the same NTK as the original (dense and unquantized) network, with its weights and activations taking values \\emph{only} in $\\{ 0, \\pm 1 \\}$ up to a scaling. Experiments on both synthetic and real-world data are conducted to support the advantages of the proposed compression scheme, with code available at \\url{https://github.com/Model-Compression/Lossless_Compression}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using the specified keywords and details, here are some key points and references that can be useful:\n\n## Spectral Filtering Techniques in Neural Networks for Sequence Modeling\n\n- While the provided sources do not directly discuss spectral filtering in the context of sequence modeling for autoregressive language models, the concept of spectral methods can be adapted from other domains. For instance, the use of graph neural networks (GNNs) and their spectral filtering layers can be insightful. In the context of image segmentation, spectral filtering layers in GNNs enable localized spectral filtering, capturing relationships between nodes (or patches in images).\n- To apply this to sequence modeling, one could consider using spectral methods to filter and process sequential data. This might involve transforming the sequence data into a spectral domain to capture long-range dependencies more effectively.\n\n## Memory Compression Methods for State Space Models and Transformers\n\n- The SSM-pooler model mentioned in the analysis is a good example of memory efficiency in state space models. It achieves better efficiency compared to attention-based models by using hierarchical compression for long sequences.\n- For transformers, memory compression can be achieved through techniques like hierarchical state space models (HiSS) that stack structured state-space models to create efficient temporal hierarchies. This approach can reduce memory usage while maintaining performance.\n\n## Hardware-Efficient Implementations of Spectral Operations and Polynomial Projections\n\n- The \"FlashFFTConv\" paper highlights the inefficiencies of FFT operations in utilizing specialized matrix multiply units and the costly I/O between layers of the memory hierarchy. This suggests the need for optimized hardware implementations for spectral operations, such as using tensor cores for efficient convolutions.\n- For polynomial projections, hardware-efficient implementations might involve designing specialized matrix multiply units or leveraging existing architectures like tensor cores to accelerate these operations.\n\n## Integration of Spectral Methods with Adaptive Scaling in Neural Architectures\n\n- Adaptive scaling in neural architectures can be integrated with spectral methods by dynamically adjusting the spectral filters based on the input data. This could involve using attention mechanisms or other adaptive techniques to scale the spectral filters, similar to how Vision Transformers (ViTs) use self-attention to capture global context.\n- The concept of modularity-based loss functions and adaptive filtering, as seen in unsupervised image segmentation using GNNs, could be adapted to sequence modeling. This involves using spectral methods to capture inherent graph topology and adjusting the model based on the data's structure.\n\n## Empirical Comparisons of Different Spectral Filtering Approaches\n\n- Empirical comparisons would involve evaluating different spectral filtering approaches in the context of sequence modeling. This could include comparing traditional Fourier transform-based methods with more modern approaches like graph convolutional layers or other spectral methods adapted from image processing or graph neural networks.\n- The review on hypergraph neural networks provides a framework for comparing different spectral analysis methods and neural network methods, which could be applied to sequence modeling by evaluating the performance of various spectral filtering techniques on sequence data.\n\n### Additional Considerations\n\n- **Hierarchical State Space Models**: The HiSS model's approach to stacking structured state-space models can be beneficial for sequence modeling, offering a way to create temporal hierarchies that are compatible with spectral filtering techniques.\n- **Graph Neural Networks**: The use of GNNs, especially those with spectral filtering layers, can provide insights into how to capture long-range dependencies in sequence data efficiently.\n\nBy integrating these concepts, researchers can develop more efficient and scalable autoregressive language models that leverage spectral filtering, adaptive scaling, and hardware-efficient implementations to improve performance and reduce memory usage.\n",
            "analysis": "Summary of Analysis:\n\nThe search results provide valuable insights into spectral methods, state space models, and hierarchical architectures, but more specific information is needed about spectral filtering in memory systems and hardware optimization.\n\nKey Findings:\n\n1. Hierarchical State Space Models:\n- HiSS demonstrates superior performance in sequence modeling [1]\n- Stacking structured state-space models creates effective temporal hierarchies\n- Shows potential for efficient scaling and compatibility with filtering techniques\n\n2. Hardware Considerations:\n- FFT operations have poor hardware utilization [2]\n- Need for specialized matrix multiply units optimization\n- Memory hierarchy I/O challenges in spectral operations\n\n3. Memory Efficiency:\n- SSM-pooler achieves 36% better efficiency than attention-based models [3]\n- Importance of hierarchical compression for long sequences\n- Trade-offs between compression and model quality\n\nUseful References with Excerpts:\n\n[1] From \"Hierarchical State Space Models\":\n\"HiSS stacks structured state-space models to create a temporal hierarchy... outperforms state-of-the-art sequence models by at least 23% on MSE\"\n\n[2] From \"FlashFFTConv\":\n\"The FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy\"\n\n[3] From \"Efficient Classification of Long Documents\":\n\"SSM-pooler model achieves comparable performance while being on average 36% more efficient\"\n\nFuture Search Plan:\nNeed additional information on:\n1. Spectral filtering techniques in neural networks\n2. Memory compression methods in state space models\n3. Hardware-efficient implementations of polynomial projections\n4. Integration of spectral methods with adaptive scaling\n\nReferences:\n[1] Bhirangi, R.M., et al. (2024). \"Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling.\" ICML.\n[2] Fu, D.Y., et al. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores.\" arXiv preprint.\n[3] Lu, P., et al. (2023). \"Efficient Classification of Long Documents via State-Space Models.\" EMNLP.\n[4] Wu, H., et al. (2023). \"Solving High-Dimensional PDEs with Latent Spectral Models.\" ICML.\n[5] Kwan, H.M., et al. (2023). \"HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation.\" NeurIPS."
        },
        {
            "ready": false,
            "query": "hardware efficient spectral operations polynomial projections",
            "detail": "Find papers discussing:\n1. Hardware-efficient implementations of spectral operations in neural networks\n2. Memory-efficient polynomial projection techniques\n3. Optimization strategies for FFT operations in deep learning\n4. Hardware acceleration methods for state space models\n5. Empirical comparisons of spectral filtering architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hardware-efficient implementations of spectral operations in neural networks\n2. Memory-efficient polynomial projection techniques\n3. Optimization strategies for FFT operations in deep learning\n4. Hardware acceleration methods for state space models\n5. Empirical comparisons of spectral filtering architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (Avg. Score: 0.99)\n\n*Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 2*)\n\n**TL;DR:** A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n##### *Relevant Chunk: No. 4/67 (Score: 0.99)*\n\n```\n0}$ | $\\mathbf{9 . 6}$ | $\\mathbf{9 . 0}$ | Attention-Free, MLP-Free |\n\n## 6 Related Work\n\nLong Convolutions Recent work proposes to use long convolution layers as a replacement for the Transformer attention layers in sequence modeling [26,63, 66-68]. Many of these models rely on the FFT convolution theorem to compute the long convolutions. We build on the insights in many of these architectures in constructing our M2 architectures, and additionally replaces the FFT operations with Monarch matrices. Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev bases [79] or orthogonal polynomial bases [32]. These approaches have analogues in our multivariate analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be able to approximate some of these operations. An interesting question for future work would be to study how well our techniques and concerns about causality and hardware utilization translate to these alternative convolution bases. Optimization of deep learning primitives There is a rich history of the optimization of deep learning primitives, as accelerating their performance can yield substantial savings in compute and cost for large models. There are many approaches to speed up these operations, but they usually either reduce data movement or compute. Reducing data movement: In many applications, the major bottleneck is the storage and movement of large amounts of memory. One popular approach to reducing data movement is checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory $[44,76]$. Another approach is kernel fusion, wherein algorithms initially described as sequential steps can often be fused in ways that improve their properties. For example, it is generally faster to implement a dot-product through a multiply-accumulate rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [62] have added kernel fusion capabilities, although the very best performance usually still arises from\nhandwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load small blocks of memory, do intensive computation on them, and then write the results a tile at a time [80]. Finally, many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [53]. Efficient algorithms usually make use of a combination of these techniques. For example, FlashAttention [13] uses all four to dramatically decrease both the latency and memory consumption of multi-head attention. Though we have made a modest effort to implement Monarch Mixer efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques. Reducing flops: A first target for optimization is the multi-layer perceptron (MLP), owing to its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this work $[5,9,12,14,15,17,24,88]$. Attention is also a popular target for optimization. Recently, a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the fact that the attention matrix is extremely sparse at long sequence lengths [2, 21, 22, 40, 51]. Others use low-rank approximations of the attention matrix [11, 77, 88] or kernel methods instead [7, 39]. A subset use a combination of these techniques, such as $[6,71]$. Finally, a third category of methods $[25,63]$ aim to replace attention entirely, relying on state-space models [31]. ## 7 Discussion and Conclusion\n\nWe explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence length and model dimension and is hardware-efficient on modern accelerators. We motivate M2 from both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept investigation into performance on masked language modeling, image classification, and causal language modeling. While our initial results are promising, our work is only a first step in this direction. The M2 layer can likely be further optimized with systems optimization techniques such as kernel fusion. Our work has also not been optimized for inference like more well-established models such as Transformers, or even more recent models such as state space models. It also remains to be seen whether M2 layers can have as widespread applicability as Transformers. We hope that these can be fruitful directions for future work. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.\n```\n\n#### 2. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.84)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 8/54 (Score: 0.96)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] George B Arfken and Hans J Weber. Mathematical methods for physicists. Elsevier Academic Press, 2005 . [3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In The International Conference on Machine Learning (ICML), pages 1120-1128, 2016. [4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [5] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019. [7] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2(1):24-47, 2020. [8] John P Boyd. Chebyshev and Fourier spectral methods.\n```\n\n##### *Relevant Chunk: No. 13/54 (Score: 0.73)*\n\n```\nIn Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2019. [19] Tri Dao, Christopher M De Sa, and Christopher R\u00e9. Gaussian quadrature for kernel features. In Advances in Neural Information Processing Systems (NeurIPS), pages 6107-6117, 2017. [20] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In The International Conference on Machine Learning (ICML), 2019. [21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In The International Conference on Learning Representations (ICLR), 2020. [22] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1060-1079. SIAM, 2018. [23] Raymond A DeCarlo. Linear systems: A state variable approach with numerical implementation. Prentice-Hall, Inc., 1989. [24] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems (NeurIPS), pages $3844-3852,2016$. [25] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci. edu $/ \\mathrm{ml}$. [26] Krzysztof Duda. Accurate, guaranteed stable, sliding discrete Fourier transform [DSP tips \\& tricks]. IEEE Signal Processing Magazine, 27(6):124-127, 2010. [27] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural ODEs. In Advances in Neural Information Processing Systems, pages 3134-3144, 2019. [28] Behrouz Farhang-Boroujeny and Saeed Gazor. Generalized sliding FFT and its application to implementation of block LMS adaptive filters.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.32)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.32)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 4. Scalable MatMul-free Language Modeling (Avg. Score: 0.27)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 8/27 (Score: 0.27)*\n\n```\nWe assume a constant clock rate for all implementations. We note that the FPGA implementation is done in RTL from top to bottom, and there are many optimizations that could be added. For example, we are not using any vendor-provided IPs, and we are not bursting DDR transactions, both of which would significantly accelerate operation. This approach is to achieve the most generic and cross-platform evaluation possible. ## 6 Conclusion\n\nWe have demonstrated the feasibility and effectiveness of the first scalable MatMul-free language model. Our work challenges the paradigm that MatMul operations are indispensable for building high-performing language models and paves the way for the development of more efficient and hardware-friendly architectures. We achieve performance on par with state-of-the-art Transformers while eliminating the need for MatMul operations, with an optimized implementation that significantly enhances both training and inference efficiency, reducing both memory usage and latency. As the demand for deploying language models on various platforms grows, MatMul-free LMs present a promising direction for creating models that are both effective and resource-efficient. However, one limitation of our work is that the MatMul-free LM has not been tested on extremely large-scale models (e.g., 100B+ parameters) due to computational constraints. This work serves as a call to action for institutions and organizations that have the resources to build the largest language models to invest in accelerating lightweight models. By prioritizing the development and deployment of MatMul-free architectures such as this one, the future of LLMs will only become more accessible, efficient, and sustainable. ## References\n\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [2] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [3] Hanting Chen, Yunhe Wang, Chunjing Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chao Xu, Chunfeng Xu, and Qi Tian. The addernet: Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $1468-1477,2020$. [4] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9):1659-1671, 1997. [5] Jason K Eshraghian, Max Ward, Emre O Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. Proceedings of the IEEE, 2023. [6] Rui-Jie Zhu, Qihang Zhao, Guoqi Li, and Jason K Eshraghian. SpikeGPT: Generative pretrained language model with spiking neural networks.\n```\n\n\n\n---\n## Found 2 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hardware efficient spectral operations polynomial projections\n\nConsidering refining your search by improving the query keywords input.\n\n### 2 related papers from Semantic Scholar\n\n#### 1. Scaling Spherical CNNs\n\n*From Search Query: hardware efficient spectral operations polynomial projections*\n\n*Carlos Esteves, J. Slotine, A. Makadia*\n\n**TL;DR:** This work shows how spherical CNNs can be scaled for much larger problems, and makes critical improvements including novel variants of common model components, an implementation of core operations to exploit hardware accelerator characteristics, and application-specific input representations that exploit the properties of the model.\n\n**Abstract:** Spherical CNNs generalize CNNs to functions on the sphere, by using spherical convolutions as the main linear operation. The most accurate and efficient way to compute spherical convolutions is in the spectral domain (via the convolution theorem), which is still costlier than the usual planar convolutions. For this reason, applications of spherical CNNs have so far been limited to small problems that can be approached with low model capacity. In this work, we show how spherical CNNs can be scaled for much larger problems. To achieve this, we make critical improvements including novel variants of common model components, an implementation of core operations to exploit hardware accelerator characteristics, and application-specific input representations that exploit the properties of our model. Experiments show our larger spherical CNNs reach state-of-the-art on several targets of the QM9 molecular benchmark, which was previously dominated by equivariant graph neural networks, and achieve competitive performance on multiple weather forecasting tasks. Our code is available at https://github.com/google-research/spherical-cnn.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 2. Kernel methods through the roof: handling billions of points efficiently\n\n*From Search Query: hardware efficient spectral operations polynomial projections*\n\n*Giacomo Meanti, Luigi Carratino, L. Rosasco, Alessandro Rudi*\n\n**TL;DR:** This work designed a preconditioned gradient solver for kernel methods exploiting both GPU acceleration and parallelization with multiple GPUs, implementing out-of-core variants of common linear algebra operations to guarantee optimal hardware utilization.\n\n**Abstract:** Kernel methods provide an elegant and principled approach to nonparametric learning, but so far could hardly be used in large scale problems, since naive implementations scale poorly with data size. Recent advances have shown the benefits of a number of algorithmic ideas, for example combining optimization, numerical linear algebra and random projections. Here, we push these efforts further to develop and test a solver that takes full advantage of GPU hardware. Towards this end, we designed a preconditioned gradient solver for kernel methods exploiting both GPU acceleration and parallelization with multiple GPUs, implementing out-of-core variants of common linear algebra operations to guarantee optimal hardware utilization. Further, we optimize the numerical precision of different operations and maximize efficiency of matrix-vector multiplications. As a result we can experimentally show dramatic speedups on datasets with billions of points, while still guaranteeing state of the art performance. Additionally, we make our software available as an easy to use library.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 101  (*Influential: 5*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, especially focusing on hardware-efficient implementations, memory efficiency, and optimization strategies, here are some relevant findings and suggestions based on the provided search results and analysis:\n\n## Hardware-Efficient Implementations of Spectral Operations\n- The paper on \"Efficient memristor accelerator for transformer self-attention\" highlights a hardware-efficient approach using memristor-based in-memory computing. This method demonstrates significant efficiency in terms of area utilization, latency, and energy consumption, which could be adapted for spectral operations in neural networks. The use of memristor technology for matrix multiplication and other linear operations can be particularly beneficial for spectral filtering.\n\n## Memory-Efficient Polynomial Projection Techniques\n- While the provided sources do not directly address polynomial projection techniques, the concept of memory efficiency in neural networks is discussed. For instance, the memristor accelerator paper shows how compact and energy-efficient designs can be achieved, which could be a starting point for optimizing polynomial projections. Additionally, the analysis note mentions the need for memory compression techniques, suggesting that hierarchical processing and optimized memory hierarchies could be key areas to explore.\n\n## Optimization Strategies for FFT Operations in Deep Learning\n- FFT operations are crucial in spectral state space models but often suffer from poor hardware utilization. The memristor accelerator paper, although not specifically focused on FFT, demonstrates how in-memory computing can optimize matrix operations. For FFT operations, exploring similar in-memory computing or specialized hardware accelerators could improve efficiency. Additionally, optimizing the memory hierarchy I/O, as mentioned in the analysis note, would be essential for efficient FFT operations.\n\n## Hardware Acceleration Methods for State Space Models\n- The memristor accelerator paper provides a clear example of how hardware acceleration can be achieved for transformer networks, which can be extended to state space models. The use of memristor-based in-memory computing shows promising results in terms of accuracy, area utilization, and energy efficiency. This approach could be adapted for state space models to improve their hardware efficiency.\n\n## Empirical Comparisons of Spectral Filtering Architectures\n- The analysis note highlights the need for empirical comparisons of different spectral filtering architectures. While the provided sources do not offer direct comparisons, they suggest that integrating spectral filtering with state space models (like HiPPO) could be beneficial. Future research should focus on comparing architectures such as S4-PTD/S5-PTD initialization schemes with other spectral filtering methods to determine the most efficient and robust approach[Analysis Note].\n\n### Additional Suggestions\n\n- **Photonic Neural Cellular Automata (PNCA):** The concept of PNCA, which uses photonic hardware with sparse connections, could offer insights into how to design more efficient and robust spectral operations. PNCA's ability to tolerate noise and its local update rule might be beneficial for spectral state space models.\n\n- **Hierarchical Processing and Memory Compression:** The analysis note emphasizes the importance of hierarchical processing and memory compression techniques. Exploring these areas could help in optimizing the memory efficiency of polynomial projections and spectral operations.\n\nBy integrating these insights, researchers can develop more efficient, robust, and scalable autoregressive language models that leverage hardware-efficient spectral operations, memory-efficient polynomial projections, and optimized FFT operations.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about spectral filtering, state space models, and memory efficiency in sequence modeling. The SpectralHiPPOQ proposal combines these elements in a novel way, but further investigation is needed on specific implementation aspects and hardware considerations.\n\n2. Key References with Excerpts:\n\nFrom \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning.\"\n\nFrom \"Block-State Transformers\":\n\"SSMs have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity.\"\n\nFrom \"Robustifying State-space Models\":\n\"Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework.\"\n\n3. Key Insights:\n\na) Spectral Filtering Benefits:\n- Provides robustness independent of spectrum dynamics\n- Enables efficient fixed convolutional filters\n- Offers theoretical guarantees for long-range dependencies\n\nb) Implementation Challenges:\n- FFT operations have poor hardware utilization\n- Memory hierarchy I/O challenges need addressing\n- Trade-offs between compression and model quality\n\nc) Integration Opportunities:\n- Potential for combining spectral filtering with HiPPO framework\n- Hierarchical processing can improve efficiency\n- Memory compression techniques can be optimized\n\n4. Future Search Plan:\nNeed additional information on:\n1. Hardware-efficient implementations of spectral operations\n2. Memory compression techniques specific to polynomial projections\n3. Empirical comparisons of different spectral filtering architectures\n4. Integration methods for spectral filtering with state space models\n\n5. References:\n\n[1] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n[2] Fathi, M., et al. (2023). Block-State Transformers. Neural Information Processing Systems.\n\n[3] Yu, A., et al. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization. arXiv preprint.\n\n[4] Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[5] Qin, Z., et al. (2024). You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet. arXiv preprint."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.53939,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.174161,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HiPPOProjector",
    "proposal": "## SpectralHiPPOQ: Spectral Filtering with Hierarchical Polynomial Memory for Enhanced Language Models\n\n### 1. Motivation\n\nCurrent language models face challenges in efficiently handling long-range dependencies and managing memory. SpectralHiPPOQ addresses these challenges by integrating spectral filtering with hierarchical polynomial memory and adaptive scale integration. This approach leverages spectral methods for efficient state space management and hierarchical processing, aiming to improve long-range dependency handling and model stability.\n\n### 2. Problem Analysis\n\n#### Limitations of Existing Models:\n- Inefficient memory management in handling long contexts.\n- Limited adaptivity to varying computational needs.\n- Challenges in maintaining stability and efficiency for long-range dependencies.\n\n#### Opportunities for Improvement:\n- Spectral filtering offers robust solutions for efficient state space management.\n- Hierarchical processing can benefit from spectral methods to ensure continuity and smoothness across segments.\n- Adaptive state management can be enhanced by integrating selective state mechanisms and hierarchical normalization.\n\n### 3. Core Idea and Philosophy\n\nSpectralHiPPOQ introduces key innovations:\n\n1. **Spectral Filtering for Memory Efficiency**:\n   - Utilizes spectral filtering to compress and manage the state space efficiently.\n   - Enhances memory efficiency and computational adaptability.\n\n2. **Hierarchical Polynomial Memory (HPM)**:\n   - Combines HiPPO with spectral filtering for scale-specific memory compression.\n   - Ensures efficient long-term dependency modeling.\n\n3. **Adaptive Scale Integration (ASI)**:\n   - Integrates multiple scales dynamically using spectral methods.\n   - Enhances feature fusion and resource allocation.\n\n### 4. Design Plan\n\n#### 4.1 Architecture Overview\n\nThe SpectralHiPPOQ block consists of three main components:\n\n1. **Spectral Filtering Module**:\n```python\nclass SpectralFilter(nn.Module):\n    def __init__(self, dim, num_scales=3):\n        super().__init__()\n        self.spectral_proj = nn.Linear(dim, dim)\n        self.scale_selector = ScaleSelector(dim, num_scales)\n        \n    def forward(self, x):\n        # Apply spectral filtering\n        spectral_x = self.spectral_proj(x)\n        \n        # Select scales based on spectral features\n        scales = self.scale_selector(spectral_x)\n        return spectral_x, scales\n```\n\n2. **Hierarchical Polynomial Memory**:\n```python\nclass HPMemory(nn.Module):\n    def __init__(self, dim, num_scales=3):\n        super().__init__()\n        self.memory_projectors = nn.ModuleList([\n            HiPPOProjector(dim // (2**i))\n            for i in range(num_scales)\n        ])\n        self.quantizers = nn.ModuleList([\n            VectorQuantizer(dim // (2**i))\n            for i in range(num_scales)\n        ])\n        \n    def forward(self, x, scales):\n        memories = []\n        for i, scale in enumerate(scales):\n            # Project to polynomial basis\n            mem = self.memory_projectors[i](x)\n            \n            # Quantize if not important\n            if not scale:\n                mem = self.quantizers[i](mem)\n            memories.append(mem)\n        return memories\n```\n\n3. **Adaptive Scale Integration**:\n```python\nclass ASIntegrator(nn.Module):\n    def __init__(self, dim, num_scales=3):\n        super().__init__()\n        self.scale_weights = nn.Parameter(\n            torch.ones(num_scales)\n        )\n        self.output_proj = nn.Linear(dim, dim)\n        \n    def forward(self, memories, scales):\n        # Weight different scales\n        weights = F.softmax(\n            self.scale_weights * scales, dim=0\n        )\n        \n        # Integrate memories\n        output = sum(m * w for m, w in zip(memories, weights))\n        return self.output_proj(output)\n```\n\n#### 4.2 Mathematical Formulation\n\n1. **Spectral Filtering**:\n   \\[\n   s(x) = \\text{spectral\\_filter}(W_s x + b_s)\n   \\]\n\n2. **Hierarchical Polynomial Memory**:\n   \\[\n   p_i(x) = \\text{Project}(x, \\text{basis}_i)\n   \\]\n\n3. **Adaptive Scale Integration**:\n   \\[\n   w = \\text{softmax}(v \\odot s)\n   \\]\n\n### 5. Implementation Guidelines\n\n1. **Spectral Filtering**:\n```python\ndef spectral_filter(x):\n    # Apply spectral projection\n    spectral_x = spectral_proj(x)\n    return spectral_x\n```\n\n2. **Memory Management**:\n```python\ndef manage_memory(x, scales):\n    # Project to polynomial basis\n    memory = project_polynomial(x)\n    \n    # Apply scale-specific compression\n    memory = compress_memory(memory, scales)\n    return memory\n```\n\n3. **Scale Integration**:\n```python\ndef integrate_scales(memories, scales):\n    # Compute scale weights\n    weights = compute_weights(scales)\n    \n    # Combine memories\n    output = combine_memories(memories, weights)\n    return output\n```\n\n### 6. Theoretical Analysis\n\n1. **Computational Complexity**:\n   - Best case: O(N) for single scale\n   - Average case: O(N log N) with multiple scales\n   - Worst case: O(N log N) due to polynomial projections\n\n2. **Memory Usage**:\n   - Reduced by factor of (1/k) where k is compression ratio\n   - Additional overhead from spectral filtering: O(d)\n   - Scale-specific storage: O(N log d)\n\n3. **Expected Benefits**:\n   - Improved efficiency through spectral filtering\n   - Better memory utilization via adaptive compression\n   - Enhanced long-term dependency modeling\n\n### 7. Conclusion\n\nSpectralHiPPOQ leverages spectral filtering and hierarchical polynomial memory to enhance language models' efficiency and adaptability. By integrating spectral methods, the model achieves improved memory management and long-range dependency handling, offering a robust solution for modern language modeling challenges.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": false,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "gpt4o_0806",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6500000000000001,
                0.15000000000000002,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.5499999999999999,
                0.10000000000000002,
                0.30000000000000004
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.44999999999999996,
                0.34999999999999964,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.6,
                0.10000000000000002,
                0.2
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.2,
                0.10000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "SpectralHiPPOQ integrates spectral filtering with hierarchical polynomial memory and adaptive scale integration to enhance memory efficiency and computational adaptability in language models. By leveraging spectral methods for efficient state space management and hierarchical processing, SpectralHiPPOQ aims to improve long-range dependency handling and model stability.",
    "ideation": null,
    "modelname": "spectralhippoq",
    "suggestions": "1. Hardware Optimization:\n- Consider alternative hardware-efficient implementations of spectral operations\n- Add detailed memory access pattern optimization strategies\n- Include hardware-specific implementation guidelines\n- Provide concrete examples of efficient parallel processing\n\n2. Training Stability:\n- Add gradient flow analysis through spectral operations\n- Include initialization and normalization strategies\n- Provide convergence analysis for adaptive components\n- Consider progressive training approach\n\n3. Implementation Details:\n- Add numerical stability analysis\n- Include ablation study design\n- Specify fallback mechanisms for edge cases\n- Provide more detailed memory management strategies\n\n4. Performance Guarantees:\n- Add theoretical bounds on compression error\n- Include analysis of worst-case scenarios\n- Specify minimum performance guarantees\n- Add robustness analysis for different input distributions\n\n5. Hardware Considerations:\n- Consider alternatives to FFT operations\n- Add memory bandwidth analysis\n- Provide cache optimization strategies\n- Specify hardware-specific implementation details",
    "user_input": ""
}