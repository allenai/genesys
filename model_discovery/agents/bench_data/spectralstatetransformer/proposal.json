{
    "variantname": "SpectralRMSNorm",
    "review": "The SpectralRMSNorm proposal presents an innovative approach to enhancing RMSNorm through the integration of spectral state space models (SSMs) and selective state compression. Based on comprehensive analysis of search results and existing research, here is a detailed review:\n\n1. CLARITY (4.5/5)\nThe proposal is exceptionally well-structured with:\n- Clear articulation of objectives and motivation\n- Detailed mathematical formulations\n- Well-defined implementation guidelines\n- Comprehensive theoretical analysis\nThe only minor weakness is in the explanation of some hardware optimization strategies.\n\n2. INNOVATION (4.3/5)\nThe proposal demonstrates significant innovation in:\n+ Novel integration of spectral SSMs with normalization\n+ Hardware-efficient block processing design\n+ Dynamic state compression mechanisms\nHowever:\n- Some concepts build on existing adaptive normalization techniques\n- Hardware optimization strategies share similarities with existing approaches\n\n3. FEASIBILITY (4.0/5)\nImplementation appears feasible within the framework:\n+ Clear mathematical formulations provided\n+ Detailed implementation guidelines\n+ Consideration of hardware constraints\nConcerns:\n- Complex integration of spectral components\n- Potential training stability challenges\n- Need for careful hardware optimization\n\n4. SCALABILITY (4.2/5)\nThe design shows strong potential for scaling:\n+ Linear computational complexity through SSM integration\n+ Efficient memory management through block processing\n+ Hardware-aware implementation\nMinor concerns:\n- Memory requirements for state management\n- Potential overhead in parameter updates\n\n5. ACCURACY AND ROBUSTNESS (4.1/5)\nExpected improvements in model performance:\n+ Better handling of varying input complexities\n+ Enhanced feature normalization\n+ Improved stability through spectral properties\nConcerns:\n- Need for empirical validation\n- Potential impact of compression on accuracy\n- Trade-off between adaptivity and stability\n\n6. EFFICIENCY (4.4/5)\nStrong focus on computational efficiency:\n+ Optimized memory access patterns\n+ Efficient block-based computation\n+ Hardware-aware implementation\n+ Reduced memory footprint through compression\n\nCOMPARATIVE ANALYSIS:\nThe proposal differs significantly from existing approaches:\n- More hardware-efficient than traditional adaptive normalization\n- Novel integration of spectral properties\n- Unique focus on memory access optimization\n- Distinct from sibling proposals (blockla_transformer, stateadaptivetransformer)\n\nTECHNICAL DEPTH:\nThe proposal demonstrates strong technical understanding:\n+ Comprehensive mathematical formulations\n+ Detailed implementation guidelines\n+ Thorough consideration of hardware aspects\n+ Clear analysis of trade-offs",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient SSM implementation memory compression",
            "detail": "Find specific implementations of SSM-based architectures focusing on memory efficiency and state compression techniques. Include details about:\n1. State update mechanisms\n2. Memory compression strategies\n3. Hardware optimization approaches\n4. Integration with existing architectures",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations of SSM-based architectures focusing on memory efficiency and state compression techniques. Include details about:\n1. State update mechanisms\n2. Memory compression strategies\n3. Hardware optimization approaches\n4. Integration with existing architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.89)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.89)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.86)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.99)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.97)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.62)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.70)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.70)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient SSM implementation memory compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: efficient SSM implementation memory compression*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 2. EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression\n\n*From Search Query: efficient SSM implementation memory compression*\n\n*Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu*\n\n**TL;DR:** An optimized GPU implementation which supports training GNNs with compressed activations for Pytorch Geometric and Pytorch, and a memory-ef\ufb01cient framework called EXACT, which for the first time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations.\n\n**Abstract:** Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously dif\ufb01cult to implement due to the lack of necessary tools in common graph learning packages. To un-leash the potential of this direction, we provide an optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-ef\ufb01cient framework called \u201cEXACT\u201d, which for the \ufb01rst time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to 32 \u00d7 with 0 . 2 - 0 . 5% accuracy drop and 10 - 25% time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on ogbn-products from a 48GB GPU to a 12GB GPU. The code is available at https://github.com/warai-0toko\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 55  (*Influential: 7*)\n\n#### 3. HM-ANN: Efficient Billion-Point Nearest Neighbor Search on Heterogeneous Memory\n\n*From Search Query: efficient SSM implementation memory compression*\n\n*Jie Ren, Minjia Zhang, Dong Li*\n\n**TL;DR:** A novel graph-based similarity search algorithm called HM-ANN is presented, which takes both memory and data heterogeneity into consideration and enables billion-scale similarity search on a single node without using compression.\n\n**Abstract:** The state-of-the-art approximate nearest neighbor search (ANNS) algorithms face a fundamental tradeoff between query latency and accuracy, because of small main memory capacity: To store indices in main memory for fast query response, They have to limit the number of data points or store compressed vectors, which hurts search accuracy. The emergence of heterogeneous memory (HM) brings opportunities to largely increase memory capacity and break the above tradeoff: Using HM, billions of data points can be placed in main memory on a single machine without using any data compression. However, HM consists of both fast (but small) memory and slow (but large) memory, and using HM inappropriately slows down query time significantly. In this work, we present a novel graph-based similarity search algorithm called HM-ANN, which takes both memory and data heterogeneity into consideration and enables billion-scale similarity search on a single node without using compression. On two billion-sized datasets BIGANN and DEEP1B, HM-ANN outperforms state-of-the-art compression-based solutions such as L&C and IMI+OPQ in recall-vs-latency by a large margin, obtaining 46% higher recall under the same search latency. We also extend existing graph-based methods such as HNSW and NSG with two strong baseline implementations on HM. At billion-point scale, HM-ANN is 2X and 5.8X faster than our HNSW and NSG baselines respectively to reach the same accuracy.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 62  (*Influential: 8*)\n\n#### 4. LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models\n\n*From Search Query: efficient SSM implementation memory compression*\n\n*Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok Kwon, S. Kwon, Byeongwook Kim, Youngjoo Lee, Dongsoo Lee*\n\n**TL;DR:** This paper introduces LUT-GEMM, an efficient kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization.\n\n**Abstract:** Recent advances in self-supervised learning and the Transformer architecture have significantly improved natural language processing (NLP), achieving remarkably low perplexity. However, the growing size of NLP models introduces a memory wall problem during the generation phase. To mitigate this issue, recent efforts have focused on quantizing model weights to sub-4-bit precision while preserving full precision for activations, resulting in practical speed-ups during inference on a single GPU. However, these improvements primarily stem from reduced memory movement, which necessitates a resource-intensive dequantization process rather than actual computational reduction. In this paper, we introduce LUT-GEMM, an efficient kernel for quantized matrix multiplication, which not only eliminates the resource-intensive dequantization process but also reduces computational costs compared to previous kernels for weight-only quantization. Furthermore, we proposed group-wise quantization to offer a flexible trade-off between compression ratio and accuracy. The impact of LUT-GEMM is facilitated by implementing high compression ratios through low-bit quantization and efficient LUT-based operations. We show experimentally that when applied to the OPT-175B model with 3-bit quantization, LUT-GEMM substantially accelerates token generation latency, achieving a remarkable 2.1$\\times$ improvement on a single GPU when compared to OPTQ, which relies on the costly dequantization process.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 5. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: efficient SSM implementation memory compression*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n### 1 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: efficient SSM implementation memory compression*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design with a focus on efficient State-Space Models (SSMs) implementation, memory compression, and integration with existing architectures, here are some key findings and strategies:\n\n## State Update Mechanisms\n- SSMs, such as Mamba, utilize linear state updates, which are efficient and scalable. These models update the state using a fixed-size hidden state, which is beneficial for online inference and memory efficiency.\n- The HiPPO framework provides a theoretical foundation for optimal polynomial projections, allowing SSMs to effectively model long-range dependencies with linear complexity.\n\n## Memory Compression Strategies\n- **Adaptive State Compression**: Models like Taipan and Zamba incorporate adaptive mechanisms to manage state information. Taipan uses Selective Attention Layers to optimize resource allocation, applying attention only to tokens that benefit from it, thus reducing overall memory and computational demands.\n- **Dynamic Parameter Adjustment**: E-Tamba, for example, achieves significant memory savings by replacing specific Transformer layers with Mamba layers, demonstrating that hybrid approaches can maintain performance while reducing memory usage.\n- **Polynomial Projections**: The HiPPO framework suggests using polynomial projections to compress continuous signals and discrete time series, which can be integrated into SSMs for efficient state management.\n\n## Hardware Optimization Approaches\n- **Efficient Inference**: SSMs like Mamba and its variants offer constant memory usage during inference, which is highly efficient compared to the quadratic memory complexity of traditional Transformer models.\n- **GPU Optimization**: Utilizing streaming multiprocessors (SMs) and sparsity-aware kernels can further optimize the computational efficiency of SSM-based models. This involves dividing computations into general-purpose cores and leveraging libraries like cuSPARSE and CUTLASS for sparse matrix operations.\n- **Memory Management**: E-Tamba demonstrates nearly 3X memory savings compared to traditional Transformer models by integrating Mamba layers, highlighting the potential for memory-efficient designs on GPU hardware.\n\n## Integration with Existing Architectures\n- **Hybrid Architectures**: Models like Zamba and Taipan combine SSMs (e.g., Mamba) with attention mechanisms. Zamba uses a globally shared self-attention layer after every six Mamba blocks to enhance long-range dependency handling, while Taipan employs Selective Attention Layers to optimize resource allocation.\n- **Layer Replacement**: Strategies like E-Tamba involve replacing specific layers of traditional models (e.g., Transformers) with SSM layers, which can retain performance while reducing memory and computational costs.\n- **Layer-wise Nonlinearity**: Integrating layer-wise nonlinearity into SSMs can enhance their expressiveness and universal approximation capabilities, as suggested by the work on stacking state-space models with nonlinear activations.\n\n### Key Implementations\n\n- **Taipan**: Combines Mamba with Selective Attention Layers to handle long-range dependencies efficiently while maintaining computational efficiency. This approach optimizes resource allocation by applying attention selectively to tokens that benefit from it.\n- **Zamba**: Integrates Mamba blocks with a globally shared self-attention layer to enhance long-range dependency handling and in-context learning, all while reducing computational demands compared to standard Transformer models.\n- **E-Tamba**: Replaces specific Transformer layers with Mamba layers, achieving significant memory savings and competitive performance on downstream NLP tasks, making it a strong alternative to traditional Transformer architectures.\n\nThese approaches and implementations provide a solid foundation for improving autoregressive language model design by leveraging the efficiency of SSMs, optimizing memory usage, and integrating with existing architectures to enhance performance and scalability.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design and references reveal several key areas for potential improvement in LM block design:\n\n1. State-Space Models (SSMs) Integration:\n- Recent work shows SSMs can effectively model long-range dependencies with linear complexity\n- HiPPO framework provides theoretical foundations for optimal polynomial projections\n- SSMs with layer-wise nonlinearity proven as universal approximators\n\n2. Memory and State Management:\n- Current designs struggle with efficient memory usage for long sequences\n- Adaptive state compression and dynamic parameter adjustment show promise\n- Need for better mechanisms to maintain and update state information\n\n3. Attention Mechanisms:\n- Max-margin token selection provides theoretical insights into attention optimization\n- Linear attention variants reduce computational complexity but may sacrifice expressiveness\n- Hybrid approaches combining attention with other mechanisms show potential\n\nKey Insights:\n\n1. SSM Integration Potential:\nFrom HiPPO paper: \"SSMs provide a principled framework for online compression of continuous signals and discrete time series through projection onto polynomial bases.\"\n\nThis suggests potential for incorporating SSM-based memory mechanisms into GAUs for better sequence modeling.\n\n2. Universal Approximation:\nFrom Wang & Xue (2023): \"Stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\"\n\nThis provides theoretical backing for SSM-based architectures while highlighting the importance of nonlinear activations.\n\n3. Hybrid Architectures:\nFrom Park et al. (2024): \"MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently.\"\n\nThis indicates potential benefits of combining different architectural components.\n\n4. Token Selection:\nFrom Tarzanagh et al. (2023): \"Running gradient descent on attention parameters converges to a max-margin solution that separates locally-optimal tokens from non-optimal ones.\"\n\nThis provides insights for designing more efficient token selection mechanisms.\n\nDetailed Analysis:\n\n1. Memory Management Opportunities:\n- Current approaches often use fixed memory structures\n- Potential for adaptive memory allocation based on input complexity\n- Need for efficient state compression without losing critical information\n\n2. Computational Efficiency:\n- Linear complexity achievable through SSM-based approaches\n- Trade-off between expressiveness and computation needs careful balance\n- Hardware considerations crucial for practical implementation\n\n3. Architecture Integration:\n- SSMs could enhance GAU's ability to capture long-range dependencies\n- Layer-wise nonlinearity important for expressiveness\n- Need for careful integration to maintain stability\n\nFuture Search Plan:\n\n1. Investigate specific SSM architectures and their implementations\n2. Explore efficient memory compression techniques\n3. Research hardware-efficient implementations of SSM-based models\n\nReferences:\n\n1. Gu, A., Dao, T., Ermon, S., Rudra, A., & R\u00e9, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. Neural Information Processing Systems.\n\n2. Wang, S., & Xue, B. (2023). State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Neural Information Processing Systems.\n\n3. Park, J., Park, J., Xiong, Z., Lee, N., Cho, J., Oymak, S., Lee, K., & Papailiopoulos, D. (2024). Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks. arXiv preprint.\n\n4. Tarzanagh, D. A., Li, Y., Zhang, X., & Oymak, S. (2023). Max-Margin Token Selection in Attention Mechanism. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "adaptive compression techniques dynamic parameter updates",
            "detail": "Find specific implementations and techniques for:\n1. Adaptive compression mechanisms in state-space models\n2. Dynamic parameter update strategies\n3. Methods for efficient state management\n4. Integration with existing model architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and techniques for:\n1. Adaptive compression mechanisms in state-space models\n2. Dynamic parameter update strategies\n3. Methods for efficient state management\n4. Integration with existing model architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.46)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.46)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.31)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.45)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.17)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 3. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.01)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.01)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n#### 4. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.01)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 15/37 (Score: 0.01)*\n\n```\nAdvances in neural information processing systems, 33:1474-1487, 2020. [16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [17] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. [18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572-585, 2021. [19] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections. International Conference on Learning Representations, 2023. [20] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. International Conference on Learning Representations, 2023. [21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive compression techniques dynamic parameter updates\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Overfitting for Fun and Profit: Instance-Adaptive Data Compression\n\n*From Search Query: adaptive compression techniques dynamic parameter updates*\n\n*T. V. Rozendaal, Iris A.M. Huijben, Taco Cohen*\n\n**TL;DR:** This paper adapts the full model to a single video, and sends model updates (quantized and compressed using a parameter-space prior) along with the latent representation, and demonstrates that full-model adaptation improves $RD$ performance by ~1 dB, with respect to encoder-only finetuning.\n\n**Abstract:** Neural data compression has been shown to outperform classical methods in terms of $RD$ performance, with results still improving rapidly. At a high level, neural compression is based on an autoencoder that tries to reconstruct the input instance from a (quantized) latent representation, coupled with a prior that is used to losslessly compress these latents. Due to limitations on model capacity and imperfect optimization and generalization, such models will suboptimally compress test data in general. However, one of the great strengths of learned compression is that if the test-time data distribution is known and relatively low-entropy (e.g. a camera watching a static scene, a dash cam in an autonomous car, etc.), the model can easily be finetuned or adapted to this distribution, leading to improved $RD$ performance. In this paper we take this concept to the extreme, adapting the full model to a single video, and sending model updates (quantized and compressed using a parameter-space prior) along with the latent representation. Unlike previous work, we finetune not only the encoder/latents but the entire model, and - during finetuning - take into account both the effect of model quantization and the additional costs incurred by sending the model updates. We evaluate an image compression model on I-frames (sampled at 2 fps) from videos of the Xiph dataset, and demonstrate that full-model adaptation improves $RD$ performance by ~1 dB, with respect to encoder-only finetuning.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 42  (*Influential: 3*)\n\n#### 2. Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization\n\n*From Search Query: adaptive compression techniques dynamic parameter updates*\n\n*Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, H. Dai, D. Dou*\n\n**TL;DR:** A Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs and a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further.\n\n**Abstract:** Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8\\% in terms of accuracy) and efficiency (up to 97.59\\% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 1*)\n\n#### 3. Dodo: Dynamic Contextual Compression for Decoder-only LMs\n\n*From Search Query: adaptive compression techniques dynamic parameter updates*\n\n*Guanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van Durme*\n\n**TL;DR:** Dodo is proposed, a solution for context compression that represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space.\n\n**Abstract:** Transformer-based language models (LMs) are inefficient in long contexts. We propose Dodo, a solution for context compression. Instead of one vector per token in a standard transformer model, Dodo represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space. Moreover, off-the-shelf models such as LLaMA can be adapted to Dodo by efficient parameter tuning methods such as LoRA. In use, Dodo can act as either an autoregressive LM or a context compressor for downstream tasks. We demonstrate through experiments in language modeling, question answering, and summarization that Dodo retains capabilities in these tasks, while drastically reducing the overhead during decoding. For example, in the autoencoding task, Dodo shrinks context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. DoCoFL: Downlink Compression for Cross-Device Federated Learning\n\n*From Search Query: adaptive compression techniques dynamic parameter updates*\n\n*Ron Dorfman, S. Vargaftik, Y. Ben-Itzhak, K. Levy*\n\n**TL;DR:** Through extensive evaluation, it is shown that $\\textsf{DoCoFL}$ offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.\n\n**Abstract:** Many compression techniques have been proposed to reduce the communication overhead of Federated Learning training procedures. However, these are typically designed for compressing model updates, which are expected to decay throughout training. As a result, such methods are inapplicable to downlink (i.e., from the parameter server to clients) compression in the cross-device setting, where heterogeneous clients $\\textit{may appear only once}$ during training and thus must download the model parameters. Accordingly, we propose $\\textsf{DoCoFL}$ -- a new framework for downlink compression in the cross-device setting. Importantly, $\\textsf{DoCoFL}$ can be seamlessly combined with many uplink compression schemes, rendering it suitable for bi-directional compression. Through extensive evaluation, we show that $\\textsf{DoCoFL}$ offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 5. Parameter-free, Dynamic, and Strongly-Adaptive Online Learning\n\n*From Search Query: adaptive compression techniques dynamic parameter updates*\n\n*Ashok Cutkosky*\n\n**TL;DR:** A new online learning algorithm is provided that for the first time combines several disparate notions of adaptivity, and obtains a \u201cparameter-free\u201d regret bound that adapts to the norm of the comparator and the squared norm of the size of the gradients it observes.\n\n**Abstract:** We provide a new online learning algorithm that for the \ufb01rst time combines several disparate notions of adaptivity. First, our algorithm obtains a \u201cparameter-free\u201d regret bound that adapts to the norm of the comparator and the squared norm of the size of the gradients it observes. Second, it obtains a \u201cstrongly-adaptive\u201d regret bound, so that for any given interval of length N , the regret over the interval is \u02dc O ( \u221a N ) . Finally, our algorithm obtains an optimal \u201cdynamic\u201d regret bound: for any sequence of comparators with path-length P , our algorithm obtains regret \u02dc O ( \u221a PN ) over intervals of length N . Our primary technique for achieving these goals is a new method of combining constrained online learning regret bounds that does not rely on an expert meta-algorithm to aggregate learners.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 46  (*Influential: 13*)\n\n### 1 related papers from Papers with Code\n\n#### 1. An Adaptive Nested Source Term Iteration for Radiative Transfer Equations\n\n*From Search Query: adaptive compression techniques dynamic parameter updates*\n\n*Olga Mula, Felix Gruber, Wolfgang Dahmen*\n\n**Abstract:** We propose a new approach to the numerical solution of radiative transfer equations with certified a posteriori error bounds. A key role is played by stable Petrov--Galerkin type variational formulations of parametric transport equations and corresponding radiative transfer equations. This allows us to formulate an iteration in a suitable, infinite dimensional function space that is guaranteed to converge with a fixed error reduction per step. The numerical scheme is then based on approximately realizing this iteration within dynamically updated accuracy tolerances that still ensure convergence to the exact solution. To advance this iteration two operations need to be performed within suitably tightened accuracy tolerances. First, the global scattering operator needs to be approximately applied to the current iterate within a tolerance comparable to the current accuracy level. Second, parameter dependent linear transport equations need to be solved, again at the required accuracy of the iteration. To ensure that the stage dependent error tolerances are met, one has to employ rigorous a posteriori error bounds which, in our case, rest on a Discontinuous Petrov--Galerkin (DPG) scheme. These a posteriori bounds are not only crucial for guaranteeing the convergence of the perturbed iteration but are also used to generate adapted parameter dependent spatial meshes. This turns out to significantly reduce overall computational complexity. Since the global operator is only applied, we avoid the need to solve linear systems with densely populated matrices. Moreover, the approximate application of the global scatterer accelerated through low-rank approximation and matrix compression techniques. The theoretical findings are illustrated and complemented by numerical experiments with non-trivial scattering kernels.\n\n**Published:** 2018-10-16\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive compression mechanisms, dynamic parameter updates, and efficient state management, here are some specific techniques and strategies that can be considered:\n\n## Adaptive Compression Mechanisms\n\n1. **Spatiotemporal Adaptive Compression**:\n   The LongVU method described in the paper on spatiotemporal adaptive compression for long videos can be adapted for language models. This involves reducing the number of tokens based on temporal and spatial dependencies, which can be translated to reducing the context size in language models by selectively preserving key tokens and applying spatial pooling or similar techniques to reduce the remaining tokens.\n\n2. **Token Reduction Methods**:\n   Techniques like TR-BERT, AdaViT, and PuMer, which are designed for vision transformers and other models, can be adapted for language models. These methods dynamically reduce the number of tokens processed in the network, avoiding redundant calculations and improving efficiency. For example, TR-BERT adapts the layer number of each token in inference to avoid redundant calculations, which could be applied to autoregressive language models to reduce computational overhead.\n\n## Dynamic Parameter Update Strategies\n\n1. **Context-Dependent Parameter Adjustment**:\n   Dynamic parameter adjustment based on input context can be implemented using adaptive models that estimate the distribution of characters or words based on what they have processed so far. This approach, similar to adaptive data compression models, can adjust model parameters in real-time to optimize performance for different input contexts.\n\n2. **Selective Mechanisms**:\n   The Mamba paper suggests using selective state spaces and kernel fusion to optimize memory IO and computation. This idea can be extended to dynamic parameter updates by selectively updating parameters based on the relevance of the input context, thus optimizing computational resources.\n\n## Methods for Efficient State Management\n\n1. **State Space Compression**:\n   The concept of proof state space compression from the NeurIPS 2024 paper can be applied to language models. By grouping similar states into strata and using conformal prediction intervals, the search space can be efficiently managed, reducing the computational overhead of state management.\n\n2. **Hardware-Aware Implementation**:\n   Optimizing state management through hardware-aware design is crucial. Techniques such as kernel fusion and recomputation can reduce memory IO and increase FLOP utilization, making the model more efficient. This is highlighted in papers like Mamba and Simple Hardware-Efficient Long Convolutions.\n\n## Integration with Existing Model Architectures\n\n1. **Combination with Linear Attention Mechanisms**:\n   Integrating state space models (SSMs) with linear attention mechanisms can simplify architectures and improve performance. This combination can leverage the strengths of both models, such as the efficient state management of SSMs and the contextual understanding of linear attention mechanisms.\n\n2. **Block-Based Processing**:\n   Implementing block-based processing, as suggested in the Samba paper, can improve throughput and efficiency. This involves processing the input sequence in blocks, which can be optimized for hardware to reduce memory IO and increase computational efficiency.\n\nBy incorporating these techniques, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage adaptive compression, dynamic parameter updates, and efficient state management.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several key developments and opportunities in SSM-based architectures:\n\n1. Hardware-Efficient Implementation:\n- Mamba introduces selective SSMs with hardware-aware parallel algorithms\n- Memory hierarchy optimization through kernel fusion and recomputation\n- Efficient state management without materializing full expanded states\n\n2. Memory Compression Techniques:\n- Adaptive state compression through selective mechanisms\n- Dynamic parameter adjustment based on input context\n- Efficient memory access patterns through hardware-aware design\n\n3. Integration Strategies:\n- Combination of SSMs with linear attention mechanisms\n- Simplified architectures without traditional attention or MLP blocks\n- Hardware-efficient block processing for improved throughput\n\nKey Excerpts and References:\n\n1. From Mamba paper (Gu & Dao, 2023):\n\"The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state h only in more efficient levels of the memory hierarchy... we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation.\"\n\n2. From State Space Models as Foundation Models (Alonso et al., 2024):\n\"The S4 model starts from a continuous time model... where the structure imposed on matrix A is diagonal plus a low-rank update... resembling a closed-loop dynamics matrix.\"\n\n3. From Simple Hardware-Efficient Long Convolutions (Fu et al., 2023):\n\"FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization.\"\n\nKey Insights:\n\n1. State Management:\n- SSMs can achieve efficient state compression through selective mechanisms\n- Hardware-aware implementation crucial for performance\n- Memory hierarchy optimization key to efficiency\n\n2. Architecture Design:\n- Simplified architectures possible without traditional attention\n- Integration of SSMs with other mechanisms can improve performance\n- Block-based processing offers efficiency advantages\n\n3. Implementation Considerations:\n- Kernel fusion and recomputation important for memory efficiency\n- Hardware-specific optimizations crucial for real-world performance\n- Memory access patterns need careful design\n\nFuture Search Plan:\n\n1. Need to investigate:\n- Specific techniques for adaptive state compression\n- Methods for efficient parameter updates\n- Integration strategies with existing architectures\n\n2. Search focus:\n- Adaptive compression mechanisms\n- Dynamic parameter adjustment techniques\n- Hardware-efficient implementation strategies\n\nReferences:\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n2. Alonso, C. A., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint.\n\n3. Fu, D. Y., et al. (2023). Simple Hardware-Efficient Long Convolutions for Sequence Modeling. International Conference on Machine Learning.\n\n4. Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n5. Chen, W., et al. (2024). Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arXiv preprint."
        },
        {
            "ready": false,
            "query": "SSM compression integration hardware optimization",
            "detail": "Find specific techniques and implementations for:\n1. Integrating SSMs with compression mechanisms\n2. Hardware-efficient SSM implementations\n3. Memory optimization in state-space models\n4. Parameter update strategies in SSMs",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific techniques and implementations for:\n1. Integrating SSMs with compression mechanisms\n2. Hardware-efficient SSM implementations\n3. Memory optimization in state-space models\n4. Parameter update strategies in SSMs\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.99)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.98)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.96)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.97)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.97)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. Structured state-space models are deep Wiener models (Avg. Score: 0.93)\n\n*Fabio Bonassi, Carl R. Andersson, Per Mattsson, Thomas B. Sch\u00f6n*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper provides a system identification-friendly introduction to the Structured State-space Models (SSMs), and highlights future research directions for which this community could provide impactful contributions.\n\n**Abstract:** The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.\n\n##### *Relevant Chunk: No. 3/22 (Score: 0.93)*\n\n```\n(2022), and mainly involves stability promotion via regularization. To address these problems, Gu et al. (2021) proposed a Structured State-space Model (SSM) architecture named S4, which consists of multiple layers composed by LTI discrete-time systems followed by a nonlinear function. The term \"structured\" stems from the fact that this LTI system is given a specific structure to improve the architecture's modeling performances while also reducing the computational cost at training (Yu et al., 2018). Nonlinear state-space models are not new, see Marconato et al. (2013), yet their adoption has been hampered by their crucial reliance on the model structure and on the initialization method of learnable parameters. The contribution of the S4 approach towards SSMs has therefore been that of providing ( $i$ ) a novel, intrinsically stable, parametrization of the LTI system obtained by discretizing a continuoustime Diagonal Plus-Low Rank (DPLR) system, (ii) a new strategy towards the parameters' initialization problem, (iii) a computationally efficient approach to simulate (and train) these models over extremely long sequences, and (iv) an empirical proof of the state-of-the-art performances of these models in long-term sequence learning problems. Motivated by these appealing features, many works have continued to build on the S4 architecture. For example, Gupta et al. (2022) and Gu et al. (2022) have explored the benefits entailed by stricter SSM structures, namely the parametrization via diagonal continuous-time systems (S4D), and by simpler initialization strategies. Smith et al. (2022) have explored a novel, and somewhat more computationally efficient, simulation method for diagonal continuous-time parametrizations, named S5. Orvieto et al. (2023) recently investigated the parametrization of the LTI subsystems directly in the discrete time domain, resulting in the Linear Recurrent Unit (LRU) architecture. Contribution Despite the appealing results achieved by SSMs in the long-range arena benchmarks sequence classification problems, their use for nonlinear system identification is still unexplored. With this paper, we want to change that by making the following contributions. First of all we show that it is possible to interpret SSMs as deep Wiener models, i.e. model structures where several Wiener models are interconnected in series. An interesting note here is that even though the Wiener models have been extremely popular within system identification - see e.g. Schoukens and Tiels (2017) and references therein - their structure has been limited to \"single-layer\" or parallel architectures (Wills and Ninness, 2012). Our second contribution is to dissect the recent developments on SSMs and explain them in terms of their structure and parameterization, and to clearly separate this from their initialization, simulation, and training strategies. The presentation in the paper is also done using the language commonly used in the system identification community in order to speed up the use of these tools within this area. Notation The imaginary unit is denoted by $i=\\sqrt{-1}$. Given a vector $v$, we denote by $v^{\\prime}$ its real transpose. For a time-dependent vector, the discrete-time index $k$ is reported as a subscript, e.g., $v_{k}$. Moreover, we denote by $v_{a: b}$ (where $a \\leqslant b$ ) the sequence $v_{a: b}=\\left(v_{a}, v_{a+1}, \\ldots, v_{b}\\right)$. For this sequence, we indicate by $\\operatorname{cat}\\left(v_{a: b}\\right)$ the concatenation of its elements, i.e. $\\operatorname{cat}\\left(v_{a: b}\\right)=\\left[v_{a}^{\\prime}, \\ldots, v_{b}^{\\prime}\\right]^{\\prime}$, and by uncat $(\\cdot)$ its inverse operation returning a sequence of vectors given their concatenation. Given a complex matrix $A$, we let $\\operatorname{conj}(A)$ be its element-wise complex conjugate and $A^{*}$ be its Hermitian transpose. Diagonal matrices may be defined via the $\\operatorname{diag}$ operator, as $A=\\operatorname{diag}\\left(a_{1}, \\ldots, a_{n}\\right)$. ## 2. STRUCTURED STATE-SPACE MODELS\n\nConsider the model depicted in Figure 1, which consists of $L$ Wiener systems interconnected in series. Each of these layers is here referred to as Structured Statespace Layer (SSL). Their interconnection results in an SSM, which can be interpreted as a specific configuration of a deep Wiener system. We let the generic $\\ell$-th SSL $(\\ell \\in\\{1, \\ldots, L\\})$ be represented by a discrete-time statespace model\n\n$$\n\\mathrm{SSL}_{\\ell}:\\left\\{\\begin{array}{l}\nx_{k+1}=A x_{k}+B u_{k} \\\\\n\\eta_{k}=C x_{k}+D u_{k} \\\\\ny_{k}=\\sigma\\left(\\eta_{k}\\right)+F u_{k}\n\\end{array}\\right. $$\n\nwhere, for compactness, the layer index is omitted. System (1) is characterized by the input vector $u \\in \\mathbb{R}^{n_{u}}$, the intermediate vector $\\eta \\in \\mathbb{R}^{n_{y}}$, the output vector $y \\in \\mathbb{R}^{n_{y}}$, and the complex-valued state vector $x \\in \\mathbb{C}^{n_{x}}$. The SSL is parametrized by the matrices $\\{A, B, C, D, F\\}$. The output transformation $\\sigma(\\cdot)$ can be any nonlinear, Lipschitzcontinuous activation function, such as the tanh, ELU, or Swish, see Ramachandran et al. (2017). In what follows, we aim to provide an overview of the possible structure, parametrization, initialization, and simulation strategies for this SSL. Remark 1. When a deep SSM is considered $(L>1)$, each layer is parametrized and initialized independently from the others. The simulation is carried out iteratively over the set of layers, meaning that the output sequence of the $\\ell$-th layer is used as input of the layer $\\ell+1$.\n```\n\n\n\n---\n## Found 0 related papers from 0 external sources\n\n\n\nYour 1 raw search queries input to the search frame: SSM compression integration hardware optimization\n\nConsidering refining your search by improving the query keywords input.\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using state-space models (SSMs) and integrate them with compression mechanisms, hardware optimization, and efficient parameter updates, here are some specific techniques and implementations based on the provided analysis and search results:\n\n## Integrating SSMs with Compression Mechanisms\n\n### Adaptive Compression Techniques\n- **Dynamic Contextual Compression**: Models like Dodo introduce dynamic contextual compression, where the number of hidden states at each layer is dynamically adjusted, reducing the cost of self-attention significantly. This approach can be integrated with SSMs to achieve efficient state compression without significant information loss.\n\n### Quantization and Compression\n- **Quantization Methods**: Techniques such as Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) can be applied to SSMs to reduce memory requirements. For example, using 8-bit integer precision (INT8) can significantly reduce memory usage while minimizing accuracy loss. This can be particularly useful when combined with hardware-aware optimizations.\n\n## Hardware-Efficient SSM Implementations\n\n### Memory Hierarchy Optimization\n- **Zero Redundancy Optimizer (ZeRO)**: ZeRO and its variants (ZeRO-Offload, ZeRO-Infinity, ZeRO++) can be adapted for SSMs to reduce redundant copies of data on GPUs, offload optimizer states and updates to CPUs, and utilize NVMe memory for offloading parameters and gradients. This can significantly improve memory efficiency and reduce communication overhead.\n\n### Kernel Fusion and Block-Based Processing\n- **Kernel Fusion**: Optimizing the memory hierarchy by fusing kernels and using block-based processing can improve the efficiency of SSM implementations. This involves minimizing memory accesses and maximizing the use of cache memory, which is crucial for hardware-aware algorithms.\n\n### Hardware-Specific Optimizations\n- **Mixed Precision Training**: Using mixed precision training (e.g., FP16 and INT8) on NVIDIA TensorCore-equipped hardware can provide both speed-up and memory consumption reduction. This can be particularly beneficial for SSMs that require intensive computations and large memory bandwidth.\n\n## Memory Optimization in State-Space Models\n\n### Selective State Space Models\n- **Mamba**: The Mamba model introduces selective SSMs that allow input-dependent parameter adjustment. This selective propagation of information can be optimized further by integrating adaptive compression techniques, ensuring efficient state management without significant information loss.\n\n### State Compression\n- **Closed-Loop Dynamics**: Models like S4, which start from a continuous time model with a diagonal plus low-rank update structure, can be optimized for state compression. This structure resembles a closed-loop dynamics matrix and can be efficiently compressed while preserving critical information.\n\n## Parameter Update Strategies in SSMs\n\n### Dynamic Parameter Adjustment\n- **Input-Dependent Parameters**: SSMs can be designed to have parameters that are functions of the input, allowing for dynamic adjustment based on the current token. This approach can be combined with quantization and compression techniques to ensure efficient parameter updates without compromising model performance.\n\n### Efficient Gradient Computation\n- **Gradient Compression**: Techniques like gradient compression (FP16, INT8) can be used to reduce the memory requirements during training. This is particularly useful when training large models, as it allows for more efficient use of GPU memory and reduces communication overhead.\n\nBy integrating these techniques, researchers can develop SSM-based autoregressive language models that are highly efficient, scalable, and optimized for both software and hardware considerations.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising directions for improving LM block design through state-space models (SSMs) and adaptive mechanisms:\n\n1. Selective State Space Models:\n- Mamba introduces selective SSMs that allow input-dependent parameter adjustment\n- Hardware-aware implementation crucial for efficiency\n- Integration with existing architectures possible without attention/MLP blocks\n\n2. Memory and State Management:\n- Adaptive compression techniques can reduce memory requirements\n- Hardware-aware algorithms key to efficient implementation\n- State compression without information loss is critical\n\n3. Implementation Considerations:\n- Kernel fusion and memory hierarchy optimization important\n- Block-based processing can improve efficiency\n- Hardware-specific optimizations necessary\n\nKey References and Excerpts:\n\n1. From Mamba paper (Gu & Dao, 2023):\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning... simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\n2. From State Space Models as Foundation Models (Alonso et al., 2024):\n\"The S4 model starts from a continuous time model... where the structure imposed on matrix A is diagonal plus a low-rank update... resembling a closed-loop dynamics matrix.\"\n\n3. From Dodo (Qin et al., 2023):\n\"Instead of one vector per token in a standard transformer model, Dodo represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space.\"\n\nKey Insights:\n\n1. State Management:\n- SSMs can achieve efficient state compression through selective mechanisms\n- Hardware-aware implementation crucial for performance\n- Memory hierarchy optimization key to efficiency\n\n2. Adaptive Mechanisms:\n- Dynamic parameter adjustment based on input context\n- Selective propagation of information\n- Efficient compression without significant loss\n\n3. Implementation Strategy:\n- Need for hardware-aware algorithms\n- Importance of memory access patterns\n- Balance between compression and information preservation\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific techniques for combining SSMs with adaptive compression\n2. Methods for efficient parameter updates in SSMs\n3. Integration strategies with existing architectures\n4. Hardware optimization techniques for SSMs\n\nNext Search Focus:\n1. SSM and compression integration techniques\n2. Hardware-efficient SSM implementations\n3. Parameter update mechanisms in state-space models\n\nReferences:\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n2. Alonso, C. A., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint.\n\n3. Qin, G., Rosset, C., Chau, E. C., Rao, N., & Van Durme, B. (2023). Dodo: Dynamic Contextual Compression for Decoder-only LMs. Annual Meeting of the Association for Computational Linguistics.\n\n4. Rozendaal, T. V., Huijben, I. A., & Cohen, T. (2021). Overfitting for Fun and Profit: Instance-Adaptive Data Compression. International Conference on Learning Representations.\n\n5. Smith, J. T. H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "block-based SSM processing selective parameter updates",
            "detail": "Find specific techniques and implementations for:\n1. Block-based processing in SSMs\n2. Efficient parameter update mechanisms for selective SSMs\n3. Hardware-efficient implementations of block processing\n4. Integration strategies with existing architectures",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific techniques and implementations for:\n1. Block-based processing in SSMs\n2. Efficient parameter update mechanisms for selective SSMs\n3. Hardware-efficient implementations of block processing\n4. Integration strategies with existing architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 1.00)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 1.00)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.99)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.99)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.99)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n\n\n---\n## Found 3 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: block-based SSM processing selective parameter updates\n\nConsidering refining your search by improving the query keywords input.\n\n### 3 related papers from Semantic Scholar\n\n#### 1. Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space\n\n*From Search Query: block-based SSM processing selective parameter updates*\n\n*Mor Geva, Avi Caciularu, Ke Wang, Yoav Goldberg*\n\n**TL;DR:** This work reverse-engineering the operation of the feed-forward network layers, one of the building blocks of transformer models, shows that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable.\n\n**Abstract:** Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 259  (*Influential: 34*)\n\n#### 2. Concise and Precise Context Compression for Tool-Using Language Models\n\n*From Search Query: block-based SSM processing selective parameter updates*\n\n*Yang Xu, ylfeng, Honglin Mu, Yutai Hou, Yitong Li, Xinghao Wang, Wanjun Zhong, Zhongyang Li, Dandan Tu, Qingfu Zhu, Min Zhang, Wanxiang Che*\n\n**TL;DR:** This work proposes two strategies for compressing tool documentation into concise and precise summary sequences for tool-using language models that reach a performance comparable to the upper-bound baseline under up to 16x compression ratio.\n\n**Abstract:** Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying the input window as well as slowing down the decoding process. Given the progress in general-purpose compression, soft context compression is a suitable approach to alleviate the problem. However, when compressing tool documentation, existing methods suffer from the weaknesses of key information loss (specifically, tool/parameter name errors) and difficulty in adjusting the length of compressed sequences based on documentation lengths. To address these problems, we propose two strategies for compressing tool documentation into concise and precise summary sequences for tool-using language models. 1) Selective compression strategy mitigates key information loss by deliberately retaining key information as raw text tokens. 2) Block compression strategy involves dividing tool documentation into short chunks and then employing a fixed-length compression model to achieve variable-length compression. This strategy facilitates the flexible adjustment of the compression ratio. Results on API-Bank and APIBench show that our approach reaches a performance comparable to the upper-bound baseline under up to 16x compression ratio.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 3. A Mixture of h - 1 Heads is Better than h Heads\n\n*From Search Query: block-based SSM processing selective parameter updates*\n\n*Hao Peng, Roy Schwartz, Dianqi Li, Noah A. Smith*\n\n**TL;DR:** This work proposes the mixture of attentive experts model (MAE), a model trained using a block coordinate descent algorithm that alternates between updating the responsibilities of the experts and their parameters and learns to activate different heads on different inputs.\n\n**Abstract:** Multi-head attentive neural architectures have achieved state-of-the-art results on a variety of natural language processing tasks. Evidence has shown that they are overparameterized; attention heads can be pruned without significant performance loss. In this work, we instead \u201creallocate\u201d them\u2014the model learns to activate different heads on different inputs. Drawing connections between multi-head attention and mixture of experts, we propose the mixture of attentive experts model (MAE). MAE is trained using a block coordinate descent algorithm that alternates between updating (1) the responsibilities of the experts and (2) their parameters. Experiments on machine translation and language modeling show that MAE outperforms strong baselines on both tasks. Particularly, on the WMT14 English to German translation dataset, MAE improves over \u201ctransformer-base\u201d by 0.8 BLEU, with a comparable number of parameters. Our analysis shows that our model learns to specialize different experts to different inputs.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 27  (*Influential: 3*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using state space models (SSMs) with a focus on block-based processing, efficient parameter updates, and hardware-efficient implementations, here are some specific techniques and implementations that can be considered:\n\n## Block-Based Processing in SSMs\n\n1. **Block Decomposition**: The work on Mamba and its successors highlights the use of block decomposition approaches for efficient computation. This involves breaking down the SSM operations into block-based structures, similar to those used in convolutional neural networks (CNNs). For example, the Centaurus network design uses a mix of full, group, and bottleneck SSM blocks, inspired by classical convolutional blocks, to balance network size and performance.\n\n2. **Semiseparable Matrices**: The SSD algorithm mentioned in the analysis uses block decompositions of semiseparable matrices to optimize SSM computations. This approach leverages both the linear SSM recurrence and quadratic dual form to achieve optimal tradeoffs in efficiency.\n\n## Efficient Parameter Update Mechanisms for Selective SSMs\n\n1. **Input-Dependent Parameter Adjustment**: Mamba introduces a method where SSM parameters are functions of the input, allowing for content-based reasoning. This approach enables the model to selectively propagate or forget information based on the input, which can be particularly useful for autoregressive models.\n\n2. **Hardware-Aware Parallel Algorithms**: Implementing hardware-aware parallel algorithms for recurrent modes can enhance the efficiency of parameter updates. This involves optimizing the algorithms to take advantage of the hardware's capabilities, such as parallel processing and memory access patterns.\n\n## Hardware-Efficient Implementations of Block Processing\n\n1. **Memory Hierarchy Optimization**: Optimizing the memory hierarchy through careful state materialization is crucial for efficient block processing. This involves managing the state updates without materializing the full expanded states, which can significantly reduce memory usage and improve performance.\n\n2. **Adaptive Compression**: Using adaptive compression mechanisms can help in efficient state management. This can involve selective compression strategies that balance between compression and information preservation, ensuring that the model maintains its expressiveness while being hardware-efficient.\n\n## Integration Strategies with Existing Architectures\n\n1. **Merging with MoE Frameworks**: BlackMamba integrates the strengths of SSMs with mixture-of-experts (MoE) frameworks. This combination allows for streamlined performance and addresses scalability issues, making it a viable approach for integrating SSMs with existing architectures.\n\n2. **Tensor Network Representations**: Expressing SSM blocks as tensor networks can facilitate integration with other architectures. This approach, as seen in the Centaurus network, allows for easy observation of the connective structure of SSM blocks and can be integrated with various other network designs.\n\n## Additional Insights\n\n- **Spiking Neural Networks (SNNs) Integration**: The SPikE-SSM model integrates spiking neurons with SSM blocks, which can provide energy-efficient and sparse computation. This could be an innovative way to enhance the efficiency and robustness of autoregressive models, especially for long sequence learning.\n\n- **Spectral State Space Models**: These models have provable robustness properties and can be integrated with existing architectures to enhance performance and efficiency. They depend neither on the spectrum of the underlying dynamics nor the dimensionality of the problem, making them a robust choice.\n\nBy incorporating these techniques, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage the strengths of SSMs and other advanced architectures.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal significant developments in SSM architectures and their hardware-efficient implementations, particularly through recent innovations like Mamba and its successors. Key areas identified:\n\n1. Hardware-Efficient Implementation:\n- Mamba-2 introduces refined SSM implementations 2-8x faster than original\n- Memory hierarchy optimization through careful state materialization\n- Block decomposition approaches for efficient computation\n\n2. Selective State Space Models:\n- Input-dependent parameter adjustment for content-based reasoning\n- Hardware-aware parallel algorithms for recurrent mode\n- Integration with simplified network architectures\n\n3. Memory and State Management:\n- Adaptive compression through selective mechanisms\n- Efficient state updates without materializing full expanded states\n- Hardware-aware memory access patterns\n\nKey Excerpts and References:\n\n1. From Transformers are SSMs (Dao & Gu, 2024):\n\"Our framework exposes new efficient and easily-implementable algorithms for computing SSMs... SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes.\"\n\n2. From Mamba (Gu & Dao, 2023):\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning... simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information.\"\n\n3. From Spectral State Space Models (Agarwal et al., 2023):\n\"Spectral state space models have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n\nKey Insights:\n\n1. State Management:\n- SSMs can achieve efficient state compression through selective mechanisms\n- Hardware-aware implementation crucial for performance\n- Memory hierarchy optimization key to efficiency\n\n2. Architecture Design:\n- Block decomposition enables efficient computation\n- Integration of selective mechanisms improves expressiveness\n- Hardware considerations crucial for implementation\n\n3. Implementation Strategy:\n- Need for hardware-aware algorithms\n- Importance of memory access patterns\n- Balance between compression and information preservation\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific techniques for block-based SSM processing\n2. Methods for efficient parameter updates in selective SSMs\n3. Integration strategies with existing architectures\n4. Hardware optimization techniques for block processing\n\nReferences:\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n3. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n4. Bonassi, F., Andersson, C. R., Mattsson, P., & Sch\u00f6n, T. B. (2023). Structured state-space models are deep Wiener models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"memory efficient normalization\", \"hardware optimization SSM\", \"stability analysis adaptive normalization\"",
            "detail": "Find papers discussing:\n1. Memory-efficient implementations of normalization layers, particularly in the context of SSMs\n2. Hardware-specific optimization strategies for combined SSM and normalization approaches\n3. Theoretical analysis of stability in adaptive normalization systems\n4. Methods for efficient state management in SSM architectures\n5. Techniques for balancing adaptivity and computational efficiency",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Memory-efficient implementations of normalization layers, particularly in the context of SSMs\n2. Hardware-specific optimization strategies for combined SSM and normalization approaches\n3. Theoretical analysis of stability in adaptive normalization systems\n4. Methods for efficient state management in SSM architectures\n5. Techniques for balancing adaptivity and computational efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.92)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.98)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.86)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.60)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.97)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n##### *Relevant Chunk: No. 7/86 (Score: 0.23)*\n\n```\n2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms. ### 2.4 Overview: Structured State Space Duality\n\nWhile this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically. Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM (2). The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:\n\n- The structure on $A$ is further simplified from diagonal to scalar times identity structure. Each $A_{t}$ can also be identified with just a scalar in this case. - We use a larger head dimension $P$, compared to $P=1$ used in Mamba. Typically $P=\\{64,128\\}$ is chosen which is similar to conventions for modern Transformers. Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators. Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as\n\n$$\n\\left(L \\circ Q K^{\\top}\\right) \\cdot V \\quad L_{i j}= \\begin{cases}a_{i} \\times \\cdots \\times a_{j+1} & i \\geq j \\\\ 0 & i<j\\end{cases}\n$$\n\nwhere $a_{i}$ are input-dependent scalars bounded in $[0,1]$. Compared to standard softmax attention, there are two main differences\n\n- The softmax is dropped. - The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$. Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the \"attention sink\" phenomenon (Darcet et al. 2024; Xiao et al. 2024). More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time. More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4. Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form $Y=M X$ for a matrix $M_{\\theta} \\in \\mathbb{R}^{(T, T)}$ that depends on $\\theta=(A, B, C)$. In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$. Going beyond these, any algorithm for multiplication by $M$ can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs (Gu and Dao 2023); Listing 1 provides a complete implementation in a few lines of code.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.45)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.45)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"memory efficient normalization\", \"hardware optimization SSM\", \"stability analysis adaptive normalization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search\n\n*From Search Query: \"memory efficient normalization\"*\n\n*Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, H. Xiong*\n\n**TL;DR:** This paper presents a novel approach, namely Partially-Connected DARTS, by sampling a small part of super-net to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance.\n\n**Abstract:** Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-net and searching for an optimal architecture. In this paper, we present a novel approach, namely Partially-Connected DARTS, by sampling a small part of super-net to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We solve it by introducing edge normalization, which adds a new set of edge-level hyper-parameters to reduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can be trained with a larger batch size and, consequently, enjoy both faster speed and higher training stability. Experiment results demonstrate the effectiveness of the proposed method. Specifically, we achieve an error rate of 2.57% on CIFAR10 within merely 0.1 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.2% on ImageNet (under the mobile setting) within 3.8 GPU-days for search. Our code has been made available at https://www.dropbox.com/sh/on9lg3rpx1r6dkf/AABG5mt0sMHjnEJyoRnLEYW4a?dl=0.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 560  (*Influential: 131*)\n\n#### 2. Norm matters: efficient and accurate normalization schemes in deep networks\n\n*From Search Query: \"memory efficient normalization\"*\n\n*Elad Hoffer, Ron Banner, Itay Golan, Daniel Soudry*\n\n**TL;DR:** A novel view is presented on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective, and a modification to weight-normalization, which improves its performance on large-scale tasks.\n\n**Abstract:** Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used $L^2$ batch-norm, using normalization in $L^1$ and $L^\\infty$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 174  (*Influential: 15*)\n\n#### 3. MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\n*From Search Query: \"memory efficient normalization\"*\n\n*Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, Michael Spranger*\n\n**Abstract:** Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve out-of-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memory-constrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for back-propagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-the-art CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 6% with constrained memory and significantly reduces the memory costs of ResNet50 on ImageNet by at least 70% with comparable accuracy. Our codes can be accessed at https://github.com/SonyAI/MECTA .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 2*)\n\n#### 4. Data-Driven Offline Optimization For Architecting Hardware Accelerators\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Aviral Kumar, A. Yazdanbakhsh, Milad Hashemi, Kevin Swersky, S. Levine*\n\n**TL;DR:** This paper develops a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization.\n\n**Abstract:** Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a\"simulation-driven\"approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a\"data-driven\", offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators -- tailored towards both single and multiple applications -- improving performance upon state-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 5. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 6. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 7. Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds\n\n*From Search Query: \"stability analysis adaptive normalization\"*\n\n*Taira Tsuchiya, Shinji Ito, J. Honda*\n\n**TL;DR:** A generic adaptive learning rate, called stability-penalty-adaptive (SPA) learning rate for Follow-the-regularized-leader (FTRL) is developed, which yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed.\n\n**Abstract:** Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-regularized-leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called stability-penalty-adaptive (SPA) learning rate for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: sparsity, game-dependency, and best-of-both-worlds (BOBW). Despite the fact that sparsity appears frequently in real problems, existing sparse multi-armed bandit algorithms with $k$-arms assume that the sparsity level $s \\leq k$ is known in advance, which is often not the case in real-world scenarios. To address this issue, we first establish $s$-agnostic algorithms with regret bounds of $\\tilde{O}(\\sqrt{sT})$ in the adversarial regime for $T$ rounds, which matches the existing lower bound up to a logarithmic factor. Meanwhile, BOBW algorithms aim to achieve a near-optimal regret in both the stochastic and adversarial regimes. Leveraging the SPA learning rate and the technique for $s$-agnostic algorithms combined with a new analysis to bound the variation in FTRL output in response to changes in a regularizer, we establish the first BOBW algorithm with a sparsity-dependent bound. Additionally, we explore partial monitoring and demonstrate that the proposed SPA learning rate framework allows us to achieve a game-dependent bound and the BOBW simultaneously.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 8. Why Spectral Normalization Stabilizes GANs: Analysis and Improvements\n\n*From Search Query: \"stability analysis adaptive normalization\"*\n\n*Zinan Lin, Vyas Sekar, G. Fanti*\n\n**TL;DR:** This work shows that SN controls two important failure modes of GAN training: exploding and vanishing gradients, and proposes Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization.\n\n**Abstract:** Spectral normalization (SN) is a widely-used technique for improving the stability of Generative Adversarial Networks (GANs) by forcing each layer of the discriminator to have unit spectral norm. This approach controls the Lipschitz constant of the discriminator, and is empirically known to improve sample quality in many GAN architectures. However, there is currently little understanding of why SN is so effective. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization technique, proposed over two decades ago to control gradients in the training of deep neural networks. This connection helps to explain why the most popular implementation of SN for GANs requires no hyperparameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, we show that SN tends to preserve this property throughout training. Finally, building on this theoretical understanding, we propose Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization. Theoretically, we show that BSN gives better gradient control than SN. Empirically, we demonstrate that BSN outperforms SN in sample quality on several benchmark datasets, while also exhibiting better training stability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 35  (*Influential: 4*)\n\n### 5 related papers from ArXiv\n\n#### 1. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"memory efficient normalization\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 2. Extracting Prompts by Inverting LLM Outputs\n\n*From Search Query: \"memory efficient normalization\"*\n\n*Collin Zhang, John X. Morris, Vitaly Shmatikov*\n\n**Abstract:** We consider the problem of language model inversion: given outputs of a\nlanguage model, we seek to extract the prompt that generated these outputs. We\ndevelop a new black-box method, output2prompt, that learns to extract prompts\nwithout access to the model's logits and without adversarial or jailbreaking\nqueries. In contrast to previous work, output2prompt only needs outputs of\nnormal user queries. To improve memory efficiency, output2prompt employs a new\nsparse encoding techique. We measure the efficacy of output2prompt on a variety\nof user and system prompts and demonstrate zero-shot transferability across\ndifferent LLMs.\n\n**Published:** 2024-05-23T19:35:03Z  (*Updated: 2024-10-08T16:30:11Z*)\n\n\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n#### 4. Parameter-Efficient Fine-Tuning of State Space Models\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, Kangwook Lee*\n\n**Abstract:** Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged\nas powerful tools for language modeling, offering high performance with\nefficient inference and linear scaling in sequence length. However, the\napplication of parameter-efficient fine-tuning (PEFT) methods to SSM-based\nmodels remains largely unexplored. This paper aims to systematically study two\nkey questions: (i) How do existing PEFT methods perform on SSM-based models?\n(ii) Which modules are most effective for fine-tuning? We conduct an empirical\nbenchmark of four basic PEFT methods on SSM-based models. Our findings reveal\nthat prompt-based methods (e.g., prefix-tuning) are no longer effective, an\nempirical result further supported by theoretical analysis. In contrast, LoRA\nremains effective for SSM-based models. We further investigate the optimal\napplication of LoRA within these models, demonstrating both theoretically and\nexperimentally that applying LoRA to linear projection matrices without\nmodifying SSM modules yields the best results, as LoRA is not effective at\ntuning SSM modules. To further improve performance, we introduce LoRA with\nSelective Dimension tuning (SDLoRA), which selectively updates certain channels\nand states on SSM modules while applying LoRA to linear projection matrices.\nExtensive experimental results show that this approach outperforms standard\nLoRA.\n\n**Published:** 2024-10-11T17:30:28Z  (*Updated: 2024-10-11T17:30:28Z*)\n\n\n\n#### 5. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"stability analysis adaptive normalization\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. I-BERT: Integer-only BERT Quantization\n\n*From Search Query: \"memory efficient normalization\"*\n\n*Kurt Keutzer, Michael W. Mahoney, Zhewei Yao, Amir Gholami, Sehoon Kim*\n\n**Abstract:** Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive efficient inference at the edge, and even at the data center. While quantization can be a viable solution for this, previous work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot efficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM processors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has been open-sourced.\n\n**Published:** 2021-01-05\n\n\n\n#### 2. QLoRA: Efficient Finetuning of Quantized LLMs\n\n*From Search Query: \"memory efficient normalization\"*\n\n*Luke Zettlemoyer, Ari Holtzman, Artidoro Pagnoni, Tim Dettmers*\n\n**Abstract:** We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-23\n\n\n\n#### 3. Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation\n\n*From Search Query: \"stability analysis adaptive normalization\"*\n\n*Liping Jing, Jun Zhu, Hang Su, Zicheng Sun, Xingxing Zhang, Liyuan Wang, Yilin Lyu*\n\n**Abstract:** Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N.\n\n**Conference:** overcoming-recency-bias-of-normalization\n\n**Published:** 2023-10-13\n\n\n\n\n\n---\n## Web search results\n\n To address the researcher's goals and the detailed requirements, here are some key points and references that can help in improving autoregressive language model design, particularly focusing on memory-efficient normalization, hardware optimization, stability analysis, and efficient state management.\n\n## Memory-Efficient Normalization Techniques\n\n### Parameter-Efficient Fine-Tuning (PEFT)\nTechniques like Low-Rank Adaptation (LoRA) can be adapted for memory-efficient fine-tuning of large language models. LoRA reduces the memory footprint by approximating only a small subset of the model's parameters, which can be applied to normalization layers as well. This approach is discussed in the context of fine-tuning large language models, which can be extended to normalization layers within these models.\n\n### Gradient Low-Rank Projection (GaLore)\nGaLore, another PEFT method, approximates the optimizer states rather than the model weights. This can be particularly useful for normalization layers, as it reduces the memory footprint associated with optimizer states, allowing for more efficient training and fine-tuning without compromising model performance.\n\n## Hardware-Specific Optimization Strategies\n\n### Data and Model Parallelism\nTechniques such as data and model parallelism, along with strategies like gradient checkpointing and memory offloading (e.g., ZeRO-Offload), can significantly optimize memory usage and computational efficiency. These methods can be applied to combined SSM and normalization approaches to ensure efficient use of hardware resources.\n\n### Block-Based Processing\nHardware-aware implementations that utilize block-based processing can optimize memory usage. For instance, algorithms designed for recurrent mode, like those mentioned in the Mamba paper, can achieve fast inference and linear scaling in sequence length, which is beneficial for integrating SSMs with normalization layers.\n\n## Stability Analysis of Adaptive Normalization\n\n### Theoretical Guarantees\nStability analysis in adaptive normalization systems is crucial. Theoretical guarantees for combined approaches involving SSMs and normalization can be derived by analyzing the convergence properties and robustness of these systems. This involves understanding the low-rank structure of gradients and how it affects the stability of the normalization process.\n\n### Impact on Convergence Properties\nThe incorporation of second-order information, as in the Natural GaLore method, can provide faster convergence and higher performance without additional memory overhead. This method can be analyzed theoretically to ensure stability and robustness in adaptive normalization systems.\n\n## Efficient State Management in SSM Architectures\n\n### State Compression\nEfficient state compression is essential for managing the state in SSM architectures. Techniques like sharding model parameters, optimizer states, and gradients across multiple devices can help in reducing the memory footprint and improving computational efficiency.\n\n### Memory Hierarchy Optimization\nOptimizing the memory hierarchy is crucial for efficient state management. This involves careful management of activations, gradients, and optimizer states to ensure that the memory usage is minimized without compromising performance. The use of mixed precision (e.g., int8, fp16, fp32) can also help in reducing memory requirements.\n\n## Balancing Adaptivity and Computational Efficiency\n\n### Trade-Offs Between Adaptivity and Efficiency\nBalancing adaptivity and computational efficiency is a key challenge. Techniques like LoRA and GaLore offer a trade-off between adaptivity and efficiency by approximating certain components of the model or optimizer states. This balance is critical for ensuring that the model remains adaptive while maintaining computational efficiency.\n\nIn summary, the following strategies and techniques can help in achieving the researcher's goals:\n\n- **Memory-Efficient Normalization**: Use PEFT techniques like LoRA and GaLore to reduce the memory footprint of normalization layers.\n- **Hardware Optimization**: Employ data and model parallelism, gradient checkpointing, and memory offloading strategies to optimize hardware usage.\n- **Stability Analysis**: Analyze the theoretical guarantees and convergence properties of adaptive normalization systems to ensure stability and robustness.\n- **Efficient State Management**: Optimize the memory hierarchy and use state compression techniques to manage the state in SSM architectures efficiently.\n- **Balancing Adaptivity and Efficiency**: Use techniques that balance adaptivity and computational efficiency, such as LoRA and GaLore, to ensure the model remains adaptive while being computationally efficient.\n",
            "analysis": "Summary of Analysis:\n\nThe search results provide comprehensive insights into SSMs, hardware efficiency, and normalization techniques. Key findings reveal important considerations for integrating spectral SSMs with normalization in an efficient manner.\n\n1. Key References with Excerpts:\n\nFrom Mamba paper:\n\"We design a hardware-aware parallel algorithm in recurrent mode... Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length\"\n\nFrom Transformers are SSMs:\n\"Our framework exposes new efficient and easily-implementable algorithms for computing SSMs... that takes advantage of both the linear SSM recurrence and quadratic dual form\"\n\nFrom State Space Models as Foundation Models:\n\"SSMs can be naturally connected to deep sequence modeling... offering the opportunity to create synergies between the corresponding research areas\"\n\n2. Key Insights:\n\na) Hardware Efficiency:\n- SSMs can achieve linear scaling with sequence length\n- Hardware-aware implementations are crucial for performance\n- Block-based processing can optimize memory usage\n\nb) Normalization Integration:\n- Need careful consideration of stability in adaptive normalization\n- Trade-offs between adaptivity and computational efficiency\n- Importance of proper initialization for stability\n\nc) Implementation Considerations:\n- Memory hierarchy optimization is crucial\n- Need for efficient state compression\n- Balance between adaptivity and stability\n\n3. Areas Needing Further Investigation:\n\na) Specific Integration Methods:\n- How to efficiently combine spectral SSMs with normalization\n- Optimal initialization strategies\n- Memory-efficient implementations\n\nb) Stability Analysis:\n- Theoretical guarantees for combined approaches\n- Impact on convergence properties\n- Robustness considerations\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\n1. Memory-efficient implementations of normalization in SSMs\n2. Stability analysis of adaptive normalization techniques\n3. Hardware optimization strategies for combined approaches\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n[2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n[3] Alonso, C. A., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint.\n\n[4] Fu, D. Y., et al. (2023). Simple Hardware-Efficient Long Convolutions for Sequence Modeling. ICML.\n\n[5] Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. ICML.\n\nNeed more information about:\n1. Memory-efficient normalization techniques\n2. Hardware-specific optimization strategies\n3. Stability analysis methods"
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"spectral state space models\", \"adaptive normalization\", \"selective compression\"",
            "detail": "Find papers discussing:\n1. Applications of spectral state space models in neural networks, particularly for normalization or adaptive processing\n2. Block-based processing techniques that optimize memory usage and hardware utilization in language models\n3. Methods for selective state compression and information preservation in deep learning models\n4. Theoretical analysis of stability and convergence in adaptive normalization methods\n5. Comparisons between different normalization techniques in terms of computational efficiency and effectiveness",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Applications of spectral state space models in neural networks, particularly for normalization or adaptive processing\n2. Block-based processing techniques that optimize memory usage and hardware utilization in language models\n3. Methods for selective state compression and information preservation in deep learning models\n4. Theoretical analysis of stability and convergence in adaptive normalization methods\n5. Comparisons between different normalization techniques in terms of computational efficiency and effectiveness\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.77)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.77)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.51)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.51)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.45)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.45)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. MambaLRP: Explaining Selective State Space Sequence Models (Avg. Score: 0.26)\n\n*F. Jafari, G. Montavon, Klaus-Robert M\u00fcller, Oliver Eberle*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work identifies specific components in the Mamba architecture, which cause unfaithful explanations and proposes MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components.\n\n**Abstract:** Recent sequence modeling approaches using Selective State Space Sequence Models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models.\n\n##### *Relevant Chunk: No. 35/59 (Score: 0.26)*\n\n```\narXiv:2401.04722, 2024. [40] H. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur. Long range language modeling via gated state spaces. arXiv:2206.13947, 2022. [41] A. Mohtashami and M. Jaggi. Random-access infinite context length for transformers. In Advances in Neural Information Processing Systems, 2023. [42] G. Montavon, W. Samek, and K.-R. M\u00fcller. Methods for interpreting and understanding deep neural networks. Digital signal processing, 73:1-15, 2018. [43] G. Montavon, A. Binder, S. Lapuschkin, W. Samek, and K.-R. M\u00fcller. Layer-wise relevance propagation: An overview. Explainable AI: interpreting, explaining and visualizing deep learning, pages 193-209, 2019. [44] E. Nguyen, K. Goel, A. Gu, G. Downs, P. Shah, T. Dao, S. Baccus, and C. R\u00e9. S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in Neural Information Processing Systems, 35:2846-2861, 2022. [45] M. Pi\u00f3ro, K. Ciebiera, K. Kr\u00f3l, J. Ludziejewski, and S. Jaszczur. MoE-Mamba: Efficient selective state space models with mixture of experts.\n```\n\n#### 5. The Hidden Attention of Mamba Models  (Avg. Score: 0.21)\n\n*Ameen Ali, Itamar Zimerman, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 19  (*Influential: 3*)\n\n**TL;DR:** A third view is added and it is shown that selective SSMs can be viewed as attention-driven models and empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers.\n\n**Abstract:** The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains, including NLP, long-range sequence processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 17/33 (Score: 0.21)*\n\n```\nIn: Proceedings of NAACLHLT. pp. 3543-3556 (2019) 14\n40. Kulikov, I., Eremeev, M., Cho, K.: Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling. In: Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 1115-1124. Association for Computational Linguistics, Online only (Nov 2022) 20\n41. Li, S., Singh, H., Grover, A.: Mamba-nd: Selective state space modeling for multidimensional data. arXiv preprint arXiv:2402.05892 (2024) 1\n42. Liang, D., Zhou, X., Wang, X., Zhu, X., Xu, W., Zou, Z., Ye, X., Bai, X.: Pointmamba: A simple state space model for point cloud analysis. arXiv preprint arXiv:2402.10739 (2024) 1\n43. Liu, J., Yang, H., Zhou, H.Y., Xi, Y., Yu, L., Yu, Y., Liang, Y., Shi, G., Zhang, S., Zheng, H., et al.: Swin-umamba: Mamba-based unet with imagenet-based pretraining. arXiv preprint arXiv:2402.03302 (2024) 1\n44. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Liu, Y.: Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166 (2024) 1, 3, 4\n45. Lu, C., Schroecker, Y., Gu, A., Parisotto, E., Foerster, J., Singh, S., Behbahani, F.: Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems 36 (2024) 3\n46. Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., Zhang, L.: Soft: Softmax-free transformer with linear complexity. Advances in Neural Information Processing Systems 34, 21297-21309 (2021) 7\n47. Lutati, S., Zimerman, I., Wolf, L.: Focus your attention (with adaptive iir filters). arXiv preprint arXiv:2305.14952 (2023) 4\n48. Ma, J., Li, F., Wang, B.: U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722 (2024) 1\n49. Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., Zettlemoyer, L.: Mega: moving average equipped gated attention. arXiv preprint arXiv:2209.10655 (2022) 7\n50. Martin, E., Cundy, C.: Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057 (2017) 4\n51. Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947 (2022) 3\n52. Mehta, H., Gupta, A., Cutkosky, A., Neyshabur, B.: Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947 (2022) 14, 27\n53. Nam, W.J., Gur, S., Choi, J., Wolf, L., Lee, S.W.: Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"spectral state space models\", \"adaptive normalization\", \"selective compression\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Hilbert Space Embeddings of Hidden Markov Models\n\n*From Search Query: \"spectral state space models\"*\n\n*Le Song, Byron Boots, S. Siddiqi, Geoffrey J. Gordon, Alex Smola*\n\n**TL;DR:** This work proposes a nonparametric HMM that extends traditional HMMs to structured and non-Gaussian continuous distributions, and derives a local-minimum-free kernel spectral algorithm for learning these HMMs.\n\n**Abstract:** Hidden Markov Models (HMMs) are important tools for modeling sequence data. However, they are restricted to discrete latent states, and are largely restricted to Gaussian and discrete observations. And, learning algorithms for HMMs have predominantly relied on local search heuristics, with the exception of spectral methods such as those described below. We propose a nonparametric HMM that extends traditional HMMs to structured and non-Gaussian continuous distributions. Furthermore, we derive a local-minimum-free kernel spectral algorithm for learning these HMMs. We apply our method to robot vision data, slot car inertial sensor data and audio event classification data, and show that in these applications, embedded HMMs exceed the previous state-of-the-art performance.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2010\n\n**Citations:** 234  (*Influential: 21*)\n\n#### 2. Learning Harmonic Molecular Representations on Riemannian Manifold\n\n*From Search Query: \"spectral state space models\"*\n\n*Yiqun Wang, Yuning Shen, Shih\u2010Ya Chen, Lihao Wang, Fei Ye, Hao Zhou*\n\n**TL;DR:** A Harmonic Molecular Representation learning (HMR) framework, which represents a molecule using the Laplace-Beltrami eigenfunctions of its molecular surface, and outperforms the state-of-the-art deep learning models for ligand-binding protein pocket classification and the rigid protein docking challenge, demonstrating its versatility in molecular representation learning.\n\n**Abstract:** Molecular representation learning plays a crucial role in AI-assisted drug discovery research. Encoding 3D molecular structures through Euclidean neural networks has become the prevailing method in the geometric deep learning community. However, the equivariance constraints and message passing in Euclidean space may limit the network expressive power. In this work, we propose a Harmonic Molecular Representation learning (HMR) framework, which represents a molecule using the Laplace-Beltrami eigenfunctions of its molecular surface. HMR offers a multi-resolution representation of molecular geometric and chemical features on 2D Riemannian manifold. We also introduce a harmonic message passing method to realize efficient spectral message passing over the surface manifold for better molecular encoding. Our proposed method shows comparable predictive power to current models in small molecule property prediction, and outperforms the state-of-the-art deep learning models for ligand-binding protein pocket classification and the rigid protein docking challenge, demonstrating its versatility in molecular representation learning.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 2*)\n\n#### 3. Streaming Coresets for Symmetric Tensor Factorization\n\n*From Search Query: \"spectral state space models\"*\n\n*Rachit Chhaya, Jayesh Choudhari, A. Dasgupta, Supratim Shit*\n\n**TL;DR:** Six algorithms to select a sublinear number of these vectors as coreset, while guaranteeing that the CP decomposition of the p-moment tensor of the coreset approximates the corresponding decomposing of the $p-moments tensor computed from the full data.\n\n**Abstract:** Factorizing tensors has recently become an important optimization module in a number of machine learning pipelines, especially in latent variable models. We show how to do this efficiently in the streaming setting. Given a set of $n$ vectors, each in $\\mathbb{R}^d$, we present algorithms to select a sublinear number of these vectors as coreset, while guaranteeing that the CP decomposition of the $p$-moment tensor of the coreset approximates the corresponding decomposition of the $p$-moment tensor computed from the full data. We introduce two novel algorithmic techniques: online filtering and kernelization. Using these two, we present six algorithms that achieve different tradeoffs of coreset size, update time and working space, beating or matching various state of the art algorithms. In the case of matrices ($2$-ordered tensor), our online row sampling algorithm guarantees $(1 \\pm \\epsilon)$ relative error spectral approximation. We show applications of our algorithms in learning single topic modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 4. Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective\n\n*From Search Query: \"adaptive normalization\"*\n\n*Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen*\n\n**TL;DR:** A novel slice-level adaptive normalization scheme, referred to SAN, is proposed, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization and could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data.\n\n**Abstract:** Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN , which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available 2 .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 7*)\n\n#### 5. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n\n*From Search Query: \"adaptive normalization\"*\n\n*Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich*\n\n**TL;DR:** A gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes is presented, showing that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks.\n\n**Abstract:** Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 1092  (*Influential: 144*)\n\n#### 6. Selective compression learning of latent representations for variable-rate image compression\n\n*From Search Query: \"selective compression\"*\n\n*Jooyoung Lee, S. Jeong, Munchurl Kim*\n\n**TL;DR:** A selective compression method that partially encodes the latent representations in a fully generalized manner for deep learning-based variable-rate image compression and can achieve comparable compression efficiency as those of the separately trained reference compression models and can reduce decoding time.\n\n**Abstract:** Recently, many neural network-based image compression methods have shown promising results superior to the existing tool-based conventional codecs. However, most of them are often trained as separate models for different target bit rates, thus increasing the model complexity. Therefore, several studies have been conducted for learned compression that supports variable rates with single models, but they require additional network modules, layers, or inputs that often lead to complexity overhead, or do not provide sufficient coding efficiency. In this paper, we firstly propose a selective compression method that partially encodes the latent representations in a fully generalized manner for deep learning-based variable-rate image compression. The proposed method adaptively determines essential representation elements for compression of different target quality levels. For this, we first generate a 3D importance map as the nature of input content to represent the underlying importance of the representation elements. The 3D importance map is then adjusted for different target quality levels using importance adjustment curves. The adjusted 3D importance map is finally converted into a 3D binary mask to determine the essential representation elements for compression. The proposed method can be easily integrated with the existing compression models with a negligible amount of overhead increase. Our method can also enable continuously variable-rate compression via simple interpolation of the importance adjustment curves among different quality levels. The extensive experimental results show that the proposed method can achieve comparable compression efficiency as those of the separately trained reference compression models and can reduce decoding time owing to the selective compression. The sample codes are publicly available at https://github.com/JooyoungLeeETRI/SCR.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 7. RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation\n\n*From Search Query: \"selective compression\"*\n\n*Fangyuan Xu, Weijia Shi, Eunsol Choi*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 40  (*Influential: 7*)\n\n#### 8. Revisiting Discriminator in GAN Compression: A Generator-discriminator Cooperative Compression Scheme\n\n*From Search Query: \"selective compression\"*\n\n*Shaojie Li, Jie Wu, Xuefeng Xiao, Fei Chao, Xudong Mao, Rongrong Ji*\n\n**TL;DR:** The role of discriminator is revisited and a novel generator-discriminator cooperative compression scheme for GAN compression, termed GCC, which contributes to reducing 80% computational costs while maintains comparable performance in image translation tasks.\n\n**Abstract:** Recently, a series of algorithms have been explored for GAN compression, which aims to reduce tremendous computational overhead and memory usages when deploying GANs on resource-constrained edge devices. However, most of the existing GAN compression work only focuses on how to compress the generator, while fails to take the discriminator into account. In this work, we revisit the role of discriminator in GAN compression and design a novel generator-discriminator cooperative compression scheme for GAN compression, termed GCC. Within GCC, a selective activation discriminator automatically selects and activates convolutional channels according to a local capacity constraint and a global coordination constraint, which help maintain the Nash equilibrium with the lightweight generator during the adversarial training and avoid mode collapse. The original generator and discriminator are also optimized from scratch, to play as a teacher model to progressively refine the pruned generator and the selective activation discriminator. A novel online collaborative distillation scheme is designed to take full advantage of the intermediate feature of the teacher generator and discriminator to further boost the performance of the lightweight generator. Extensive experiments on various GAN-based generation tasks demonstrate the effectiveness and generalization of GCC. Among them, GCC contributes to reducing 80% computational costs while maintains comparable performance in image translation tasks. Our code and models are available at https://github.com/SJLeo/GCC.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 31  (*Influential: 7*)\n\n### 6 related papers from ArXiv\n\n#### 1. Deep Clustering with Measure Propagation\n\n*From Search Query: \"spectral state space models\"*\n\n*Minhua Chen, Badrinath Jayakumar, Padmasundari Gopalakrishnan, Qiming Huang, Michael Johnston, Patrick Haffner*\n\n**Abstract:** Deep models have improved state-of-the-art for both supervised and\nunsupervised learning. For example, deep embedded clustering (DEC) has greatly\nimproved the unsupervised clustering performance, by using stacked autoencoders\nfor representation learning. However, one weakness of deep modeling is that the\nlocal neighborhood structure in the original space is not necessarily preserved\nin the latent space. To preserve local geometry, various methods have been\nproposed in the supervised and semi-supervised learning literature (e.g.,\nspectral clustering and label propagation) using graph Laplacian\nregularization. In this paper, we combine the strength of deep representation\nlearning with measure propagation (MP), a KL-divergence based graph\nregularization method originally used in the semi-supervised scenario. The main\nassumption of MP is that if two data points are close in the original space,\nthey are likely to belong to the same class, measured by KL-divergence of class\nmembership distribution. By taking the same assumption in the unsupervised\nlearning scenario, we propose our Deep Embedded Clustering Aided by Measure\nPropagation (DECAMP) model. We evaluate DECAMP on short text clustering tasks.\nOn three public datasets, DECAMP performs competitively with other\nstate-of-the-art baselines, including baselines using additional data to\ngenerate word embeddings used in the clustering process. As an example, on the\nStackoverflow dataset, DECAMP achieved a clustering accuracy of 79%, which is\nabout 5% higher than all existing baselines. These empirical results suggest\nthat DECAMP is a very effective method for unsupervised learning.\n\n**Published:** 2021-04-18T22:02:43Z  (*Updated: 2021-04-26T14:32:28Z*)\n\n\n\n#### 2. Mimetic Initialization Helps State Space Models Learn to Recall\n\n*From Search Query: \"spectral state space models\"*\n\n*Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, Srinadh Bhojanapalli*\n\n**Abstract:** Recent work has shown that state space models such as Mamba are significantly\nworse than Transformers on recall-based tasks due to the fact that their state\nsize is constant with respect to their input sequence length. But in practice,\nstate space models have fairly large state sizes, and we conjecture that they\nshould be able to perform much better at these tasks than previously reported.\nWe investigate whether their poor copying and recall performance could be due\nin part to training difficulties rather than fundamental capacity constraints.\nBased on observations of their \"attention\" maps, we propose a structured\ninitialization technique that allows state space layers to more readily mimic\nattention. Across a variety of architecture settings, our initialization makes\nit substantially easier for Mamba to learn to copy and do associative recall\nfrom scratch.\n\n**Published:** 2024-10-14T23:17:46Z  (*Updated: 2024-10-14T23:17:46Z*)\n\n\n\n#### 3. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive normalization\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"adaptive normalization\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 5. Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model\n  Compression\n\n*From Search Query: \"selective compression\"*\n\n*Jingcun Wang, Yu-Guang Chen, Ing-Chao Lin, Bing Li, Grace Li Zhang*\n\n**Abstract:** Large Language Models (LLMs) have achieved remarkable breakthroughs. However,\nthe huge number of parameters in LLMs require significant amount of memory\nstorage in inference, which prevents their practical deployment in many\napplications. To reduce memory storage of LLMs, singular value decomposition\n(SVD) provides a promising solution to approximate weight matrices for\ncompressing LLMs. In this paper, we take a step further to explore parameter\nsharing across different layers with SVD to achieve more effective compression\nfor LLMs. Specifically, weight matrices in different layers are decomposed and\nrepresented as a linear combination of a set of shared basis vectors and unique\ncoefficients. The types of weight matrices and the layer selection for basis\nsharing are examined when compressing LLMs to maintain the performance.\nComprehensive experiments demonstrate that Basis Sharing outperforms\nstate-of-the-art SVD-based compression approaches and parameter sharing\ntechniques, especially under large compression ratios. Code is available at:\nhttps://github.com/TUDa-HWAI/Basis_Sharing\n\n**Published:** 2024-10-02T14:30:02Z  (*Updated: 2024-10-02T14:30:02Z*)\n\n\n\n#### 6. Entropy Law: The Story Behind Data Compression and LLM Performance\n\n*From Search Query: \"selective compression\"*\n\n*Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang, Defu Lian, Enhong Chen*\n\n**Abstract:** Data is the cornerstone of large language models (LLMs), but not all data is\nuseful for model learning. Carefully selected data can better elicit the\ncapabilities of LLMs with much less computational overhead. Most methods\nconcentrate on evaluating the quality of individual samples in data selection,\nwhile the combinatorial effects among samples are neglected. Even if each\nsample is of perfect quality, their combinations may be suboptimal in teaching\nLLMs due to their intrinsic homogeneity or contradiction. In this paper, we aim\nto uncover the underlying relationships between LLM performance and data\nselection. Inspired by the information compression nature of LLMs, we uncover\nan ``entropy law'' that connects LLM performance with data compression ratio\nand first-epoch training loss, which reflect the information redundancy of a\ndataset and the mastery of inherent knowledge encoded in this dataset,\nrespectively. Through both theoretical deduction and empirical evaluation, we\nfind that model performance is negatively correlated to the compression ratio\nof training data, which usually yields a lower training loss. Based on the\nfindings of the entropy law, we propose a quite efficient and universal data\nselection method named \\textbf{ZIP} for training LLMs, which aim to prioritize\ndata subsets exhibiting a low compression ratio. Based on a multi-stage\nalgorithm that selects diverse data in a greedy manner, we can obtain a good\ndata subset with satisfactory diversity. Extensive experiments have been\nconducted to validate the entropy law and the superiority of ZIP across\ndifferent LLM backbones and alignment stages. We also present an interesting\napplication of entropy law that can detect potential performance risks at the\nbeginning of model training.\n\n**Published:** 2024-07-09T08:14:29Z  (*Updated: 2024-07-11T03:06:45Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Spectral State Space Models\n\n*From Search Query: \"spectral state space models\"*\n\n*Elad Hazan, Xinyi Chen, Daniel Suo, Naman Agarwal*\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n**Published:** 2023-12-11\n\n\n\n#### 2. SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral Image Denoising\n\n*From Search Query: \"spectral state space models\"*\n\n*Jun Zhou, Jianfeng Lu, Fengchao Xiong, Guanyiman Fu*\n\n**Abstract:** Denoising is a crucial preprocessing step for hyperspectral images (HSIs) due to noise arising from intra-imaging mechanisms and environmental factors. Long-range spatial-spectral correlation modeling is beneficial for HSI denoising but often comes with high computational complexity. Based on the state space model (SSM), Mamba is known for its remarkable long-range dependency modeling capabilities and computational efficiency. Building on this, we introduce a memory-efficient spatial-spectral UMamba (SSUMamba) for HSI denoising, with the spatial-spectral continuous scan (SSCS) Mamba being the core component. SSCS Mamba alternates the row, column, and band in six different orders to generate the sequence and uses the bidirectional SSM to exploit long-range spatial-spectral dependencies. In each order, the images are rearranged between adjacent scans to ensure spatial-spectral continuity. Additionally, 3D convolutions are embedded into the SSCS Mamba to enhance local spatial-spectral modeling. Experiments demonstrate that SSUMamba achieves superior denoising results with lower memory consumption per batch compared to transformer-based methods. The source code is available at https://github.com/lronkitty/SSUMamba.\n\n**Published:** 2024-05-02\n\n\n\n#### 3. Semantic Image Synthesis with Spatially-Adaptive Normalization\n\n*From Search Query: \"adaptive normalization\"*\n\n*Jun-Yan Zhu, Ting-Chun Wang, Ming-Yu Liu, Taesung Park*\n\n**Abstract:** We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .\n\n**Conference:** semantic-image-synthesis-with-spatially-1\n\n**Published:** 2019-03-18\n\n\n\n#### 4. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n*From Search Query: \"adaptive normalization\"*\n\n*Junho Kim, Kwanghee Lee, Hyeonwoo Kang, Minjae Kim*\n\n**Abstract:** We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-07-25\n\n\n\n#### 5. RECOMP: Improving Retrieval-Augmented LMs with Compression and Selective Augmentation\n\n*From Search Query: \"selective compression\"*\n\n*Eunsol Choi, Weijia Shi, Fangyuan Xu*\n\n**Abstract:** Retrieving documents and prepending them in-context at inference time improves performance of language model (LMs) on a wide range of tasks. However, these documents, often spanning hundreds of words, make inference substantially more expensive. We propose compressing the retrieved documents into textual summaries prior to in-context integration. This not only reduces the computational costs but also relieves the burden of LMs to identify relevant information in long retrieved documents. We present two compressors -- an extractive compressor which selects useful sentences from retrieved documents and an abstractive compressor which generates summaries by synthesizing information from multiple documents. Both compressors are trained to improve LMs' performance on end tasks when the generated summaries are prepended to the LMs' input, while keeping the summary concise.If the retrieved documents are irrelevant to the input or offer no additional information to LM, our compressor can return an empty string, implementing selective augmentation.We evaluate our approach on language modeling task and open domain question answering task. We achieve a compression rate of as low as 6% with minimal loss in performance for both tasks, significantly outperforming the off-the-shelf summarization models. We show that our compressors trained for one LM can transfer to other LMs on the language modeling task and provide summaries largely faithful to the retrieved documents.\n\n**Published:** 2023-10-06\n\n\n\n#### 6. Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering\n\n*From Search Query: \"selective compression\"*\n\n*Lingpeng Kong, Jiacheng Ye, Yaoxiang Wang, Zhiyong Wu*\n\n**Abstract:** Despite the surprising few-shot performance of in-context learning (ICL), it is still a common practice to randomly sample examples to serve as context. This paper advocates a new principle for ICL: self-adaptive in-context learning. The self-adaption mechanism is introduced to help each sample find an in-context example permutation (i.e., selection and ordering) that can derive the correct prediction, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general select-then-rank framework and instantiate it with new selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the enormous potential of self-adaptive ICL that it might be able to close the gap between ICL and finetuning given more advanced algorithms. Our code is released to facilitate future research in this area: https://github.com/Shark-NLP/self-adaptive-ICL\n\n**Published:** 2022-12-20\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using spectral state space models, adaptive normalization, and selective compression, here are some key findings and suggestions based on the provided sources and the detailed analysis:\n\n## Applications of Spectral State Space Models in Neural Networks\n\n- **State Space Models (SSMs) in Sequence Modeling**: SSMs have been shown to be effective in sequence modeling due to their ability to handle long-range dependencies and their efficient inferential time complexity. The work on GraphSSM extends SSM theory to temporal graphs, integrating structural information and using Laplacian regularization, which could be adapted for normalization and adaptive processing in language models.\n\n## Block-Based Processing Techniques\n\n- **Efficient Processing in SSMs**: SSMs can be optimized using techniques such as convolutions or parallel scans, which enhance their computational efficacy. These methods can be integrated into block-based processing to optimize memory usage and hardware utilization in language models. For instance, the use of convolutions in SSMs can help in processing large sequences efficiently.\n\n## Methods for Selective State Compression and Information Preservation\n\n- **State Compression in SSMs**: The GraphSSM framework imposes smoothness constraints that help in achieving a more effective denoising effect during node memory compression. This approach can be adapted for selective state compression in language models, ensuring that critical information is preserved while reducing the state size. The smoothness criterion determined via the choice of graph Laplacian can be particularly useful.\n\n## Theoretical Analysis of Stability and Convergence in Adaptive Normalization Methods\n\n- **Spectral Regularization**: The use of spectral regularization can maintain the stability and trainability of neural networks by controlling the spectral norm of each layer. This method ensures that the maximum singular value of each layer remains close to one, which is crucial for preventing exploding gradients and maintaining effective gradient diversity. This approach can be integrated into adaptive normalization methods to ensure stability and convergence.\n\n## Comparisons Between Different Normalization Techniques\n\n- **Spectral Regularization vs. Other Regularizers**: Spectral regularization has been shown to be more performant and less sensitive to hyperparameters compared to other regularizers like L2 regularization. This makes it a promising candidate for adaptive normalization in language models, offering better generalization performance and trainability.\n\n### Additional Insights\n\n- **Generalization and Out-of-Distribution Performance**: For language models, especially those dealing with complex and dynamic data, ensuring good out-of-distribution generalization is crucial. The approach of using spectral state space models and adaptive normalization can be complemented by statistical significance tests and ensemble methods to improve generalization performance, as discussed in the context of dynamical systems.\n\n### Investigation Plan\n\n1. **First Round: Spectral SSMs in Normalization**\n   - Investigate how spectral SSMs can be integrated with RMSNorm for adaptive normalization. Look into the theoretical guarantees for stability and convergence, and assess the computational efficiency of this integration.\n\n2. **Second Round: Block Processing and Memory Optimization**\n   - Research block-based processing techniques that optimize memory usage and hardware utilization. Compare these with existing block-processing approaches and evaluate the trade-offs between block size and model performance.\n\n3. **Third Round: Selective State Compression**\n   - Evaluate dynamic compression strategies and their effectiveness in preserving critical information. Analyze the computational overhead of compression mechanisms and their impact on model performance.\n\n4. **Final Round: Comparison with Existing Approaches**\n   - Compare the proposed approach with traditional adaptive normalization methods and standard RMSNorm. Highlight the novelty and advantages of using spectral SSMs and selective state compression in language models.\n\nBy focusing on these areas, the researchers can develop a robust and efficient autoregressive language model that leverages the strengths of spectral state space models, adaptive normalization, and selective compression.\n",
            "analysis": "Initial Analysis of SpectralRMSNorm Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Integration of Spectral State Space Models (SSMs) with RMSNorm:\n- How do spectral SSMs compare to other state space models in normalization contexts?\n- What are the theoretical guarantees for stability and convergence?\n- How does this integration affect computational efficiency?\n\n2. Block-Based Processing:\n- Impact on memory efficiency and hardware utilization\n- Trade-offs between block size and model performance\n- Comparison with existing block-processing approaches\n\n3. Selective State Compression:\n- Effectiveness of dynamic compression strategies\n- Information preservation in compressed states\n- Computational overhead of compression mechanisms\n\n4. Comparison with Existing Approaches:\n- Differences from traditional adaptive normalization methods\n- Advantages over standard RMSNorm\n- Novelty compared to existing SSM-based approaches\n\nInvestigation Plan:\n1. First Round: Focus on spectral SSMs and their applications in normalization\n2. Second Round: Investigate block processing and memory optimization techniques\n3. Third Round: Research state compression methods and their effectiveness\n4. Final Round: Compare with existing adaptive normalization approaches\n\nInitial search results suggest this is a novel approach combining spectral SSMs with normalization in a unique way. The proposal differs from previous designs by:\n- Using spectral properties for adaptive normalization\n- Implementing block-based processing with SSM integration\n- Incorporating selective state compression\n\nKey references found:\n1. Wang & Xue (2023) - Theoretical foundations of SSMs\n2. Gu & Dao (2023) - Mamba architecture and selective state spaces\n3. Zhang & Sennrich (2019) - RMSNorm foundations\n\nFuture search needed to:\n1. Find more specific applications of SSMs in normalization\n2. Identify similar block-processing approaches\n3. Evaluate state compression techniques in language models\n\nThe proposal appears to be distinct from sibling designs:\n- blockla_transformer focuses on linear attention mechanisms\n- stateadaptivetransformer emphasizes general state adaptation\nThis proposal uniquely combines spectral SSMs with normalization for adaptive processing."
        },
        {
            "ready": false,
            "query": "\"spectral SSM implementation\", \"normalization stability analysis\"",
            "detail": "Find papers discussing:\n1. Practical implementations of spectral SSMs, particularly in normalization contexts\n2. Stability analysis and theoretical guarantees for adaptive normalization methods\n3. Memory-efficient implementations and optimization techniques\n4. Integration methods for combining different normalization approaches\n5. Hardware-aware design considerations for efficient normalization",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Practical implementations of spectral SSMs, particularly in normalization contexts\n2. Stability analysis and theoretical guarantees for adaptive normalization methods\n3. Memory-efficient implementations and optimization techniques\n4. Integration methods for combining different normalization approaches\n5. Hardware-aware design considerations for efficient normalization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.31)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.31)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 2. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.06)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.06)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.04)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.07)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 7/31 (Score: 0.02)*\n\n```\nIn our experiments we search over two values of $k_{y}=\\{2,32\\}$. For non-image tasks, ListOps, Text and Retrieval, we find that setting $k_{y}=2$ is sufficient to get optimal results. For the image tasks, CIFAR, Pathfinder and PathX, we found that $k_{y}=32$ led to significant performance gains. A performance ablation over this parameter can be found in the appendix (Table 2]. Overall we find that the STU model provides improvements over baselines such as S4 and LRU on 4 out of the 6 tasks and performs comparably to the best baseline on the others. Remarkably, the STU layers come with provable guarantees and thus performs well out of the box without the need for specific initializations, discretizations or normalizations. We initialize all parameters $M_{i}^{y}, M_{i}^{u}, M_{k}^{\\phi+}, M_{k}^{\\phi-}$ with 0 . We provide details of the experimental setup, including hyperparameter tuning in the appendix (Section E). ## 6 Conclusion\n\nInsprired by the success of SSMs, we present a new theoretically-founded deep neural network architecture, Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for\nlearning Linear Dynamical Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz. robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark. We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without the need for designing specific initializations, discretizations or normalizations which were necessary for existing SSMs to learn in such settings. While spectral SSMs only model symmetric A, our presented set of experiments on the LRA benchmark suggest that the gap between symmetric and general A is potentially small in real world tasks. Indeed more recent SSM models like [GD23, $\\mathrm{DSF}^{+}$24] work with real diagonals (i.e. symmetric case) as they do not find evidence that adding complex eigenvalues help. Spectral filtering has been extended in certain settings to asymmetric A [HLS ${ }^{+}$18] and a similar extension to our proposal is straightforward but comes with efficiency losses and we leave it to future work. ## References\n\n[ASB16] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120-1128. PMLR, 2016. [Ble89] Guy E Blelloch. Scans as primitive parallel operations. IEEE Transactions on computers, 38(11):1526-1538, 1989. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [BSF94] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994. [CVMG ${ }^{+}$14] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. $\\left[\\mathrm{DBK}^{+}\\right.$20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.\n```\n\n#### 4. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.02)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 13/24 (Score: 0.02)*\n\n```\nCoRR, abs/1609.07843, 2016. [20] Microsoft. Introducing phi-3: Redefining what's possible with slms. https://azure.microsoft.com/en-us/blog/ introducing-phi-3-redefining-whats-possible-with-slms/, 2024. [21] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization, 2021. [22] NERSC. Perlmutter system architecture. https://docs.nersc.gov/systems/ perlmutter/architecture/\n[23] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022.\n```\n\n\n\n---\n## Found 11 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: \"spectral SSM implementation\", \"normalization stability analysis\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Spectral Clustering with Graph Neural Networks for Graph Pooling\n\n*From Search Query: \"spectral SSM implementation\"*\n\n*F. Bianchi, Daniele Grattarola, C. Alippi*\n\n**TL;DR:** This paper forms a continuous relaxation of the normalized minCUT problem and trains a GNN to compute cluster assignments that minimize this objective, and designs a graph pooling operator that overcomes some important limitations of state-of-the-art graph Pooling techniques and achieves the best performance in several supervised and unsupervised tasks.\n\n**Abstract:** Spectral clustering (SC) is a popular clustering technique to find strongly connected communities on a graph. SC can be used in Graph Neural Networks (GNNs) to implement pooling operations that aggregate nodes belonging to the same cluster. However, the eigendecomposition of the Laplacian is expensive and, since clustering results are graph-specific, pooling methods based on SC must perform a new optimization for each new sample. In this paper, we propose a graph clustering approach that addresses these limitations of SC. We formulate a continuous relaxation of the normalized minCUT problem and train a GNN to compute cluster assignments that minimize this objective. Our GNN-based implementation is differentiable, does not require to compute the spectral decomposition, and learns a clustering function that can be quickly evaluated on out-of-sample graphs. From the proposed clustering method, we design a graph pooling operator that overcomes some important limitations of state-of-the-art graph pooling techniques and achieves the best performance in several supervised and unsupervised tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 374  (*Influential: 45*)\n\n#### 2. Spectral Normalization for Generative Adversarial Networks\n\n*From Search Query: \"spectral SSM implementation\"*\n\n*Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida*\n\n**TL;DR:** This paper proposes a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator and confirms that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.\n\n**Abstract:** One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 4168  (*Influential: 587*)\n\n#### 3. Why Spectral Normalization Stabilizes GANs: Analysis and Improvements\n\n*From Search Query: \"spectral SSM implementation\"*\n\n*Zinan Lin, Vyas Sekar, G. Fanti*\n\n**TL;DR:** This work shows that SN controls two important failure modes of GAN training: exploding and vanishing gradients, and proposes Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization.\n\n**Abstract:** Spectral normalization (SN) is a widely-used technique for improving the stability of Generative Adversarial Networks (GANs) by forcing each layer of the discriminator to have unit spectral norm. This approach controls the Lipschitz constant of the discriminator, and is empirically known to improve sample quality in many GAN architectures. However, there is currently little understanding of why SN is so effective. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization technique, proposed over two decades ago to control gradients in the training of deep neural networks. This connection helps to explain why the most popular implementation of SN for GANs requires no hyperparameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, we show that SN tends to preserve this property throughout training. Finally, building on this theoretical understanding, we propose Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization. Theoretically, we show that BSN gives better gradient control than SN. Empirically, we demonstrate that BSN outperforms SN in sample quality on several benchmark datasets, while also exhibiting better training stability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 35  (*Influential: 4*)\n\n#### 4. Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction\n\n*From Search Query: \"normalization stability analysis\"*\n\n*Kaifeng Lyu, Zhiyuan Li, Sanjeev Arora*\n\n**TL;DR:** For a fairly broad class of neural nets with normalization, the theory explains how GD with a finite learning rate enters the so-called Edge of Stability (EoS) regime, and characterizes the trajectory of GD in this regime via a continuous sharpness-reduction flow.\n\n**Abstract:** Normalization layers (e.g., Batch Normalization, Layer Normalization) were introduced to help with optimization difficulties in very deep nets, but they clearly also help generalization, even in not-so-deep nets. Motivated by the long-held belief that flatter minima lead to better generalization, this paper gives mathematical analysis and supporting experiments suggesting that normalization (together with accompanying weight-decay) encourages GD to reduce the sharpness of loss surface. Here\"sharpness\"is carefully defined given that the loss is scale-invariant, a known consequence of normalization. Specifically, for a fairly broad class of neural nets with normalization, our theory explains how GD with a finite learning rate enters the so-called Edge of Stability (EoS) regime, and characterizes the trajectory of GD in this regime via a continuous sharpness-reduction flow.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 60  (*Influential: 2*)\n\n#### 5. Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation\n\n*From Search Query: \"normalization stability analysis\"*\n\n*Yilin Lyu, Liyuan Wang, Xingxing Zhang, Zicheng Sun, Hang Su, Jun Zhu, Liping Jing*\n\n**TL;DR:** This work focuses on the most popular Batch Normalization and provides an in-depth theoretical analysis of its sub-optimality in continual learning, and proposes Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages.\n\n**Abstract:** Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n### 4 related papers from ArXiv\n\n#### 1. The Expressive Capacity of State Space Models: A Formal Language\n  Perspective\n\n*From Search Query: \"spectral SSM implementation\"*\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have\nshown promising performance in language modeling (LM), competititve with\ntransformers. However, there is little understanding of the in-principle\nabilities of such models, which could provide useful guidance to the search for\nbetter LM architectures. We present a comprehensive theoretical study of the\ncapacity of such SSMs as it compares to that of transformers and traditional\nRNNs. We find that SSMs and transformers have overlapping but distinct\nstrengths. In star-free state tracking, SSMs implement straightforward and\nexact solutions to problems that transformers struggle to represent exactly.\nThey can also model bounded hierarchical structure with optimal memory even\nwithout simulating a stack. On the other hand, we identify a design choice in\ncurrent SSMs that limits their expressive power. We discuss implications for\nSSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n**Published:** 2024-05-27T17:46:57Z  (*Updated: 2024-06-02T19:43:55Z*)\n\n\n\n#### 2. MambaByte: Token-free Selective State Space Model\n\n*From Search Query: \"spectral SSM implementation\"*\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.\n\n**Published:** 2024-01-24T18:53:53Z  (*Updated: 2024-08-09T20:18:57Z*)\n\n\n\n#### 3. Sparse Attention with Linear Units\n\n*From Search Query: \"normalization stability analysis\"*\n\n*Biao Zhang, Ivan Titov, Rico Sennrich*\n\n**Abstract:** Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.\n\n**Published:** 2021-04-14T17:52:38Z  (*Updated: 2021-10-06T14:04:59Z*)\n\n\n\n#### 4. DeepNet: Scaling Transformers to 1,000 Layers\n\n*From Search Query: \"normalization stability analysis\"*\n\n*Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei*\n\n**Abstract:** In this paper, we propose a simple yet effective method to stabilize\nextremely deep Transformers. Specifically, we introduce a new normalization\nfunction (DeepNorm) to modify the residual connection in Transformer,\naccompanying with theoretically derived initialization. In-depth theoretical\nanalysis shows that model updates can be bounded in a stable way. The proposed\nmethod combines the best of two worlds, i.e., good performance of Post-LN and\nstable training of Pre-LN, making DeepNorm a preferred alternative. We\nsuccessfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and\nfeed-forward network sublayers) without difficulty, which is one order of\nmagnitude deeper than previous deep Transformers. Remarkably, on a multilingual\nbenchmark with 7,482 translation directions, our 200-layer model with 3.2B\nparameters significantly outperforms the 48-layer state-of-the-art model with\n12B parameters by 5 BLEU points, which indicates a promising scaling direction.\n\n**Published:** 2022-03-01T15:36:38Z  (*Updated: 2022-03-01T15:36:38Z*)\n\n\n\n### 2 related papers from Papers with Code\n\n#### 1. Data-Assisted Non-Intrusive Model Reduction for Forced Nonlinear Finite Elements Models\n\n*From Search Query: \"spectral SSM implementation\"*\n\n*Anonymous*\n\n**Abstract:** Spectral submanifolds (SSMs) have emerged as accurate and predictive model reduction tools for dynamical systems defined either by equations or data sets. While finite-elements (FE) models belong to the equation-based class of problems, their implementations in commercial solvers do not generally provide information on the nonlinearities required for the analytical construction of SSMs. Here, we overcome this limitation by developing a data-driven construction of SSM-reduced models from a small number of unforced FE simulations. We then use these models to predict the forced response of the FE model without performing any costly forced simulation. This approach yields accurate forced response predictions even in the presence of internal resonances or quasi-periodic forcing, as we illustrate on several FE models. Our examples range from simple structures, such as beams and shells, to more complex geometries, such as a micro-resonator model containing more than a million degrees of freedom. In the latter case, our algorithm predicts accurate forced response curves in a small fraction of the time it takes to verify just a few points on those curves by simulating the full forced-response.\n\n**Published:** 2023-11-29\n\n\n\n#### 2. Using Spectral Submanifolds for Optimal Mode Selection in Model Reduction\n\n*From Search Query: \"spectral SSM implementation\"*\n\n*George Haller, Shobhit Jain, Gergely Buza*\n\n**Abstract:** Model reduction of large nonlinear systems often involves the projection of the governing equations onto linear subspaces spanned by carefully-selected modes. The criteria to select the modes relevant for reduction are usually problem-specific and heuristic. In this work, we propose a rigorous mode-selection criterion based on the recent theory of Spectral Submanifolds (SSM), which facilitates a reliable projection of the governing nonlinear equations onto modal subspaces. SSMs are exact invariant manifolds in the phase space that act as nonlinear continuations of linear normal modes. Our criterion identifies critical linear normal modes whose associated SSMs have locally the largest curvature. These modes should then be included in any projection-based model reduction as they are the most sensitive to nonlinearities. To make this mode selection automatic, we develop explicit formulas for the scalar curvature of an SSM and provide an open-source numerical implementation of our mode-selection procedure. We illustrate the power of this procedure by accurately reproducing the forced-response curves on three examples of varying complexity, including high-dimensional finite element models.\n\n**Published:** 2020-09-09\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, particularly focusing on the integration of spectral State Space Models (SSMs) with normalization layers, stability analysis, and efficient implementations, here are some key points and references that align with the given details:\n\n## Practical Implementations of Spectral SSMs in Normalization Contexts\n\n- The paper on \"SPikE-SSM\" provides insights into the implementation of spectral SSMs, particularly in the context of spiking neural networks. It discusses how the original SSM block can be modified to incorporate a refractory neuron model, which can be relevant for long sequence modeling and potentially for normalization stability. The SPikE-SSM block includes a forward computation graph and utilizes trainable parameters to regulate the neuron\u2019s spiking rate, which could be adapted for normalization contexts.\n\n## Stability Analysis and Theoretical Guarantees for Adaptive Normalization\n\n- While the provided sources do not directly address stability analysis of adaptive normalization in the context of SSMs, the general concept of spectral SSMs offers provable robustness properties. These properties, as mentioned by Agarwal et al., suggest that spectral SSMs can provide stable foundations for adaptive normalization methods. This stability is crucial for maintaining the performance of the model under varying data distributions.\n\n## Memory-Efficient Implementations and Optimization Techniques\n\n- The \"SPikE-SSM\" paper also touches on the efficiency of the model by using a discretized version of the continuous SSM formulation, which can be more memory-efficient. Additionally, the use of a Conv1D operation followed by a Gated Linear Unit (GLU) activation function helps in optimizing the computational efficiency of the model.\n\n## Integration Methods for Combining Different Normalization Approaches\n\n- For integrating SSMs with normalization, one could consider the approach used in the \"DenseMamba\" model, which combines state space models with dense hidden connections. This integration can be adapted to include normalization layers, potentially using the spectral properties of SSMs to stabilize the normalization process. The idea is to leverage the robustness of SSMs to enhance the stability and performance of adaptive normalization methods.\n\n## Hardware-Aware Design Considerations for Efficient Normalization\n\n- While the provided sources do not explicitly discuss hardware-aware design considerations, the efficiency and optimization techniques mentioned (such as using Conv1D and GLU) are generally hardware-friendly. For more specific hardware optimization, focusing on parallelizable operations and leveraging GPU architectures can be beneficial. The \"DenseMamba\" model, for instance, is designed for efficient large language models, which implies considerations for hardware efficiency.\n\n### Next Steps\n\n- **Implementation Details**: Continue to explore papers and implementations that specifically integrate spectral SSMs with normalization layers, such as the SPikE-SSM model.\n- **Hardware Optimization**: Look for studies or papers that discuss optimizing SSMs and normalization layers for specific hardware architectures, such as GPUs or TPUs.\n- **Integration Methods**: Investigate how different normalization approaches can be combined with SSMs, potentially using the robustness properties of SSMs to enhance stability.\n\nBy focusing on these areas, the researchers can develop a more robust, efficient, and scalable autoregressive language model that leverages the strengths of spectral SSMs and adaptive normalization.\n",
            "analysis": "Summary of Analysis:\nThe initial search results provide valuable insights into spectral state space models (SSMs), adaptive normalization, and selective compression techniques. However, more specific information is needed about their integration in normalization layers and practical implementations.\n\nKey Findings:\n\n1. Spectral State Space Models:\n- Offer provable robustness properties independent of underlying dynamics\n- Use fixed convolutional filters that don't require learning\n- Show promise in long-range memory tasks\nFrom Agarwal et al. (2023): \"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n\n2. Adaptive Normalization:\n- Can handle non-stationary data distributions\n- Improves model performance through dynamic parameter adjustment\n- Requires careful consideration of stability\nFrom Liu et al. (2023): \"SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice rather than a global instance.\"\n\n3. Selective Compression:\n- Enables efficient variable-rate processing\n- Can maintain performance while reducing computational costs\n- Benefits from importance-based selection\nFrom Lee et al. (2022): \"The proposed method adaptively determines essential representation elements for compression of different target quality levels.\"\n\nKey Insights for Design:\n1. Integration Strategy:\n- Spectral SSMs could provide stable foundations for adaptive normalization\n- Block-based processing may help optimize memory usage\n- Selective compression could be guided by spectral properties\n\n2. Implementation Considerations:\n- Need to balance adaptivity with computational efficiency\n- Important to maintain stability in normalization operations\n- Should leverage theoretical guarantees from spectral methods\n\nFuture Search Needed:\n1. More specific information about:\n- Practical implementations of spectral SSMs in normalization layers\n- Stability analysis of adaptive normalization techniques\n- Memory-efficient implementations of selective compression\n\n2. Details on:\n- Integration methods for combining SSMs with normalization\n- Hardware optimization techniques for efficient implementation\n- Empirical comparisons of different normalization approaches\n\nReferences:\n1. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv:2312.06837.\n2. Liu, Z., Cheng, M., Li, Z., Huang, Z., Liu, Q., Xie, Y., & Chen, E. (2023). Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective. Neural Information Processing Systems.\n3. Lee, J., Jeong, S., & Kim, M. (2022). Selective compression learning of latent representations for variable-rate image compression. Neural Information Processing Systems.\n4. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv:2401.04722.\n\nNext Search Plan:\n1. Focus on implementation details:\nKeywords: \"spectral SSM implementation\", \"normalization stability analysis\"\n2. Look for hardware optimization:\nKeywords: \"efficient normalization computation\", \"memory-optimized SSM\"\n3. Investigate integration methods:\nKeywords: \"combining SSM normalization\", \"adaptive state compression\""
        },
        {
            "ready": false,
            "query": "\"hardware efficient SSM\", \"adaptive normalization implementation\", \"SSM normalization comparison\"",
            "detail": "Find papers discussing:\n1. Hardware-efficient implementations of SSMs, particularly in the context of normalization\n2. Practical implementations combining SSMs with adaptive normalization techniques\n3. Empirical comparisons of different normalization approaches in SSM architectures\n4. Memory optimization strategies for SSM-based models\n5. Stability analysis of combined SSM-normalization architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hardware-efficient implementations of SSMs, particularly in the context of normalization\n2. Practical implementations combining SSMs with adaptive normalization techniques\n3. Empirical comparisons of different normalization approaches in SSM architectures\n4. Memory optimization strategies for SSM-based models\n5. Stability analysis of combined SSM-normalization architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.99)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.98)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.98)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.98)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 3. State Space Models are Comparable to Transformers in Estimating Functions with Dynamic Smoothness (Avg. Score: 0.94)\n\n*Naoki Nishikawa, Taiji Suzuki*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice and prove that SSMs can estimate such functions with the same convergence rate as Transformers.\n\n**Abstract:** Deep neural networks based on state space models (SSMs) are attracting much attention in sequence modeling since their computational cost is significantly smaller than that of Transformers. While the capabilities of SSMs have been primarily investigated through experimental comparisons, theoretical understanding of SSMs is still limited. In particular, there is a lack of statistical and quantitative evaluation of whether SSM can replace Transformers. In this paper, we theoretically explore in which tasks SSMs can be alternatives of Transformers from the perspective of estimating sequence-to-sequence functions. We consider the setting where the target function has direction-dependent smoothness and prove that SSMs can estimate such functions with the same convergence rate as Transformers. Additionally, we prove that SSMs can estimate the target function, even if the smoothness changes depending on the input sequence, as well as Transformers. Our results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice.\n\n##### *Relevant Chunk: No. 2/38 (Score: 0.94)*\n\n```\nWhile the capabilities of SSMs have been primarily investigated through experimental comparisons, theoretical understanding of SSMs is still limited. In particular, there is a lack of statistical and quantitative evaluation of whether SSM can replace Transformers. In this paper, we theoretically explore in which tasks SSMs can be alternatives of Transformers from the perspective of estimating sequence-to-sequence functions. We consider the setting where the target function has direction-dependent smoothness and prove that SSMs can estimate such functions with the same convergence rate as Transformers. Additionally, we prove that SSMs can estimate the target function, even if the smoothness changes depending on the input sequence, as well as Transformers. Our results show the possibility that SSMs can replace Transformers when estimating the functions in certain classes that appear in practice. ## 1 Introduction\n\nFoundation models based on Transformers have achieved remarkable success in various sequence modeling tasks such as natural language processing (Vaswani et al., 2017), computer vision (Dosovitskiy et al., 2020), and speech recognition (Radford et al., 2023). The superior performance of Transformers is attributed to the self-attention mechanism, which enables the model to aggregate the information from the input sequence. In contrast to its success, self-attention mechanism has a potential problem that it requires a large amount of computation and memory. To deal with this issue, many studies have been attempted to develop efficient models that can replace Transformers. Among them, Structured State Space Models (SSMs) have garnered considerable interest recently. One advantage of SSMs is that the output can be computed with a significantly small time using convolution via FFT algorithm or recursive computation. Based on the original SSMs, many improvements have been proposed, such as HiPPO-based intialization (Gu et al. 2021) and architectures using gated convolutions (Fu et al. 2022, Poli et al., 2023). Networks based on SSMs have accomplished high performance in various applications such as gene analysis (Nguyen et al., 2024), audio generation (Goel et al., 2022) and speech recognition (Saon et al. 2023). On the other hand, some of the recent studies pointed out the limitations of SSMs, especially for their abilities to solve tasks. For example, Merrill et al. (2024) show that SSMs cannot solve sequential problems from the view of computational complexity theory. Additionally, Gu and Dao (2023) pointed out that SSMs are less effective for the tasks to handle discrete and information-dense data such as language processing. Therefore, it is still unclear in what situation we can replace Transformers with SSMs. ![](https://cdn.mathpix.com/cropped/2024_09_12_5e098f2beaae0c90c478g-02.jpg?height=258&width=1373&top_left_y=245&top_left_x=368)\n\nFigure 1.1: Conceptual illustrations of the abilities of SSMs. Left: This represents the classification task in which the existence of the token \" $A$ \" or \" $B$ \" is important. We prove that SSMs can extract such tokens, even if the position of the important token is different for each input. Right: This illustrate the task to predict the next token in the sequence. In this task, models have to output the token associated with the last token. We also prove that SSMs can solve such tasks, i.e., SSMs can extract appropriate tokens even if the position of essential tokens are different for each token in the same sequence. Recently, some studies have theoretically investigated the abilities of SSMs. For instance, Wang and Xue (2024) show that SSMs are universal approximators for continuous sequence-to-sequence functions. Additionally, Massaroli et al. (2024) constructed the parameters of SSMs to solve the task called associated recall. Moreover, Cirone et al. (2024) studied the abilities of SSMs using rough path theory. However, they mainly focus on the expressive power of SSMs and do not provide statistical understanding. Furthermore, quantitative evaluations to compare SSMs and Transformers is limited. Leaving aside SSMs, many studies have investigated the abilities of deep neural networks to estimate functions. Some of them analyze the estimation abilities of fully connected neural networks (FNNs) with the assumption that the target function is in certain function classes (Schmidt-Hieber, 2020, Suzuki, 2018) or have a specific smoothness structure (Suzuki and Nitanda, 2021). Moreover, Nakada and Imaizumi (2020) and Chen et al. (2022) consider the setting that the data distribution has a low-dimensional structure. Additionally, Okumoto and Suzuki (2021) studied convolutional neural networks (CNNs) and showed that CNNs can estimate the functions that have smoothness structures with the minimax optimal rate even if the input is infinite-dimensional. As for the Transformers, Takakura and Suzuki (2023) showed that Transformers can estimate the functions with infinitedimensional input as well as CNNs. Additionally, they showed that Transformers can estimate the functions whose smoothness structure changes depending on the input. Our contributions. In this paper, we explore the abilities of SSMs with gated convolution to replace Transformers from the perspective of statistical learning theory. More specifically, we investigate the estimation ability of SSMs for the function classes called $\\gamma$-smooth and piecewise $\\gamma$-smooth. For the function in these classes, Takakura and Suzuki (2023) showed that Transformers can estimate them effectively. We prove that SSMs can also estimate those functions with the same convergence rate as Transformers, and show that SSMs can replace Transformers when estimating those functions. The essential point of the two function classes above is that they have smoothness structures. As for $\\gamma$-smooth functions, the smoothness of the function is the same for all input sequences, i.e., the important features to extract are fixed. On the other hand, piecewise $\\gamma$-smooth functions have different smoothness depending on the input. This function class characterizes the ability of Transformers and SSMs to extract important features dynamically. In addition to the settings considered in Takakura and Suzuki (2023), we also consider the functions whose smoothness structure also changes depending on the position output token. This setting is inspired by the ability of Transformers to solve the task called associative recall (Ba et al. 2016). We show that SSMs can also replace Transformers in this setting. See Figure 1.1 for the conceptual illustrations of those function classes. The contributions of this paper are summarized as follows:\n\n1. We theoretically investigate the estimation ability of SSMs with gated convolutions for $\\gamma$ smooth functions and piecewise $\\gamma$-smooth functions. We show that SSMs can achieve the same estimation error as Transformers, which implies that SSMs may be alternative to Transformers in terms of estimating functions in those classes. 2. Inspired by recent research on the abilities of Transformers, we define function classes that are extensions of the piecewise $\\gamma$-smooth function class. Then, we prove that SSMs can also attain the similar estimation error for those classes. Other related works. The function classes with piecewise smoothness are also considered in Petersen and Voigtlaender (2018) and Imaizumi and Fukumizu (2019). They do not consider anisotropic smoothness or the sequence-to-sequence functions, while we consider such situations. One of the other directions to investigate the abilities of SSMs is to utilize the control theory, like Alonso et al. (2024). Instead of focusing on the statistical aspect, they mainly provide a comprehensive understanding of existing SSMs. Gu and DaO (2023) proposed an SSM-based architecture called Mamba, whose filter is controlled by the input. While convolution with FFT algorithms cannot be used for Mamba, they proposed a hardware-aware efficient implementation. In this paper, we do not focus on the setting that filters are controlled by the input, and we consider SSMs with gated convolution with data-independent filters.\n```\n\n#### 4. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.88)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 1/27 (Score: 0.88)*\n\n```\n# State Space Models as Foundation Models: A Control Theoretic Overview \n\nCarmen Amo Alonso*, Jerome Sieber*, and Melanie N. Zeilinger\n\n\n#### Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences. Index Terms-Machine learning, Linear systems, Timevarying systems. ## I. INTRODUCTION\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models' capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window ${ }^{1}$ and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the\n\n[^0]attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inferencein contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section $\\Pi$, we provide an overview of the essential components and considerations in SSMs. In Section III we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. ## II. State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. ## A. Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\n$$\ny(k)=f(u(k), \\ldots, u(k-T) ; \\theta)\n$$\n\nwhere at each time $k$, the output $y(k)$ is produced after evaluating an input signal of length $k-T$, i.e., $u(k), \\ldots, u(k-$ $T)$, and a set of parameters $\\theta$. The parameters $\\theta$ are task dependent, and can be fine-tuned accordingly. Since the search space of general $f(\\cdot ; \\theta)$ is too broad, different parameterizations of $f(\\cdot ; \\theta)$ can be used to render the problem tractable. For instance, the model $f(\\cdot ; \\theta)$ can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of $f(\\cdot ; \\theta)$ is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model $f(\\cdot ; \\theta)$ is presented with input-output pairs $(u(k), y(k)) \\forall k$, where $\\theta$ represents the parameters. The parameters $\\theta$ are then iteratively updated to minimize a loss function $\\mathscr{L}(\\cdot)$, i.e., iteratively solving the following optimization problem\n\n$$\n\\min _{\\theta} \\mathscr{L}(y-f(u ; \\theta))\n$$\n\nFor a language model the inputs $u$ are tokenized ${ }^{2}$ sentences and the outputs $y$ are a shifted version of the same inputs, i.e., an auto-regressive setup. ## B. Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\n$$\n\\begin{aligned}\n& \\dot{x}(t)=A x(t)+B u(t) \\\\\n& y(t)=C x(t)+D u(t)\n\\end{aligned}\n$$\n\nwhere $x \\in \\mathbb{C}^{p}$ represents the complex-valued state, $u, y \\in \\mathbb{R}^{q}$ are the input and the output, respectively, and $t$ denotes the continuous-time index. We note that the input fed into the system denoted as $u$ is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. $A, B, C, D$ are complex-valued matrices of appropriate dimensions and in representation (3), these matrices\n\n[^1]are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., $A_{t}, B_{t}, C_{t}, D_{t}$. In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input $u(t)$ represents a signal or input token fed into the model at a given time $t$. The state $x(t)$ represents the hidden state that stores the relevant information about the current and previous inputs up to time $t$, and $y(t)$ is the output of the model at time $t$. In a learning setup, the matrices $A, B, C, D$ are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix $A$ is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption 2.1: The dynamic matrix in dynamics (3) has a diagonal structure, i.e., $A=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)$ with $\\lambda_{i} \\in \\mathbb{C} \\forall i$. Although initial proposals [11], [12] deviate slightly from Assumption 2.1, most of the Structured SSMs literature assumes a diagonal $A$ matrix. Specific choices will be discussed in Section III\n\n## C. Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\n$$\n\\begin{aligned}\nx(k+1) & =\\bar{A} x(k)+\\bar{B} u(k) \\\\\ny(k) & =\\bar{C} x(k)+\\bar{D} u(k)\n\\end{aligned}\n$$\n\nwhere $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ are the discrete-time dynamic matrices discretized with time-step $\\Delta \\in \\mathbb{R}$, possibly with complexvalued components, and $k$ denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III\nWe note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. ## D. Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix $A$ have a significant impact on the performance after training: on a simple classification task, performance increases from $67 \\%$ when $A$ is randomly initialized, to $80 \\%$ when $A$ is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state $x(k)$ being able to capture the recent history of the inputs $u(k), \\ldots, u(k-T)$ for some time horizon $T$. This property is referred to as memory in the standard SSM literature. As is well-known in control\ntheory, the memory of system (4) is directly linked to the eigenvalues of matrix $A$. Lemma 2.2: (Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of $A$ are inside the unit circle and very close to the unit circumference, i.e. $|\\operatorname{eig}(A)| \\leq 1$ and $|\\operatorname{eig}(A)| \\approx 1 \\forall \\operatorname{eig}(A)$. Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned $A$ matrix is approximately equal to (but not bigger than) 1 . For the initialization of the other matrices, i.e., $B, C$, and $D$, standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of $B, C$, and $D$ in the following and refer the reader to the original papers [14], [15]. ## E. Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation $u(1)$ and zero initial state $x(1)=0$. In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a noncausal representation since input-output pairs $(u(k), y(k))$ are available for all $k$. Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\n$$\ny(k)=\\sum_{\\tau=0}^{k} \\bar{C} \\bar{A}^{k-\\tau} \\bar{B} u(\\tau)\n$$\n\nThis convolutional representation allows for faster learning because the complete input sequence $u(k) \\forall k$ can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. ## F. Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input $u$ and postprocessing of the output $y$ is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding\nA\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-3.jpg?height=776&width=874&top_left_y=211&top_left_x=1078)\n\nFig. 1: A. General scaffolding of a SSM. The dynamical model (4) is represented in green. The input to the SSM is pre-processed and forked off in a skip connection (lower signal). The nature of the pre-processing map (linear or nonlinear) depends on the specific scaffolding. The output of the recursion is then post-processed with a nonlinear gate. B. Overall architecture of a SSM. Each of the SSMs including its scaffolding (Fig. 1.A.) is structured in a layered fashion, where the output from one layer is the input to the next. the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition 2.3. In general, a linear or nonlinear map is performed on the input $\\bar{u}$ before it is fed into system (4). Once the output $y$ has been computed, a gating operation is generally performed to control the flow of information from the input $\\tilde{u}$ to the output $\\tilde{y}$. Intuitively, the gate $g(\\tilde{y}, \\tilde{u})$ controls which outputs $\\tilde{y}$ are set to zero based on the inputs $\\tilde{u}$ via the softmax operation. Definition 2.3: Given two vectors $x_{1}, x_{2} \\in \\mathbb{R}^{p}$, a gating operation is defined as $g\\left(x_{1}, x_{2}\\right):=x_{1} \\odot \\sigma\\left(W x_{2}\\right)$, where $W \\in \\mathbb{R}^{p \\times p}$, $\\odot$ is the element-wise multiplication, and $\\sigma$ is the softmax operation ${ }^{3}$\n\nAs is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input $y$ and output $u$ are of the same dimension $\\mathbb{R}^{q}$. For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matri-\n\n[^2]ces), and the size of the systems ranges in $p \\in[64,512], q \\in$ [32, 1024]. For language modelling the number of layers and system size can be significantly larger.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hardware efficient SSM\", \"adaptive normalization implementation\", \"SSM normalization comparison\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: \"hardware efficient SSM\"*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 2. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient SSM\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 3. Simple Hardware-Efficient PCFGs with Independent Left and Right Productions\n\n*From Search Query: \"hardware efficient SSM\"*\n\n*Wei Liu, Songlin Yang, Yoon Kim, Kewei Tu*\n\n**TL;DR:** This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions, and finds that this formalism scales more effectively both as a language model and as an unsupervised parser.\n\n**Abstract:** Scaling dense PCFGs to thousands of nonterminals via a low-rank parameterization of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions. Despite imposing a stronger independence assumption than the low-rank approach, we find that this formalism scales more effectively both as a language model and as an unsupervised parser. As an unsupervised parser, our simple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language model, it obtains a perplexity of 119.0, outperforming similarly-sized low-rank PCFGs. We further introduce \\emph{FlashInside}, a hardware IO-aware implementation of the inside algorithm for efficiently scaling simple PCFGs.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen*\n\n**TL;DR:** A novel slice-level adaptive normalization scheme, referred to SAN, is proposed, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization and could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data.\n\n**Abstract:** Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN , which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available 2 .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 7*)\n\n#### 5. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich*\n\n**TL;DR:** A gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes is presented, showing that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks.\n\n**Abstract:** Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 1092  (*Influential: 144*)\n\n#### 6. A Large-Scale Comparison of Historical Text Normalization Systems\n\n*From Search Query: \"SSM normalization comparison\"*\n\n*Marcel Bollmann*\n\n**TL;DR:** This paper presents the largest study of historical text normalization done so far, comparing systems spanning all categories of proposed normalization techniques, analysing the effect of training data quantity, and using different evaluation methods.\n\n**Abstract:** There is no consensus on the state-of-the-art approach to historical text normalization. Many techniques have been proposed, including rule-based methods, distance metrics, character-based statistical machine translation, and neural encoder\u2013decoder models, but studies have used different datasets, different evaluation methods, and have come to different conclusions. This paper presents the largest study of historical text normalization done so far. We critically survey the existing literature and report experiments on eight languages, comparing systems spanning all categories of proposed normalization techniques, analysing the effect of training data quantity, and using different evaluation methods. The datasets and scripts are made publicly available.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 72  (*Influential: 5*)\n\n#### 7. Global Normalization for Streaming Speech Recognition in a Modular Framework\n\n*From Search Query: \"SSM normalization comparison\"*\n\n*Ehsan Variani, Ke Wu, M. Riley, David Rybach, Matt Shannon, Cyril Allauzen*\n\n**TL;DR:** By switching to a globally normalized model, the word error rate gap between streaming and non-streaming speech-recognition models can be greatly reduced, and the label bias problem in streaming speech recognition is addressed.\n\n**Abstract:** We introduce the Globally Normalized Autoregressive Transducer (GNAT) for addressing the label bias problem in streaming speech recognition. Our solution admits a tractable exact computation of the denominator for the sequence-level normalization. Through theoretical and empirical results, we demonstrate that by switching to a globally normalized model, the word error rate gap between streaming and non-streaming speech-recognition models can be greatly reduced (by more than 50\\% on the Librispeech dataset). This model is developed in a modular framework which encompasses all the common neural speech recognition models. The modularity of this framework enables controlled comparison of modelling choices and creation of new models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 1*)\n\n### 6 related papers from ArXiv\n\n#### 1. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"hardware efficient SSM\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n#### 2. Parameter-Efficient Fine-Tuning of State Space Models\n\n*From Search Query: \"hardware efficient SSM\"*\n\n*Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, Kangwook Lee*\n\n**Abstract:** Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged\nas powerful tools for language modeling, offering high performance with\nefficient inference and linear scaling in sequence length. However, the\napplication of parameter-efficient fine-tuning (PEFT) methods to SSM-based\nmodels remains largely unexplored. This paper aims to systematically study two\nkey questions: (i) How do existing PEFT methods perform on SSM-based models?\n(ii) Which modules are most effective for fine-tuning? We conduct an empirical\nbenchmark of four basic PEFT methods on SSM-based models. Our findings reveal\nthat prompt-based methods (e.g., prefix-tuning) are no longer effective, an\nempirical result further supported by theoretical analysis. In contrast, LoRA\nremains effective for SSM-based models. We further investigate the optimal\napplication of LoRA within these models, demonstrating both theoretically and\nexperimentally that applying LoRA to linear projection matrices without\nmodifying SSM modules yields the best results, as LoRA is not effective at\ntuning SSM modules. To further improve performance, we introduce LoRA with\nSelective Dimension tuning (SDLoRA), which selectively updates certain channels\nand states on SSM modules while applying LoRA to linear projection matrices.\nExtensive experimental results show that this approach outperforms standard\nLoRA.\n\n**Published:** 2024-10-11T17:30:28Z  (*Updated: 2024-10-11T17:30:28Z*)\n\n\n\n#### 3. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 4. Differentiable Pooling for Unsupervised Acoustic Model Adaptation\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Pawel Swietojanski, Steve Renals*\n\n**Abstract:** We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches.\n\n**Published:** 2016-03-31T15:10:40Z  (*Updated: 2016-07-13T18:12:49Z*)\n\n\n\n#### 5. Never Train from Scratch: Fair Comparison of Long-Sequence Models\n  Requires Data-Driven Priors\n\n*From Search Query: \"SSM normalization comparison\"*\n\n*Ido Amos, Jonathan Berant, Ankit Gupta*\n\n**Abstract:** Modeling long-range dependencies across sequences is a longstanding goal in\nmachine learning and has led to architectures, such as state space models, that\ndramatically outperform Transformers on long sequences. However, these\nimpressive empirical gains have been by and large demonstrated on benchmarks\n(e.g. Long Range Arena), where models are randomly initialized and trained to\npredict a target label from an input sequence. In this work, we show that\nrandom initialization leads to gross overestimation of the differences between\narchitectures and that pretraining with standard denoising objectives, using\n$\\textit{only the downstream task data}$, leads to dramatic gains across\nmultiple architectures and to very small gaps between Transformers and state\nspace models (SSMs). In stark contrast to prior works, we find vanilla\nTransformers to match the performance of S4 on Long Range Arena when properly\npretrained, and we improve the best reported results of SSMs on the PathX-256\ntask by 20 absolute points. Subsequently, we analyze the utility of\npreviously-proposed structured parameterizations for SSMs and show they become\nmostly redundant in the presence of data-driven initialization obtained through\npretraining. Our work shows that, when evaluating different architectures on\nsupervised tasks, incorporation of data-driven priors via pretraining is\nessential for reliable performance estimation, and can be done efficiently.\n\n**Published:** 2023-10-04T17:17:06Z  (*Updated: 2024-04-28T13:52:55Z*)\n\n\n\n#### 6. An Empirical Study of Mamba-based Language Models\n\n*From Search Query: \"SSM normalization comparison\"*\n\n*Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro*\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the\nshortcomings of Transformers, such as quadratic computational complexity with\nsequence length and large inference-time memory requirements from the key-value\ncache. Moreover, recent studies have shown that SSMs can match or exceed the\nlanguage modeling capabilities of Transformers, making them an attractive\nalternative. In a controlled setting (e.g., same data), however, studies so far\nhave only presented small scale experiments comparing SSMs to Transformers. To\nunderstand the strengths and weaknesses of these architectures at larger\nscales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and\nTransformer models trained on the same datasets of up to 3.5T tokens. We also\ncompare these models to a hybrid architecture consisting of 43% Mamba-2, 7%\nattention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks,\nwe answer the question of whether Mamba models can match Transformers at larger\ntraining budgets. Our results show that while pure SSMs match or exceed\nTransformers on many tasks, they lag behind Transformers on tasks which require\nstrong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook)\nor long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid\nexceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points\non average) and is predicted to be up to 8x faster when generating tokens at\ninference time. To validate long-context capabilities, we provide additional\nexperiments evaluating variants of the Mamba-2-Hybrid and Transformer extended\nto support 16K, 32K, and 128K sequences. On an additional 23 long-context\ntasks, the hybrid model continues to closely match or exceed the Transformer on\naverage. To enable further study, we release the checkpoints as well as the\ncode used to train our models as part of NVIDIA's Megatron-LM project.\n\n**Published:** 2024-06-12T05:25:15Z  (*Updated: 2024-06-12T05:25:15Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: \"hardware efficient SSM\"*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Junho Kim, Kwanghee Lee, Hyeonwoo Kang, Minjae Kim*\n\n**Abstract:** We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-07-25\n\n\n\n#### 3. Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Serge Belongie, Xun Huang*\n\n**Abstract:** Gatys et al. recently introduced a neural algorithm that renders a content\nimage in the style of another image, achieving so-called style transfer.\nHowever, their framework requires a slow iterative optimization process, which\nlimits its practical application. Fast approximations with feed-forward neural\nnetworks have been proposed to speed up neural style transfer. Unfortunately,\nthe speed improvement comes at a cost: the network is usually tied to a fixed\nset of styles and cannot adapt to arbitrary new styles. In this paper, we\npresent a simple yet effective approach that for the first time enables\narbitrary style transfer in real-time. At the heart of our method is a novel\nadaptive instance normalization (AdaIN) layer that aligns the mean and variance\nof the content features with those of the style features. Our method achieves\nspeed comparable to the fastest existing approach, without the restriction to a\npre-defined set of styles. In addition, our approach allows flexible user\ncontrols such as content-style trade-off, style interpolation, color & spatial\ncontrols, all using a single feed-forward neural network.\n\n**Conference:** arbitrary-style-transfer-in-real-time-with-1\n\n**Published:** 2017-03-20\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using State Space Models (SSMs) and incorporating hardware-efficient implementations, adaptive normalization, and other optimization strategies, here are some key points and references from the provided sources:\n\n## Hardware-Efficient Implementations of SSMs\n\n- The paper on \"LC-PLM: LONG-CONTEXT PROTEIN LANGUAGE MODEL\" discusses how SSMs can be made more efficient through linearization of RNN dynamics and the use of hardware-aware algorithms. For example, Mamba and Mamba-2 models leverage structured matrix multiplications to better utilize modern Tensor cores, which is crucial for efficient processing of long sequences.\n- The work on \"LET SSMS BE CONVNETS: STATE-SPACE MODELING\" highlights that SSM layers can be configured for efficient online inference with small memory and computational requirements. It also suggests using Fast Fourier Transforms (FFTs) to perform convolutions efficiently, which is a hardware-efficient approach.\n\n## Practical Implementations Combining SSMs with Adaptive Normalization Techniques\n\n- While the provided sources do not explicitly discuss the combination of SSMs with adaptive normalization techniques, the concept of integrating normalization layers into SSM architectures can be inferred. For instance, the \"LC-PLM\" paper mentions the use of gated mechanisms and residual connections, which could be adapted to include normalization layers. However, specific implementations combining SSMs with adaptive normalization are not detailed in these sources.\n- The analysis note suggests that spectral SSMs could provide a stable foundation for adaptive normalization, but this would require further research to implement practically.\n\n## Empirical Comparisons of Different Normalization Approaches in SSM Architectures\n\n- There is a lack of direct empirical comparisons of different normalization approaches within the context of SSM architectures in the provided sources. However, the general importance of proper initialization and parameterization for stability is emphasized, which could be a starting point for such comparisons.\n\n## Memory Optimization Strategies for SSM-Based Models\n\n- The \"LET SSMS BE CONVNETS\" paper discusses optimizing memory usage by expressing SSM blocks as tensor networks and dynamically optimizing the contraction order based on the input features and system matrices. This approach can lead to significant speedups during training and inference.\n- The \"LC-PLM\" paper mentions the use of shared linear projection layers to make the model deeper with almost the same parameter count, which is a memory-efficient design choice.\n\n## Stability Analysis of Combined SSM-Normalization Architectures\n\n- The analysis note highlights the importance of proper initialization and parameterization for stability in normalization layers. It also mentions the trade-offs between adaptivity and stability, which would be crucial in analyzing the stability of combined SSM-normalization architectures. However, specific stability analyses are not provided in the sources.\n\n### Additional Insights\n\n- **Linear Attention and Memory Efficiency**: The \"TABFLEX\" paper discusses how linear attention can reduce computational complexity from quadratic to linear time, which is beneficial for SSMs. This approach can be interpreted as a form of RNN and is efficient in terms of memory access and usage.\n- **State Space Duality**: The concept of state space duality, as mentioned in the analysis note, allows for efficient algorithms that take advantage of both the linear SSM recurrence and quadratic dual form, which can be crucial for optimizing SSMs in hardware-efficient ways.\n\nTo further achieve the goal of improving autoregressive language model design, additional research should focus on:\n\n1. **Practical Implementations**: Investigate how to practically combine SSMs with adaptive normalization techniques.\n2. **Empirical Studies**: Conduct empirical studies to compare different normalization approaches within SSM architectures.\n3. **Hardware-Specific Optimizations**: Explore hardware-specific optimizations for SSM-based models, including memory and computational efficiency.\n4. **Stability Analysis**: Perform detailed stability analyses for combined SSM-normalization architectures to ensure robust performance.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about spectral SSMs, normalization techniques, and their potential integration:\n\na) Spectral SSMs offer unique advantages:\n- Provable robustness properties independent of spectrum and dimensionality\n- Fixed convolutional filters that don't require learning\n- Efficient implementation possibilities through state space duality\n\nb) Normalization stability considerations:\n- Importance of proper initialization and parameterization\n- Trade-offs between adaptivity and stability\n- Need for hardware-efficient implementations\n\n2. Useful References with Excerpts:\n\nFrom \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning.\"\n\nFrom \"Transformers are SSMs\":\n\"Our framework exposes new efficient and easily-implementable algorithms for computing SSMs... that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes.\"\n\nFrom \"Understanding the Generalization Benefit of Normalization Layers\":\n\"Our theory explains how GD with a finite learning rate enters the so-called Edge of Stability (EoS) regime, and characterizes the trajectory of GD in this regime via a continuous sharpness-reduction flow.\"\n\n3. Key Insights:\n\na) Integration Strategy:\n- Spectral SSMs could provide stable foundations for adaptive normalization\n- Block-based processing may help optimize memory usage\n- Need to balance adaptivity with computational efficiency\n\nb) Implementation Considerations:\n- Hardware-efficient algorithms through state space duality\n- Importance of proper initialization and stability analysis\n- Trade-offs between memory usage and computational speed\n\nc) Research Gaps:\n- Limited information on practical implementations combining SSMs with normalization\n- Need more details on hardware-specific optimizations\n- Lack of empirical comparisons in normalization contexts\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\n1. Hardware-efficient implementations of SSMs and normalization\n2. Empirical studies comparing different normalization approaches with SSMs\n3. Specific architectures combining state space models with adaptive normalization\n\n5. References:\n\n[1] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint arXiv:2312.06837.\n\n[2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n[3] Lyu, K., Li, Z., & Arora, S. (2022). Understanding the Generalization Benefit of Normalization Layers: Sharpness Reduction. Neural Information Processing Systems.\n\n[4] Zhang, B., Titov, I., & Sennrich, R. (2021). Sparse Attention with Linear Units. Proceedings of EMNLP, 10155\u201310168.\n\n[5] Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., & Wei, F. (2022). DeepNet: Scaling Transformers to 1,000 Layers. arXiv preprint."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.835209,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.045401,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "RMSNorm",
    "proposal": "\n# SpectralRMSNorm: Enhancing RMSNorm with Spectral State Space Models for Adaptive Normalization\n\n## Motivation\n\nCurrent language models face several key challenges in processing long sequences:\n\n1. Fixed normalization parameters limit adaptability to varying input complexities\n2. Inefficient memory usage in processing long sequences\n3. Poor hardware utilization in normalization computations\n4. Training instability in deep networks\n\nWhile RMSNorm provides efficient normalization, it lacks mechanisms to adapt to input context and maintain long-range dependencies. Recent advances in state space models (SSMs), particularly spectral SSMs, offer promising approaches for enhancing normalization through adaptive, input-dependent processing.\n\n## Problem Analysis\n\n### Current RMSNorm Limitations\n\n1. **Fixed Parameters**:\n   - Uses static scaling parameters\n   - Cannot adapt to input complexity\n   - Limited context awareness\n\n2. **Memory Management**:\n   - High memory requirements for long sequences\n   - Inefficient cache utilization\n   - Poor hardware utilization\n\n3. **Training Stability**:\n   - Gradient flow issues in deep networks\n   - Inconsistent optimization objectives\n   - Limited adaptability to input variations\n\n### Opportunities from SSMs\n\nRecent work shows that spectral state space models offer:\n- Provable robustness properties\n- Independence from spectrum of underlying dynamics\n- Efficient hardware implementation possibilities\n\n## Core Idea and Philosophy\n\nSpectralRMSNorm introduces three key innovations:\n\n1. **Spectral State Integration**:\n   - Input-dependent normalization parameters\n   - Adaptive computation based on spectral properties\n   - Efficient parameter updates\n\n2. **Block-Based Processing**:\n   - Hardware-efficient block decomposition\n   - Optimized memory access patterns\n   - Reduced memory footprint\n\n3. **Selective State Compression**:\n   - Dynamic state compression based on input importance\n   - Efficient memory management\n   - Information-preserving compression\n\n## Design Plan\n\n### Architecture Overview\n\n```python\nclass SpectralRMSNorm(nn.Module):\n    def __init__(self, embed_dim, block_size=256, state_dim=64):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.block_size = block_size\n        self.state_dim = state_dim\n        \n        # Base normalization parameters\n        self.weight = nn.Parameter(torch.ones(embed_dim))\n        \n        # Spectral SSM parameters\n        self.A = nn.Parameter(torch.randn(state_dim, state_dim))\n        self.B = nn.Parameter(torch.randn(state_dim, 1))\n        self.C = nn.Parameter(torch.randn(1, state_dim))\n        \n        # State compression\n        self.compress = nn.Linear(embed_dim, state_dim)\n        self.expand = nn.Linear(state_dim, embed_dim)\n        \n    def forward(self, x):\n        B, L, D = x.shape\n        \n        # Block processing\n        blocks = x.view(B, -1, self.block_size, D)\n        \n        # Compute RMS statistics per block\n        variance = blocks.pow(2).mean(dim=-1, keepdim=True)\n        \n        # Compress state\n        state = self.compress(blocks)\n        \n        # Update state through SSM\n        for i in range(state.size(2)):\n            state_out = torch.tanh(\n                torch.matmul(state, self.A.T) + \n                torch.matmul(variance, self.B.T)\n            )\n            state = state_out\n        \n        # Generate adaptive scaling\n        scaling = self.expand(state)\n        \n        # Apply normalization with adaptive scaling\n        x_norm = blocks * torch.rsqrt(variance + 1e-5)\n        output = x_norm * (self.weight + scaling)\n        \n        return output.view(B, L, D)\n```\n\n### Key Components\n\n1. **Spectral State Processing**:\n```python\ndef update_state(self, state, variance):\n    \"\"\"Update state using spectral SSM dynamics\"\"\"\n    state_out = torch.tanh(\n        torch.matmul(state, self.A.T) + \n        torch.matmul(variance, self.B.T)\n    )\n    return state_out\n```\n\n2. **Block-Based Processing**:\n```python\ndef process_blocks(self, x):\n    \"\"\"Process input in hardware-efficient blocks\"\"\"\n    blocks = x.view(-1, self.block_size, self.embed_dim)\n    variance = blocks.pow(2).mean(dim=-1, keepdim=True)\n    return blocks, variance\n```\n\n3. **Selective State Compression**:\n```python\ndef compress_state(self, x):\n    \"\"\"Compress state based on input importance\"\"\"\n    importance = torch.sigmoid(self.compress(x))\n    compressed = x * importance\n    return compressed\n```\n\n### Mathematical Formulation\n\n1. **Spectral State Update**:\n   \\[\n   h_{t+1} = \\tanh(A h_t + B v_t)\n   \\]\n   where:\n   - \\(h_t\\) is the state at time t\n   - \\(v_t\\) is the input variance\n   - \\(A, B\\) are learnable parameters\n\n2. **Adaptive Normalization**:\n   \\[\n   y = \\frac{x}{\\sqrt{\\mathbb{E}[x^2] + \\epsilon}} \\cdot (w + C h_t)\n   \\]\n   where:\n   - \\(w\\) is the base scaling parameter\n   - \\(C h_t\\) is the adaptive scaling from the state\n\n3. **State Compression**:\n   \\[\n   h_{\\text{compressed}} = \\sigma(W_c x) \\odot h\n   \\]\n   where:\n   - \\(W_c\\) is the compression matrix\n   - \\(\\sigma\\) is the sigmoid function\n\n### Implementation Guidelines\n\n1. **Initialization**:\n```python\ndef initialize_parameters(self):\n    \"\"\"Initialize SSM parameters for stability\"\"\"\n    nn.init.orthogonal_(self.A)\n    nn.init.zeros_(self.B)\n    nn.init.zeros_(self.C)\n    nn.init.ones_(self.weight)\n```\n\n2. **Memory Management**:\n```python\ndef optimize_memory(self):\n    \"\"\"Optimize memory usage through block processing\"\"\"\n    torch.cuda.empty_cache()\n    return self.use_checkpointing()\n```\n\n3. **Hardware Optimization**:\n```python\ndef hardware_optimize(self):\n    \"\"\"Optimize for hardware efficiency\"\"\"\n    with torch.cuda.amp.autocast():\n        self.use_fused_kernels()\n```\n\n## Theoretical Analysis\n\n1. **Complexity Analysis**:\n   - Time Complexity: O(N) for sequence length N\n   - Space Complexity: O(B) for block size B\n   - Memory Access: O(1) per block\n\n2. **Stability Analysis**:\n   - Bounded gradients through spectral constraints\n   - Stable variance through epsilon term\n   - Consistent optimization through block processing\n\n3. **Convergence Properties**:\n   - Faster convergence through adaptive normalization\n   - Improved gradient flow through spectral dynamics\n   - Better stability through block-based processing\n\n## Advantages\n\n1. **Improved Adaptability**:\n   - Input-dependent normalization\n   - Dynamic parameter adjustment\n   - Better handling of varying complexities\n\n2. **Enhanced Efficiency**:\n   - Optimized memory usage\n   - Efficient computation\n   - Better hardware utilization\n\n3. **Training Stability**:\n   - Stable gradient flow\n   - Consistent optimization\n   - Improved convergence\n\n## References\n\n1. Wang, S., & Xue, B. (2023). State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Neural Information Processing Systems.\n\n2. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n3. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n4. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n5. Zhang, B., & Sennrich, R. (2019). Root Mean Square Layer Normalization. Neural Information Processing Systems.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel normalization layer that integrates spectral state space models with RMSNorm to enable adaptive, input-dependent normalization while maintaining computational efficiency and stability. The design leverages hardware-aware block processing and selective state compression to enhance performance on long sequences.",
    "ideation": null,
    "modelname": "spectralstatetransformer",
    "suggestions": "1. Implementation Strategy:\n- Provide more detailed hardware optimization guidelines\n- Include specific memory access patterns\n- Add benchmarking methodology\n- Consider different hardware architectures\n\n2. Training Stability:\n- Elaborate on gradient stabilization mechanisms\n- Include theoretical bounds on parameter updates\n- Add regularization techniques\n- Consider adaptive learning rate strategies\n\n3. Memory Management:\n- Detail memory allocation strategies\n- Include cache optimization techniques\n- Consider different memory hierarchies\n- Add memory bandwidth analysis\n\n4. Parameter Updates:\n- Specify update frequency\n- Include parameter initialization strategies\n- Consider different update schemes\n- Add stability analysis\n\n5. Hardware Optimization:\n- Provide specific hardware-aware optimizations\n- Include CUDA kernel implementations\n- Consider different accelerator types\n- Add resource utilization analysis\n\n6. Empirical Validation:\n- Define specific benchmarks\n- Include comparison methodology\n- Add ablation studies\n- Consider different model sizes\n\n7. Integration Guidelines:\n- Provide detailed integration steps\n- Include compatibility considerations\n- Add testing strategies\n- Consider different frameworks",
    "user_input": ""
}