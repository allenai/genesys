{
    "variantname": "AdaptiveRMSNorm",
    "review": "The AdaptiveNormTransformer proposal presents an innovative approach to enhancing RMSNorm through dynamic normalization and hardware-efficient implementation. After comprehensive analysis of search results and comparison with existing work, here is the detailed review:\n\n1. CLARITY AND OBJECTIVES (4.5/5)\nThe proposal is exceptionally well-structured with:\n- Clear articulation of objectives and motivation\n- Detailed mathematical formulations\n- Well-defined implementation guidelines\n- Comprehensive theoretical analysis\nThe only minor weakness is in the explanation of some hardware optimization strategies.\n\n2. INNOVATION AND NOVELTY (4.2/5)\nThe proposal demonstrates significant innovation in:\n+ Novel integration of content-dependent normalization parameters\n+ Hardware-efficient implementation strategies\n+ Dynamic parameter adaptation mechanisms\nHowever:\n- Some concepts build on existing adaptive normalization techniques\n- Hardware optimization strategies share similarities with existing approaches\n\n3. FEASIBILITY (4.0/5)\nImplementation appears feasible within the framework:\n+ Clear mathematical formulations provided\n+ Detailed implementation guidelines\n+ Consideration of hardware constraints\nConcerns:\n- Complex integration of dynamic parameters\n- Potential training stability challenges\n- Need for careful hardware optimization\n\n4. SCALABILITY (4.3/5)\nThe design shows strong potential for scalability:\n+ Efficient memory access patterns\n+ Linear computational complexity\n+ Hardware-aware implementation\nMinor concerns:\n- Memory requirements for dynamic parameters\n- Potential overhead in parameter updates\n\n5. ACCURACY AND ROBUSTNESS (4.2/5)\nExpected improvements in model performance:\n+ Better handling of varying input complexities\n+ Improved feature normalization\n+ Enhanced stability through adaptive mechanisms\nConcerns:\n- Need for empirical validation\n- Potential impact of dynamic parameters on stability\n- Trade-off between adaptivity and consistency\n\n6. EFFICIENCY (4.4/5)\nStrong focus on computational efficiency:\n+ Optimized memory access patterns\n+ Efficient computation scheduling\n+ Hardware-aware implementation\n+ Reduced memory footprint through optimization\n\nCOMPARATIVE ANALYSIS:\nThe proposal differs significantly from existing approaches:\n- More hardware-efficient than traditional adaptive normalization\n- Novel integration of content-dependent parameters\n- Unique focus on memory access optimization\n- Distinct from sibling proposals (h2_hierarchicallineartransformer, compressivelineartransformer)\n\nTECHNICAL DEPTH:\nThe proposal demonstrates strong technical understanding:\n+ Comprehensive mathematical formulations\n+ Detailed implementation guidelines\n+ Thorough consideration of hardware aspects\n+ Clear analysis of trade-offs",
    "search_stack": [
        {
            "ready": false,
            "query": "dynamic routing neural networks, memory compression techniques, hybrid attention mechanisms",
            "detail": "Show me papers discussing dynamic routing mechanisms in neural networks, particularly those that could be applied to transformer architectures. Include information about how routing decisions are made and how they affect computational efficiency.",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nShow me papers discussing dynamic routing mechanisms in neural networks, particularly those that could be applied to transformer architectures. Include information about how routing decisions are made and how they affect computational efficiency.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization  (Avg. Score: 0.82)\n\n*R. Csord\u00e1s, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 42  (*Influential: 7*)\n\n**TL;DR:** This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.\n\n**Abstract:** Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.\n\n##### *Relevant Chunk: No. 2/47 (Score: 0.90)*\n\n```\nThe situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100\\% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public $1^{1}$\n\n\n## 1 INTRODUCTION\n\nNeural networks (NNs) may easily learn certain training sets, but typically they do not generalize on systematically different test sets. Examples of systematic generalization (Fodor et al, 1988)\ninclude generalization to sequences longer than those seen during training-productivity, and algorithmic combinations of previously learned rules-systematicity. Despite recent efforts (Bahdanau et al., 2019, Korrel et al. 2019, Lake, 2019, Li et al., 2019, Russin et al., 2019, Csord\u00e1s et al., 2021), systematic generalization generally remains unsolved (Fodor \\& McLaughlin, 1990, Lake \\& Baroni, 2018, Liska et al., 2018, Greff et al., 2020, Hupkes et al., 2020). On some datasets, the best performing models are neuro-symbolic hybrids (Chen et al.| 2020, Liu et al. 2020) using task-specific symbolic functions. However, their applicability to other datasets remains limited (Furrer et al. 2020, Shaw et al., 2020). A big question is: which type of architectural inductive bias encourages the training process to select \"good\" solutions which generalize systematically? The popular Transformers (Vaswani et al. 2017) also often fail to generalize on algorithmic tasks (e.g. Liska et al. (2018); Dubois et al. (2020); Chaabouni et al. (2021); Csord\u00e1s et al. (2021); Onta\u00f1\u00f3n et al.(2021)), even on tasks with intuitive solutions that can be simply expressed in terms of Transformer attention patterns. Given an input sequence of length $N$ and a Transformer encoder of depth $T$, solving an algorithmic task is often all about routing the relevant information to the right node/operation at the right time in the $T$-by- $N$ grid represented by Transformer columns (illustrated in Figure 11(Left). Effectively the task is to learn to draw an adaptive control flow on the canvas of Transformer columns. In fact, recent work by Weiss et al. (2021) introduced a programming language called RASP, which is specifically designed to express solutions to sequence processing problems, and which has a direct equivalent to the operations in Transformer encoders. However, it is shown that Transformers learn solutions expressed in RASP only through intermediate supervision of attention patterns, and sometimes, even such supervision fails. Generally speaking, Transformers fail to find easily interpretable and/or symbolic solutions to algorithmic tasks. We conversely hypothesize that attention-based NNs that are able to find intuitive solutions (achieving interpretable attention patterns) could improve systematic generalization. [^0]Here we point out that regular Transformers lack some basic ingredients for learning such \"intuitive\" solutions to algorithmic problems. As a remedy, we propose simple architectural modifications to help them learn data routing. As a first step towards validating our model, we focus on the popular length generalization task of compositional table lookup (CTL; Liska et al. (2018); Hupkes et al. (2019); Dubois et al. (2020), as well as two more complex tasks: a simple arithmetic task and a variant of ListOps (Nangia \\& Bowman, 2018) designed to test the compositional generalization ability of NNs. Our novel Neural Data Router (NDR) achieves 100\\% generalization accuracy (never reported before; Dubois et al. (2020)) on the CTL task, and obtains nearly perfect accuracy on both the proposed simple arithmetic and ListOps tasks. We show that the attention and gating patterns of NDR tend to be interpretable as plausible control flows. ## 2 Improving Transformers FOR LEARning Adaptive Control Flow\n\nWe argue that the following components are needed to build Transformers capable of learning adaptive control flow. First, composing known operations in an arbitrary order requires that all operations are available at every computational step. This can be easily achieved by sharing the weights of the layers, as is done in Universal Transformers (Dehghani et al. 2019). Second, the network should be sufficiently deep, at least as deep as the deepest data dependency in the computational graph built from elementary operations (e.g., in the case of a parse tree, this is the depth of the tree). Otherwise, multiple operations must be fused into a single layer and hinder natural and elegant compositions. Third, inputs in some columns should be kept unchanged until it is their turn to be processed. The regular Transformer lacks a mechanism for skipping the whole transformation step by simply copying the input to the next step/layer. We propose a special gating function, copy gate, to implement such a mechanism (Sec.\n```\n\n##### *Relevant Chunk: No. 3/47 (Score: 0.87)*\n\n```\n2.1). Finally, many algorithmic tasks require combining several local computations in the right order. This typically implies that attention should not focus on all possible matches at a given time but only on the closest match. We propose and investigate a new type of attention with a corresponding inductive bias called geometric attention (Sec.2.2. Using both the geometric attention and copy gate, our model implements a \"neural data routing mechanism\", which can adaptively serialize the input problem. We refer to the resulting new Transformer as Neural Data Router (NDR). In the experimental section (Sec.3), we evaluate this model on three algorithmic tasks requiring length generalization and demonstrate its effectiveness. ### 2.1 Copy Gate: LEARNING TO SKIP OPERATIONS (VERTICAL Flow)\n\nEach layer of the regular Transformer consists of one self-attention and one feedforward block. The input to each of these blocks is directly connected to the corresponding output via a residual connection (Srivastava et al. 2015, He et al. 2016. However, such a connection does not allow for skipping the transformation of the entire layer and simply passing the unchanged input to the next layer. Here we propose to add an explicit gate, which we call copy gate, to facilitate such a behavior. We consider a $T$-layer Transformer encoder and an input sequence of length $N$. Since each layer corresponds to one computational step, we often refer to a layer as a step $t$. We denote the Transformer state of column $i$ in layer $t$ as $\\boldsymbol{h}^{(i, t)}=\\mathbf{H}_{t, i} \\in \\mathbb{R}^{d}$ where $d$ is the state size, and $\\mathbf{H}_{t} \\in \\mathbb{R}^{N \\times d}$ denotes the states of all $N$ columns in layer $t$. In the copy gate-augmented Transformer (Figure 5 in the appendix), each column $i$ in layer $(t+1)$ processes the input $\\mathbf{H}_{t}$ similarly to regular Transformers:\n\n$$\n\\begin{aligned}\n& \\left.\\boldsymbol{a}^{(i, t+1)}=\\text { LayerNorm(MultiHeadAttention }\\left(\\boldsymbol{h}^{(i, t)}, \\mathbf{H}_{t}, \\mathbf{H}_{t}\\right)+\\boldsymbol{h}^{(i, t)}\\right) \\\\\n& \\boldsymbol{u}^{(i, t+1)}=\\operatorname{LayerNorm}\\left(\\operatorname{FFN}^{\\text {data }}\\left(\\boldsymbol{a}^{(i, t+1)}\\right)\\right)\n\\end{aligned}\n$$\n\nusing the standard multi-head attention operation (Vaswani et al. 2017) MultiHeadAttention with a query obtained from $\\boldsymbol{h}^{(i, t)}$ and keys/values from $\\mathbf{H}_{t}$, but the output is gated (using $\\boldsymbol{g}^{(i, t+1)} \\in \\mathbb{R}^{d}$ ) as:\n\n$$\n\\begin{aligned}\n& \\boldsymbol{g}^{(i, t+1)}=\\sigma\\left(\\mathrm{FFN}^{\\mathrm{gate}}\\left(\\boldsymbol{a}^{(i, t+1)}\\right)\\right) \\\\\n& \\boldsymbol{h}^{(i, t+1)}=\\boldsymbol{g}^{(i, t+1)} \\odot \\boldsymbol{u}^{(i, t+1)}+\\left(1-\\boldsymbol{g}^{(i, t+1)}\\right) \\odot \\boldsymbol{h}^{(i, t)}\n\\end{aligned}\n$$\n\nWe use the basic two-layer feedforward block (Vaswani et al. 2017) for both FFN ${ }^{\\text {data }}$ and FFN ${ }^{\\text {gate }}$ which transforms input $\\boldsymbol{x} \\in \\mathbb{R}^{d}$ to:\n\n$$\n\\operatorname{FFN}(\\boldsymbol{x})=\\boldsymbol{W}_{2} \\max \\left(\\boldsymbol{W}_{1} \\boldsymbol{x}+\\boldsymbol{b}_{1}, 0\\right)+\\boldsymbol{b}_{2}\n$$\n\nbut with separate parameters and different dimensionalities: for FFN ${ }^{\\text {data }} \\boldsymbol{W}_{1}^{\\text {data }} \\in \\mathbb{R}^{d_{\\mathrm{FF}} \\times d}, \\boldsymbol{W}_{2}^{\\text {data }} \\in$ $\\mathbb{R}^{d \\times d_{\\mathrm{FF}}}$, while for $\\mathrm{FFN}^{\\text {gate }} \\boldsymbol{W}_{1}^{\\text {gate }}, \\boldsymbol{W}_{2}^{\\text {gate }} \\in \\mathbb{R}^{d \\times d}$, with biases $\\boldsymbol{b}_{1}^{\\text {data }} \\in \\mathbb{R}^{d_{\\mathrm{FF}}}$ and $\\boldsymbol{b}_{2}^{\\text {data }}, \\boldsymbol{b}_{1}^{\\text {gate }}, \\boldsymbol{b}_{2}^{\\text {gate }} \\in \\mathbb{R}^{d}$. When the gate is closed i.e. $g^{(i, t+1)}=0$ in Eq. 4 the entire transformation is skipped and the input is copied over to the next layer $\\boldsymbol{h}^{(i, t+1)}=\\boldsymbol{h}^{(i, t)}$. Crucially, we parameterize the gate (Eq. 3 ) as a function of the output of the self-attention (Eq. 11, such that the decision to copy or transform the input for each column depends on the states of all columns. This is a crucial difference compared to previously proposed gatings in Transformers, which are solely motivated by training stability (Parisotto et al., 2020) or by a common practice from convolution-based models (Chaabouni et al., 2021). None of the previous approaches can implement the behavior of our copy gate (see Sec. 6 on related work). The bias of the gate $\\boldsymbol{b}_{2}^{\\text {gate }}$ is initialized to -3 (Hochreiter \\& Schmidhuber, 1997).\n```\n\n##### *Relevant Chunk: No. 1/47 (Score: 0.69)*\n\n```\n# The Neural Data RoutER: ADAPTIVE CONTROL Flow in TranSFORMERS IMPROVES SYSTEMATIC GENERALIZATION \n\nR\u00f3bert Csord\u00e1s ${ }^{1}$ Kazuki Irie ${ }^{1}$ J\u00fcrgen Schmidhuber ${ }^{1,2}$<br>${ }^{1}$ The Swiss AI Lab, IDSIA, University of Lugano (USI) \\& SUPSI, Lugano, Switzerland<br>${ }^{2}$ King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia<br>\\{robert, kazuki, juergen\\}@idsia.ch\n\n\n#### Abstract\n\nDespite progress across a broad range of applications, Transformers have limited success in systematic generalization.\n```\n\n#### 2. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.80)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 18/50 (Score: 0.80)*\n\n```\nSpringer. Andr\u00e9 F. T. Martins and Julia Kreutzer. 2017. Learning what's easy: Fully differentiable neural easy-first taggers. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 349-362, Copenhagen, Denmark. Association for Computational Linguistics. Jacob Menick and Nal Kalchbrenner. 2018. Generating high fidelity images with subscale pixel networks and multidimensional upscaling. In International Conference on Learning Representations. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. 2016. Conditional image generation with pixelenn decoders. In Advances in neural information processing systems, pages 4790-4798. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image transformer. In International Conference on Machine Learning, pages 4055-4064. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf. Jack Rae, Jonathan J Hunt, Ivo Danihelka, Timothy Harley, Andrew W Senior, Gregory Wayne, Alex Graves, and Timothy Lillicrap. 2016. Scaling memory-augmented neural networks with sparse reads and writes. In Advances in Neural Information Processing Systems, pages 36213629 . Jack Rae and Ali Razavi. 2020. Do transformers need deep long-range memory? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7524-7529, Online. Association for Computational Linguistics. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive transformers for longrange sequence modelling. In International Conference on Learning Representations. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages $464-468$. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-ofexperts layer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4596-4604. Sainbayar Sukhbaatar, \u00c9douard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331-335. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 2048-2057. JMLR.org. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages $5753-5763$. ## A Samples from Routing Transformer\n\nIn the following sections we present a few samples generated from the Routing Transformer trained on the PG-19 data-set with sequence length 8192 .\n```\n\n#### 3. Addressing Some Limitations of Transformers with Feedback Memory (Avg. Score: 0.78)\n\n*Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, Sainbayar Sukhbaatar*\n\n**Published in:** arXiv (2020)\t**Cited by** None  (*Influential: None*)\n\n**TL;DR:** Transformers have shortcomings - limited memory and limited state update - but Feedback Memory is a straightforward way to resolve these. \n\n**Abstract:** Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\n\n##### *Relevant Chunk: No. 21/30 (Score: 0.78)*\n\n```\nArXiv, abs/1910.06764, 2019. Jorge P\u00e9rez, Javier Marinkovi\u0107, and Pablo Barcel\u00f3. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URLhttps://openreview.net/forum?id=SylKikSYDH. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. arXiv preprint arXiv:2003.05997, 2020. Abigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-generator networks.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic routing neural networks, memory compression techniques, hybrid attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\n\n*From Search Query: dynamic routing neural networks*\n\n*Ningyu Zhang, Shumin Deng, Zhanlin Sun, Xi Chen, Wei Zhang, Huajun Chen*\n\n**TL;DR:** This paper explores the capsule networks used for relation extraction in a multi-instance multi-label learning framework and proposes a novel neural approach based on capsule networks with attention mechanisms that improves the precision of the predicted relations.\n\n**Abstract:** A capsule is a group of neurons, whose activity vector represents the instantiation parameters of a specific type of entity. In this paper, we explore the capsule networks used for relation extraction in a multi-instance multi-label learning framework and propose a novel neural approach based on capsule networks with attention mechanisms. We evaluate our method with different benchmarks, and it is demonstrated that our method improves the precision of the predicted relations. Particularly, we show that capsule networks improve multiple entity pairs relation extraction.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 94  (*Influential: 1*)\n\n#### 2. Joint Slot Filling and Intent Detection via Capsule Neural Networks\n\n*From Search Query: dynamic routing neural networks*\n\n*Chenwei Zhang, Yaliang Li, Nan Du, Wei Fan, Philip S. Yu*\n\n**TL;DR:** A capsule-based neural network model is proposed which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema and a re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation.\n\n**Abstract:** Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2018\n\n**Citations:** 224  (*Influential: 31*)\n\n#### 3. A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models\n\n*From Search Query: memory compression techniques*\n\n*Krithika Ramesh, Arnav Chavan, Shrey Pandit, Sunayana Sitaram*\n\n**TL;DR:** This analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification and indicates that compression strategies can have an adverse effect on fairness measures.\n\n**Abstract:** Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 0*)\n\n#### 4. The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models\n\n*From Search Query: memory compression techniques*\n\n*Satya Sai, Srinath Namburi, Makesh Narsimhan Sreedhar, Srinath Srinivasan, Frederic Sala*\n\n**TL;DR:** A comprehensive analysis across multiple model families using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance, with a particular focus on tradeoffs involving parametric knowledge.\n\n**Abstract:** Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families (ENCODER, ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. We release our codebase1 to enable further research.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 5. Hardware-Aware Compression with Random Operation Access Specific Tile (ROAST) Hashing\n\n*From Search Query: memory compression techniques*\n\n*Aditya Desai, K. Zhou, Anshumali Shrivastava*\n\n**TL;DR:** With ROAST, the authors can efficiently train and deploy the model using a much smaller memory footprint in text and image classification tasks, and introduce global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest.\n\n**Abstract:** Advancements in deep learning are often associated with increasing model sizes. Training and deploying large models require sophisticated hardware and incur significantly higher costs. Thus, model compression is a widely explored approach to solving the problem. However, SOTA techniques fall short in one or more desirable aspects of compression - for instance, pruning does not reduce memory for training, quantization can only provide up to 32 \u00d7 compression, Hashed-Net is cache-inefficient, etc. This paper proposes a model-agnostic, cache-friendly, and hardware-aware model compression approach: Random Operation Access Specific Tile (ROAST) hashing. ROAST collapses the parameters by club-bing them through a lightweight mapping. While clubbing these parameters, ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern. ROAST is up to \u223c 25 \u00d7 faster to train and \u223c 50 \u00d7 faster to infer than the popular parameter sharing method HashedNet. Additionally, ROAST introduces global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest. With ROAST, we can efficiently train and deploy the model using a much smaller memory footprint ( \u223c 10 \u2212 100 \u00d7 lesser) in text and image classification tasks. ROAST-MM kernel implementation is open-source 1\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 6. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid attention mechanisms*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 7. AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios\n\n*From Search Query: hybrid attention mechanisms*\n\n*Zhongzhan Huang, Mingfu Liang, Shan Zhong, Liang Lin*\n\n**Abstract:** We propose the attention-inspired numerical solver (AttNS), a concise method that helps the generalization and robustness issues faced by the AI-Hybrid numerical solver in solving differential equations due to limited data. AttNS is inspired by the effectiveness of attention modules in Residual Neural Networks (ResNet) in enhancing model generalization and robustness for conventional deep learning tasks. Drawing from the dynamical system perspective of ResNet, We seamlessly incorporate attention mechanisms into the design of numerical methods tailored for the characteristics of solving differential equations. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, show-case AttNS consistently enhancing various numerical solvers without any intricate model crafting. Finally, we analyze AttNS experimentally and theoretically, demonstrating its ability to achieve strong generalization and robustness while ensuring the convergence of the solver. This includes requiring less data compared to other advanced methods to achieve comparable generalization errors and better prevention of numerical explosion issues when solving differential equations.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention\n\n*From Search Query: hybrid attention mechanisms*\n\n*Ekta Sood, Simon Tannert, Philipp Mueller, A. Bulling*\n\n**TL;DR:** This work proposes a novel hybrid text saliency model that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Abstract:** A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing(NLP). We propose a novel hybrid text saliency model(TSM) that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 65  (*Influential: 4*)\n\n### 2 related papers from ArXiv\n\n#### 1. Semantic-Unit-Based Dilated Convolution for Multi-Label Text\n  Classification\n\n*From Search Query: hybrid attention mechanisms*\n\n*Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma, Xu Sun*\n\n**Abstract:** We propose a novel model for multi-label text classification, which is based\non sequence-to-sequence learning. The model generates higher-level semantic\nunit representations with multi-level dilated convolution as well as a\ncorresponding hybrid attention mechanism that extracts both the information at\nthe word-level and the level of the semantic unit. Our designed dilated\nconvolution effectively reduces dimension and supports an exponential expansion\nof receptive fields without loss of local information, and the\nattention-over-attention mechanism is able to capture more summary relevant\ninformation from the source context. Results of our experiments show that the\nproposed model has significant advantages over the baseline models on the\ndataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is\ncompetitive to the deterministic hierarchical models and it is more robust to\nclassifying low-frequency labels.\n\n**Published:** 2018-08-26T14:36:22Z  (*Updated: 2018-11-11T19:12:35Z*)\n\n\n\n#### 2. MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation\n\n*From Search Query: hybrid attention mechanisms*\n\n*Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel*\n\n**Abstract:** Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs\n\n**Published:** 2024-10-03T14:50:33Z  (*Updated: 2024-10-04T14:19:33Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Deciding How to Decide: Dynamic Routing in Artificial Neural Networks\n\n*From Search Query: dynamic routing neural networks*\n\n*Mason McGill, Pietro Perona*\n\n**Abstract:** We propose and systematically evaluate three strategies for training\ndynamically-routed artificial neural networks: graphs of learned\ntransformations through which different input signals may take different paths.\nThough some approaches have advantages over others, the resulting networks are\noften qualitatively similar. We find that, in dynamically-routed networks\ntrained to classify images, layers and branches become specialized to process\ndistinct categories of images. Additionally, given a fixed computational\nbudget, dynamically-routed networks tend to perform better than comparable\nstatically-routed networks.\n\n**Conference:** deciding-how-to-decide-dynamic-routing-in-1\n\n**Published:** 2017-03-17\n\n\n\n#### 2. Fast Dynamic Routing Based on Weighted Kernel Density Estimation\n\n*From Search Query: dynamic routing neural networks*\n\n*Xiaofu Wu, Wei Zhao, Suofei Zhang, Quan Zhou*\n\n**Abstract:** Capsules as well as dynamic routing between them are most recently proposed\nstructures for deep neural networks. A capsule groups data into vectors or\nmatrices as poses rather than conventional scalars to represent specific\nproperties of target instance. Besides of pose, a capsule should be attached\nwith a probability (often denoted as activation) for its presence. The dynamic\nrouting helps capsules achieve more generalization capacity with many fewer\nmodel parameters. However, the bottleneck that prevents widespread applications\nof capsule is the expense of computation during routing. To address this\nproblem, we generalize existing routing methods within the framework of\nweighted kernel density estimation, and propose two fast routing methods with\ndifferent optimization strategies. Our methods prompt the time efficiency of\nrouting by nearly 40\\% with negligible performance degradation. By stacking a\nhybrid of convolutional layers and capsule layers, we construct a network\narchitecture to handle inputs at a resolution of $64\\times{64}$ pixels. The\nproposed models achieve a parallel performance with other leading methods in\nmultiple benchmarks.\n\n**Published:** 2018-05-28\n\n\n\n#### 3. XGBoost: Scalable GPU Accelerated Learning\n\n*From Search Query: memory compression techniques*\n\n*Rory Mitchell, Eibe Frank, Thejaswi Rao, Andrey Adinets*\n\n**Abstract:** We describe the multi-GPU gradient boosting algorithm implemented in the\nXGBoost library (https://github.com/dmlc/xgboost). Our algorithm allows fast,\nscalable training on multi-GPU systems with all of the features of the XGBoost\nlibrary. We employ data compression techniques to minimise the usage of scarce\nGPU memory while still allowing highly efficient implementation. Using our\nalgorithm we show that it is possible to process 115 million training instances\nin under three minutes on a publicly available cloud computing instance. The\nalgorithm is implemented using end-to-end GPU parallelism, with prediction,\ngradient calculation, feature quantisation, decision tree construction and\nevaluation phases all computed on device.\n\n**Published:** 2018-06-29\n\n\n\n#### 4. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n\n*From Search Query: memory compression techniques*\n\n*Song Han, Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer*\n\n**Abstract:** Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet\n\n**Published:** 2016-02-24\n\n\n\n#### 5. Hybrid intelligence for dynamic job-shop scheduling with deep reinforcement learning and attention mechanism\n\n*From Search Query: hybrid attention mechanisms*\n\n*Bo Yuan, Xiu Li, Rong Wang, Yuanzhi Dai, Zijun Liao, Yunhui Zeng*\n\n**Abstract:** The dynamic job-shop scheduling problem (DJSP) is a class of scheduling tasks that specifically consider the inherent uncertainties such as changing order requirements and possible machine breakdown in realistic smart manufacturing settings. Since traditional methods cannot dynamically generate effective scheduling strategies in face of the disturbance of environments, we formulate the DJSP as a Markov decision process (MDP) to be tackled by reinforcement learning (RL). For this purpose, we propose a flexible hybrid framework that takes disjunctive graphs as states and a set of general dispatching rules as the action space with minimum prior domain knowledge. The attention mechanism is used as the graph representation learning (GRL) module for the feature extraction of states, and the double dueling deep Q-network with prioritized replay and noisy networks (D3QPN) is employed to map each state to the most appropriate dispatching rule. Furthermore, we present Gymjsp, a public benchmark based on the well-known OR-Library, to provide a standardized off-the-shelf facility for RL and DJSP research communities. Comprehensive experiments on various DJSP instances confirm that our proposed framework is superior to baseline algorithms with smaller makespan across all instances and provide empirical justification for the validity of the various components in the hybrid framework.\n\n**Published:** 2022-01-03\n\n\n\n#### 6. A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization\n\n*From Search Query: hybrid attention mechanisms*\n\n*Richard Radke, Chengjiang Long, Ashraful Islam*\n\n**Abstract:** Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an \"action-ness\" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.\n\n**Published:** 2021-01-03\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on dynamic routing mechanisms, memory compression techniques, and hybrid attention mechanisms, here are some key insights and references that can be useful:\n\n## Dynamic Routing Mechanisms\n\n- The concept of dynamic routing can be seen in the \"Neural Routing Transformer Architecture (NRT)\" which incorporates dynamic routing mechanisms. This allows the model to allocate computational resources dynamically, focusing on the more challenging parts of the input. This adaptive computation can enhance efficiency and performance by not wasting resources on simpler inputs.\n\n- Early-exit neural networks, discussed in the comprehensive survey on early-exit DNNs, also involve dynamic routing through the use of multiple side branches. These networks can terminate the inference process early for simpler inputs, reducing computational overhead and improving efficiency. This approach can be adapted to transformer architectures to enable dynamic routing based on input complexity.\n\n## Memory Compression Techniques\n\n- The \"Memorizing Transformers\" concept involves extending language models with the ability to memorize internal representations of past inputs. This can be achieved through an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs, which improves language modeling efficiency and reduces the need for extensive memory capacity. Integrating such external memory mechanisms with hierarchical processing could enhance memory efficiency in transformer models[Key References and Excerpts].\n\n- Early-exit DNNs also provide a framework for distributed training and deployment, which can help in managing memory more efficiently by partitioning the model via side branches. This can be particularly useful in resource-constrained environments and for long sequences where memory management is a challenge.\n\n## Hybrid Attention Mechanisms\n\n- The \"SOFT: Softmax-free Transformer\" introduces a Gaussian kernel function to replace the dot-product similarity in self-attention, enabling a full self-attention matrix to be approximated via a low-rank matrix decomposition. This approach can be adapted for hierarchical structures, providing a more efficient and theoretically sound attention mechanism. Combining this with other attention mechanisms, such as those using softmax, could create hybrid attention mechanisms that leverage the strengths of each approach[Key References and Excerpts].\n\n- The \"Attention Alignment and Flexible Positional Embeddings\" work highlights the need for better attention alignment strategies, especially for long sequences. A hybrid approach could incorporate different attention mechanisms, such as multi-head self-attention from transformers and alternative mechanisms like those using Gaussian kernel functions, to improve performance and efficiency[Key References and Excerpts].\n\n## Integration and Adaptive Processing\n\n- Dynamic routing and adaptive computation can be integrated into transformer architectures by incorporating mechanisms like early-exit points and conditional computation. For example, the Soft Conditional Gate used in some early-exit networks can be adapted to select the optimal path during inference based on input characteristics, which can enhance test-time adaptation and efficiency.\n\n- The \"IDS-MTran\" model, which uses a multi-scale transformer for network intrusion detection, demonstrates how different features at various scales can be integrated efficiently. This approach can be applied to language models by incorporating multi-scale attention mechanisms and dynamic routing to handle complex patterns and dependencies in data more effectively.\n\nBy combining these concepts, researchers can develop an autoregressive language model that leverages dynamic routing mechanisms for efficient computation, employs memory compression techniques to manage long sequences effectively, and utilizes hybrid attention mechanisms to enhance expressiveness and efficiency.\n",
            "analysis": "### Analysis Summary\n\nAfter reviewing the seed design (meta_hierarchical_linear_transfo) and examining related work, I've identified several key areas for potential innovation:\n\n1. **Memory Management Challenges**:\n   - The seed design uses temporal latent bottlenecks for information flow between streams\n   - Existing approaches struggle with efficient memory usage for long sequences\n   - There's potential for improvement in memory compression and state management\n\n2. **Attention Mechanism Limitations**:\n   - Current linear attention methods often sacrifice expressiveness for efficiency\n   - The seed's hierarchical approach could be enhanced with more sophisticated attention patterns\n   - Softmax-free approaches (like SOFT) show promise but haven't been fully explored in hierarchical contexts\n\n3. **Test-Time Adaptation**:\n   - The memorizing transformer approach suggests benefits of dynamic memory updates\n   - Current designs lack efficient test-time adaptation mechanisms\n   - Potential to integrate adaptive components without excessive overhead\n\n4. **Sibling Design Differentiation**:\n   - H2_hierarchicallineartransformer focuses on heavy-hitter token identification\n   - CompressiveLinearTransformer emphasizes lossless compression\n   - Opportunity to explore different aspects like dynamic routing and adaptive computation\n\n### Key References and Excerpts\n\n1. From \"Memorizing Transformers\":\n```\nWe extend language models with the ability to memorize the internal representations of past inputs... demonstrates that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks.\n```\n- Suggests potential for integrating external memory mechanisms\n- Shows benefits of non-differentiable memory for efficiency\n\n2. From \"SOFT: Softmax-free Transformer\":\n```\nTo remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition.\n```\n- Demonstrates alternative attention mechanisms without softmax\n- Provides mathematical framework for efficient attention computation\n\n3. From \"Attention Alignment and Flexible Positional Embeddings\":\n```\nT5 suffers from the dispersed attention issue: the longer the input sequence, the flatter the attention distribution.\n```\n- Identifies key limitation in attention mechanisms for long sequences\n- Suggests need for better attention alignment strategies\n\n4. From \"An Empirical Study of Mamba-based Language Models\":\n```\nWhile pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning.\n```\n- Highlights importance of maintaining strong copying and in-context learning abilities\n- Suggests potential for hybrid approaches\n\n### Key Insights and Analysis\n\n1. **Memory Architecture Opportunities**:\n   - Combining external memory with hierarchical processing could improve efficiency\n   - Non-differentiable memory components might reduce computational overhead\n   - Need to balance memory capacity with access efficiency\n\n2. **Attention Mechanism Innovation**:\n   - Softmax-free approaches could be adapted for hierarchical structures\n   - Gaussian kernel functions might provide better theoretical properties\n   - Opportunity to develop hybrid attention mechanisms\n\n3. **Adaptive Processing**:\n   - Test-time adaptation could enhance model flexibility\n   - Need for efficient memory update mechanisms\n   - Potential for dynamic routing based on input characteristics\n\n4. **Integration Strategy**:\n   - Combine strengths of memorizing transformers with hierarchical processing\n   - Leverage softmax-free attention for improved efficiency\n   - Maintain strong copying and in-context learning capabilities\n\n### Future Search Plan\n\n1. **Next Search Topics**:\n   - Dynamic routing mechanisms in neural networks\n   - Efficient memory update strategies\n   - Hybrid attention mechanisms combining different approaches\n\n2. **Specific Areas to Investigate**:\n   - Mathematical formulations for memory compression\n   - Techniques for adaptive computation\n   - Methods for maintaining stability in dynamic systems\n\n### References\n\n1. Wu, Y., Rabe, M., Hutchins, D. S., & Szegedy, C. (2022). Memorizing Transformers. International Conference on Learning Representations.\n\n2. Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., ... & Zhang, L. (2021). SOFT: Softmax-free Transformer with Linear Complexity. Neural Information Processing Systems.\n\n3. Chi, T. C., Fan, T. H., & Rudnicky, A. I. (2023). Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. arXiv preprint.\n\n4. Waleffe, R., Byeon, W., Riach, D., Norick, B., Korthikanti, V., Dao, T., ... & Catanzaro, B. (2024). An Empirical Study of Mamba-based Language Models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hierarchical memory compression, adaptive compression neural networks, multi-scale attention mechanisms",
            "detail": "Show me papers discussing hierarchical memory organization and compression in neural networks, particularly focusing on adaptive compression techniques and their integration with attention mechanisms.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nShow me papers discussing hierarchical memory organization and compression in neural networks, particularly focusing on adaptive compression techniques and their integration with attention mechanisms.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Recurrent Memory Transformer (Avg. Score: 0.93)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 22/29 (Score: 0.93)*\n\n```\nJie Lei, Liwei Wang, Yelong Shen, Dong Yu, Tamara L. Berg, and Mohit Bansal. Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning, 2020. Yang Liu, Jiaxiang Liu, Li Chen, Yuxiang Lu, Shikun Feng, Zhida Feng, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-sparse: Learning hierarchical efficient transformer through regularized self-attention. arXiv preprint arXiv:2203.12276, 2022. Matt Mahoney. Large text compression benchmark, 2006. URL http://www.mattmahoney.net/dc/text html\n\nPedro Henrique Martins, Zita Marinho, and Andr\u00e9 FT Martins. $\\infty$-former: Infinite memory transformer. arXiv preprint arXiv:2109.00301, 2021. Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in nervous activity. The bulletin of mathematical biophysics, 5(4):115-133, 1943. Yuanliang Meng and Anna Rumshisky. Context-aware neural model for temporal information extraction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 527-536, 2018. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum? id=Byj72udxe\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. URL https://www.cs.ubc.ca/ amuham01/LING530/papers/ radford2018improving.pdf\n\nJack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, and Timothy P Lillicrap. Scaling memory-augmented neural networks with sparse reads and writes, 2016.\n```\n\n#### 2. Universal Transformers  (Avg. Score: 0.81)\n\n*Mostafa Dehghani, Stephan Gouws, O. Vinyals, Jakob Uszkoreit, Lukasz Kaiser*\n\n**Published in:** International Conference on Learning Representations (2018)\t**Cited by** 673  (*Influential: 65*)\n\n**TL;DR:** The Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses issues of parallelizability and global receptive field, is proposed.\n\n**Abstract:** Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.\n\n##### *Relevant Chunk: No. 26/32 (Score: 0.81)*\n\n```\nJournal of Machine Learning Research, 15(1): $1929-1958,2014$. Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems 28, pp. 2440-2448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pp. 3104-3112, 2014. URL http://arxiv.org/abs/1409.3215\n\nKe Tran, Arianna Bisazza, and Christof Monz. The importance of being recurrent for modeling hierarchical structure. In Proceedings of NAACL'18, 2018. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, 2017. URLhttp://arxiv.org/abs/1706.03762\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, \u0141ukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. CoRR, abs/1803.07416, 2018. Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merri\u00ebnboer, Armand Joulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks.\n```\n\n#### 3. Reformer: The Efficient Transformer (Avg. Score: 0.57)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 7/19 (Score: 0.70)*\n\n```\nCoRR, abs/1506.02075, 2015. URL/http://arxiv. org/ $\\mathrm{abs} / 1506.02075$. Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, and Yoshua Bengio. Hierarchical memory networks. arXiv preprint arXiv:1605.07427, 2016. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. URL https://openai.com/blog/sparse-transformers, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations.\n```\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.44)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n#### 4. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.56)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.56)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical memory compression, adaptive compression neural networks, multi-scale attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Weighted Mutual Learning with Diversity-Driven Model Compression\n\n*From Search Query: hierarchical memory compression*\n\n*Miao Zhang, Li Wang, David Campos, Wei Huang, Chenjuan Guo, B. Yang*\n\n**TL;DR:** A framework called Weighted Mutual Learning with Diversity-Driven Model Compression (WML), which outperforms existing online distillation methods on a variety of deep neural networks and produces a series of students with different model sizes in a single run, which also achieves competitive results compared with existing channel pruning methods.\n\n**Abstract:** Online distillation attracts attention from the community as it simplifies the traditional two-stage knowledge distillation process into a single stage. Online distillation collaboratively trains a group of peer models, which are treated as students, and all students gain extra knowledge from each other. However, memory consumption and diversity among students are two key challenges to the scalability and quality of online distillation. To address the two challenges, this paper presents a framework called Weighted Mutual Learning with Diversity-Driven Model Compression ( WML ) for online distillation. First, at the base of a hierarchical structure where students share different parts, we leverage the structured network pruning to generate diversified students with different models sizes, thus also helping reduce the memory requirements. Second, rather than taking the average of students, this paper, for the first time, leverages a bi-level formulation to estimate the relative importance of students with a close-form, to further boost the effectiveness of the distillation from each other. Extensive experiments show the generalization of the proposed framework, which outperforms existing online distillation methods on a variety of deep neural networks. More interesting, as a byproduct, WML produces a series of students with different model sizes in a single run, which also achieves competitive results compared with existing channel pruning methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 2. The Kanerva Machine: A Generative Distributed Memory\n\n*From Search Query: hierarchical memory compression*\n\n*Yan Wu, Greg Wayne, Alex Graves, T. Lillicrap*\n\n**TL;DR:** An end-to-end trained memory system that quickly adapts to new data and generates samples like them that significantly improves generative models trained on both the Omniglot and CIFAR datasets is presented.\n\n**Abstract:** We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 37  (*Influential: 6*)\n\n#### 3. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: hierarchical memory compression*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 2*)\n\n#### 4. Adaptive Estimators Show Information Compression in Deep Neural Networks\n\n*From Search Query: adaptive compression neural networks*\n\n*Ivan Chelombiev, Conor J. Houghton, Cian O\u2019Donnell*\n\n**TL;DR:** More robust mutual information estimation techniques are developed, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions, which explore compression in networks with a range of different activation functions.\n\n**Abstract:** To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 7*)\n\n#### 5. RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks\n\n*From Search Query: adaptive compression neural networks*\n\n*Edouard Yvinec, Arnaud Dapogny, M. Cord, K\u00e9vin Bailly*\n\n**TL;DR:** Red is presented, a data-free structured, unified approach to tackle structured pruning ofDeep Neural Networks by proposing a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors.\n\n**Abstract:** Deep Neural Networks (DNNs) are ubiquitous in today's computer vision land-scape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to re-train the model. In this paper, we present RED, a data-free structured, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 0*)\n\n#### 6. Entropy and mutual information in models of deep neural networks\n\n*From Search Query: adaptive compression neural networks*\n\n*Marylou Gabri\u00e9, Andre Manoel, Cl\u00e9ment Luneau, Jean Barbier, N. Macris, Florent Krzakala, L. Zdeborov\u00e1*\n\n**TL;DR:** It is concluded that, in the proposed setting, the relationship between compression and generalization remains elusive and an experiment framework with generative models of synthetic datasets is proposed, on which deep neural networks are trained with a weight constraint designed so that the assumption in (i) is verified during learning.\n\n**Abstract:** We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) we show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 167  (*Influential: 3*)\n\n#### 7. IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation\n\n*From Search Query: multi-scale attention mechanisms*\n\n*Kai Li, Run Yang, Fuchun Sun, Xiaolin Hu*\n\n**TL;DR:** The fast version of IIANet (IIANet-fast) has only 7% of CTCNet's MACs and is 40% faster than CTCNet on CPUs while achieving better separation quality, showing the great potential of attention mechanism for efficient and effective multimodal fusion.\n\n**Abstract:** Recent research has made significant progress in designing fusion modules for audio-visual speech separation. However, they predominantly focus on multi-modal fusion at a single temporal scale of auditory and visual features without employing selective attention mechanisms, which is in sharp contrast with the brain. To address this issue, We propose a novel model called Intra- and Inter-Attention Network (IIANet), which leverages the attention mechanism for efficient audio-visual feature fusion. IIANet consists of two types of attention blocks: intra-attention (IntraA) and inter-attention (InterA) blocks, where the InterA blocks are distributed at the top, middle and bottom of IIANet. Heavily inspired by the way how human brain selectively focuses on relevant content at various temporal scales, these blocks maintain the ability to learn modality-specific features and enable the extraction of different semantics from audio-visual features. Comprehensive experiments on three standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2) demonstrate the effectiveness of IIANet, outperforming previous state-of-the-art methods while maintaining comparable inference time. In particular, the fast version of IIANet (IIANet-fast) has only 7% of CTCNet's MACs and is 40% faster than CTCNet on CPUs while achieving better separation quality, showing the great potential of attention mechanism for efficient and effective multimodal fusion.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 8. BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model\n\n*From Search Query: multi-scale attention mechanisms*\n\n*Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, H. Goan, Han Liu*\n\n**TL;DR:** The BiSHop framework, a novel end-to-end framework for deep tabular learning, is introduced, demonstrating that BiSHop surpasses current SOTA methods with significantly less HPO runs, marking it a robust solution for deep tabular learning.\n\n**Abstract:** We introduce the \\textbf{B}i-Directional \\textbf{S}parse \\textbf{Hop}field Network (\\textbf{BiSHop}), a novel end-to-end framework for deep tabular learning. BiSHop handles the two major challenges of deep tabular learning: non-rotationally invariant data structure and feature sparsity in tabular data. Our key motivation comes from the recent established connection between associative memory and attention mechanisms. Consequently, BiSHop uses a dual-component approach, sequentially processing data both column-wise and row-wise through two interconnected directional learning modules. Computationally, these modules house layers of generalized sparse modern Hopfield layers, a sparse extension of the modern Hopfield model with adaptable sparsity. Methodologically, BiSHop facilitates multi-scale representation learning, capturing both intra-feature and inter-feature interactions, with adaptive sparsity at each scale. Empirically, through experiments on diverse real-world datasets, we demonstrate that BiSHop surpasses current SOTA methods with significantly less HPO runs, marking it a robust solution for deep tabular learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 9. Visual News: Benchmark and Challenges in News Image Captioning\n\n*From Search Query: multi-scale attention mechanisms*\n\n*Fuxiao Liu, Yinghan Wang, Tianlu Wang, Vicente Ordonez*\n\n**TL;DR:** This work proposes Visual News Captioner, an entity-aware model for the task of news image captioning, built upon the Transformer architecture, which is further equipped with novel multi-modal feature fusion techniques and attention mechanisms designed to generate named entities more accurately.\n\n**Abstract:** We propose Visual News Captioner, an entity-aware model for the task of news image captioning. We also introduce Visual News, a large-scale benchmark consisting of more than one million news images along with associated news articles, image captions, author information, and other metadata. Unlike the standard image captioning task, news images depict situations where people, locations, and events are of paramount importance. Our proposed method can effectively combine visual and textual features to generate captions with richer information such as events and entities. More specifically, built upon the Transformer architecture, our model is further equipped with novel multi-modal feature fusion techniques and attention mechanisms, which are designed to generate named entities more accurately. Our method utilizes much fewer parameters while achieving slightly better prediction results than competing methods. Our larger and more diverse Visual News dataset further highlights the remaining challenges in captioning news images.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 91  (*Influential: 18*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Residual Dense Network for Image Restoration\n\n*From Search Query: hierarchical memory compression*\n\n*Yu Kong, Yulun Zhang, Yapeng Tian, Bineng Zhong, Yun Fu*\n\n**Abstract:** Convolutional neural network has recently achieved great success for image restoration (IR) and also offered hierarchical features. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in IR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.\n\n**Published:** 2018-12-25\n\n\n\n#### 2. Learning for Video Compression with Hierarchical Quality and Recurrent Enhancement\n\n*From Search Query: hierarchical memory compression*\n\n*Luc van Gool, Radu Timofte, Ren Yang, Fabian Mentzer*\n\n**Abstract:** In this paper, we propose a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. The frames in the first layer are compressed by an image compression method with the highest quality. Using these frames as references, we propose the Bi-Directional Deep Compression (BDDC) network to compress the second layer with relatively high quality. Then, the third layer frames are compressed with the lowest quality, by the proposed Single Motion Deep Compression (SMDC) network, which adopts a single motion map to estimate the motions of multiple frames, thus saving bits for motion information. In our deep decoder, we develop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement. In our HLVC approach, the hierarchical quality benefits the coding efficiency, since the high quality information facilitates the compression and enhancement of low quality frames at encoder and decoder sides, respectively. Finally, the experiments validate that our HLVC approach advances the state-of-the-art of deep video compression methods, and outperforms the \"Low-Delay P (LDP) very fast\" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at https://github.com/RenYang-home/HLVC.\n\n**Conference:** learning-for-video-compression-with-1\n\n**Published:** 2020-03-04\n\n\n\n#### 3. Context-adaptive neural network based prediction for image compression\n\n*From Search Query: adaptive compression neural networks*\n\n*Thierry Dumas, Aline Roumy, Christine Guillemot*\n\n**Abstract:** This paper describes a set of neural network architectures, called Prediction Neural Networks Set (PNNS), based on both fully-connected and convolutional neural networks, for intra image prediction. The choice of neural network for predicting a given image block depends on the block size, hence does not need to be signalled to the decoder. It is shown that, while fully-connected neural networks give good performance for small block sizes, convolutional neural networks provide better predictions in large blocks with complex textures. Thanks to the use of masks of random sizes during training, the neural networks of PNNS well adapt to the available context that may vary, depending on the position of the image block to be predicted. When integrating PNNS into a H.265 codec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained. These gains are on average 0.99% larger than those of prior neural network based methods. Unlike the H.265 intra prediction modes, which are each specialized in predicting a specific texture, the proposed PNNS can model a large set of complex textures.\n\n**Published:** 2018-07-17\n\n\n\n#### 4. Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel Pruning\n\n*From Search Query: adaptive compression neural networks*\n\n*Jindong Wang, Chaohui Yu, Zijing Wu, Yiqiang Chen*\n\n**Abstract:** Deep unsupervised domain adaptation (UDA) has recently received increasing\nattention from researchers. However, existing methods are computationally\nintensive due to the computation cost of Convolutional Neural Networks (CNN)\nadopted by most work. To date, there is no effective network compression method\nfor accelerating these models. In this paper, we propose a unified Transfer\nChannel Pruning (TCP) approach for accelerating UDA models. TCP is capable of\ncompressing the deep UDA model by pruning less important channels while\nsimultaneously learning transferable features by reducing the cross-domain\ndistribution divergence. Therefore, it reduces the impact of negative transfer\nand maintains competitive performance on the target task. To the best of our\nknowledge, TCP is the first approach that aims at accelerating deep UDA models.\nTCP is validated on two benchmark datasets-Office-31 and ImageCLEF-DA with two\ncommon backbone networks-VGG16 and ResNet50. Experimental results demonstrate\nthat TCP achieves comparable or better classification accuracy than other\ncomparison methods while significantly reducing the computational cost. To be\nmore specific, in VGG16, we get even higher accuracy after pruning 26% floating\npoint operations (FLOPs); in ResNet50, we also get higher accuracy on half of\nthe tasks after pruning 12% FLOPs. We hope that TCP will open a new door for\nfuture research on accelerating transfer learning models.\n\n**Published:** 2019-03-25\n\n\n\n#### 5. Enhancing Few-Shot Image Classification through Learnable Multi-Scale Embedding and Attention Mechanisms\n\n*From Search Query: multi-scale attention mechanisms*\n\n*Mohammad Reza Mohammadi, Amirreza Fateh, Fatemeh Askari*\n\n**Abstract:** In the context of few-shot classification, the goal is to train a classifier using a limited number of samples while maintaining satisfactory performance. However, traditional metric-based methods exhibit certain limitations in achieving this objective. These methods typically rely on a single distance value between the query feature and support feature, thereby overlooking the contribution of shallow features. To overcome this challenge, we propose a novel approach in this paper. Our approach involves utilizing multi-output embedding network that maps samples into distinct feature spaces. The proposed method extract feature vectors at different stages, enabling the model to capture both global and abstract features. By utilizing these diverse feature spaces, our model enhances its performance. Moreover, employing a self-attention mechanism improves the refinement of features at each stage, leading to even more robust representations and improved overall performance. Furthermore, assigning learnable weights to each stage significantly improved performance and results. We conducted comprehensive evaluations on the MiniImageNet and FC100 datasets, specifically in the 5-way 1-shot and 5-way 5-shot scenarios. Additionally, we performed a cross-domain task from MiniImageNet to the CUB dataset, achieving high accuracy in the testing domain. These evaluations demonstrate the efficacy of our proposed method in comparison to state-of-the-art approaches. https://github.com/FatemehAskari/MSENet\n\n**Published:** 2024-09-12\n\n\n\n#### 6. Frame-Level Multi-Label Playing Technique Detection Using Multi-Scale Network and Self-Attention Mechanism\n\n*From Search Query: multi-scale attention mechanisms*\n\n*Wei Li, Fan Xia, Yi Yu, Yulun Wu, Wenwu Meng, Mingjin Che, Dichucheng Li*\n\n**Abstract:** Instrument playing technique (IPT) is a key element of musical presentation. However, most of the existing works for IPT detection only concern monophonic music signals, yet little has been done to detect IPTs in polyphonic instrumental solo pieces with overlapping IPTs or mixed IPTs. In this paper, we formulate it as a frame-level multi-label classification problem and apply it to Guzheng, a Chinese plucked string instrument. We create a new dataset, Guzheng\\_Tech99, containing Guzheng recordings and onset, offset, pitch, IPT annotations of each note. Because different IPTs vary a lot in their lengths, we propose a new method to solve this problem using multi-scale network and self-attention. The multi-scale network extracts features from different scales, and the self-attention mechanism applied to the feature maps at the coarsest scale further enhances the long-range feature extraction. Our approach outperforms existing works by a large margin, indicating its effectiveness in IPT detection.\n\n**Published:** 2023-03-23\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical memory organization, adaptive compression techniques, and their integration with attention mechanisms, here are some relevant insights and references:\n\n## Hierarchical Memory Organization and Compression\n\n### Hierarchical Neural Codec Language Model\nThe paper on HALL-E introduces a hierarchical neural codec language model that incorporates multi-resolution residual vector quantization (MRVQ) modules. This approach hierarchically reorganizes discrete audio tokens, which can be analogously applied to text sequences. The MRVQ module reduces the frame rate at lower quantization layers, enabling more efficient handling of long contexts. This hierarchical organization can be adapted for text data, potentially improving memory efficiency and compression in language models.\n\n### Hierarchical Mixed-Precision Post-Training Quantization\nThe work on hierarchical mixed-precision post-training quantization for neural networks discusses methods to compress neural networks by quantizing weights and activations to lower precision. This approach can be extended to language models, where hierarchical quantization can help in reducing the memory footprint while maintaining performance. The use of mixed-precision quantization can be seen as a form of adaptive compression, where different layers or components of the model are quantized to different precision levels based on their sensitivity and importance.\n\n## Adaptive Compression Techniques\n\n### Learning both Weights and Connections for Efficient Neural Networks\nThe concept of pruning and quantization discussed in the guide to advances in deep learning efficiency can be applied to language models. Techniques like the lottery ticket hypothesis and SynFlow suggest that neural networks can be significantly pruned without losing much performance, which can be adapted for adaptive compression in language models. These methods involve identifying and retaining only the most important weights and connections, thereby reducing the overall memory and computational requirements.\n\n## Integration with Attention Mechanisms\n\n### Recursive Neural Networks and Hierarchical Attention\nRecursive neural networks, which excel at processing hierarchical data, can be integrated with attention mechanisms to improve the handling of long-range dependencies. By applying the same set of weights recursively to a variable-sized input, these networks can capture hierarchical structures effectively. Combining this with attention mechanisms, such as geometric attention or sparse attention, can enhance the model's ability to focus on relevant parts of the input sequence while maintaining efficiency.\n\n### Dynamic Routing and Attention Mechanisms\nThe Neural Data Router (NDR) approach, which introduces copy gate and geometric attention mechanisms, can be integrated with hierarchical memory organization. These mechanisms facilitate selective information flow and provide an inductive bias for routing, which can be crucial for hierarchical processing. By combining these with adaptive compression techniques, the model can efficiently manage memory and improve performance on long sequences.\n\n## Key Insights for Design\n\n- **Hierarchical Processing**: Implement hierarchical memory organization using techniques like MRVQ or mixed-precision quantization to reduce memory footprint and improve efficiency.\n- **Adaptive Compression**: Use pruning and quantization methods to adaptively compress the model, retaining only the most important weights and connections.\n- **Attention Integration**: Combine hierarchical processing with attention mechanisms such as geometric attention or sparse attention to enhance the model's ability to handle long-range dependencies efficiently.\n\nBy integrating these concepts, researchers can develop autoregressive language models that are more efficient, scalable, and performant, especially in handling long sequences and complex hierarchical data structures.\n",
            "analysis": "### Analysis Summary\n\nAfter reviewing the search results, I've identified several promising directions for improving the seed design through dynamic routing, memory compression, and hybrid attention mechanisms:\n\n1. **Dynamic Routing Innovations**:\n   - Neural Data Router (NDR) approach shows potential for improving systematic generalization\n   - Copy gate and geometric attention mechanisms could enhance information flow\n   - Routing decisions can be made content-dependent using learned parameters\n\n2. **Memory Management Approaches**:\n   - KNN-based memory mechanisms show promise for efficient information retrieval\n   - Lossless compression techniques can reduce memory footprint\n   - Hierarchical memory organization could improve efficiency\n\n3. **Attention Mechanism Improvements**:\n   - Softmax-free approaches using Gaussian kernels offer theoretical advantages\n   - Hybrid attention combining multiple mechanisms could enhance performance\n   - Attention alignment strategies are crucial for long sequences\n\n### Key References and Excerpts\n\n1. From \"The Neural Data Router\":\n```\nTo facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention... NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing.\n```\n- Introduces copy gate mechanism for selective information flow\n- Geometric attention provides inductive bias for routing\n\n2. From \"Efficient Content-Based Sparse Attention\":\n```\nOur work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest.\n```\n- Demonstrates efficient sparse attention implementation\n- Shows how to reduce memory and computation requirements\n\n3. From \"Memorizing Transformers\":\n```\nWe extend language models with the ability to memorize the internal representations of past inputs... demonstrates that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling.\n```\n- Provides framework for external memory integration\n- Shows benefits of non-differentiable memory components\n\n### Key Insights for Design\n\n1. **Dynamic Routing Architecture**:\n   - Copy gate mechanism could be adapted for hierarchical processing\n   - Geometric attention could improve information flow between streams\n   - Content-dependent routing decisions could enhance adaptability\n\n2. **Memory Integration**:\n   - KNN-based memory could complement hierarchical processing\n   - Non-differentiable memory components could reduce overhead\n   - Hierarchical memory organization could improve efficiency\n\n3. **Attention Mechanisms**:\n   - Combining sparse and dense attention could balance efficiency and expressiveness\n   - Geometric attention could provide better inductive bias\n   - Memory-augmented attention could enhance long-range dependencies\n\n### Future Search Plan\n\nNeed to investigate:\n\n1. **Memory Compression Techniques**:\n   - Specific methods for compressing key-value pairs\n   - Adaptive compression strategies\n   - Impact on model performance\n\n2. **Hierarchical Processing**:\n   - Methods for combining multiple scales of processing\n   - Information flow between hierarchical levels\n   - Efficient implementation strategies\n\n3. **Attention Integration**:\n   - Ways to combine different attention mechanisms\n   - Efficient implementation of hybrid attention\n   - Impact on model performance\n\n### References\n\n1. Csord\u00e1s, R., Irie, K., & Schmidhuber, J. (2021). The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization. International Conference on Learning Representations.\n\n2. Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). Efficient Content-Based Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics.\n\n3. Wu, Y., Rabe, M., Hutchins, D. S., & Szegedy, C. (2022). Memorizing Transformers. International Conference on Learning Representations.\n\n4. Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., ... & Zhang, L. (2021). SOFT: Softmax-free Transformer with Linear Complexity. Neural Information Processing Systems.\n\n5. Chi, T. C., Fan, T. H., & Rudnicky, A. I. (2023). Attention Alignment and Flexible Positional Embeddings Improve Transformer Length Extrapolation. arXiv preprint."
        },
        {
            "ready": false,
            "query": "adaptive routing neural networks, memory update mechanisms, hierarchical integration methods",
            "detail": "Show me papers discussing adaptive routing mechanisms in neural networks, particularly focusing on content-dependent routing and dynamic path selection. Include information about memory update strategies and their integration with hierarchical processing.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nShow me papers discussing adaptive routing mechanisms in neural networks, particularly focusing on content-dependent routing and dynamic path selection. Include information about memory update strategies and their integration with hierarchical processing.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Block-Recurrent Transformers (Avg. Score: 0.39)\n\n*DeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, Behnam Neyshabur*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 66  (*Influential: 9*)\n\n**TL;DR:** N/A\n\n**Abstract:** We introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to sequence length. Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block in order to make efficient use of accelerator hardware. The cell itself is strikingly simple. It is merely a transformer layer: it uses self-attention and cross-attention to efficiently compute a recurrent function over a large set of state vectors and tokens. Our design was inspired in part by LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM cell up by several orders of magnitude. Our implementation of recurrence has the same cost in both computation time and parameter count as a conventional transformer layer, but offers dramatically improved perplexity in language modeling tasks over very long sequences. Our model out-performs a long-range Transformer XL baseline by a wide margin, while running twice as fast. We demonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source code. Our code has been released as open source.\n\n##### *Relevant Chunk: No. 30/53 (Score: 0.39)*\n\n```\nCsord\u00e1s, K. Irie, and J. Schmidhuber, \"The neural data router: Adaptive control flow in transformers improves systematic generalization,\" in International Conference on Learning Representations, 2022. [38] K.\n```\n\n#### 2. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization  (Avg. Score: 0.08)\n\n*R. Csord\u00e1s, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 42  (*Influential: 7*)\n\n**TL;DR:** This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.\n\n**Abstract:** Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.\n\n##### *Relevant Chunk: No. 1/47 (Score: 0.11)*\n\n```\n# The Neural Data RoutER: ADAPTIVE CONTROL Flow in TranSFORMERS IMPROVES SYSTEMATIC GENERALIZATION \n\nR\u00f3bert Csord\u00e1s ${ }^{1}$ Kazuki Irie ${ }^{1}$ J\u00fcrgen Schmidhuber ${ }^{1,2}$<br>${ }^{1}$ The Swiss AI Lab, IDSIA, University of Lugano (USI) \\& SUPSI, Lugano, Switzerland<br>${ }^{2}$ King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia<br>\\{robert, kazuki, juergen\\}@idsia.ch\n\n\n#### Abstract\n\nDespite progress across a broad range of applications, Transformers have limited success in systematic generalization.\n```\n\n##### *Relevant Chunk: No. 2/47 (Score: 0.05)*\n\n```\nThe situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100\\% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public $1^{1}$\n\n\n## 1 INTRODUCTION\n\nNeural networks (NNs) may easily learn certain training sets, but typically they do not generalize on systematically different test sets. Examples of systematic generalization (Fodor et al, 1988)\ninclude generalization to sequences longer than those seen during training-productivity, and algorithmic combinations of previously learned rules-systematicity. Despite recent efforts (Bahdanau et al., 2019, Korrel et al. 2019, Lake, 2019, Li et al., 2019, Russin et al., 2019, Csord\u00e1s et al., 2021), systematic generalization generally remains unsolved (Fodor \\& McLaughlin, 1990, Lake \\& Baroni, 2018, Liska et al., 2018, Greff et al., 2020, Hupkes et al., 2020). On some datasets, the best performing models are neuro-symbolic hybrids (Chen et al.| 2020, Liu et al. 2020) using task-specific symbolic functions. However, their applicability to other datasets remains limited (Furrer et al. 2020, Shaw et al., 2020). A big question is: which type of architectural inductive bias encourages the training process to select \"good\" solutions which generalize systematically? The popular Transformers (Vaswani et al. 2017) also often fail to generalize on algorithmic tasks (e.g. Liska et al. (2018); Dubois et al. (2020); Chaabouni et al. (2021); Csord\u00e1s et al. (2021); Onta\u00f1\u00f3n et al.(2021)), even on tasks with intuitive solutions that can be simply expressed in terms of Transformer attention patterns. Given an input sequence of length $N$ and a Transformer encoder of depth $T$, solving an algorithmic task is often all about routing the relevant information to the right node/operation at the right time in the $T$-by- $N$ grid represented by Transformer columns (illustrated in Figure 11(Left). Effectively the task is to learn to draw an adaptive control flow on the canvas of Transformer columns. In fact, recent work by Weiss et al. (2021) introduced a programming language called RASP, which is specifically designed to express solutions to sequence processing problems, and which has a direct equivalent to the operations in Transformer encoders. However, it is shown that Transformers learn solutions expressed in RASP only through intermediate supervision of attention patterns, and sometimes, even such supervision fails. Generally speaking, Transformers fail to find easily interpretable and/or symbolic solutions to algorithmic tasks. We conversely hypothesize that attention-based NNs that are able to find intuitive solutions (achieving interpretable attention patterns) could improve systematic generalization. [^0]Here we point out that regular Transformers lack some basic ingredients for learning such \"intuitive\" solutions to algorithmic problems. As a remedy, we propose simple architectural modifications to help them learn data routing. As a first step towards validating our model, we focus on the popular length generalization task of compositional table lookup (CTL; Liska et al. (2018); Hupkes et al. (2019); Dubois et al. (2020), as well as two more complex tasks: a simple arithmetic task and a variant of ListOps (Nangia \\& Bowman, 2018) designed to test the compositional generalization ability of NNs. Our novel Neural Data Router (NDR) achieves 100\\% generalization accuracy (never reported before; Dubois et al. (2020)) on the CTL task, and obtains nearly perfect accuracy on both the proposed simple arithmetic and ListOps tasks. We show that the attention and gating patterns of NDR tend to be interpretable as plausible control flows. ## 2 Improving Transformers FOR LEARning Adaptive Control Flow\n\nWe argue that the following components are needed to build Transformers capable of learning adaptive control flow. First, composing known operations in an arbitrary order requires that all operations are available at every computational step. This can be easily achieved by sharing the weights of the layers, as is done in Universal Transformers (Dehghani et al. 2019). Second, the network should be sufficiently deep, at least as deep as the deepest data dependency in the computational graph built from elementary operations (e.g., in the case of a parse tree, this is the depth of the tree). Otherwise, multiple operations must be fused into a single layer and hinder natural and elegant compositions. Third, inputs in some columns should be kept unchanged until it is their turn to be processed. The regular Transformer lacks a mechanism for skipping the whole transformation step by simply copying the input to the next step/layer. We propose a special gating function, copy gate, to implement such a mechanism (Sec.\n```\n\n#### 3. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.05)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 2/50 (Score: 0.05)*\n\n```\nDespite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows selfattention with a sparse routing module based on online $k$-means while reducing the overall complexity of attention to $O\\left(n^{1.5} d\\right)$ from $O\\left(n^{2} d\\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs $3.44 \\mathrm{bits} / \\mathrm{dim}$ ) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow. *\n\n\n## 1 Introduction\n\nGenerative models of sequences have witnessed rapid progress driven by the application of attention to neural networks. In particular, Bahdanau et al. (2015); Cho et al. (2014); Vaswani et al. (2017) relied on attention to drastically improve the state-of-the art in machine translation. Subsequent research (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) demonstrated the power of\n\n[^0]self-attention in learning powerful representations of language to address several natural language processing tasks. Self-attention also brought impressive progress for generative modeling outside of language, e.g. image (Parmar et al., 2018; Menick and Kalchbrenner, 2018; Child et al., 2019) and music generation (Huang et al., 2018; Child et al., 2019). Self-attention operates over sequences in a stepwise manner: at every time-step, attention assigns an attention weight to each previous input element (representation of past time-steps) and uses these weights to compute the representation of the current time-step as a weighted sum of the past input elements (Vaswani et al., 2017). Self-attention (Shaw et al., 2018) is a particular case of attention (Bahdanau et al., 2015; Chorowski et al., 2015; Luong et al., 2015). Self-attention is commonly used in autoregressive generative models. These models generate observations step-by-step, modeling the probability of the next symbol given the previously generated ones. At every time step, self-attentive generative models can directly focus on any part of the previous context. In contrast, recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have direct interactions with only a local neighborhood of context around the current time step. This advantage however comes at a price: unlike recurrent networks or convolution networks, the time and space complexity of self-attention is quadratic in $n$, the length of the sequence. Specifically, for every position $i \\leq n$, self-attention computes weights for its whole context of length $i$, which induces a complexity of $\\sum_{i \\leq n} i=n(n-1) / 2$. This makes it difficult to scale attention based models to modeling long sequences. However, long sequences are the norm in many domains, including music, image, speech, video generation and document level machine translation. Therefore, an important research direction is to investigate sparse and memory efficient forms of attention in order to scale to tasks with large sequence lengths. Previous work has proposed data independent or fixed sparsity patterns bounding temporal\ndependencies, such as local or strided attention. At each time step, the model attends only to a fixed number of time steps in the past (Child et al., 2019). Extensions to local attention have suggested learning the length of the temporal sparsity for each attention module in the network (Sukhbaatar et al., 2019). These strategies draw their inspiration from RNNs and CNNs and bound their complexity by attending only to representations summarizing a local neighborhood of the current time step. Their attention matrices (matrices containing the attention weights for every pair of previous, current timestep) are natively sparse and require instantiating only non-zero entries. While these approaches have achieved good results, fixing the sparsity pattern of a content based mechanism such as self-attention can limit its ability to pool in information from large contexts. As an alternative to local attention, Correia et al. (2019) consider content-based sparsity, an approach allowing for arbitrary sparsity patterns. This formulation however does require instantiating a full dense attention matrix prior to sparsification through variants of $L_{0}$-sparsity or sparsemax approximations (Blondel et al., 2019). The present work builds upon these two lines of research and proposes to retain the modeling flexibility of content-based sparse attention while leveraging the efficiency of natively sparse attention matrices. Our formulation avoids sparsemax variants and relies on clustering of attention instead. Each attention module considers a clustering of the space: the current time-step only attends to context belonging to the same cluster. In other words, the current time-step query is routed to a limited number of context elements through its cluster assignment. This strategy draws inspiration from the application of spherical $k$-means clustering to the Maximum Inner Product Search (MIPS) problem. Our proposed model, Routing Transformer, combines our efficient clustering-based sparse attention with classical local attention to reach excellent performance both for language and image generation.\n```\n\n#### 4. Ordered Memory  (Avg. Score: 0.05)\n\n*Daniel Borisov, Matthew D\u2019Iorio, Jeffrey Hyacinthe*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.\n\n**Abstract:** Natural language semantics can be modeled using the phrase-structured model, which can be represented using a tree-type architecture. As a result, recent advances in natural language processing have been made utilising recursive neural networks using memory models that allow them to infer tree-type representations of the input sentence sequence. These new tree models have allowed for improvements in sentiment analysis and semantic recognition. Here we review the Ordered Memory model proposed by Shen et al. (2019) at the NeurIPS 2019 conference, and try to either create baselines that can perform better or create simpler models that can perform equally as well. We found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.\n\n##### *Relevant Chunk: No. 17/24 (Score: 0.05)*\n\n```\nIn Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University, page 14, 1992. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. David Dowty. 4. Direct compositionality, 14:23-101, 2007. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 199-209, 2016. Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990. Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193-202, 1980. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv: $1410.5401,2014$. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems, pages $1828-1836,2015$. Caglar Gulcehre, Sarath Chandar, and Yoshua Bengio. Memory augmented neural networks with wormhole connections.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive routing neural networks, memory update mechanisms, hierarchical integration methods\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning\n\n*From Search Query: adaptive routing neural networks*\n\n*C. Rosenbaum, Tim Klinger, M. Riemer*\n\n**TL;DR:** A collaborative multi-agent reinforcement learning (MARL) approach is employed to jointly train the router and function blocks of a routing network, a kind of self-organizing neural network consisting of a router and a set of one or more function blocks.\n\n**Abstract:** Multi-task learning (MTL) with neural networks leverages commonalities in tasks to improve performance, but often suffers from task interference which reduces the benefits of transfer. To address this issue we introduce the routing network paradigm, a novel neural network and training algorithm. A routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks. A function block may be any neural network - for example a fully-connected or a convolutional layer. Given an input the router makes a routing decision, choosing a function block to apply and passing the output back to the router recursively, terminating when a fixed recursion depth is reached. In this way the routing network dynamically composes different function blocks for each input. We employ a collaborative multi-agent reinforcement learning (MARL) approach to jointly train the router and function blocks. We evaluate our model against cross-stitch networks and shared-layer baselines on multi-task settings of the MNIST, mini-imagenet, and CIFAR-100 datasets. Our experiments demonstrate a significant improvement in accuracy, with sharper convergence. In addition, routing networks have nearly constant per-task training cost while cross-stitch networks scale linearly with the number of tasks. On CIFAR-100 (20 tasks) we obtain cross-stitch performance levels with an 85% reduction in training time.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2017\n\n**Citations:** 229  (*Influential: 21*)\n\n#### 2. Adaptive Neural Trees\n\n*From Search Query: adaptive routing neural networks*\n\n*Ryutaro Tanno, Kai Arulkumaran, D. Alexander, A. Criminisi, A. Nori*\n\n**TL;DR:** Adapt neural trees via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers).\n\n**Abstract:** Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 156  (*Influential: 17*)\n\n#### 3. Scaling Vision with Sparse Mixture of Experts\n\n*From Search Query: adaptive routing neural networks*\n\n*C. Riquelme, J. Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, N. Houlsby*\n\n**TL;DR:** This work presents a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks, when applied to image recognition, while requiring as little as half of the compute at inference time.\n\n**Abstract:** Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are\"dense\", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 456  (*Influential: 38*)\n\n#### 4. A Framework for Inference Inspired by Human Memory Mechanisms\n\n*From Search Query: memory update mechanisms*\n\n*Xiangyu Zeng, Jie Lin, Piao Hu, Ruizheng Huang, Zhicheng Zhang*\n\n**TL;DR:** Inspired by human brain's memory system and cognitive architectures, a PMI framework that consists of perception, memory and inference components is proposed that substantially contributes to the model effectiveness on inference tasks.\n\n**Abstract:** How humans and machines make sense of current inputs for relation reasoning and question-answering while putting the perceived information into context of our past memories, has been a challenging conundrum in cognitive science and artificial intelligence. Inspired by human brain's memory system and cognitive architectures, we propose a PMI framework that consists of perception, memory and inference components. Notably, the memory module comprises working and long-term memory, with the latter endowed with a higher-order structure to retain extensive and complex relational knowledge and experience. Through a differentiable competitive write access, current perceptions update working memory, which is later merged with long-term memory via outer product associations, reducing information conflicts and averting memory overflow. In the inference module, relevant information is retrieved from two separate memory origins and associatively integrated to attain a more comprehensive and precise interpretation of current perceptions. We exploratively apply our PMI to improve prevailing Transformers and CNN models on question-answering tasks like bAbI-20k and Sort-of-CLEVR datasets, as well as detecting equilateral triangles, language modeling and image classification tasks, and in each case, our PMI enhancements consistently outshine their original counterparts significantly. Visualization analyses reveal that relational memory consolidation, along with the interaction and integration of information from diverse memory sources, substantially contributes to the model effectiveness on inference tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: memory update mechanisms*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 6. Learning to Learn Variational Semantic Memory\n\n*From Search Query: memory update mechanisms*\n\n*Xiantong Zhen, Yingjun Du, Huan Xiong, Qiang Qiu, Cees G. M. Snoek, Ling Shao*\n\n**TL;DR:** Variational semantic memory is introduced into meta-learning to acquire long-term knowledge for few-shot learning and confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning.\n\n**Abstract:** In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the benefit of variational semantic memory in boosting few-shot recognition.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 7. Enhancing Hierarchical Text Classification through Knowledge Graph Integration\n\n*From Search Query: hierarchical integration methods*\n\n*Ye Liu, Kai Zhang, Zhenya Huang, Kehang Wang, Yanghai Zhang, Qi Liu, Enhong Chen*\n\n**TL;DR:** This work proposes a novel Knowledge-enabled Hierarchical Text Classification model (K-HTC), which incorporates knowledge graphs into HTC and innovatively integrates knowledge into both the text representation and hierarchical label learning process, addressing the knowledge limitations of traditional methods.\n\n**Abstract:** Hierarchical Text Classification (HTC) is an essential and challenging subtask of multi-label text classification with a taxonomic hierarchy. Recent advances in deep learning and pre-trained language models have led to significant breakthroughs in the HTC problem. However, despite their effectiveness, these methods are often restricted by a lack of domain knowledge, which leads them to make mistakes in a variety of situations. Generally, when manually classifying a specific document to the taxonomic hierarchy, experts make inference based on their prior knowledge and experience. For machines to achieve this capability, we propose a novel Knowledge-enabled Hierarchical Text Classification model (K-HTC), which incorporates knowledge graphs into HTC. Specifically, K-HTC innovatively integrates knowledge into both the text representation and hierarchical label learning process, addressing the knowledge limitations of traditional methods. Additionally, a novel knowledge-aware contrastive learning strategy is proposed to further exploit the information inherent in the data. Extensive experiments on two publicly available HTC datasets show the efficacy of our proposed method, and indicate the necessity of incorporating knowledge graphs in HTC tasks.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 8. Hi-ArG: Exploring the Integration of Hierarchical Argumentation Graphs in Language Pretraining\n\n*From Search Query: hierarchical integration methods*\n\n*Jingcong Liang, Rong Ye, Meng Han, Qi Zhang, Ruofei Lai, Xinyu Zhang, Zhao Cao, Xuanjing Huang, Zhongyu Wei*\n\n**TL;DR:** This paper proposes the Hierarchical Argumentation Graph (Hi-ArG), a new structure to organize arguments, and introduces two approaches to exploit Hi-G, including a text-graph multi-modal model GreaseArG and a new pre-training framework augmented with graph information.\n\n**Abstract:** The knowledge graph is a structure to store and represent knowledge, and recent studies have discussed its capability to assist language models for various applications. Some variations of knowledge graphs aim to record arguments and their relations for computational argumentation tasks. However, many must simplify semantic types to fit specific schemas, thus losing flexibility and expression ability. In this paper, we propose the Hierarchical Argumentation Graph (Hi-ArG), a new structure to organize arguments. We also introduce two approaches to exploit Hi-ArG, including a text-graph multi-modal model GreaseArG and a new pre-training framework augmented with graph information. Experiments on two argumentation tasks have shown that after further pre-training and fine-tuning, GreaseArG supersedes same-scale language models on these tasks, while incorporating graph information during further pre-training can also improve the performance of vanilla language models. Code for this paper is available at https://github.com/ljcleo/Hi-ArG .\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 9. Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration\n\n*From Search Query: hierarchical integration methods*\n\n*Xinmeng Li, Qian Li, Wansen Wu, Quanjun Yin*\n\n**TL;DR:** In state value decoding stage, the model explores the hierarchical semantic of ontology and enhances the interrelation between slots with masked hierarchical attention and solves the out-of-vocabulary problem by combining generation method and extraction method together.\n\n**Abstract:** Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantic of ontology and enhance the interrelation between slots with masked hierarchical attention. In state value decoding stage, we solve the out-of-vocabulary problem by combining generation method and extraction method together. We evaluate the performance of our model on two representative datasets, MultiWOZ in English and CrossWOZ in Chinese. The results show that our model yields a significant performance gain over current state-of-the-art state tracking model and it is more robust to out-of-vocabulary problem compared with other methods.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 9  (*Influential: 1*)\n\n### 2 related papers from ArXiv\n\n#### 1. Recurrent Entity Networks with Delayed Memory Update for Targeted\n  Aspect-based Sentiment Analysis\n\n*From Search Query: memory update mechanisms*\n\n*Fei Liu, Trevor Cohn, Timothy Baldwin*\n\n**Abstract:** While neural networks have been shown to achieve impressive results for\nsentence-level sentiment analysis, targeted aspect-based sentiment analysis\n(TABSA) --- extraction of fine-grained opinion polarity w.r.t. a pre-defined\nset of aspects --- remains a difficult task. Motivated by recent advances in\nmemory-augmented models for machine reading, we propose a novel architecture,\nutilising external \"memory chains\" with a delayed memory update mechanism to\ntrack entities. On a TABSA task, the proposed model demonstrates substantial\nimprovements over state-of-the-art approaches, including those using external\nknowledge bases.\n\n**Published:** 2018-04-30T01:57:31Z  (*Updated: 2018-04-30T01:57:31Z*)\n\n\n\n#### 2. MemoryBank: Enhancing Large Language Models with Long-Term Memory\n\n*From Search Query: memory update mechanisms*\n\n*Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang*\n\n**Abstract:** Revolutionary advancements in Large Language Models have drastically reshaped\nour interactions with artificial intelligence systems. Despite this, a notable\nhindrance remains-the deficiency of a long-term memory mechanism within these\nmodels. This shortfall becomes increasingly evident in situations demanding\nsustained interaction, such as personal companion systems and psychological\ncounseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored\nfor LLMs. MemoryBank enables the models to summon relevant memories,\ncontinually evolve through continuous memory updates, comprehend, and adapt to\na user personality by synthesizing information from past interactions. To mimic\nanthropomorphic behaviors and selectively preserve memory, MemoryBank\nincorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting\nCurve theory, which permits the AI to forget and reinforce memory based on time\nelapsed and the relative significance of the memory, thereby offering a\nhuman-like memory mechanism. MemoryBank is versatile in accommodating both\nclosed-source models like ChatGPT and open-source models like ChatGLM. We\nexemplify application of MemoryBank through the creation of an LLM-based\nchatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned\nwith psychological dialogs, SiliconFriend displays heightened empathy in its\ninteractions. Experiment involves both qualitative analysis with real-world\nuser dialogs and quantitative analysis with simulated dialogs. In the latter,\nChatGPT acts as users with diverse characteristics and generates long-term\ndialog contexts covering a wide array of topics. The results of our analysis\nreveal that SiliconFriend, equipped with MemoryBank, exhibits a strong\ncapability for long-term companionship as it can provide emphatic response,\nrecall relevant memories and understand user personality.\n\n**Published:** 2023-05-17T14:40:29Z  (*Updated: 2023-05-21T06:20:28Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models\n\n*From Search Query: adaptive routing neural networks*\n\n*Rongrong Ji, Xiaoshuai Sun, Shengxin Chen, Tianhe Ren, Yiyi Zhou, Gen Luo*\n\n**Abstract:** Recently, growing interest has been aroused in extending the multimodal capability of large language models (LLMs), e.g., vision-language (VL) learning, which is regarded as the next milestone of artificial general intelligence. However, existing solutions are prohibitively expensive, which not only need to optimize excessive parameters, but also require another large-scale pre-training before VL instruction tuning. In this paper, we propose a novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA). Instead of using large neural networks to connect the image encoder and LLM, MMA adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models. Meanwhile, MMA is also equipped with a routing algorithm to help LLMs achieve an automatic shift between single- and multi-modal instructions without compromising their ability of natural language understanding. To validate MMA, we apply it to a recent LLM called LLaMA and term this formed large vision-language instructed model as LaVIN. To validate MMA and LaVIN, we conduct extensive experiments under two setups, namely multimodal science question answering and multimodal dialogue. The experimental results not only demonstrate the competitive performance and the superior training efficiency of LaVIN than existing multimodal LLMs, but also confirm its great potential as a general-purpose chatbot. More importantly, the actual expenditure of LaVIN is extremely cheap, e.g., only 1.4 training hours with 3.8M trainable parameters, greatly confirming the effectiveness of MMA. Our project is released at https://luogen1996.github.io/lavin.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-24\n\n\n\n#### 2. Avoiding Catastrophe: Active Dendrites Enable Multi-Task Learning in Dynamic Environments\n\n*From Search Query: adaptive routing neural networks*\n\n*Subutai Ahmad, Jeremy Forest, Lucas Oliveira Souza, Akash Velu, Karan Grewal, Abhiram Iyer*\n\n**Abstract:** A key challenge for AI is to build embodied systems that operate in dynamically changing environments. Such systems must adapt to changing task contexts and learn continuously. Although standard deep learning systems achieve state of the art results on static benchmarks, they often struggle in dynamic scenarios. In these settings, error signals from multiple contexts can interfere with one another, ultimately leading to a phenomenon known as catastrophic forgetting. In this article we investigate biologically inspired architectures as solutions to these problems. Specifically, we show that the biophysical properties of dendrites and local inhibitory systems enable networks to dynamically restrict and route information in a context-specific manner. Our key contributions are as follows. First, we propose a novel artificial neural network architecture that incorporates active dendrites and sparse representations into the standard deep learning framework. Next, we study the performance of this architecture on two separate benchmarks requiring task-based adaptation: Meta-World, a multi-task reinforcement learning environment where a robotic agent must learn to solve a variety of manipulation tasks simultaneously; and a continual learning benchmark in which the model's prediction task changes throughout training. Analysis on both benchmarks demonstrates the emergence of overlapping but distinct and sparse subnetworks, allowing the system to fluidly learn multiple tasks with minimal forgetting. Our neural implementation marks the first time a single architecture has achieved competitive results on both multi-task and continual learning settings. Our research sheds light on how biological properties of neurons can inform deep learning systems to address dynamic scenarios that are typically impossible for traditional ANNs to solve.\n\n**Published:** 2021-12-31\n\n\n\n#### 3. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n\n*From Search Query: memory update mechanisms*\n\n*Aleksandra Piktus, Tim Rockt\u00e4schel, Wen-tau Yih, Heinrich K\u00fcttler, Patrick Lewis, Sebastian Riedel, Mike Lewis, Vladimir Karpukhin, Ethan Perez, Naman Goyal, Fabio Petroni, Douwe Kiela*\n\n**Abstract:** Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-05-22\n\n\n\n#### 4. Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks\n\n*From Search Query: memory update mechanisms*\n\n*Shi-Min Hu, Tai-Jiang Mu, Zheng-Ning Liu, Meng-Hao Guo*\n\n**Abstract:** Attention mechanisms, especially self-attention, have played an increasingly important role in deep feature representation for visual tasks. Self-attention updates the feature at each position by computing a weighted sum of features using pair-wise affinities across all positions to capture the long-range dependency within a single sample. However, self-attention has quadratic complexity and ignores potential correlation between different samples. This paper proposes a novel attention mechanism which we call external attention, based on two external, small, learnable, shared memories, which can be implemented easily by simply using two cascaded linear layers and two normalization layers; it conveniently replaces self-attention in existing popular architectures. External attention has linear complexity and implicitly considers the correlations between all data samples. We further incorporate the multi-head mechanism into external attention to provide an all-MLP architecture, external attention MLP (EAMLP), for image classification. Extensive experiments on image classification, object detection, semantic segmentation, instance segmentation, image generation, and point cloud analysis reveal that our method provides results comparable or superior to the self-attention mechanism and some of its variants, with much lower computational and memory costs.\n\n**Published:** 2021-05-05\n\n\n\n#### 5. Hierarchical Optimization Time Integration for CFL-rate MPM Stepping\n\n*From Search Query: hierarchical integration methods*\n\n*Chenfanfu Jiang, Danny M. Kaufman, Min Tang, Ming Gao, Xinxin Zhang, Yu Fang, Minchen Li, Xinlei Wang*\n\n**Abstract:** We propose Hierarchical Optimization Time Integration (HOT) for efficient implicit time-stepping of the Material Point Method (MPM) irrespective of simulated materials and conditions. HOT is an MPM-specialized hierarchical optimization algorithm that solves nonlinear time step problems for large-scale MPM systems near the CFL-limit. HOT provides convergent simulations \"out-of-the-box\" across widely varying materials and computational resolutions without parameter tuning. As an implicit MPM time stepper accelerated by a custom-designed Galerkin multigrid wrapped in a quasi-Newton solver, HOT is both highly parallelizable and robustly convergent. As we show in our analysis, HOT maintains consistent and efficient performance even as we grow stiffness, increase deformation, and vary materials over a wide range of finite strain, elastodynamic and plastic examples. Through careful benchmark ablation studies, we compare the effectiveness of HOT against seemingly plausible alternative combinations of MPM with standard multigrid and other Newton-Krylov models. We show how these alternative designs result in severe issues and poor performance. In contrast, HOT outperforms the existing state-of-the-art, heavily optimized implicit MPM codes with an up to 10x performance speedup across a wide range of challenging benchmark test simulations.\n\n**Published:** 2019-11-18\n\n\n\n#### 6. Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth\n\n*From Search Query: hierarchical integration methods*\n\n*Woonghyun Ka, Junmo Kim, Sehwan Chun, Donggyu Joo, Pyungwhan Ahn, Doyeon Kim*\n\n**Abstract:** Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.\n\n**Published:** 2022-01-19\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive routing mechanisms, memory update strategies, and hierarchical integration methods, here are some relevant insights and references:\n\n### Adaptive Routing Mechanisms\n\nWhile the provided sources do not directly discuss adaptive routing within the context of neural networks for language models, we can draw parallels from other domains and adapt these concepts:\n\n1. **Graph Neural Networks (GNNs) for Routing Optimization**:\n   - The paper on GNNs for routing optimization discusses dynamic and adaptive routing methods, although in the context of communication networks. These techniques, such as reinforcement learning for routing optimization, can be conceptually applied to neural networks. For example, using GNNs to dynamically adjust the flow of information within a neural network based on the content and context could be a promising approach.\n\n2. **Dynamic Path Selection**:\n   - The concept of dynamic path selection can be inspired by the adaptive routing mechanisms used in network optimization. For instance, using attention mechanisms that dynamically select paths based on the input content can be seen as a form of adaptive routing. This is similar to the multi-scale attention mechanisms discussed in the analysis summary, which can be adapted to select different paths or focus on different parts of the input sequence based on its content.\n\n### Memory Update Mechanisms\n\n1. **Recurrent Memory Transformer (RMT)**:\n   - The RMT model mentioned in the analysis summary demonstrates effective segment-level processing and memory updates. This model uses recurrence to pass information between segments of long sequences, which can be a powerful strategy for updating memory in autoregressive language models.\n\n2. **Lossless Compressed Memory Attention (LoMA)**:\n   - LoMA introduces a method for lossless compression of the KV cache, which can be integrated with memory update mechanisms to ensure efficient and lossless updates of the memory. This approach can help in maintaining the integrity of the information stored in the memory.\n\n### Hierarchical Integration Methods\n\n1. **Hierarchical Memory Organization**:\n   - Implementing hierarchical memory with multiple compression levels, as suggested in the analysis summary, can be highly effective. This involves using different levels of compression and memory organization to handle different types of information efficiently. For example, using lossless compression for critical information and adaptive compression for less important data.\n\n2. **Multi-Scale Attention Mechanisms**:\n   - The IIANet model, which leverages intra- and inter-modality attention, demonstrates how to maintain the ability to learn modality-specific features and extract different semantics at various temporal scales. This multi-scale attention approach can be integrated into a hierarchical processing framework to ensure that different levels of the model focus on different aspects of the input.\n\n### Integration Strategies\n\n1. **Combining Hierarchical Processing with Adaptive Routing**:\n   - By integrating adaptive routing mechanisms with hierarchical processing, the model can dynamically select the most relevant paths and focus on different parts of the input sequence based on its content. This can be achieved by using attention mechanisms that adaptively route information through different layers or modules of the hierarchical model.\n\n2. **Efficient Implementation Strategies**:\n   - Using techniques like those described in NVIDIA SuperNICs, such as direct data placement and adaptive routing, can help in optimizing the performance of large-scale AI models by minimizing latency and network congestion. While these are primarily network infrastructure solutions, the principles of adaptive routing and efficient data placement can be applied to the internal workings of neural networks.\n\nIn summary, to achieve the goal of improving autoregressive language model design, researchers can explore the following:\n\n- **Adaptive Routing**: Implement content-dependent routing strategies inspired by GNNs and dynamic path selection methods.\n- **Memory Update Mechanisms**: Use models like RMT and LoMA to ensure efficient and lossless memory updates.\n- **Hierarchical Integration**: Implement hierarchical memory organization with multi-scale attention mechanisms to focus on different aspects of the input sequence.\n\nBy combining these approaches, researchers can develop more efficient, scalable, and robust autoregressive language models.\n",
            "analysis": "### Analysis Summary\n\nAfter analyzing the search results, I've identified several promising directions for enhancing the seed design through memory organization, compression, and attention mechanisms:\n\n1. **Memory Architecture Innovations**:\n   - Lossless memory compression (LoMA) shows potential for reducing KV cache size without information loss\n   - Recurrent Memory Transformer (RMT) demonstrates effective segment-level processing\n   - Hierarchical memory organization with adaptive compression ratios could improve efficiency\n\n2. **Attention Mechanism Improvements**:\n   - Softmax-free approaches using Gaussian kernels offer theoretical advantages\n   - Multi-scale attention mechanisms show promise for capturing dependencies at different levels\n   - Intra- and inter-modality attention could inspire better hierarchical processing\n\n3. **Compression Techniques**:\n   - Adaptive compression based on content importance\n   - Lossless compression of key-value pairs\n   - Hierarchical encoding for efficient representation\n\n### Key References and Excerpts\n\n1. From \"LoMA: Lossless Compressed Memory Attention\":\n```\nWe introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache... Our method compresses the KV cache after every tc generated tokens with a compression ratio of c and a target compressed length t.\n```\n- Demonstrates feasibility of lossless KV cache compression\n- Provides framework for dynamic compression during generation\n\n2. From \"Recurrent Memory Transformer\":\n```\nMemory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence... Results show that RMT performs on par with the Transformer-XL on language modeling.\n```\n- Shows effectiveness of memory-augmented processing\n- Validates segment-level recurrent approach\n\n3. From \"IIANet: Intra- and Inter-Modality Attention\":\n```\nLeverages the attention mechanism for efficient feature fusion... maintains the ability to learn modality-specific features and enables the extraction of different semantics at various temporal scales.\n```\n- Demonstrates benefits of multi-scale attention\n- Shows how to combine different types of attention mechanisms\n\n### Key Insights for Design\n\n1. **Memory Organization**:\n   - Implement hierarchical memory with multiple compression levels\n   - Use lossless compression for KV cache to maintain information integrity\n   - Incorporate segment-level processing for efficient long sequence handling\n\n2. **Attention Integration**:\n   - Design multi-scale attention mechanisms for different hierarchical levels\n   - Combine local and global attention patterns\n   - Implement content-dependent attention routing\n\n3. **Compression Strategy**:\n   - Develop adaptive compression based on content importance\n   - Use hierarchical encoding for efficient representation\n   - Implement dynamic compression ratios\n\n### Future Search Plan\n\nNeed to investigate:\n1. **Adaptive Routing Mechanisms**:\n   - Content-dependent routing strategies\n   - Dynamic path selection methods\n   - Efficiency of different routing architectures\n\n2. **Memory Update Mechanisms**:\n   - Efficient memory update strategies\n   - Memory consolidation techniques\n   - Long-term information retention methods\n\n3. **Integration Methods**:\n   - Ways to combine hierarchical processing with adaptive routing\n   - Efficient implementation strategies\n   - Performance optimization techniques\n\n### References\n\n1. Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv preprint.\n\n2. Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). Recurrent Memory Transformer. Neural Information Processing Systems.\n\n3. Li, K., Yang, R., Sun, F., & Hu, X. (2023). IIANet: An Intra- and Inter-Modality Attention Network. International Conference on Machine Learning.\n\n4. Wu, Y., Wayne, G., Graves, A., & Lillicrap, T. (2018). The Kanerva Machine: A Generative Distributed Memory. International Conference on Learning Representations.\n\n5. Chelombiev, I., Houghton, C. J., & O'Donnell, C. (2019). Adaptive Estimators Show Information Compression in Deep Neural Networks. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "efficiency optimization neural networks, gradient flow hierarchical networks, hardware optimization transformers",
            "detail": "Show me papers discussing efficiency optimization techniques for neural networks, particularly focusing on memory-efficient implementations and hardware-aware optimizations. Include information about gradient flow in deep hierarchical networks and methods for ensuring training stability.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nShow me papers discussing efficiency optimization techniques for neural networks, particularly focusing on memory-efficient implementations and hardware-aware optimizations. Include information about gradient flow in deep hierarchical networks and methods for ensuring training stability.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.91)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.91)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.34)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.34)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 3. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention (Avg. Score: 0.15)\n\n*Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, G. Fung, Yin Li, Vikas Singh*\n\n**Published in:** AAAI Conference on Artificial Intelligence (2021)\t**Cited by** 375  (*Influential: 62*)\n\n**TL;DR:** This work proposes Nystr\u00f6mformer - a model that exhibits favorable scalability as a function of sequence length and performs favorably relative to other efficient self-attention methods.\n\n**Abstract:** Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.\n\n##### *Relevant Chunk: No. 25/36 (Score: 0.15)*\n\n```\narXiv preprint arXiv:1905.12346. Frostig, R.; Johnson, M. J.; and Leary, C. 2018. Compiling machine learning programs via high-level tracing. Systems for Machine Learning . Howard, J.; and Ruder, S. 2018. Universal Language Model Finetuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), 328-339. Huang, Y.; Cheng, Y.; Bapna, A.; Firat, O.; Chen, D.; Chen, M.; Lee, H.; Ngiam, J.; Le, Q. V.; Wu, Y.; et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems (NeurIPS), $103-112$. Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings of the International Conference on Machine Learning (ICML). Kitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer: The Efficient Transformer. In International Conference on Learning Representations (ICLR). Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple layers of features from tiny images. Technical Report TR-2009, University of Toronto . Kumar, S.; Mohri, M.; and Talwalkar, A. 2009. Ensemble Nystr\u00f6m method. In Advances in Neural Information Processing Systems (NeurIPS), 1060-1068. Lample, G.; Sablayrolles, A.; Ranzato, M.; Denoyer, L.; and J\u00e9gou, H. 2019. Large memory layers with product keys. In Advances in Neural Information Processing Systems (NeurIPS), 8548-8559. Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In International Conference on Learning Representations (ICLR). Lee, J.; Lee, Y.; Kim, J.; Kosiorek, A.; Choi, S.; and Teh, Y. W. 2019. Set transformer: A framework for attention-based permutation-invariant neural networks.\n```\n\n#### 4. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.15)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.15)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 5. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.14)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.14)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficiency optimization neural networks, gradient flow hierarchical networks, hardware optimization transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths\n\n*From Search Query: efficiency optimization neural networks*\n\n*Charles Guille-Escuret, Hiroki Naganuma, Kilian Fatras, Ioannis Mitliagkas*\n\n**TL;DR:** This study delves into the fundamental geometric properties of sampled gradients along optimization paths, and suggests that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training.\n\n**Abstract:** Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice. Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks. This efficiency, however, contrasts with the non-convex and seemingly complex structure of neural loss landscapes. In this study, we delve into the fundamental geometric properties of sampled gradients along optimization paths. We focus on two key quantities, which appear in the restricted secant inequality and error bound. Both hold high significance for first-order optimization. Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches. Our findings suggest that not only do optimization trajectories never encounter significant obstacles, but they also maintain stable dynamics during the majority of training. These observed properties are sufficiently expressive to theoretically guarantee linear convergence and prescribe learning rate schedules mirroring empirical practices. We conduct our experiments on image classification, semantic segmentation and language modeling across different batch sizes, network architectures, datasets, optimizers, and initialization seeds. We discuss the impact of each factor. Our work provides novel insights into the properties of neural network loss functions, and opens the door to theoretical frameworks more relevant to prevalent practice.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Optimization and Bayes: A Trade-off for Overparameterized Neural Networks\n\n*From Search Query: efficiency optimization neural networks*\n\n*Zhengmian Hu, Heng Huang*\n\n**TL;DR:** The proposed algorithm TransBL enables a trade-off between generalization and sampling efficiency, and derives the first algorithm-dependent PAC-Bayesian generalization bound for infinitely wide networks based on an exact KL divergence between the trained posterior distribution obtained by infinitesimal step size gradient descent and a Gaussian prior.\n\n**Abstract:** This paper proposes a novel algorithm, Transformative Bayesian Learning (TransBL), which bridges the gap between empirical risk minimization (ERM) and Bayesian learning for neural networks. We compare ERM, which uses gradient descent to optimize, and Bayesian learning with importance sampling for their generalization and computational complexity. We derive the first algorithm-dependent PAC-Bayesian generalization bound for infinitely wide networks based on an exact KL divergence between the trained posterior distribution obtained by infinitesimal step size gradient descent and a Gaussian prior. Moreover, we show how to transform gradient-based optimization into importance sampling by incorporating a weight. While Bayesian learning has better generalization, it suffers from low sampling efficiency. Optimization methods, on the other hand, have good sampling efficiency but poor generalization. Our proposed algorithm TransBL enables a trade-off between generalization and sampling efficiency.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks\n\n*From Search Query: efficiency optimization neural networks*\n\n*Y. Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi, Vikas Chandra, Yingyan Lin*\n\n**TL;DR:** This work opens up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy, and proposes a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient Dnns that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real- hardware efficiency.\n\n**Abstract:** Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 4. Focused Hierarchical RNNs for Conditional Sequence Processing\n\n*From Search Query: gradient flow hierarchical networks*\n\n*Nan Rosemary Ke, Konrad Zolna, Alessandro Sordoni, Zhouhan Lin, Adam Trischler, Yoshua Bengio, Joelle Pineau, Laurent Charlin, C. Pal*\n\n**TL;DR:** A mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed and generalize significantly better on synthetic tasks as compared to the baselines is presented.\n\n**Abstract:** Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 24  (*Influential: 0*)\n\n#### 5. GFlowNets and variational inference\n\n*From Search Query: gradient flow hierarchical networks*\n\n*Nikolay Malkin, Salem Lahlou, T. Deleu, Xu Ji, J. E. Hu, K. Everett, Dinghuai Zhang, Y. Bengio*\n\n**Abstract:** This paper builds bridges between two families of probabilistic algorithms: (hierarchical) variational inference (VI), which is typically used to model distributions over continuous spaces, and generative flow networks (GFlowNets), which have been used for distributions over discrete structures such as graphs. We demonstrate that, in certain cases, VI algorithms are equivalent to special cases of GFlowNets in the sense of equality of expected gradients of their learning objectives. We then point out the differences between the two families and show how these differences emerge experimentally. Notably, GFlowNets, which borrow ideas from reinforcement learning, are more amenable than VI to off-policy training without the cost of high gradient variance induced by importance sampling. We argue that this property of GFlowNets can provide advantages for capturing diversity in multimodal target distributions.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 62  (*Influential: 3*)\n\n#### 6. Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions in Scalar Networks and Beyond\n\n*From Search Query: gradient flow hierarchical networks*\n\n*Itai Kreisler, M. S. Nacson, Daniel Soudry, Y. Carmon*\n\n**TL;DR:** This model proves that the GFS sharpness decreases monotonically, and characterize settings where GD provably converges to the EoS in scalar networks with the squared loss.\n\n**Abstract:** Recent research shows that when Gradient Descent (GD) is applied to neural networks, the loss almost never decreases monotonically. Instead, the loss oscillates as gradient descent converges to its ''Edge of Stability'' (EoS). Here, we find a quantity that does decrease monotonically throughout GD training: the sharpness attained by the gradient flow solution (GFS)-the solution that would be obtained if, from now until convergence, we train with an infinitesimal step size. Theoretically, we analyze scalar neural networks with the squared loss, perhaps the simplest setting where the EoS phenomena still occur. In this model, we prove that the GFS sharpness decreases monotonically. Using this result, we characterize settings where GD provably converges to the EoS in scalar networks. Empirically, we show that GD monotonically decreases the GFS sharpness in a squared regression model as well as practical neural network architectures.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 7. DevFormer: A Symmetric Transformer for Context-Aware Device Placement\n\n*From Search Query: hardware optimization transformers*\n\n*Haeyeon Kim, Minsu Kim, Federico Berto, Joungho Kim, Jinkyoo Park*\n\n**TL;DR:** This paper applies DevFoemer to the problem of decoupling capacitor placement and shows that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\\%$.\n\n**Abstract:** In this paper, we present DevFormer, a novel transformer-based architecture for addressing the complex and computationally demanding problem of hardware design optimization. Despite the demonstrated efficacy of transformers in domains including natural language processing and computer vision, their use in hardware design has been limited by the scarcity of offline data. Our approach addresses this limitation by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetricity that effectively capture the hardware context and enable efficient design optimization with limited offline data. We apply DevFoemer to the problem of decoupling capacitor placement and show that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\\%$. Finally, we show that our approach achieves promising results in other offline contextual learning-based combinatorial optimization tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 8. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware optimization transformers*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 71  (*Influential: 13*)\n\n#### 9. Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers\n\n*From Search Query: hardware optimization transformers*\n\n*Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, P. Jaillet, B. Low*\n\n**TL;DR:** A neural bandit algorithm is adopted which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs and uses extensive experiments to show that INSTINCT consistently outperforms baselines in different tasks, e.g., various instruction induction tasks and the task of improving zero-shot chain-of-thought instructions.\n\n**Abstract:** Large language models (LLMs) have shown remarkable instruction-following capabilities and achieved impressive performances in various applications. However, the performances of LLMs depend heavily on the instructions given to them, which are typically manually tuned with substantial human efforts. Recent work has used the query-efficient Bayesian optimization (BO) algorithm to automatically optimize the instructions given to black-box LLMs. However, BO usually falls short when optimizing highly sophisticated (e.g., high-dimensional) objective functions, such as the functions mapping an instruction to the performance of an LLM. This is mainly due to the limited expressive power of the Gaussian process (GP) which is used by BO as a surrogate to model the objective function. Meanwhile, it has been repeatedly shown that neural networks (NNs), especially pre-trained transformers, possess strong expressive power and can model highly complex functions. So, we adopt a neural bandit algorithm which replaces the GP in BO by an NN surrogate to optimize instructions for black-box LLMs. More importantly, the neural bandit algorithm allows us to naturally couple the NN surrogate with the hidden representation learned by a pre-trained transformer (i.e., an open-source LLM), which significantly boosts its performance. These motivate us to propose our INSTruction optimization usIng Neural bandits Coupled with Transformers (INSTINCT) algorithm. We perform instruction optimization for ChatGPT and use extensive experiments to show that INSTINCT consistently outperforms baselines in different tasks, e.g., various instruction induction tasks and the task of improving zero-shot chain-of-thought instructions. Our code is available at https://github.com/xqlin98/INSTINCT.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Delta-STN: Efficient Bilevel Optimization for Neural Networks using Structured Response Jacobians\n\n*From Search Query: efficiency optimization neural networks*\n\n*Roger Grosse, Juhan Bae*\n\n**Abstract:** Hyperparameter optimization of neural networks can be elegantly formulated as a bilevel optimization problem. While research on bilevel optimization of neural networks has been dominated by implicit differentiation and unrolling, hypernetworks such as Self-Tuning Networks (STNs) have recently gained traction due to their ability to amortize the optimization of the inner objective. In this paper, we diagnose several subtle pathologies in the training of STNs. Based on these observations, we propose the $\\Delta$-STN, an improved hypernetwork architecture which stabilizes training and optimizes hyperparameters much more efficiently than STNs. The key idea is to focus on accurately approximating the best-response Jacobian rather than the full best-response function; we achieve this by reparameterizing the hypernetwork and linearizing the network around the current parameters. We demonstrate empirically that our $\\Delta$-STN can tune regularization hyperparameters (e.g. weight decay, dropout, number of cutout holes) with higher accuracy, faster convergence, and improved stability compared to existing approaches.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-26\n\n\n\n#### 2. Efficient On-Chip Learning for Optical Neural Networks Through Power-Aware Sparse Zeroth-Order Optimization\n\n*From Search Query: efficiency optimization neural networks*\n\n*David Z. Pan, Ray T. Chen, Zhoufeng Ying, Zheng Zhao, Chenghao Feng, Jiaqi Gu*\n\n**Abstract:** Optical neural networks (ONNs) have demonstrated record-breaking potential in high-performance neuromorphic computing due to their ultra-high execution speed and low energy consumption. However, current learning protocols fail to provide scalable and efficient solutions to photonic circuit optimization in practical applications. In this work, we propose a novel on-chip learning framework to release the full potential of ONNs for power-efficient in situ training. Instead of deploying implementation-costly back-propagation, we directly optimize the device configurations with computation budgets and power constraints. We are the first to model the ONN on-chip learning as a resource-constrained stochastic noisy zeroth-order optimization problem, and propose a novel mixed-training strategy with two-level sparsity and power-aware dynamic pruning to offer a scalable on-chip training solution in practical ONN deployment. Compared with previous methods, we are the first to optimize over 2,500 optical components on chip. We can achieve much better optimization stability, 3.7x-7.6x higher efficiency, and save >90% power under practical device variations and thermal crosstalk.\n\n**Published:** 2020-12-21\n\n\n\n#### 3. Dynamic Hierarchical Mimicking Towards Consistent Optimization Objectives\n\n*From Search Query: gradient flow hierarchical networks*\n\n*Duo Li, Qifeng Chen*\n\n**Abstract:** While the depth of modern Convolutional Neural Networks (CNNs) surpasses that of the pioneering networks with a significant margin, the traditional way of appending supervision only over the final classifier and progressively propagating gradient flow upstream remains the training mainstay. Seminal Deeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of optimization arising from gradient flow through a long chain. However, it is still vulnerable to issues including interference to the hierarchical representation generation process and inconsistent optimization objectives, as illustrated theoretically and empirically in this paper. Complementary to previous training strategies, we propose Dynamic Hierarchical Mimicking, a generic feature learning mechanism, to advance CNN training with enhanced generalization ability. Partially inspired by DSN, we fork delicately designed side branches from the intermediate layers of a given neural network. Each branch can emerge from certain locations of the main branch dynamically, which not only retains representation rooted in the backbone network but also generates more diverse representations along its own pathway. We go one step further to promote multi-level interactions among different branches through an optimization formula with probabilistic prediction matching losses, thus guaranteeing a more robust optimization process and better representation ability. Experiments on both category and instance recognition tasks demonstrate the substantial improvements of our proposed method over its corresponding counterparts using diverse state-of-the-art CNN architectures. Code and models are publicly available at https://github.com/d-li14/DHM\n\n**Conference:** dynamic-hierarchical-mimicking-towards-1\n\n**Published:** 2020-03-24\n\n\n\n#### 4. A Compositional Framework for First-Order Optimization\n\n*From Search Query: gradient flow hierarchical networks*\n\n*Anonymous*\n\n**Abstract:** Optimization decomposition methods are a fundamental tool to develop distributed solution algorithms for large scale optimization problems arising in fields such as machine learning and optimal control. In this paper, we present an algebraic framework for hierarchically composing optimization problems defined on hypergraphs and automatically generating distributed solution algorithms that respect the given hierarchical structure. The central abstractions of our framework are operads, operad algebras, and algebra morphisms, which formalize notions of syntax, semantics, and structure preserving semantic transformations respectively. These abstractions allow us to formally relate composite optimization problems to the distributed algorithms that solve them. Specifically, we show that certain classes of optimization problems form operad algebras, and a collection of first-order solution methods, namely gradient descent, Uzawa's algorithm (also called gradient ascent-descent), and their subgradient variants, yield algebra morphisms from these problem algebras to algebras of dynamical systems. Primal and dual decomposition methods are then recovered by applying these morphisms to certain classes of composite problems. Using this framework, we also derive a novel sufficient condition for when a problem defined by compositional data is solvable by a decomposition method. We show that the minimum cost network flow problem satisfies this condition, thereby allowing us to automatically derive a hierarchical dual decomposition algorithm for finding minimum cost flows on composite flow networks. We implement our operads, algebras, and algebra morphisms in a Julia package called AlgebraicOptimization.jl and use our implementation to empirically demonstrate that hierarchical dual decomposition outperforms standard dual decomposition on classes of flow networks with hierarchical structure.\n\n**Published:** 2024-03-08\n\n\n\n#### 5. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: hardware optimization transformers*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n#### 6. Rethinking Vision Transformers for MobileNet Size and Speed\n\n*From Search Query: hardware optimization transformers*\n\n*Jian Ren, Sergey Tulyakov, Yanzhi Wang, Kamyar Salahi, Georgios Evangelidis, Yang Wen, Ju Hu, Yanyu Li*\n\n**Abstract:** With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously. The proposed models, EfficientFormerV2, achieve 3.5% higher top-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and parameters. This work demonstrate that properly designed and optimized vision transformers can achieve high performance even with MobileNet-level size and speed.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2022-12-15\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on efficiency optimization, memory-efficient implementations, hardware-aware optimizations, and ensuring training stability, here are some key points and references that can be helpful:\n\n## Efficiency Optimization Techniques\n\n### Memory-Efficient Implementations\n- **Sparse Neural Networks**: Implementing sparse neural networks can significantly reduce memory usage and computational overhead. Techniques like SNIP (Single-shot Network Pruning based on Gradients) and SynFlow (Synaptic Flow) are effective in pruning unnecessary weights and connections, making the model leaner and more efficient.\n- **Lossless Compressed Memory Attention**: The LoMA (Lossless Compressed Memory Attention) approach allows for the lossless compression of the KV cache, which can be particularly beneficial in memory-constrained environments. This method compresses the KV cache dynamically, maintaining performance while reducing memory usage.\n\n### Hardware-Aware Optimizations\n- **Pruning and Sparsity-Aware Kernels**: While random pruning does not directly provide significant speed-ups due to the lack of locality in weight matrix multiplication, using sparsity-aware kernels like cuSPARSE and CUTLASS can optimize the performance of pruned models on hardware.\n- **Efficient Optimizers**: Optimizers like Adam, which combine adaptive learning rates with momentum, can be more efficient than traditional SGD. Additionally, optimizing the computation of optimizers and reducing the memory load of stored statistics can further enhance efficiency.\n\n## Gradient Flow in Deep Hierarchical Networks\n\n### Stable Training Methods\n- **Gradient Flow and Back-Propagation**: Ensuring stable gradient flow is crucial in deep hierarchical networks. Techniques like back-propagation, which calculate gradients by propagating errors backward through the network, are fundamental. However, for stability, methods such as using momentum or adaptive learning rates (e.g., Adam) can help in handling sparse gradients and accelerating convergence.\n- **Subset Pretraining**: The approach of using subset pretraining can help in stabilizing the training process by first optimizing over subsets of the training data, which can provide a good initial point for fine-tuning over the entire training set. This method can reduce the computational overhead and improve stability.\n\n## Hierarchical Processing and Memory Organization\n\n### Hierarchical Memory Organization\n- **Recurrent Memory Transformer (RMT)**: Implementing segment-level processing with recurrent connections can enhance the ability of the model to store and process local and global information. RMT has shown performance on par with Transformer-XL in language modeling tasks.\n- **Hierarchical Routing Decisions**: Combining copy gates with geometric attention and using adaptive selection of processing paths based on input characteristics can improve the routing mechanism in hierarchical networks. This approach can enhance systematic generalization and information flow.\n\n## Integration Strategies\n\n### Multi-Scale Attention Mechanisms\n- **Combining Local and Global Attention**: Using multi-scale attention mechanisms for different hierarchical levels can improve the model's ability to capture both local and global context. This approach can be integrated with sparse and dense attention patterns to optimize memory usage and computational efficiency.\n\n### Efficient Memory Updates\n- **Dynamic Compression and Update Mechanisms**: Implementing dynamic compression and update mechanisms, such as those in LoMA, can ensure efficient memory updates. This helps in maintaining performance while reducing the memory footprint of the model.\n\nBy incorporating these techniques, researchers can develop more efficient, stable, and scalable autoregressive language models that leverage advanced optimization algorithms, memory-efficient implementations, and hardware-aware optimizations.\n",
            "analysis": "### Analysis Summary\n\nAfter analyzing three rounds of search results, I've identified several promising directions for enhancing the seed design through adaptive routing, memory mechanisms, and hierarchical processing:\n\n1. **Adaptive Routing Innovations**:\n   - Neural Data Router (NDR) shows potential for improving systematic generalization\n   - Content-dependent routing can enhance information flow\n   - Dynamic path selection based on input characteristics\n\n2. **Memory Architecture**:\n   - Lossless memory compression (LoMA) for efficient KV cache\n   - Recurrent Memory Transformer (RMT) for segment-level processing\n   - Hierarchical memory organization with adaptive compression\n\n3. **Integration Strategies**:\n   - Multi-scale attention mechanisms for different hierarchical levels\n   - Combination of sparse and dense attention patterns\n   - Efficient memory update mechanisms\n\n### Key References and Excerpts\n\n1. From \"The Neural Data Router\":\n```\nTo facilitate learning of useful control flow, we propose two modifications to the Transformer architecture: copy gate and geometric attention... NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing.\n```\n- Introduces copy gate for selective information flow\n- Geometric attention provides routing inductive bias\n\n2. From \"LoMA: Lossless Compressed Memory Attention\":\n```\nWe introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache... Our method compresses the KV cache after every tc generated tokens with a compression ratio of c and a target compressed length t.\n```\n- Demonstrates feasibility of lossless KV cache compression\n- Provides framework for dynamic compression\n\n3. From \"Recurrent Memory Transformer\":\n```\nMemory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence... Results show that RMT performs on par with the Transformer-XL on language modeling.\n```\n- Shows effectiveness of memory-augmented processing\n- Validates segment-level recurrent approach\n\n4. From \"Routing Networks: Adaptive Selection\":\n```\nA routing network is a kind of self-organizing neural network consisting of two components: a router and a set of one or more function blocks... Given an input the router makes a routing decision, choosing a function block to apply.\n```\n- Demonstrates effective routing architecture\n- Shows benefits of dynamic function selection\n\n### Key Insights for Design\n\n1. **Routing Mechanism**:\n   - Combine copy gate with geometric attention for content-dependent routing\n   - Use adaptive selection of processing paths based on input characteristics\n   - Implement hierarchical routing decisions\n\n2. **Memory Organization**:\n   - Use lossless compression for KV cache\n   - Implement segment-level processing with recurrent connections\n   - Organize memory hierarchically with multiple compression levels\n\n3. **Integration Strategy**:\n   - Combine local and global attention patterns\n   - Use multi-scale processing for different hierarchical levels\n   - Implement efficient memory updates\n\n### Future Search Plan\n\nNeed to investigate:\n\n1. **Efficiency Optimization**:\n   - Techniques for reducing computational overhead\n   - Memory-efficient implementations\n   - Hardware-aware optimizations\n\n2. **Stability Analysis**:\n   - Methods for ensuring stable training\n   - Gradient flow in deep hierarchical networks\n   - Convergence properties\n\n### References\n\n1. Csord\u00e1s, R., Irie, K., & Schmidhuber, J. (2021). The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization. ICLR.\n\n2. Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv preprint.\n\n3. Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). Recurrent Memory Transformer. NeurIPS.\n\n4. Rosenbaum, C., Klinger, T., & Riemer, M. (2017). Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning. ICLR.\n\n5. Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). Efficient Content-Based Sparse Attention with Routing Transformers. TACL.\n\n6. Liu, F., Cohn, T., & Baldwin, T. (2018). Recurrent Entity Networks with Delayed Memory Update for Targeted Aspect-based Sentiment Analysis. arXiv preprint.\n\n7. Zhong, W., Guo, L., Gao, Q., Ye, H., & Wang, Y. (2023). MemoryBank: Enhancing Large Language Models with Long-Term Memory. arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"adaptive normalization implementation\", \"hardware optimization techniques\", \"training stability analysis\"",
            "detail": "Search for papers and implementations discussing:\n\n1. Production implementations of adaptive normalization:\n- Real-world deployment strategies\n- Performance metrics and benchmarks\n- Integration case studies\n\n2. Hardware optimization techniques:\n- Memory management strategies\n- Computation scheduling methods\n- Resource utilization patterns\n\n3. Training stability analysis:\n- Empirical stability studies\n- Convergence analysis methods\n- Loss landscape investigation\n\nInclude both theoretical frameworks and practical implementations with performance metrics.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations discussing:\n\n1. Production implementations of adaptive normalization:\n- Real-world deployment strategies\n- Performance metrics and benchmarks\n- Integration case studies\n\n2. Hardware optimization techniques:\n- Memory management strategies\n- Computation scheduling methods\n- Resource utilization patterns\n\n3. Training stability analysis:\n- Empirical stability studies\n- Convergence analysis methods\n- Loss landscape investigation\n\nInclude both theoretical frameworks and practical implementations with performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.00)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.00)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.00)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.00)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 3. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.00)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.00)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n#### 4. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.00)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.00)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 5. PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation (Avg. Score: 0.00)\n\n*Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yunhe Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work presents a new efficient model architecture for establishing modern language models, namely, PanGu-$\\pi$, and develops an LLM named YunShan for practical application, which can surpass other models with similar scales on benchmarks.\n\n**Abstract:** The recent trend of large language models (LLMs) is to increase the scale of both model size (\\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama. However, large models often involve massive computational costs, and practical applications cannot afford such high prices. However, the method of constructing a strong model architecture for LLMs is rarely discussed. We first analyze the state-of-the-art language model architectures and observe the feature collapse problem. Based on the theoretical analysis, we propose that the nonlinearity is also very important for language models, which is usually studied in convolutional neural networks for vision tasks. The series informed activation function is then introduced with tiny calculations that can be ignored, and an augmented shortcut is further used to enhance the model nonlinearity. We then demonstrate that the proposed approach is significantly effective for enhancing the model nonlinearity through carefully designed ablations; thus, we present a new efficient model architecture for establishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using the same dataset and training strategy to compare PanGu-$\\pi$ with state-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a comparable performance to that of benchmarks with about 10\\% inference speed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms of accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the high-value domains of finance and law, developing an LLM named YunShan for practical application. The results show that YunShan can surpass other models with similar scales on benchmarks.\n\n##### *Relevant Chunk: No. 39/62 (Score: 0.00)*\n\n```\narXiv preprint arXiv:1808.06226, 2018. [107] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [108] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [109] B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [110] J. Su et al. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, pp. 127063, 2023. [111] O. Contributors. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass 2023.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive normalization implementation\", \"hardware optimization techniques\", \"training stability analysis\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen*\n\n**TL;DR:** A novel slice-level adaptive normalization scheme, referred to SAN, is proposed, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization and could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data.\n\n**Abstract:** Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN , which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available 2 .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 7*)\n\n#### 2. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich*\n\n**TL;DR:** A gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes is presented, showing that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks.\n\n**Abstract:** Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 1088  (*Influential: 144*)\n\n#### 3. Structured Evolution with Compact Architectures for Scalable Policy Optimization\n\n*From Search Query: \"hardware optimization techniques\"*\n\n*K. Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, Adrian Weller*\n\n**TL;DR:** It is shown that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies algorithm.\n\n**Abstract:** We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al. (2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 141  (*Influential: 13*)\n\n#### 4. SliceGPT: Compress Large Language Models by Deleting Rows and Columns\n\n*From Search Query: \"hardware optimization techniques\"*\n\n*Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James Hensman*\n\n**TL;DR:** SliceGPT is presented, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network.\n\n**Abstract:** Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 69  (*Influential: 14*)\n\n#### 5. Pruning vs Quantization: Which is Better?\n\n*From Search Query: \"hardware optimization techniques\"*\n\n*Andrey Kuzmin, Markus Nagel, M. V. Baalen, A. Behboodi, Tijmen Blankevoort*\n\n**TL;DR:** The question on which is better: neural network quantization or pruning?\n\n**Abstract:** Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, pruning might be beneficial from an accuracy standpoint.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 20  (*Influential: 0*)\n\n#### 6. The alignment property of SGD noise and how it helps select flat minima: A stability analysis\n\n*From Search Query: \"training stability analysis\"*\n\n*Lei Wu, Mingze Wang, Weijie J. Su*\n\n**TL;DR:** It is proved that if a global minimum is linearly stable for SGD, then it must satisfy the Frobenius norm of Hessian at $\\theta^*$, batch size, and learning rate, respectively, otherwise SGD will escape from that minimum fast.\n\n**Abstract:** The phenomenon that stochastic gradient descent (SGD) favors flat minima has played a critical role in understanding the implicit regularization of SGD. In this paper, we provide an explanation of this striking phenomenon by relating the particular noise structure of SGD to its \\emph{linear stability} (Wu et al., 2018). Specifically, we consider training over-parameterized models with square loss. We prove that if a global minimum $\\theta^*$ is linearly stable for SGD, then it must satisfy $\\|H(\\theta^*)\\|_F\\leq O(\\sqrt{B}/\\eta)$, where $\\|H(\\theta^*)\\|_F, B,\\eta$ denote the Frobenius norm of Hessian at $\\theta^*$, batch size, and learning rate, respectively. Otherwise, SGD will escape from that minimum \\emph{exponentially} fast. Hence, for minima accessible to SGD, the sharpness -- as measured by the Frobenius norm of the Hessian -- is bounded \\emph{independently} of the model size and sample size. The key to obtaining these results is exploiting the particular structure of SGD noise: The noise concentrates in sharp directions of local landscape and the magnitude is proportional to loss value. This alignment property of SGD noise provably holds for linear networks and random feature models (RFMs), and is empirically verified for nonlinear networks. Moreover, the validity and practical relevance of our theoretical findings are also justified by extensive experiments on CIFAR-10 dataset.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 31  (*Influential: 4*)\n\n#### 7. On the Algorithmic Stability of Adversarial Training\n\n*From Search Query: \"training stability analysis\"*\n\n*Yue Xing, Qifan Song, Guang Cheng*\n\n**TL;DR:** It is argued that the non-differentiability issue of adversarial training causes worse algorithmic stability than their natu-ral counterparts, and injecting noise enables the training trajectory to avoid the occurrence of non-differentiability with dominating probability, hence enhancing the stability performance of adversarial training.\n\n**Abstract:** The adversarial training is a popular tool to remedy the vulnerability of deep learning models against adversarial attacks, and there is rich theoretical literature on the training loss of adversarial training algorithms. In contrast, this paper studies the algorithmic stability of a generic adversarial training algorithm, which can further help to establish an upper bound for generalization error. By \ufb01guring out the stability upper bound and lower bound, we argue that the non-differentiability issue of adversarial training causes worse algorithmic stability than their natu-ral counterparts. To tackle this problem, we consider a noise injection method. While the non-differentiability problem seriously affects the stability of adversarial training, injecting noise enables the training trajectory to avoid the occurrence of non-differentiability with dominating probability, hence enhancing the stability performance of adversarial training. Our analysis also studies the relation between the algorithm stability and numerical approximation error of adversarial attacks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 48  (*Influential: 7*)\n\n### 6 related papers from ArXiv\n\n#### 1. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 2. Differentiable Pooling for Unsupervised Acoustic Model Adaptation\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Pawel Swietojanski, Steve Renals*\n\n**Abstract:** We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches.\n\n**Published:** 2016-03-31T15:10:40Z  (*Updated: 2016-07-13T18:12:49Z*)\n\n\n\n#### 3. ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training\n  Quantization Framework for W8A8 Transformers\n\n*From Search Query: \"hardware optimization techniques\"*\n\n*Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He*\n\n**Abstract:** Quantization techniques are pivotal in reducing the memory and computational\ndemands of deep neural network inference. Existing solutions, such as\nZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook\ncrucial memory-bounded operators and the complexities of per-token\nquantization. Addressing these gaps, we present a novel, fully\nhardware-enhanced robust optimized post-training W8A8 quantization framework,\nZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and\ncompute-intensive operators, aiming for optimal hardware performance.\nAdditionally, it offers flexibility by allowing specific INT8 modules to switch\nto FP16/BF16 mode, enhancing accuracy.\n\n**Published:** 2023-10-26T18:34:41Z  (*Updated: 2023-10-26T18:34:41Z*)\n\n\n\n#### 4. FlashDecoding++: Faster Large Language Model Inference on GPUs\n\n*From Search Query: \"hardware optimization techniques\"*\n\n*Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, Yu Wang*\n\n**Abstract:** As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n\n**Published:** 2023-11-02T14:57:03Z  (*Updated: 2024-01-05T12:41:13Z*)\n\n\n\n#### 5. Empirical Analysis of Efficient Fine-Tuning Methods for Large\n  Pre-Trained Language Models\n\n*From Search Query: \"training stability analysis\"*\n\n*Nigel Doering, Cyril Gorlla, Trevor Tuttle, Adhvaith Vijay*\n\n**Abstract:** Fine-tuning large pre-trained language models for downstream tasks remains a\ncritical challenge in natural language processing. This paper presents an\nempirical analysis comparing two efficient fine-tuning methods - BitFit and\nadapter modules - to standard full model fine-tuning. Experiments conducted on\nGLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The\nBitFit approach, which trains only bias terms and task heads, matches full\nfine-tuning performance across varying amounts of training data and time\nconstraints. It demonstrates remarkable stability even with only 30\\% of data,\noutperforming full fine-tuning at intermediate data levels. Adapter modules\nexhibit high variability, with inconsistent gains over default models. The\nfindings indicate BitFit offers an attractive balance between performance and\nparameter efficiency. Our work provides valuable perspectives on model tuning,\nemphasizing robustness and highlighting BitFit as a promising alternative for\nresource-constrained or streaming task settings. The analysis offers actionable\nguidelines for efficient adaptation of large pre-trained models, while\nillustrating open challenges in stabilizing techniques like adapter modules.\n\n**Published:** 2024-01-08T17:44:43Z  (*Updated: 2024-01-08T17:44:43Z*)\n\n\n\n#### 6. Neighborhood-Regularized Self-Training for Learning with Few Labels\n\n*From Search Query: \"training stability analysis\"*\n\n*Ran Xu, Yue Yu, Hejie Cui, Xuan Kan, Yanqiao Zhu, Joyce Ho, Chao Zhang, Carl Yang*\n\n**Abstract:** Training deep neural networks (DNNs) with limited supervision has been a\npopular research topic as it can significantly alleviate the annotation burden.\nSelf-training has been successfully applied in semi-supervised learning tasks,\nbut one drawback of self-training is that it is vulnerable to the label noise\nfrom incorrect pseudo labels. Inspired by the fact that samples with similar\nlabels tend to share similar representations, we develop a neighborhood-based\nsample selection approach to tackle the issue of noisy pseudo labels. We\nfurther stabilize self-training via aggregating the predictions from different\nrounds during sample selection. Experiments on eight tasks show that our\nproposed method outperforms the strongest self-training baseline with 1.83% and\n2.51% performance gain for text and graph datasets on average. Our further\nanalysis demonstrates that our proposed data selection strategy reduces the\nnoise of pseudo labels by 36.8% and saves 57.3% of the time when compared with\nthe best baseline. Our code and appendices will be uploaded to\nhttps://github.com/ritaranx/NeST.\n\n**Published:** 2023-01-10T00:07:33Z  (*Updated: 2023-02-15T20:29:50Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Junho Kim, Kwanghee Lee, Hyeonwoo Kang, Minjae Kim*\n\n**Abstract:** We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-07-25\n\n\n\n#### 2. Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Serge Belongie, Xun Huang*\n\n**Abstract:** Gatys et al. recently introduced a neural algorithm that renders a content\nimage in the style of another image, achieving so-called style transfer.\nHowever, their framework requires a slow iterative optimization process, which\nlimits its practical application. Fast approximations with feed-forward neural\nnetworks have been proposed to speed up neural style transfer. Unfortunately,\nthe speed improvement comes at a cost: the network is usually tied to a fixed\nset of styles and cannot adapt to arbitrary new styles. In this paper, we\npresent a simple yet effective approach that for the first time enables\narbitrary style transfer in real-time. At the heart of our method is a novel\nadaptive instance normalization (AdaIN) layer that aligns the mean and variance\nof the content features with those of the style features. Our method achieves\nspeed comparable to the fastest existing approach, without the restriction to a\npre-defined set of styles. In addition, our approach allows flexible user\ncontrols such as content-style trade-off, style interpolation, color & spatial\ncontrols, all using a single feed-forward neural network.\n\n**Conference:** arbitrary-style-transfer-in-real-time-with-1\n\n**Published:** 2017-03-20\n\n\n\n#### 3. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\n\n*From Search Query: \"hardware optimization techniques\"*\n\n*Shen Li, Ajit Mathews, Yuchen Hao, Geeta Chauhan, Pritam Damania, Bernard Nguyen, Can Balioglu, Alban Desmaison, Sam Shleifer, Myle Ott, Hamid Shojanazeri, Less Wright, Min Xu, Chien-chin Huang, Liang Luo, Rohan Varma, Andrew Gu, Yanli Zhao*\n\n**Abstract:** It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.\n\n**Published:** 2023-04-21\n\n\n\n#### 4. Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training\n\n*From Search Query: \"hardware optimization techniques\"*\n\n*Minjia Zhang, Olatunji Ruwase, Stas Bekman, Masahiro Tanaka, Lev Kurilenko, Sam Ade Jacobs, Xinyu Lian*\n\n**Abstract:** Existing checkpointing approaches seem ill-suited for distributed training even though hardware limitations make model parallelism, i.e., sharding model state across multiple accelerators, a requirement for model scaling. Consolidating distributed model state into a single checkpoint unacceptably slows down training, and is impractical at extreme scales. Distributed checkpoints, in contrast, are tightly coupled to the model parallelism and hardware configurations of the training run, and thus unusable on different configurations. To address this problem, we propose Universal Checkpointing, a technique that enables efficient checkpoint creation while providing the flexibility of resuming on arbitrary parallelism strategy and hardware configurations. Universal Checkpointing unlocks unprecedented capabilities for large-scale training such as improved resilience to hardware failures through continued training on remaining healthy hardware, and reduced training time through opportunistic exploitation of elastic capacity. The key insight of Universal Checkpointing is the selection of the optimal representation in each phase of the checkpointing life cycle: distributed representation for saving, and consolidated representation for loading. This is achieved using two key mechanisms. First, the universal checkpoint format, which consists of a consolidated representation of each model parameter and metadata for mapping parameter fragments into training ranks of arbitrary model-parallelism configuration. Second, the universal checkpoint language, a simple but powerful specification language for converting distributed checkpoints into the universal checkpoint format. Our evaluation demonstrates the effectiveness and generality of Universal Checkpointing on state-of-the-art model architectures and a wide range of parallelism techniques.\n\n**Published:** 2024-06-27\n\n\n\n#### 5. Stability Analysis and Generalization Bounds of Adversarial Training\n\n*From Search Query: \"training stability analysis\"*\n\n*Zhi-Quan Luo, Jue Wang, Ruoyu Sun, Yanbo Fan, Jiancong Xiao*\n\n**Abstract:** In adversarial machine learning, deep neural networks can fit the adversarial examples on the training dataset but have poor generalization ability on the test set. This phenomenon is called robust overfitting, and it can be observed when adversarially training neural nets on common datasets, including SVHN, CIFAR-10, CIFAR-100, and ImageNet. In this paper, we study the robust overfitting issue of adversarial training by using tools from uniform stability. One major challenge is that the outer function (as a maximization of the inner function) is nonsmooth, so the standard technique (e.g., hardt et al., 2016) cannot be applied. Our approach is to consider $\\eta$-approximate smoothness: we show that the outer function satisfies this modified smoothness assumption with $\\eta$ being a constant related to the adversarial perturbation $\\epsilon$. Based on this, we derive stability-based generalization bounds for stochastic gradient descent (SGD) on the general class of $\\eta$-approximate smooth functions, which covers the adversarial loss. Our results suggest that robust test accuracy decreases in $\\epsilon$ when $T$ is large, with a speed between $\\Omega(\\epsilon\\sqrt{T})$ and $\\mathcal{O}(\\epsilon T)$. This phenomenon is also observed in practice. Additionally, we show that a few popular techniques for adversarial training (e.g., early stopping, cyclic learning rate, and stochastic weight averaging) are stability-promoting in theory.\n\n**Published:** 2022-10-03\n\n\n\n#### 6. Adaptive Smoothness-weighted Adversarial Training for Multiple Perturbations with Its Stability Analysis\n\n*From Search Query: \"training stability analysis\"*\n\n*Zhi-Quan Luo, Jue Wang, Baoyuan Wu, Yanbo Fan, Zeyu Qin, Jiancong Xiao*\n\n**Abstract:** Adversarial Training (AT) has been demonstrated as one of the most effective methods against adversarial examples. While most existing works focus on AT with a single type of perturbation e.g., the $\\ell_\\infty$ attacks), DNNs are facing threats from different types of adversarial examples. Therefore, adversarial training for multiple perturbations (ATMP) is proposed to generalize the adversarial robustness over different perturbation types (in $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ norm-bounded perturbations). However, the resulting model exhibits trade-off between different attacks. Meanwhile, there is no theoretical analysis of ATMP, limiting its further development. In this paper, we first provide the smoothness analysis of ATMP and show that $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ adversaries give different contributions to the smoothness of the loss function of ATMP. Based on this, we develop the stability-based excess risk bounds and propose adaptive smoothness-weighted adversarial training for multiple perturbations. Theoretically, our algorithm yields better bounds. Empirically, our experiments on CIFAR10 and CIFAR100 achieve the state-of-the-art performance against the mixture of multiple perturbations attacks.\n\n**Published:** 2022-10-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on adaptive normalization, hardware optimization, and training stability is crucial. Here are some key points and references that can help achieve these goals:\n\n## Adaptive Normalization Implementation\n\n### Real-world Deployment Strategies and Performance Metrics\n- **Normalized Knowledge Distillation (NKD)**: This technique, discussed in the context of hybrid AI systems, involves normalizing the student model's outputs to match the target distribution. This can be particularly useful in autoregressive models to ensure that the predictions align with the expected outcomes. The process includes output normalization, proxy task design, and fine-tuning, which can enhance model performance and stability.\n- **Layer Normalization and Batch Normalization**: These techniques are essential for stabilizing the training process. Layer normalization, which normalizes across features, is beneficial in recurrent neural networks (RNNs) and transformer models, while batch normalization normalizes the inputs of each layer to improve training speed and stability. These methods can be integrated into autoregressive models to improve convergence and stability.\n\n### Integration Case Studies\n- **Adaptive Adversarial Transformer (AAT)**: This novel architecture combines the strengths of Transformers with a dual attention mechanism. While not specifically focused on language models, the AAT's use of adaptive mechanisms can provide insights into how to integrate adaptive normalization with attention mechanisms in autoregressive models. The AAT's design can help in understanding how to dynamically adjust parameters based on input characteristics, which is crucial for improving model expressivity and trainability.\n\n## Hardware Optimization Techniques\n\n### Memory Management Strategies\n- **Selective Parameter Protection**: Studies like \"AWQ: Activation-aware Weight Quantization\" highlight the importance of protecting only the most salient weights in large language models. This approach can significantly reduce quantization error and improve memory efficiency. By identifying and protecting critical weight channels based on activation distribution, autoregressive models can be optimized for better performance with limited resources[Analysis Note, Reference: Han et al., 2023].\n\n### Computation Scheduling Methods\n- **Computation Scheduling Strategies**: Efficient computation scheduling is vital for optimizing resource utilization. Papers like \"Gated Linear Attention Transformers with Hardware-Efficient Training\" discuss how to schedule computations to minimize memory access patterns and optimize resource utilization. These strategies can be adapted to autoregressive models to improve their hardware efficiency[Analysis Note, Reference: Yang et al., 2023].\n\n### Resource Utilization Patterns\n- **Memory Access Patterns**: Optimizing memory access patterns is crucial for performance. Studies on hardware-efficient training highlight the importance of minimizing memory accesses and optimizing computation schedules. By analyzing and optimizing these patterns, autoregressive models can achieve better performance and efficiency[Analysis Note, Reference: Schmidt et al., 2023].\n\n## Training Stability Analysis\n\n### Empirical Stability Studies\n- **Gradient Flow Control Mechanisms**: The stability of training can be significantly improved by controlling gradient flow. Research on \"Stable and low-precision training\" shows that loss spikes often occur due to under-estimated squared gradients by the AdamW second moment estimator. By analyzing and addressing these issues, autoregressive models can achieve more stable training[Analysis Note, Reference: Schmidt et al., 2023].\n\n### Convergence Analysis Methods\n- **Eigenspectrum Variance**: The study on \"Self-attention Networks Localize\" reveals that a small eigenspectrum variance in self-attention mechanisms can prevent rank and entropy collapse, leading to better model expressivity and trainability. This insight can be applied to autoregressive models to improve their convergence properties[Analysis Note, Reference: Bao et al., 2024].\n\n### Loss Landscape Investigation\n- **Loss Spike Patterns**: Analyzing loss spike patterns can provide valuable insights into optimization. By understanding when and why loss spikes occur, researchers can develop strategies to mitigate these issues and improve the overall stability and convergence of the model[Analysis Note, Reference: Schmidt et al., 2023].\n\nIn summary, integrating adaptive normalization techniques like NKD, layer normalization, and batch normalization can enhance model performance and stability. Optimizing hardware efficiency through selective parameter protection, computation scheduling, and resource utilization patterns can improve the model's efficiency. Finally, analyzing gradient flow, eigenspectrum variance, and loss spike patterns can help in achieving stable and convergent training for autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about adaptive normalization, hardware efficiency, and training stability:\n\na) Adaptive Normalization:\n- Dynamic parameter adjustment based on input characteristics\n- Integration with attention mechanisms\n- Impact on model convergence and stability\n- Hardware-aware implementation considerations\n\nb) Hardware Efficiency:\n- Memory access patterns optimization\n- Computation scheduling strategies\n- Resource utilization techniques\n- Implementation trade-offs\n\nc) Training Stability:\n- Gradient flow control mechanisms\n- Convergence properties\n- Adaptive update strategies\n- Stabilization techniques\n\n2. Useful References with Excerpts:\n\nFrom \"Self-attention Networks Localize\":\n\"...reveals that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\"\n\nFrom \"AWQ: Activation-aware Weight Quantization\":\n\"...finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights.\"\n\nFrom \"Stable and low-precision training\":\n\"...analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator.\"\n\n3. Key Insights:\n\na) Normalization Stability:\n- Eigenspectrum variance affects attention localization\n- Dynamic normalization improves training stability\n- Activation-aware approaches outperform static methods\n\nb) Hardware Optimization:\n- Selective parameter protection improves efficiency\n- Memory access patterns crucial for performance\n- Computation scheduling affects resource utilization\n\nc) Training Dynamics:\n- Gradient estimation accuracy impacts stability\n- Adaptive update mechanisms improve convergence\n- Loss spike patterns provide optimization insights\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Specific implementations of adaptive normalization in production systems\n2. Real-world performance metrics and benchmarks\n3. Integration strategies with existing architectures\n4. Stability analysis methods\n\n5. References:\n\n[1] Bao, H., Hataya, R., & Karakida, R. (2024). Self-attention Networks Localize When QK-eigenspectrum Concentrates. arXiv preprint.\n\n[2] Han, S., et al. (2023). AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration. arXiv preprint.\n\n[3] Schmidt, L., et al. (2023). Stable and low-precision training for large-scale vision-language models. NeurIPS 2023.\n\n[4] Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint.\n\n[5] Xu, J., et al. (2019). Understanding and Improving Layer Normalization. arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"adaptive normalization language models\", \"hardware efficient normalization\", \"dynamic layer normalization\"",
            "detail": "Search for papers and implementations that discuss:\n\n1. Adaptive or dynamic normalization techniques in neural networks, particularly:\n- Content-dependent parameter adjustment\n- Hardware-efficient implementations\n- Training stability considerations\n\n2. Specific focus on:\n- RMSNorm variants and improvements\n- Memory access optimization for normalization\n- Gradient flow in adaptive systems\n\n3. Implementation details of:\n- Dynamic parameter computation\n- Efficient memory management\n- Hardware optimization techniques\n\nInclude both theoretical analyses and practical implementations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations that discuss:\n\n1. Adaptive or dynamic normalization techniques in neural networks, particularly:\n- Content-dependent parameter adjustment\n- Hardware-efficient implementations\n- Training stability considerations\n\n2. Specific focus on:\n- RMSNorm variants and improvements\n- Memory access optimization for normalization\n- Gradient flow in adaptive systems\n\n3. Implementation details of:\n- Dynamic parameter computation\n- Efficient memory management\n- Hardware optimization techniques\n\nInclude both theoretical analyses and practical implementations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.12)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.12)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 2. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.06)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.06)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 3. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.04)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 28/54 (Score: 0.04)*\n\n```\nIn Advances in Neural Information Processing Systems, pages 15544-15553, 2019. [72] Aaron R Voelker and Chris Eliasmith. Improving spiking dynamical networks: Accurate delays, higherorder synapses, and time cells. Neural computation, 30(3):569-609, 2018. [73] Aaron Russell Voelker. Dynamical systems in spiking neuromorphic hardware. PhD thesis, University of Waterloo, 2019. [74] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin, and Michael Auli. Pay less attention with lightweight and dynamic convolutions. In The International Conference on Learning Representations $(I C L R), 2019$. [75] Jiasong Wu, Lu Wang, Guanyu Yang, Lotfi Senhadji, Limin Luo, and Huazhong Shu. Sliding conjugate symmetric sequency-ordered complex Hadamard transform: fast algorithm and applications. IEEE Transactions on Circuits and Systems I: Regular Papers, 59(6):1321-1334, 2012. [76] Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel S Schoenholz. A mean field theory of batch normalization.\n```\n\n#### 4. Linearizing Large Language Models (Avg. Score: 0.03)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.03)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 5. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.03)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.03)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive normalization language models\", \"hardware efficient normalization\", \"dynamic layer normalization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"adaptive normalization language models\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**TL;DR:** The low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter, is introduced and achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 3*)\n\n#### 2. Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts\n\n*From Search Query: \"adaptive normalization language models\"*\n\n*Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su*\n\n**TL;DR:** This first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts is presented, and it is found that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing.\n\n**Abstract:** By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also demonstrate a strong confirmation bias when the external evidence contains some information that is consistent with their parametric memory, despite being presented with conflicting evidence at the same time. These results pose important implications that are worth careful consideration for the further development and deployment of tool- and retrieval-augmented LLMs. Resources are available at https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 105  (*Influential: 20*)\n\n#### 3. AdaPlanner: Adaptive Planning from Feedback with Language Models\n\n*From Search Query: \"adaptive normalization language models\"*\n\n*Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang*\n\n**TL;DR:** A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\n\n**Abstract:** Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 86  (*Influential: 18*)\n\n#### 4. MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Neo Wei Ming, Zhehui Wang, Cheng Liu, R. Goh, Tao Luo*\n\n**TL;DR:** MA-BERT is proposed, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss and enables hardware-friendly processing on various computing engines, including CPUs and GPUs.\n\n**Abstract:** Due to their superior results, Transformer-based models such as BERT have become de facto standards in many Natural Language Processing (NLP) applications. However, the intensive use of complex non-linear functions within the Transformer architecture impairs its computing efficiency and complicates corresponding accelerator designs, because non-linear functions are generally computation-intensive and require special hardware support. In light of this, we propose MA-BERT, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss. Specifically, we propose four correlated techniques that include approximating softmax with a two-layer neural network, replacing GELU with ReLU, fusing normalization layers with adjacent linear layers, and leveraging knowledge transfer from baseline models. Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy. With mainly regular matrix arithmetic operations, MA-BERT enables hardware-friendly processing on various computing engines, including CPUs and GPUs. Our experimental results show that MA-BERT achieves up to 27% and 41% reduction in inference time on CPU and GPU, respectively, with comparable accuracy on many downstream tasks compared to\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 71  (*Influential: 13*)\n\n#### 6. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 7. Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Dongkyu Lee, Zhiliang Tian, Lanqing Xue, N. Zhang*\n\n**TL;DR:** This paper proposes to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content, and creates not only style-independent content representation, but also content-dependent style representation in transferring style.\n\n**Abstract:** Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content. A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style. Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information. In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content. Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content. Our method creates not only style-independent content representation, but also content-dependent style representation in transferring style. Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation. In addition, it is also competitive in terms of style transfer accuracy and fluency.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 42  (*Influential: 8*)\n\n#### 8. Dynamic Token Normalization Improves Vision Transformer\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Wenqi Shao, Yixiao Ge, Zhaoyang Zhang, Xuyuan Xu, Xiaogang Wang, Ying Shan, P. Luo*\n\n**TL;DR:** This work proposes a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token), which can represent various existing normalization methods.\n\n**Abstract:** Vision Transformer (ViT) and its variants (e.g., Swin, PVT) have achieved great success in various computer vision tasks, owing to their capability to learn long-range contextual information. Layer Normalization (LN) is an essential ingredient in these models. However, we found that the ordinary LN makes tokens at different positions similar in magnitude because it normalizes embeddings within each token. It is difficult for Transformers to capture inductive bias such as the positional context in an image with LN. We tackle this problem by proposing a new normalizer, termed Dynamic Token Normalization (DTN), where normalization is performed both within each token (intra-token) and across different tokens (inter-token). DTN has several merits. Firstly, it is built on a unified formulation and thus can represent various existing normalization methods. Secondly, DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context. {Thirdly, by simply replacing LN layers, DTN can be readily plugged into various vision transformers, such as ViT, Swin, PVT, LeViT, T2T-ViT, BigBird and Reformer. Extensive experiments show that the transformer equipped with DTN consistently outperforms baseline model with minimal extra parameters and computational overhead. For example, DTN outperforms LN by $0.5\\%$ - $1.2\\%$ top-1 accuracy on ImageNet, by $1.2$ - $1.4$ box AP in object detection on COCO benchmark, by $2.3\\%$ - $3.9\\%$ mCE in robustness experiments on ImageNet-C, and by $0.5\\%$ - $0.8\\%$ accuracy in Long ListOps on Long-Range Arena.} Codes will be made public at \\url{https://github.com/wqshao126/DTN}\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 11  (*Influential: 1*)\n\n#### 9. Dynamic Normalization and Relay for Video Action Recognition\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Dongqi Cai, Anbang Yao, Yurong Chen*\n\n**TL;DR:** Dynamic Normalization and Relay (DNR), an improved normalization design, is presented, to augment the spatial-temporal representation learning of any deep action recognition model, adapting to small batch size training settings.\n\n**Abstract:** Convolutional Neural Networks (CNNs) have been the dominant model for video action recognition. Due to the huge memory and compute demand, popular action recognition networks need to be trained with small batch sizes, which makes learning discriminative spatial-temporal representations for videos become a challenging problem. In this paper, we present Dynamic Normalization and Relay (DNR), an improved normalization design, to augment the spatial-temporal representation learning of any deep action recognition model, adapting to small batch size training settings. We observe that state-of-the-art action recognition networks usually apply the same normalization parameters to all video data, and ignore the dependencies of the estimated normalization parameters between neighboring frames (at the same layer) and between neighboring layers (with all frames of a video clip). Inspired by this, DNR introduces two dynamic normalization relay modules to explore the potentials of cross-temporal and cross-layer feature distribution dependencies for estimating accurate layer-wise normalization parameters. These two DNR modules are instantiated as a light-weight recurrent structure conditioned on the current input features, and the normalization parameters estimated from the neighboring frames based features at the same layer or from the whole video clip based features at the preceding layers. We \ufb01rst plug DNR into prevailing 2D CNN backbones and test its performance on public action recognition datasets including Kinetics and Something-Something. Experimental results show that DNR brings large performance improvements to the baselines, achieving over 4.4 % absolute margins in top-1 accuracy without training bells and whistles. More experiments on 3D backbones and several latest 2D spatial-temporal networks further validate its effectiveness. Code will be available at https://github.com/caidonkey/dnr .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 4  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive normalization language models\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 2. Direct Multi-Turn Preference Optimization for Language Agents\n\n*From Search Query: \"adaptive normalization language models\"*\n\n*Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, Fuli Feng*\n\n**Abstract:** Adapting Large Language Models (LLMs) for agent tasks is critical in\ndeveloping language agents. Direct Preference Optimization (DPO) is a promising\ntechnique for this adaptation with the alleviation of compounding errors,\noffering a means to directly optimize Reinforcement Learning (RL) objectives.\nHowever, applying DPO to multi-turn tasks presents challenges due to the\ninability to cancel the partition function. Overcoming this obstacle involves\nmaking the partition function independent of the current state and addressing\nlength disparities between preferred and dis-preferred trajectories. In this\nlight, we replace the policy constraint with the state-action occupancy measure\nconstraint in the RL objective and add length normalization to the\nBradley-Terry model, yielding a novel loss function named DMPO for multi-turn\nagent tasks with theoretical explanations. Extensive experiments on three\nmulti-turn agent task datasets confirm the effectiveness and superiority of the\nDMPO loss.\n\n**Published:** 2024-06-21T05:13:20Z  (*Updated: 2024-08-17T09:33:12Z*)\n\n\n\n#### 3. Full Stack Optimization of Transformer Inference: a Survey\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, Amir Gholami*\n\n**Abstract:** Recent advances in state-of-the-art DNN architecture design have been moving\ntoward Transformer models. These models achieve superior accuracy across a wide\nrange of applications. This trend has been consistent over the past several\nyears since Transformer models were originally introduced. However, the amount\nof compute and bandwidth required for inference of recent Transformer models is\ngrowing at a significant rate, and this has made their deployment in\nlatency-sensitive applications challenging. As such, there has been an\nincreased focus on making Transformer models more efficient, with methods that\nrange from changing the architecture design, all the way to developing\ndedicated domain-specific accelerators. In this work, we survey different\napproaches for efficient Transformer inference, including: (i) analysis and\nprofiling of the bottlenecks in existing Transformer architectures and their\nsimilarities and differences with previous convolutional models; (ii)\nimplications of Transformer architecture on hardware, including the impact of\nnon-linear operations such as Layer Normalization, Softmax, and GELU, as well\nas linear operations, on hardware design; (iii) approaches for optimizing a\nfixed Transformer architecture; (iv) challenges in finding the right mapping\nand scheduling of operations for Transformer models; and (v) approaches for\noptimizing Transformer models by adapting the architecture using neural\narchitecture search. Finally, we perform a case study by applying the surveyed\noptimizations on Gemmini, the open-source, full-stack DNN accelerator\ngenerator, and we show how each of these approaches can yield improvements,\ncompared to previous benchmark results on Gemmini. Among other things, we find\nthat a full-stack co-design approach with the aforementioned methods can result\nin up to 88.7x speedup with a minimal performance degradation for Transformer\ninference.\n\n**Published:** 2023-02-27T18:18:13Z  (*Updated: 2023-02-27T18:18:13Z*)\n\n\n\n#### 4. Methods of improving LLM training stability\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Oleg Rybakov, Mike Chrzanowski, Peter Dykas, Jinze Xue, Ben Lanir*\n\n**Abstract:** Training stability of large language models(LLMs) is an important research\ntopic. Reproducing training instabilities can be costly, so we use a small\nlanguage model with 830M parameters and experiment with higher learning rates\nto force models to diverge. One of the sources of training instability is the\ngrowth of logits in attention layers. We extend the focus of the previous work\nand look not only at the magnitude of the logits but at all outputs of linear\nlayers in the Transformer block. We observe that with a high learning rate the\nL2 norm of all linear layer outputs can grow with each training step and the\nmodel diverges. Specifically we observe that QKV, Proj and FC2 layers have the\nlargest growth of the output magnitude. This prompts us to explore several\noptions: 1) apply layer normalization not only after QK layers but also after\nProj and FC2 layers too; 2) apply layer normalization after the QKV layer (and\nremove pre normalization). 3) apply QK layer normalization together with\nsoftmax capping. We show that with the last two methods we can increase\nlearning rate by 1.5x (without model divergence) in comparison to an approach\nbased on QK layer normalization only. Also we observe significant perplexity\nimprovements for all three methods in comparison to the baseline model.\n\n**Published:** 2024-10-22T04:27:03Z  (*Updated: 2024-10-22T04:27:03Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. QLoRA: Efficient Finetuning of Quantized LLMs\n\n*From Search Query: \"adaptive normalization language models\"*\n\n*Luke Zettlemoyer, Ari Holtzman, Artidoro Pagnoni, Tim Dettmers*\n\n**Abstract:** We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-23\n\n\n\n#### 2. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks\n\n*From Search Query: \"adaptive normalization language models\"*\n\n*Jie Tang, Zhilin Yang, Zhengxiao Du, Weng Lam Tam, Yicheng Fu, Kaixuan Ji, Xiao Liu*\n\n**Abstract:** Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning \\cite{li2021prefix,qin2021learning} optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.Our code and data are released at https://github.com/THUDM/P-tuning-v2.\n\n**Published:** 2021-10-14\n\n\n\n#### 3. Training wide residual networks for deployment using a single bit for each weight\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Mark D. McDonnell*\n\n**Abstract:** For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .\n\n**Conference:** training-wide-residual-networks-for-1\n\n**Published:** 2018-02-23\n\n\n\n#### 4. MimicNorm: Weight Mean and Last BN Layer Mimic the Dynamic of Batch Normalization\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Wenrui Dai, Hongkai Xiong, Junni Zou, Chenglin Li, Wen Fei*\n\n**Abstract:** Substantial experiments have validated the success of Batch Normalization (BN) Layer in benefiting convergence and generalization. However, BN requires extra memory and float-point calculation. Moreover, BN would be inaccurate on micro-batch, as it depends on batch statistics. In this paper, we address these problems by simplifying BN regularization while keeping two fundamental impacts of BN layers, i.e., data decorrelation and adaptive learning rate. We propose a novel normalization method, named MimicNorm, to improve the convergence and efficiency in network training. MimicNorm consists of only two light operations, including modified weight mean operations (subtract mean values from weight parameter tensor) and one BN layer before loss function (last BN layer). We leverage the neural tangent kernel (NTK) theory to prove that our weight mean operation whitens activations and transits network into the chaotic regime like BN layer, and consequently, leads to an enhanced convergence. The last BN layer provides autotuned learning rates and also improves accuracy. Experimental results show that MimicNorm achieves similar accuracy for various network structures, including ResNets and lightweight networks like ShuffleNet, with a reduction of about 20% memory consumption. The code is publicly available at https://github.com/Kid-key/MimicNorm.\n\n**Published:** 2020-10-19\n\n\n\n#### 5. Layer Normalization\n\n*From Search Query: \"dynamic layer normalization\"*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using adaptive or dynamic normalization techniques, here are some key findings and implementations that align with your requirements:\n\n## Adaptive Normalization Techniques\n\n### Content-Dependent Parameter Adjustment\n- The concept of **Adaptive RMSNorm** is particularly relevant, as it provides context-sensitive normalization by scaling features dynamically based on the input\u2019s global context. This approach is discussed in the context of the ModifiedLlamaDecoderLayer, where Adaptive RMSNorm normalizes the input while applying an adaptive scaling, which helps in maintaining stability in training and improving convergence.\n\n### RMSNorm Variants and Improvements\n- **Adaptive RMSNorm** is a variant of RMSNorm that adapts the normalization process based on the global context of each input batch. This is implemented in the ModifiedLlamaDecoderLayer, where it enhances the model\u2019s ability to generalize across diverse inputs.\n\n### Dynamic Layer Normalization\n- The **Mixture-of-Depths (MoD) framework** introduces an additional normalization module for each late layer, which is tuned individually to ensure specific adaptation for each layer. This approach leverages the predictive power from intermediate representations during training and includes dynamic normalization to adapt the outputs of late layers to language modeling tasks.\n\n## Hardware-Efficient Implementations\n\n### Memory Access Optimization\n- **Cluster-Based Normalization (CBN)** organizes data into predefined structures or contexts prior to training and normalizes based on these contexts. This approach can reduce computational demands by grouping data with similar characteristics, which can be more hardware-efficient compared to traditional normalization techniques.\n\n### Memory Footprint Reduction and Computation Scheduling\n- The **MoD framework** minimally increases trainable parameters by leveraging intermediate layers, which can reduce the memory footprint and computational requirements. This is achieved by training late layers as ensembles contributing to the final logits through learned routing weights, without adding significant additional parameters.\n\n## Training Stability Considerations\n\n### Gradient Flow in Adaptive Systems\n- **Universal Self-Knowledge Distillation (USKD)** and **fine-tuning** with normalized outputs can help in stabilizing the training process. By using the student model as its own teacher, USKD refines the model's outputs based on its previous learning, which can improve training stability.\n\n### Gradient Stabilization Through Adaptive Updates\n- The use of **adaptive normalization** techniques, such as Adaptive RMSNorm, helps in smoothing out variations in the input data, which can lead to improved convergence and robustness during training. This is particularly beneficial in complex tasks where input-specific details need to be retained.\n\n## Implementation Details\n\n### Dynamic Parameter Computation\n- In the **ModifiedLlamaDecoderLayer**, dynamic parameter computation is achieved through Adaptive RMSNorm, which adjusts the normalization parameters based on the global context of each input batch. This dynamic computation is integrated into the model\u2019s decoder layer and complements other components like Token Mixing and SEBlock.\n\n### Efficient Memory Management\n- **Adaptive Context Normalization (ACN)** dynamically builds contexts in the latent space during training, which helps in organizing data into groups with similar characteristics. This approach can optimize memory usage by normalizing data within these contexts, reducing the computational demands associated with traditional normalization methods.\n\n### Hardware Optimization Techniques\n- The **MoD framework** and **CBN** both offer hardware optimization by leveraging intermediate representations and organizing data into contexts, respectively. These approaches can be integrated into existing architectures to improve performance with minimal additional parameters and computational overhead.\n\nBy incorporating these adaptive normalization techniques, focusing on content-dependent parameter adjustment, and optimizing for hardware efficiency, the researchers can potentially improve the performance, stability, and scalability of their autoregressive language models.\n",
            "analysis": "Initial Analysis of AdaptiveNormTransformer Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) Adaptive Normalization Mechanism:\n- Novel content-dependent scaling parameters\n- Dynamic computation based on input complexity\n- Integration with RMSNorm\n- Potential impact on training stability\n\nb) Hardware Efficiency Claims:\n- Memory access optimization\n- Computation scheduling\n- Memory footprint reduction\n- Real-world performance implications\n\nc) Training Stability:\n- Gradient stabilization through adaptive updates\n- Consistency of optimization objectives\n- Impact on deep network training\n\n2. Investigation Plan:\n\nPhase 1: Examine Existing Adaptive Normalization Approaches\n- Search for papers on adaptive/dynamic normalization\n- Analyze implementations in current architectures\n- Identify key differences from proposed approach\n\nPhase 2: Hardware Efficiency Validation\n- Research hardware-optimized normalization techniques\n- Study memory access patterns and scheduling\n- Evaluate real-world performance metrics\n\nPhase 3: Training Stability Analysis\n- Investigate gradient flow in adaptive systems\n- Study optimization techniques for dynamic parameters\n- Analyze stability in deep networks\n\n3. Initial Assessment:\n\nStrengths:\n- Novel integration of content-dependent normalization\n- Focus on hardware efficiency\n- Potential for improved adaptability\n\nConcerns:\n- Implementation complexity\n- Training stability with dynamic parameters\n- Hardware optimization challenges\n\n4. Comparison with Previous Designs:\n- More focused on normalization compared to sibling designs\n- Different approach to efficiency (normalization vs. compression)\n- Unique emphasis on hardware optimization\n\nThe proposal appears to offer a novel approach to improving language model performance through adaptive normalization, but requires thorough investigation of existing work and potential implementation challenges."
        },
        {
            "ready": false,
            "query": "\"adaptive parameter computation language models\", \"memory efficient dynamic normalization\", \"training stability adaptive systems\"",
            "detail": "Search for papers and implementations discussing:\n\n1. Adaptive parameter computation techniques:\n- Dynamic parameter generation methods\n- Efficient update mechanisms\n- Integration with transformer architectures\n\n2. Memory-efficient implementations:\n- Optimized memory access patterns\n- Parameter sharing strategies\n- Hardware-aware design considerations\n\n3. Training stability mechanisms:\n- Gradient flow control\n- Adaptive learning rate schemes\n- Normalization stability techniques\n\nInclude both theoretical frameworks and practical implementations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations discussing:\n\n1. Adaptive parameter computation techniques:\n- Dynamic parameter generation methods\n- Efficient update mechanisms\n- Integration with transformer architectures\n\n2. Memory-efficient implementations:\n- Optimized memory access patterns\n- Parameter sharing strategies\n- Hardware-aware design considerations\n\n3. Training stability mechanisms:\n- Gradient flow control\n- Adaptive learning rate schemes\n- Normalization stability techniques\n\nInclude both theoretical frameworks and practical implementations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.26)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.26)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.21)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 37/49 (Score: 0.21)*\n\n```\nCoRR, abs/1911.02150, 2019. URLhttp://arxiv.org/abs/1911.02150. Noam Shazeer. GLU variants improve transformer, 2020. URLhttps://arxiv.org/abs/ 2002.05202\n\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. CoRR, abs/1804.04235, 2018. URL http://arxiv.org/abs/1804.04235. Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2022. URLhttps://arxiv.org/abs/2208.04933. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 331-335, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1032. URLhttps://aclanthology.org/P19-1032. Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 9902-9912. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/sukhbaatar21a.html. Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. Do long-range language models actually use long-range context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 807-822, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.62. URL/https://aclanthology.org/2021.emnlp-main.\n```\n\n#### 3. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.14)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.14)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 4. Fast Transformers with Clustered Attention (Avg. Score: 0.12)\n\n*Apoorv Vyas, Angelos Katharopoulos, Franccois Fleuret*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 125  (*Influential: 7*)\n\n**TL;DR:** Closed attention is proposed, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids, which results in a model with linear complexity with respect to the sequence length for a fixed number of clusters.\n\n**Abstract:** Transformers have been proven a successful model for a variety of tasks in sequence modeling. However, computing the attention matrix, which is their key component, has quadratic complexity with respect to the sequence length, thus making them prohibitively expensive for large sequences. To address this, we propose clustered attention, which instead of computing the attention for every query, groups queries into clusters and computes attention just for the centroids. To further improve this approximation, we use the computed clusters to identify the keys with the highest attention per query and compute the exact key/query dot products. This results in a model with linear complexity with respect to the sequence length for a fixed number of clusters. We evaluate our approach on two automatic speech recognition datasets and show that our model consistently outperforms vanilla transformers for a given computational budget. Finally, we demonstrate that our model can approximate arbitrarily complex attention distributions with a minimal number of clusters by approximating a pretrained BERT model on GLUE and SQuAD benchmarks with only 25 clusters and no loss in performance.\n\n##### *Relevant Chunk: No. 8/27 (Score: 0.12)*\n\n```\n517-520. IEEE, 1992. [11] Graves, A., Fern\u00e1ndez, S., Gomez, F., and Schmidhuber, J. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, 2006. [12] Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. [13] Kitaev, N., Kaiser, L., and Levskaya, A. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. URL https: / / openreview. net/ forum?id=rkgNKkHtvB\n[14] Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., and Teh, Y. W. Set transformer: A framework for attention-based permutation-invariant neural networks. In International Conference on Machine Learning, 2019. [15] Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J. On the variance of the adaptive learning rate and beyond.\n```\n\n#### 5. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.12)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 39/51 (Score: 0.12)*\n\n```\nIn International conference on machine learning, pages 5156-5165. PMLR, 2020. [42] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [43] Louis Kirsch and J\u00fcrgen Schmidhuber. Meta learning backpropagation and improving it. Advances in Neural Information Processing Systems, 34:14122-14134, 2021. [44] Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. In International Conference on Machine Learning, pages 2766-2775. PMLR, 2018. [45] Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of transformer language models. arXiv preprint arXiv:1904.08378, 2019. [46] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023. [47] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. [48] Quoc V Le. Building high-level features using large scale unsupervised learning. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 8595-8598. IEEE, 2013. [49] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. [50] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics (ToG), 39(4):71-1, 2020. [51] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International conference on machine learning, pages 2113-2122. PMLR, 2015. [52] Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning update rules for unsupervised representation learning.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive parameter computation language models\", \"memory efficient dynamic normalization\", \"training stability adaptive systems\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. MicroNet for Efficient Language Modeling\n\n*From Search Query: \"adaptive parameter computation language models\"*\n\n*Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han*\n\n**TL;DR:** This work improves upon recent advances in both the language modeling domain and the model-compression domain to construct parameter and computation efficient language models that are 90 times more parameter-efficient and 36 times more computation-efficient while achieving the required test perplexity of 35 on the Wikitext-103 dataset.\n\n**Abstract:** It is important to design compact language models for efficient deployment. We improve upon recent advances in both the language modeling domain and the model-compression domain to construct parameter and computation efficient language models. We use an efficient transformer-based architecture with adaptive embedding and softmax, differentiable non-parametric cache, Hebbian softmax, knowledge distillation, network pruning, and low-bit quantization. In this paper, we provide the winning solution to the NeurIPS 2019 MicroNet Challenge in the language modeling track. Compared to the baseline language model provided by the MicroNet Challenge, our model is 90 times more parameter-efficient and 36 times more computation-efficient while achieving the required test perplexity of 35 on the Wikitext-103 dataset. We hope that this work will aid future research into efficient language models, and we have released our full source code at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 2. Dynamic and Efficient Inference for Text Generation via BERT Family\n\n*From Search Query: \"adaptive parameter computation language models\"*\n\n*Xiaobo Liang, Juntao Li, Lijun Wu, Ziqiang Cao, M. Zhang*\n\n**TL;DR:** A novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency and explores the effectiveness of the pre- trained MLMs for text generation tasks.\n\n**Abstract:** Despite the excellent performance of Pre-trained Language Models on many text generation tasks, they suffer from inefficient inference on computation and memory due to their large-scale parameters and the universal autoregressive decoding paradigm. In this work, we propose a novel fine-tuning method DEER, which can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency. In particular, our critical insight is to jointly utilize the non-autoregressive (NAR) generation and dynamic parameter pruning techniques, which can flexibly control the decoding iteration steps and model sizes according to memory and latency limitations. Besides, we also explore the effectiveness of the pre-trained MLMs (i.e., the BERT family) for text generation tasks since their bidirectional attention nature is more suitable for the NAR training objective. Extensive experiments on both monolingual and multilingual pre-trained MLMs demonstrate the effectiveness of our proposed DEER method by consistently achieving (1) higher BLEU scores than the strong autoregressive Transformer model on three neural machine translation tasks with 3 \\to 12 times speedup, (2) competitive performance (but with much faster inference speed) compared with the BART model on four GLGE benchmark tasks. Our code will be publicly available at GitHubhttps://github.com/dropreg/DEER.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 3. Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization\n\n*From Search Query: \"adaptive parameter computation language models\"*\n\n*Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, H. Dai, D. Dou*\n\n**TL;DR:** A Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs and a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further.\n\n**Abstract:** Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8\\% in terms of accuracy) and efficiency (up to 97.59\\% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 36  (*Influential: 1*)\n\n#### 4. Dynamic Sparse Graph for Efficient Deep Learning\n\n*From Search Query: \"memory efficient dynamic normalization\"*\n\n*L. Liu, Lei Deng, Xing Hu, Maohua Zhu, Guoqi Li, Yufei Ding, Yuan Xie*\n\n**TL;DR:** DSG activates only a small amount of neurons with high selectivity at each iteration via a dimension-reduction search (DRS) and obtains the BN compatibility via a double-mask selection (DMS) with little accuracy loss on various benchmarks.\n\n**Abstract:** We propose to execute deep neural networks (DNNs) with dynamic and sparse graph (DSG) structure for compressive memory and accelerative execution during both training and inference. The great success of DNNs motivates the pursuing of lightweight models for the deployment onto embedded devices. However, most of the previous studies optimize for inference while neglect training or even complicate it. Training is far more intractable, since (i) the neurons dominate the memory cost rather than the weights in inference; (ii) the dynamic activation makes previous sparse acceleration via one-off optimization on fixed weight invalid; (iii) batch normalization (BN) is critical for maintaining accuracy while its activation reorganization damages the sparsity. To address these issues, DSG activates only a small amount of neurons with high selectivity at each iteration via a dimension-reduction search (DRS) and obtains the BN compatibility via a double-mask selection (DMS). Experiments show significant memory saving (1.7-4.5x) and operation reduction (2.3-4.4x) with little accuracy loss on various benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 40  (*Influential: 6*)\n\n#### 5. Masked Space-Time Hash Encoding for Efficient Dynamic Scene Reconstruction\n\n*From Search Query: \"memory efficient dynamic normalization\"*\n\n*Feng Wang, Zilong Chen, Guokang Wang, Yafei Song, Huaping Liu*\n\n**TL;DR:** The Masked Space-Time Hash encoding (MSTH) is a novel method for efficiently reconstructing dynamic 3D scenes from multi-view or monocular videos that can reduce the hash collision rate by avoiding redundant queries and modifications on static areas.\n\n**Abstract:** In this paper, we propose the Masked Space-Time Hash encoding (MSTH), a novel method for efficiently reconstructing dynamic 3D scenes from multi-view or monocular videos. Based on the observation that dynamic scenes often contain substantial static areas that result in redundancy in storage and computations, MSTH represents a dynamic scene as a weighted combination of a 3D hash encoding and a 4D hash encoding. The weights for the two components are represented by a learnable mask which is guided by an uncertainty-based objective to reflect the spatial and temporal importance of each 3D position. With this design, our method can reduce the hash collision rate by avoiding redundant queries and modifications on static areas, making it feasible to represent a large number of space-time voxels by hash tables with small size.Besides, without the requirements to fit the large numbers of temporally redundant features independently, our method is easier to optimize and converge rapidly with only twenty minutes of training for a 300-frame dynamic scene.As a result, MSTH obtains consistently better results than previous methods with only 20 minutes of training time and 130 MB of memory storage. Code is available at https://github.com/masked-spacetime-hashing/msth\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 2*)\n\n#### 6. SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization\n\n*From Search Query: \"training stability adaptive systems\"*\n\n*Amir Hertz, Or Perel, R. Giryes, O. Sorkine-Hornung, D. Cohen-Or*\n\n**TL;DR:** A spatially adaptive progressive encoding scheme for input signals of MLP networks is presented, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing.\n\n**Abstract:** Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 60  (*Influential: 6*)\n\n#### 7. Can Adversarial Training Be Manipulated By Non-Robust Features?\n\n*From Search Query: \"training stability adaptive systems\"*\n\n*Lue Tao, Lei Feng, Hongxin Wei, Jinfeng Yi, Sheng-Jun Huang, Songcan Chen*\n\n**TL;DR:** This paper identifies a novel threat model named stability attack, which aims to hinder robust availability by slightly manipulating the training data, and shows that adversarial training using a conventional defense budget $\\epsilon$ provably fails to provide test robustness in a simple statistical setting.\n\n**Abstract:** Adversarial training, originally designed to resist test-time adversarial examples, has shown to be promising in mitigating training-time availability attacks. This defense ability, however, is challenged in this paper. We identify a novel threat model named stability attack, which aims to hinder robust availability by slightly manipulating the training data. Under this threat, we show that adversarial training using a conventional defense budget $\\epsilon$ provably fails to provide test robustness in a simple statistical setting, where the non-robust features of the training data can be reinforced by $\\epsilon$-bounded perturbation. Further, we analyze the necessity of enlarging the defense budget to counter stability attacks. Finally, comprehensive experiments demonstrate that stability attacks are harmful on benchmark datasets, and thus the adaptive defense is necessary to maintain robustness. Our code is available at https://github.com/TLMichael/Hypocritical-Perturbation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 11  (*Influential: 2*)\n\n#### 8. AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients\n\n*From Search Query: \"training stability adaptive systems\"*\n\n*Juntang Zhuang, Tommy M. Tang, Yifan Ding, S. Tatikonda, N. Dvornek, X. Papademetris, J. Duncan*\n\n**TL;DR:** AdaBelief is proposed to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability; it outperforms other methods with fast convergence and high accuracy on image classification and language modeling.\n\n**Abstract:** Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the \"belief\" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at this https URL\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 437  (*Influential: 103*)\n\n### 6 related papers from ArXiv\n\n#### 1. Hadamard Adapter: An Extreme Parameter-Efficient Adapter Tuning Method\n  for Pre-trained Language Models\n\n*From Search Query: \"adaptive parameter computation language models\"*\n\n*Yuyan Chen, Qiang Fu, Ge Fan, Lun Du, Jian-Guang Lou, Shi Han, Dongmei Zhang, Zhixu Li, Yanghua Xiao*\n\n**Abstract:** Recent years, Pre-trained Language models (PLMs) have swept into various\nfields of artificial intelligence and achieved great success. However, most\nPLMs, such as T5 and GPT3, have a huge amount of parameters, fine-tuning them\nis often expensive and time consuming, and storing them takes up a lot of\nspace. Therefore, it is necessary to adopt a parameter-efficient approach to\nreduce parameters of PLMs in fine-tuning without compromising their performance\nin downstream tasks. In this paper, we design a novel adapter which only acts\non self-attention outputs in PLMs. This adapter adopts element-wise linear\ntransformation using Hadamard product, hence named as Hadamard adapter,\nrequires the fewest parameters compared to previous parameter-efficient\nadapters. In addition, we also summarize some tuning patterns for Hadamard\nadapter shared by various downstream tasks, expecting to provide some guidance\nfor further parameter reduction with shared adapters in future studies. The\nexperiments conducted on the widely-used GLUE benchmark with several SOTA PLMs\nprove that the Hadamard adapter achieves competitive performance with only\n0.033\\% parameters compared with full fine-tuning, and it has the fewest\nparameters compared with other adapters. Moreover, we further find that there\nis also some redundant layers in the Hadamard adapter which can be removed to\nachieve more parameter efficiency with only 0.022\\% parameters.\n\n**Published:** 2024-07-04T18:21:28Z  (*Updated: 2024-07-04T18:21:28Z*)\n\n\n\n#### 2. Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain\n\n*From Search Query: \"adaptive parameter computation language models\"*\n\n*Aryo Pradipta Gema, Pasquale Minervini, Luke Daines, Tom Hope, Beatrice Alex*\n\n**Abstract:** Adapting pretrained language models to novel domains, such as clinical\napplications, traditionally involves retraining their entire set of parameters.\nParameter-Efficient Fine-Tuning (PEFT) techniques for fine-tuning language\nmodels significantly reduce computational requirements by selectively\nfine-tuning small subsets of parameters. In this study, we propose a two-step\nPEFT framework and evaluate it in the clinical domain. Our approach combines a\nspecialised PEFT adapter layer designed for clinical domain adaptation with\nanother adapter specialised for downstream tasks. We evaluate the framework on\nmultiple clinical outcome prediction datasets, comparing it to clinically\ntrained language models. Our framework achieves a better AUROC score averaged\nacross all clinical downstream tasks compared to clinical language models. In\nparticular, we observe large improvements of 4-5% AUROC in large-scale\nmultilabel classification tasks, such as diagnoses and procedures\nclassification. To our knowledge, this study is the first to provide an\nextensive empirical analysis of the interplay between PEFT techniques and\ndomain adaptation in an important real-world domain of clinical applications.\n\n**Published:** 2023-07-06T15:06:41Z  (*Updated: 2024-06-09T17:00:36Z*)\n\n\n\n#### 3. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"memory efficient dynamic normalization\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 4. APT: Adaptive Pruning and Tuning Pretrained Language Models for\n  Efficient Training and Inference\n\n*From Search Query: \"memory efficient dynamic normalization\"*\n\n*Bowen Zhao, Hannaneh Hajishirzi, Qingqing Cao*\n\n**Abstract:** Fine-tuning and inference with large Language Models (LM) are generally known\nto be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces\ntraining memory by updating a small number of LM parameters but does not\nimprove inference efficiency. Structured pruning improves LM inference\nefficiency by removing consistent parameter blocks, yet often increases\ntraining memory and time. To improve both training and inference efficiency, we\nintroduce APT that adaptively prunes and tunes parameters for the LMs. At the\nearly stage of fine-tuning, APT dynamically adds salient tuning parameters for\nfast and accurate convergence while discarding unimportant parameters for\nefficiency. Compared to baselines, our experiments show that APT maintains up\nto 98% task performance when pruning RoBERTa and T5 models with 40% parameters\nleft while keeping 86.4% LLaMA models' performance with 70% parameters\nremained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces\nlarge LMs memory training footprint by up to 70%.\n\n**Published:** 2024-01-22T18:39:40Z  (*Updated: 2024-06-04T06:39:23Z*)\n\n\n\n#### 5. Empirical Analysis of Efficient Fine-Tuning Methods for Large\n  Pre-Trained Language Models\n\n*From Search Query: \"training stability adaptive systems\"*\n\n*Nigel Doering, Cyril Gorlla, Trevor Tuttle, Adhvaith Vijay*\n\n**Abstract:** Fine-tuning large pre-trained language models for downstream tasks remains a\ncritical challenge in natural language processing. This paper presents an\nempirical analysis comparing two efficient fine-tuning methods - BitFit and\nadapter modules - to standard full model fine-tuning. Experiments conducted on\nGLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The\nBitFit approach, which trains only bias terms and task heads, matches full\nfine-tuning performance across varying amounts of training data and time\nconstraints. It demonstrates remarkable stability even with only 30\\% of data,\noutperforming full fine-tuning at intermediate data levels. Adapter modules\nexhibit high variability, with inconsistent gains over default models. The\nfindings indicate BitFit offers an attractive balance between performance and\nparameter efficiency. Our work provides valuable perspectives on model tuning,\nemphasizing robustness and highlighting BitFit as a promising alternative for\nresource-constrained or streaming task settings. The analysis offers actionable\nguidelines for efficient adaptation of large pre-trained models, while\nillustrating open challenges in stabilizing techniques like adapter modules.\n\n**Published:** 2024-01-08T17:44:43Z  (*Updated: 2024-01-08T17:44:43Z*)\n\n\n\n#### 6. A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA\n\n*From Search Query: \"training stability adaptive systems\"*\n\n*Damjan Kalajdzievski*\n\n**Abstract:** As large language models (LLMs) have become increasingly compute and memory\nintensive, parameter-efficient fine-tuning (PEFT) methods are now a common\nstrategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA),\nwhich adds trainable low-rank \"adapters\" to selected layers. Each adapter\nconsists of a low-rank matrix product, multiplicatively scaled by a\nrank-dependent factor. This scaling factor, which divides adapters by a factor\nof the rank, results in slowed learning and stunted performance for LoRA with\nhigher-rank adapters. Consequently, the use of LoRA in practice has generally\nbeen limited to very low ranks. In this work, we study the impact of the\nscaling factor on the learning process and prove that LoRA adapters should be\ndivided by a factor of the square root of the rank. Modifying LoRA with the\nappropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA)\nmethod, easily provides for a fine-tuning compute/performance trade-off, where\nlarger ranks can be used to trade off increased computational resources during\ntraining for better fine-tuning performance, with no change in inference\ncomputing cost.\n\n**Published:** 2023-11-28T03:23:20Z  (*Updated: 2023-11-28T03:23:20Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Adaptive Computation Time for Recurrent Neural Networks\n\n*From Search Query: \"adaptive parameter computation language models\"*\n\n*Alex Graves*\n\n**Abstract:** This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.\n\n**Published:** 2016-03-29\n\n\n\n#### 2. Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training\n\n*From Search Query: \"adaptive parameter computation language models\"*\n\n*Tengyu Ma, Percy Liang, David Hall, Zhiyuan Li, Hong Liu*\n\n**Abstract:** Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT models of sizes ranging from 125M to 1.5B, Sophia achieves a 2x speed-up compared to Adam in the number of steps, total compute, and wall-clock time, achieving the same perplexity with 50% fewer steps, less total compute, and reduced wall-clock time. Theoretically, we show that Sophia, in a much simplified setting, adapts to the heterogeneous curvatures in different parameter dimensions, and thus has a run-time bound that does not depend on the condition number of the loss.\n\n**Published:** 2023-05-23\n\n\n\n#### 3. Long-Short Transformer: Efficient Transformers for Language and Vision\n\n*From Search Query: \"memory efficient dynamic normalization\"*\n\n*Bryan Catanzaro, Anima Anandkumar, Tom Goldstein, Mohammad Shoeybi, Chaowei Xiao, Wei Ping, Chen Zhu*\n\n**Abstract:** Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-05\n\n\n\n#### 4. Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications\n\n*From Search Query: \"memory efficient dynamic normalization\"*\n\n*Jifeng Dai, Jie zhou, Lewei Lu, Yu Qiao, Hongsheng Li, Tong Lu, Wenhai Wang, Jiapeng Luo, Xizhou Zhu, Feng Wang, Yuntao Chen, Zhiqi Li, Yuwen Xiong*\n\n**Abstract:** We introduce Deformable Convolution v4 (DCNv4), a highly efficient and effective operator designed for a broad spectrum of vision applications. DCNv4 addresses the limitations of its predecessor, DCNv3, with two key enhancements: 1. removing softmax normalization in spatial aggregation to enhance its dynamic property and expressive power and 2. optimizing memory access to minimize redundant operations for speedup. These improvements result in a significantly faster convergence compared to DCNv3 and a substantial increase in processing speed, with DCNv4 achieving more than three times the forward speed. DCNv4 demonstrates exceptional performance across various tasks, including image classification, instance and semantic segmentation, and notably, image generation. When integrated into generative models like U-Net in the latent diffusion model, DCNv4 outperforms its baseline, underscoring its possibility to enhance generative models. In practical applications, replacing DCNv3 with DCNv4 in the InternImage model to create FlashInternImage results in up to 80% speed increase and further performance improvement without further modifications. The advancements in speed and efficiency of DCNv4, combined with its robust performance across diverse vision tasks, show its potential as a foundational building block for future vision models.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2024-01-11\n\n\n\n#### 5. On-Device Training Under 256KB Memory\n\n*From Search Query: \"training stability adaptive systems\"*\n\n*Song Han, Chuang Gan, Wei-Chen Wang, Wei-Ming Chen, Ligeng Zhu, Ji Lin*\n\n**Abstract:** On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource does not allow full back-propagation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/0pUFZYdoMY8.\n\n**Published:** 2022-06-30\n\n\n\n#### 6. Language and Noise Transfer in Speech Enhancement Generative Adversarial Network\n\n*From Search Query: \"training stability adaptive systems\"*\n\n*Kang-Hun Ahn, Joan Serr\u00e0, Santiago Pascual, Antonio Bonafonte, Maruchan Park*\n\n**Abstract:** Speech enhancement deep learning systems usually require large amounts of\ntraining data to operate in broad conditions or real applications. This makes\nthe adaptability of those systems into new, low resource environments an\nimportant topic. In this work, we present the results of adapting a speech\nenhancement generative adversarial network by finetuning the generator with\nsmall amounts of data. We investigate the minimum requirements to obtain a\nstable behavior in terms of several objective metrics in two very different\nlanguages: Catalan and Korean. We also study the variability of test\nperformance to unseen noise as a function of the amount of different types of\nnoise available for training. Results show that adapting a pre-trained English\nmodel with 10 min of data already achieves a comparable performance to having\ntwo orders of magnitude more data. They also demonstrate the relative stability\nin test performance with respect to the number of training noise types.\n\n**Published:** 2017-12-18\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on adaptive parameter computation, memory-efficient implementations, and training stability mechanisms, here are some key points and references that can be useful:\n\n## Adaptive Parameter Computation Techniques\n\n### Dynamic Parameter Generation Methods\n- **Dynamic Token Normalization (DTN):** This method learns to normalize tokens in both intra-token and inter-token manners, enabling transformers to capture global and local contextual information. This approach can be integrated into transformer architectures to improve stability and performance.\n\n### Efficient Update Mechanisms\n- **Parameter-Efficient Fine-Tuning (PEFT):** Techniques like PEFT involve updating only relevant parts of the model for specific downstream tasks, using trainable modules called adapters. This method is particularly useful for adapting large models without full fine-tuning, which can be computationally expensive and prone to overfitting.\n\n### Integration with Transformer Architectures\n- **Mixture of Experts (MoE):** This approach allows for training large models by distributing the computation across multiple experts, each handling a subset of the input data. This can be integrated with transformer architectures to make them more scalable and efficient.\n\n## Memory-Efficient Implementations\n\n### Optimized Memory Access Patterns\n- **Fused Operations:** Implementing fused operations can improve hardware utilization by reducing the number of memory accesses. For example, combining multiple linear layers or normalization layers into a single operation can enhance performance.\n\n### Parameter Sharing Strategies\n- **Adapter Frameworks:** Using adapters in transformer models allows for parameter sharing across different tasks and languages. This framework can significantly reduce the number of parameters that need to be updated during fine-tuning, making the model more memory-efficient.\n\n### Hardware-Aware Design Considerations\n- **MA-BERT:** This approach eliminates complex non-linear functions in transformer-based models, reducing them to matrix arithmetic and trivial ReLU operations. This simplification can lead to more hardware-efficient inference without compromising accuracy.\n\n## Training Stability Mechanisms\n\n### Gradient Flow Control\n- **Gradient Clipping and Normalization:** Techniques such as gradient clipping and normalization can help control the growth of logits in attention layers, which is a common source of training instability. Proper management of layer-wise normalization parameters is crucial.\n\n### Adaptive Learning Rate Schemes\n- **Learning Rate Schedules:** Using adaptive learning rate schedules, such as log-log learning rate schedules, can help in stabilizing the training process. These schedules adjust the learning rate based on the model's performance and can be particularly useful in large-scale training.\n\n### Normalization Stability Techniques\n- **Dynamic Token Normalization:** As mentioned earlier, DTN can help in stabilizing the model by learning to normalize tokens dynamically. This approach ensures that the model captures both global and local contextual information without introducing instability.\n\n## Additional Insights and References\n\n- **Relaxed Recursive Transformers:** This architecture introduces layer-wise low-rank adaptation, which can achieve high performance with reduced computational cost. This approach is particularly relevant for large language models and can be integrated with adaptive parameter computation techniques.\n\n- **Chain-of-Thought Prompting:** While not directly related to the technical aspects of model architecture, chain-of-thought prompting can improve model outputs, especially for larger models. This can be seen as a complementary approach to improve overall model performance.\n\nBy incorporating these techniques, researchers can develop autoregressive language models that are more efficient, stable, and scalable:\n\n- **Adaptive parameter computation:** Using DTN and PEFT can enhance model performance and stability.\n- **Memory-efficient implementations:** Fused operations, adapter frameworks, and hardware-aware designs like MA-BERT can optimize memory usage.\n- **Training stability mechanisms:** Gradient flow control, adaptive learning rate schemes, and dynamic normalization techniques can ensure stable training.\n\nThese approaches align with the goal of improving autoregressive language model design by addressing key challenges such as computational efficiency, training stability, and memory optimization.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe initial search reveals several important aspects of adaptive normalization in language models:\n\na) Core Concepts:\n- Content-dependent parameter adjustment for normalization\n- Hardware-efficient implementations of normalization layers\n- Dynamic adaptation mechanisms for improved stability\n- Memory optimization techniques\n\nb) Key Challenges:\n- Balancing computational efficiency with model performance\n- Maintaining training stability with adaptive parameters\n- Optimizing memory access patterns\n- Scaling to long sequences efficiently\n\n2. Useful References with Excerpts:\n\nFrom \"Methods of improving LLM training stability\":\n\"One of the sources of training instability is the growth of logits in attention layers... We observe that with a high learning rate the L2 norm of all linear layer outputs can grow with each training step and the model diverges.\"\n\nFrom \"Dynamic Token Normalization Improves Vision Transformer\":\n\"DTN learns to normalize tokens in both intra-token and inter-token manners, enabling Transformers to capture both the global contextual information and the local positional context.\"\n\nFrom \"MA-BERT\":\n\"Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy.\"\n\n3. Key Insights:\n\na) Normalization Stability:\n- Growth of logits in attention layers is a key source of instability\n- Layer-wise normalization parameters need careful management\n- Dynamic adaptation can improve stability but requires proper constraints\n\nb) Hardware Efficiency:\n- Memory access patterns significantly impact performance\n- Fused operations can improve hardware utilization\n- Careful balance needed between adaptivity and computational cost\n\nc) Implementation Considerations:\n- Need for efficient parameter updates\n- Importance of proper initialization\n- Trade-offs between flexibility and complexity\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Specific techniques for adaptive parameter computation\n2. Memory-efficient implementations of dynamic normalization\n3. Training stability mechanisms for adaptive systems\n4. Integration with existing transformer architectures\n\n5. References:\n\n[1] Rybakov, O., Chrzanowski, M., Dykas, P., Xue, J., & Lanir, B. (2024). Methods of improving LLM training stability. arXiv preprint.\n\n[2] Shao, W., Ge, Y., Zhang, Z., Xu, X., Wang, X., Shan, Y., & Luo, P. (2021). Dynamic Token Normalization Improves Vision Transformer. ICLR.\n\n[3] Wei Ming, N., Wang, Z., Liu, C., Goh, R., & Luo, T. (2023). MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions. ICLR.\n\n[4] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. ICML.\n\n[5] Fu, D. Y., Epstein, E. L., Nguyen, E. N. D., Zhang, M., & Dao, T. (2023). Simple Hardware-Efficient Long Convolutions for Sequence Modeling. ICML."
        },
        {
            "ready": false,
            "query": "\"adaptive normalization transformer\", \"hardware efficient transformer implementation\", \"gradient stabilization techniques\"",
            "detail": "Search for papers and implementations discussing:\n\n1. Adaptive normalization in transformer architectures:\n- Integration with attention mechanisms\n- Dynamic parameter adjustment\n- Impact on model performance\n\n2. Hardware-efficient implementations:\n- Memory access optimization\n- Computation scheduling\n- Resource utilization\n\n3. Gradient stabilization techniques:\n- Adaptive update mechanisms\n- Convergence properties\n- Training dynamics\n\nInclude both theoretical frameworks and practical implementations with empirical results.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations discussing:\n\n1. Adaptive normalization in transformer architectures:\n- Integration with attention mechanisms\n- Dynamic parameter adjustment\n- Impact on model performance\n\n2. Hardware-efficient implementations:\n- Memory access optimization\n- Computation scheduling\n- Resource utilization\n\n3. Gradient stabilization techniques:\n- Adaptive update mechanisms\n- Convergence properties\n- Training dynamics\n\nInclude both theoretical frameworks and practical implementations with empirical results.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.99)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.99)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.59)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 50/56 (Score: 0.59)*\n\n```\narXiv preprint arXiv:1609.08144, 2016. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.\n```\n\n#### 3. Normalized Attention Without Probability Cage (Avg. Score: 0.33)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.33)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 4. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.29)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 28/35 (Score: 0.29)*\n\n```\nApoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. 2020. Fast transformers with clustered attention. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In $7 t h$ International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and\nJingjing Liu. 2020a. Cluster-former: Clusteringbased sparse transformer for long-range dependency encoding. ArXiv preprint, abs/2009.06097. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020b. Linformer: Self-attention with linear complexity. ArXiv preprint, abs/2006.04768. Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman. 2019. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625-641. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 10524-10533. PMLR. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. Nystr\u00f6mformer: A nyst\u00f6m-based algorithm for approximating selfattention.\n```\n\n#### 5. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.22)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.22)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive normalization transformer\", \"hardware efficient transformer implementation\", \"gradient stabilization techniques\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo\n\n*From Search Query: \"adaptive normalization transformer\"*\n\n*Chenjie Cao, Xinlin Ren, Yanwei Fu*\n\n**TL;DR:** This paper introduces MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline, and achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Abstract:** Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Sketching as a Tool for Understanding and Accelerating Self-attention for Long Sequences\n\n*From Search Query: \"adaptive normalization transformer\"*\n\n*Yifan Chen, Qi Zeng, Dilek Z. Hakkani-T\u00fcr, Di Jin, Heng Ji, Yun Yang*\n\n**TL;DR:** This work proposes Skeinformer to accelerate self-attention and further improve the accuracy of matrix approximation to self-Attention with column sampling, adaptive row normalization and pilot sampling reutilization.\n\n**Abstract:** Transformer-based models are not efficient in processing long sequences due to the quadratic space and time complexity of the self-attention modules. To address this limitation, Linformer and Informer reduce the quadratic complexity to linear (modulo logarithmic factors) via low-dimensional projection and row selection, respectively. These two models are intrinsically connected, and to understand their connection we introduce a theoretical framework of matrix sketching. Based on the theoretical analysis, we propose Skeinformer to accelerate self-attention and further improve the accuracy of matrix approximation to self-attention with column sampling, adaptive row normalization and pilot sampling reutilization. Experiments on the Long Range Arena benchmark demonstrate that our methods outperform alternatives with a consistently smaller time/space footprint.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 3. Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective\n\n*From Search Query: \"adaptive normalization transformer\"*\n\n*Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen*\n\n**TL;DR:** A novel slice-level adaptive normalization scheme, referred to SAN, is proposed, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization and could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data.\n\n**Abstract:** Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN , which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available 2 .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 7*)\n\n#### 4. Block-Recurrent Transformers\n\n*From Search Query: \"hardware efficient transformer implementation\"*\n\n*DeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, Behnam Neyshabur*\n\n**Abstract:** We introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to sequence length. Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block in order to make efficient use of accelerator hardware. The cell itself is strikingly simple. It is merely a transformer layer: it uses self-attention and cross-attention to efficiently compute a recurrent function over a large set of state vectors and tokens. Our design was inspired in part by LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM cell up by several orders of magnitude. Our implementation of recurrence has the same cost in both computation time and parameter count as a conventional transformer layer, but offers dramatically improved perplexity in language modeling tasks over very long sequences. Our model out-performs a long-range Transformer XL baseline by a wide margin, while running twice as fast. We demonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source code. Our code has been released as open source.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 79  (*Influential: 11*)\n\n#### 5. Multi Resolution Analysis (MRA) for Approximate Self-Attention\n\n*From Search Query: \"hardware efficient transformer implementation\"*\n\n*Zhanpeng Zeng, Sourav Pal, Jeffery Kline, G. Fung, Vikas Singh*\n\n**TL;DR:** This paper revisits classical Multiresolution Analysis concepts such as Wavelets and shows that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a MRA-based approach for self-attention with an excellent performance profile across most criteria of interest.\n\n**Abstract:** Transformers have emerged as a preferred model for many tasks in natural langugage processing and vision. Recent efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified sparsity patterns, low-rank basis expansions and combinations thereof. In this paper, we revisit classical Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value in this setting remains underexplored thus far. We show that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a MRA-based approach for self-attention with an excellent performance profile across most criteria of interest. We undertake an extensive set of experiments and demonstrate that this multi-resolution scheme outperforms most efficient self-attention proposals and is favorable for both short and long sequences. Code is available at https://github.com/mlpen/mra-attention.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 6. Smoothness and Stability in GANs\n\n*From Search Query: \"gradient stabilization techniques\"*\n\n*Casey Chu, Kentaro Minami, K. Fukumizu*\n\n**TL;DR:** This work develops a principled theoretical framework for understanding the stability of various types of GANs and derives conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture.\n\n**Abstract:** Generative adversarial networks, or GANs, commonly display unstable behavior during training. In this work, we develop a principled theoretical framework for understanding the stability of various types of GANs. In particular, we derive conditions that guarantee eventual stationarity of the generator when it is trained with gradient descent, conditions that must be satisfied by the divergence that is minimized by the GAN and the generator's architecture. We find that existing GAN variants satisfy some, but not all, of these conditions. Using tools from convex analysis, optimal transport, and reproducing kernels, we construct a GAN that fulfills these conditions simultaneously. In the process, we explain and clarify the need for various existing GAN stabilization techniques, including Lipschitz constraints, gradient penalties, and smooth activation functions.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 52  (*Influential: 4*)\n\n#### 7. Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability\n\n*From Search Query: \"gradient stabilization techniques\"*\n\n*Alexandru Damian, Eshaan Nichani, Jason D. Lee*\n\n**TL;DR:** It is demonstrated that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored.\n\n**Abstract:** Traditional analyses of gradient descent show that when the largest eigenvalue of the Hessian, also known as the sharpness $S(\\theta)$, is bounded by $2/\\eta$, training is\"stable\"and the training loss decreases monotonically. Recent works, however, have observed that this assumption does not hold when training modern neural networks with full batch or large batch gradient descent. Most recently, Cohen et al. (2021) observed two important phenomena. The first, dubbed progressive sharpening, is that the sharpness steadily increases throughout training until it reaches the instability cutoff $2/\\eta$. The second, dubbed edge of stability, is that the sharpness hovers at $2/\\eta$ for the remainder of training while the loss continues decreasing, albeit non-monotonically. We demonstrate that, far from being chaotic, the dynamics of gradient descent at the edge of stability can be captured by a cubic Taylor expansion: as the iterates diverge in direction of the top eigenvector of the Hessian due to instability, the cubic term in the local Taylor expansion of the loss function causes the curvature to decrease until stability is restored. This property, which we call self-stabilization, is a general property of gradient descent and explains its behavior at the edge of stability. A key consequence of self-stabilization is that gradient descent at the edge of stability implicitly follows projected gradient descent (PGD) under the constraint $S(\\theta) \\le 2/\\eta$. Our analysis provides precise predictions for the loss, sharpness, and deviation from the PGD trajectory throughout training, which we verify both empirically in a number of standard settings and theoretically under mild conditions. Our analysis uncovers the mechanism for gradient descent's implicit bias towards stability.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 63  (*Influential: 8*)\n\n#### 8. Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search\n\n*From Search Query: \"gradient stabilization techniques\"*\n\n*Reid Pryzant, Dan Iter, Jerry Li, Y. Lee, Chenguang Zhu, Michael Zeng*\n\n**TL;DR:** Preliminary results suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.\n\n**Abstract:** Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language\"gradients\"that criticize the current prompt. The gradients are then\"propagated\"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 204  (*Influential: 36*)\n\n### 6 related papers from ArXiv\n\n#### 1. Full Stack Optimization of Transformer Inference: a Survey\n\n*From Search Query: \"adaptive normalization transformer\"*\n\n*Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, Amir Gholami*\n\n**Abstract:** Recent advances in state-of-the-art DNN architecture design have been moving\ntoward Transformer models. These models achieve superior accuracy across a wide\nrange of applications. This trend has been consistent over the past several\nyears since Transformer models were originally introduced. However, the amount\nof compute and bandwidth required for inference of recent Transformer models is\ngrowing at a significant rate, and this has made their deployment in\nlatency-sensitive applications challenging. As such, there has been an\nincreased focus on making Transformer models more efficient, with methods that\nrange from changing the architecture design, all the way to developing\ndedicated domain-specific accelerators. In this work, we survey different\napproaches for efficient Transformer inference, including: (i) analysis and\nprofiling of the bottlenecks in existing Transformer architectures and their\nsimilarities and differences with previous convolutional models; (ii)\nimplications of Transformer architecture on hardware, including the impact of\nnon-linear operations such as Layer Normalization, Softmax, and GELU, as well\nas linear operations, on hardware design; (iii) approaches for optimizing a\nfixed Transformer architecture; (iv) challenges in finding the right mapping\nand scheduling of operations for Transformer models; and (v) approaches for\noptimizing Transformer models by adapting the architecture using neural\narchitecture search. Finally, we perform a case study by applying the surveyed\noptimizations on Gemmini, the open-source, full-stack DNN accelerator\ngenerator, and we show how each of these approaches can yield improvements,\ncompared to previous benchmark results on Gemmini. Among other things, we find\nthat a full-stack co-design approach with the aforementioned methods can result\nin up to 88.7x speedup with a minimal performance degradation for Transformer\ninference.\n\n**Published:** 2023-02-27T18:18:13Z  (*Updated: 2023-02-27T18:18:13Z*)\n\n\n\n#### 2. Understanding and Improving Layer Normalization\n\n*From Search Query: \"adaptive normalization transformer\"*\n\n*Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, Junyang Lin*\n\n**Abstract:** Layer normalization (LayerNorm) is a technique to normalize the distributions\nof intermediate layers. It enables smoother gradients, faster training, and\nbetter generalization accuracy. However, it is still unclear where the\neffectiveness stems from. In this paper, our main contribution is to take a\nstep further in understanding LayerNorm. Many of previous studies believe that\nthe success of LayerNorm comes from forward normalization. Unlike them, we find\nthat the derivatives of the mean and variance are more important than forward\nnormalization by re-centering and re-scaling backward gradients. Furthermore,\nwe find that the parameters of LayerNorm, including the bias and gain, increase\nthe risk of over-fitting and do not work in most cases. Experiments show that a\nsimple version of LayerNorm (LayerNorm-simple) without the bias and gain\noutperforms LayerNorm on four datasets. It obtains the state-of-the-art\nperformance on En-Vi machine translation. To address the over-fitting problem,\nwe propose a new normalization method, Adaptive Normalization (AdaNorm), by\nreplacing the bias and gain with a new transformation function. Experiments\nshow that AdaNorm demonstrates better results than LayerNorm on seven out of\neight datasets.\n\n**Published:** 2019-11-16T11:00:16Z  (*Updated: 2019-11-16T11:00:16Z*)\n\n\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient transformer implementation\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 4. Dynamic Transformer for Efficient Machine Translation on Embedded\n  Devices\n\n*From Search Query: \"hardware efficient transformer implementation\"*\n\n*Hishan Parry, Lei Xun, Amin Sabet, Jia Bi, Jonathon Hare, Geoff V. Merrett*\n\n**Abstract:** The Transformer architecture is widely used for machine translation tasks.\nHowever, its resource-intensive nature makes it challenging to implement on\nconstrained embedded devices, particularly where available hardware resources\ncan vary at run-time. We propose a dynamic machine translation model that\nscales the Transformer architecture based on the available resources at any\nparticular time. The proposed approach, 'Dynamic-HAT', uses a HAT\nSuperTransformer as the backbone to search for SubTransformers with different\naccuracy-latency trade-offs at design time. The optimal SubTransformers are\nsampled from the SuperTransformer at run-time, depending on latency\nconstraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses\ninherited SubTransformers sampled directly from the SuperTransformer with a\nswitching time of <1s. Using inherited SubTransformers results in a BLEU score\nloss of <1.5% because the SubTransformer configuration is not retrained from\nscratch after sampling. However, to recover this loss in performance, the\ndimensions of the design space can be reduced to tailor it to a family of\ntarget hardware. The new reduced design space results in a BLEU score increase\nof approximately 1% for sub-optimal models from the original design space, with\na wide range for performance scaling between 0.356s - 1.526s for the GPU and\n2.9s - 7.31s for the CPU.\n\n**Published:** 2021-07-17T07:36:29Z  (*Updated: 2021-07-30T19:36:54Z*)\n\n\n\n#### 5. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"gradient stabilization techniques\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 6. Is Reinforcement Learning (Not) for Natural Language Processing:\n  Benchmarks, Baselines, and Building Blocks for Natural Language Policy\n  Optimization\n\n*From Search Query: \"gradient stabilization techniques\"*\n\n*Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, Yejin Choi*\n\n**Abstract:** We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference. GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization) that learns to effectively reduce the combinatorial action\nspace in language generation. We show 1) that RL techniques are generally\nbetter than supervised methods at aligning LMs to human preferences; and 2)\nthat NLPO exhibits greater stability and performance than previous policy\ngradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic\nand human evaluations.\n\n**Published:** 2022-10-03T21:38:29Z  (*Updated: 2023-03-01T01:31:17Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. LaPE: Layer-adaptive Position Embedding for Vision Transformers with Independent Layer Normalization\n\n*From Search Query: \"adaptive normalization transformer\"*\n\n*Jie Chen, Xiangyang Ji, Haoyi Duan, Chang Liu, Kehan Li, Yinhuai Wang, Zhennan Wang, Runyi Yu*\n\n**Abstract:**     Position information is critical for Vision Transformers (VTs) due to the permutation-invariance of self-attention operations. A typical way to introduce position information is adding the absolute Position Embedding (PE) to patch embedding before entering VTs. However, this approach operates the same Layer Normalization (LN) to token embedding and PE, and delivers the same PE to each layer. This results in restricted and monotonic PE across layers, as the shared LN affine parameters are not dedicated to PE, and the PE cannot be adjusted on a per-layer basis. To overcome these limitations, we propose using two independent LNs for token embeddings and PE in each layer, and progressively delivering PE across layers. By implementing this approach, VTs will receive layer-adaptive and hierarchical PE. We name our method as Layer-adaptive Position Embedding, abbreviated as LaPE, which is simple, effective, and robust. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that LaPE significantly outperforms the default PE method. For example, LaPE improves +1.06% for CCT on CIFAR100, +1.57% for DeiT-Ti on ImageNet-1K, +0.7 box AP and +0.5 mask AP for ViT-Adapter-Ti on COCO, and +1.37 mIoU for tiny Segmenter on ADE20K. This is remarkable considering LaPE only increases negligible parameters, memory, and computational cost.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 2. Semantic Image Synthesis with Spatially-Adaptive Normalization\n\n*From Search Query: \"adaptive normalization transformer\"*\n\n*Jun-Yan Zhu, Ting-Chun Wang, Ming-Yu Liu, Taesung Park*\n\n**Abstract:** We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .\n\n**Conference:** semantic-image-synthesis-with-spatially-1\n\n**Published:** 2019-03-18\n\n\n\n#### 3. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\n\n*From Search Query: \"hardware efficient transformer implementation\"*\n\n*Song Han, Xingyu Dang, Guangxuan Xiao, Wei-Chen Wang, Wei-Ming Chen, Chuang Gan, Shang Yang, Haotian Tang, Jiaming Tang, Ji Lin*\n\n**Abstract:** Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.\n\n**Published:** 2023-06-01\n\n\n\n#### 4. AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights\n\n*From Search Query: \"gradient stabilization techniques\"*\n\n*Jung-Woo Ha, Gyuwan Kim, Youngjung Uh, Dongyoon Han, Sangdoo Yun, Sanghyuk Chun, Byeongho Heo, Seong Joon Oh*\n\n**Abstract:** Normalization techniques are a boon for modern deep learning. They let weights converge more quickly with often better generalization performances. It has been argued that the normalization-induced scale invariance among the weights provides an advantageous ground for gradient descent (GD) optimizers: the effective step sizes are automatically reduced over time, stabilizing the overall training procedure. It is often overlooked, however, that the additional introduction of momentum in GD optimizers results in a far more rapid reduction in effective step sizes for scale-invariant weights, a phenomenon that has not yet been studied and may have caused unwanted side effects in the current practice. This is a crucial issue because arguably the vast majority of modern deep neural networks consist of (1) momentum-based GD (e.g. SGD or Adam) and (2) scale-invariant parameters. In this paper, we verify that the widely-adopted combination of the two ingredients lead to the premature decay of effective step sizes and sub-optimal model performances. We propose a simple and effective remedy, SGDP and AdamP: get rid of the radial component, or the norm-increasing direction, at each optimizer step. Because of the scale invariance, this modification only alters the effective step sizes without changing the effective update directions, thus enjoying the original convergence properties of GD optimizers. Given the ubiquity of momentum GD and scale invariance in machine learning, we have evaluated our methods against the baselines on 13 benchmarks. They range from vision tasks like classification (e.g. ImageNet), retrieval (e.g. CUB and SOP), and detection (e.g. COCO) to language modelling (e.g. WikiText) and audio classification (e.g. DCASE) tasks. We verify that our solution brings about uniform gains in those benchmarks. Source code is available at https://github.com/clovaai/AdamP.\n\n**Conference:** adamp-slowing-down-the-slowdown-for-momentum\n\n**Published:** 2020-06-15\n\n\n\n#### 5. Stable and low-precision training for large-scale vision-language models\n\n*From Search Query: \"gradient stabilization techniques\"*\n\n*Ludwig Schmidt, Ali Farhadi, Ari Morcos, Luke Zettlemoyer, Tim Dettmers, Mitchell Wortsman*\n\n**Abstract:** We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-04-25\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive normalization, hardware-efficient implementations, and gradient stabilization techniques, here are some key findings and references that align with your requirements:\n\n## Adaptive Normalization in Transformer Architectures\n\n### Integration with Attention Mechanisms and Dynamic Parameter Adjustment\n- The **normalized Transformer (nGPT)** architecture introduced in the paper on \"nGPT: Normalized Transformer with Representation Learning on the Hypersphere\" is particularly relevant. This architecture normalizes all vectors to unit norm, which affects the attention mechanisms and MLP blocks. It uses learnable eigen learning rates to control the contributions of these blocks, leading to faster convergence and improved stability.\n- The concept of **dynamic normalization** is also crucial. For instance, the nGPT architecture ensures that all vectors involved in the model are constrained to the hypersphere, which can be seen as a form of dynamic normalization that improves model stability and convergence.\n\n### Impact on Model Performance\n- The nGPT architecture has been shown to significantly accelerate learning, reducing the number of training steps needed to reach comparable accuracy. This indicates a positive impact on model performance in terms of training efficiency and stability.\n\n## Hardware-Efficient Implementations\n\n### Memory Access Optimization and Computation Scheduling\n- The **PETAH (Parameter Efficient Task Adaptation for Hybrid Transformers)** framework discusses efficient task adaptation methods that can be applied to transformer models. It highlights the importance of adapting both attention and convolutional layers in hybrid models to achieve better performance with fewer parameters, which is crucial for memory access optimization and computation scheduling.\n- **Pruning techniques** combined with task adaptation, as mentioned in the PETAH paper, can help create performant backbones with fewer parameters, reducing memory footprint and improving computational efficiency.\n\n### Resource Utilization\n- **Hybrid transformer architectures**, such as those discussed in the PETAH paper, combine the benefits of convolutional and attention layers. These architectures are designed to be more resource-efficient, making them suitable for hardware-constrained applications.\n\n## Gradient Stabilization Techniques\n\n### Adaptive Update Mechanisms and Convergence Properties\n- The **AdaBelief Optimizer**, mentioned in your analysis, is an example of an adaptive update mechanism that balances fast convergence with good generalization and training stability. It adapts the stepsize based on the 'belief' in the current gradient direction, which is relevant for gradient stabilization in transformer models.\n- The **normalized Transformer (nGPT)** also employs adaptive update mechanisms through its use of learnable eigen learning rates, which modulate the contributions of attention and MLP blocks. This helps in achieving faster convergence and improved stability.\n\n### Training Dynamics\n- The nGPT architecture's use of hyperspherical parameter optimization ensures that weight decay becomes redundant, and the model's training dynamics are more stable. This is because the matrix-vector multiplications are interpreted as dot products bounded within the range of [-1,1], leading to more stable training.\n\n## Practical Implementations and Theoretical Frameworks\n\n### Integration with Existing Transformer Blocks\n- The **PatchTST** and **Crossformer** models, listed in the repository of time series forecasting papers, although focused on time series forecasting, demonstrate innovative ways to integrate new mechanisms into transformer architectures. For example, PatchTST uses a transformer-based approach for long-term forecasting, which could be adapted for language models by integrating similar attention mechanisms and normalization techniques.\n- The **iTransformer** and **ContiFormer** models also show how transformers can be adapted for specific tasks with unique normalization and attention mechanisms, which could be applied to language models to improve their performance and efficiency.\n\nIn summary, the nGPT architecture, PETAH framework, and concepts like AdaBelief Optimizer provide valuable insights into adaptive normalization, hardware-efficient implementations, and gradient stabilization techniques that can be integrated into autoregressive language models to improve their performance, efficiency, and stability.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several important aspects relevant to adaptive normalization in language models:\n\na) Key Technical Components:\n- Dynamic parameter adaptation mechanisms\n- Memory-efficient implementations\n- Training stability techniques\n- Hardware optimization strategies\n\nb) Novel Approaches:\n- Adaptive learning rate schemes\n- Dynamic token normalization\n- Parameter-efficient fine-tuning methods\n- Hardware-aware optimization techniques\n\n2. Useful References with Excerpts:\n\nFrom \"AdaBelief Optimizer\":\n\"AdaBelief simultaneously achieves three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability... adapts the stepsize according to the 'belief' in the current gradient direction.\"\n\nFrom \"Dynamic and Efficient Inference\":\n\"DEER method can make a single pre-trained model support Dynamic and Efficient infERence and achieve an adaptive trade-off between model performance and latency.\"\n\nFrom \"Long-Short Transformer\":\n\"aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations.\"\n\n3. Key Insights:\n\na) Adaptive Mechanisms:\n- Need for balance between adaptivity and stability\n- Importance of gradient belief in parameter updates\n- Trade-off between computational efficiency and model expressiveness\n\nb) Memory Optimization:\n- Efficient memory access patterns crucial for performance\n- Parameter sharing can reduce memory footprint\n- Hardware-aware design important for practical implementation\n\nc) Training Stability:\n- Gradient belief helps control update magnitude\n- Dynamic normalization improves stability\n- Need for careful parameter initialization\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Specific implementations of adaptive normalization in transformer architectures\n2. Impact of adaptive mechanisms on model convergence\n3. Hardware optimization techniques for normalization layers\n4. Integration with existing transformer blocks\n\n5. References:\n\n[1] Zhuang, J., Tang, T.M., Ding, Y., Tatikonda, S., Dvornek, N., Papademetris, X., & Duncan, J. (2020). AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients. Neural Information Processing Systems.\n\n[2] Liang, X., Li, J., Wu, L., Cao, Z., Zhang, M. (2023). Dynamic and Efficient Inference for Text Generation via BERT Family. Annual Meeting of the Association for Computational Linguistics.\n\n[3] Catanzaro, B., et al. (2021). Long-Short Transformer: Efficient Transformers for Language and Vision. Neural Information Processing Systems.\n\n[4] Yan, Z., Wang, H., Guo, D., & Han, S. (2020). MicroNet for Efficient Language Modeling. Neural Information Processing Systems.\n\n[5] Liu, L., et al. (2018). Dynamic Sparse Graph for Efficient Deep Learning. International Conference on Learning Representations."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.9055440000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.9679920000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "RMSNorm",
    "proposal": "\n# AdaptiveNormTransformer: Enhancing Language Models with Dynamic Normalization and Efficient Routing\n\n## Motivation\n\nCurrent language models face several key challenges:\n1. Fixed normalization parameters limit adaptability to varying input complexities\n2. Inefficient memory usage in processing long sequences\n3. Suboptimal hardware utilization in normalization computations\n4. Training instability in deep hierarchical networks\n\nThe seed design (meta_hierarchical_linear_transfo) and existing approaches like FlashAttention and SOFT demonstrate the importance of efficient computation and memory management. However, they don't fully address the need for adaptive processing based on input characteristics.\n\n## Problem Analysis\n\n### Limitations of Current Approaches\n\n1. **Standard RMSNorm**:\n   - Uses fixed scaling parameters\n   - Doesn't adapt to input complexity\n   - Limited hardware efficiency\n\n2. **Memory Management**:\n   - High memory requirements for long sequences\n   - Inefficient cache utilization\n   - Poor hardware utilization\n\n3. **Training Stability**:\n   - Gradient flow issues in deep networks\n   - Inconsistent optimization objectives\n   - Limited adaptability to input variations\n\n## Core Idea and Philosophy\n\nAdaptiveRMSNorm introduces three key innovations:\n\n1. **Dynamic Normalization**:\n   - Content-dependent scaling parameters\n   - Adaptive computation based on input complexity\n   - Efficient parameter updates\n\n2. **Hardware-Efficient Implementation**:\n   - Optimized memory access patterns\n   - Efficient computation scheduling\n   - Reduced memory footprint\n\n3. **Stable Training Mechanism**:\n   - Gradient stabilization through adaptive updates\n   - Consistent optimization objectives\n   - Efficient backpropagation\n\n## Design Plan\n\n### Architecture Overview\n\n```python\nclass AdaptiveRMSNorm(nn.Module):\n    def __init__(self, embed_dim, block_loc, kwarg_all, device=None, dtype=None):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.weight = nn.Parameter(torch.ones(embed_dim))\n        self.scale_generator = nn.Linear(embed_dim, embed_dim)\n        self.gate = nn.Linear(embed_dim, 1)\n        self.variance_epsilon = 1e-5\n        \n    def forward(self, x):\n        # Compute dynamic scaling parameters\n        scale = self.scale_generator(x)\n        importance = torch.sigmoid(self.gate(x))\n        \n        # Compute RMS normalization\n        variance = x.pow(2).mean(dim=-1, keepdim=True)\n        x_norm = x * torch.rsqrt(variance + self.variance_epsilon)\n        \n        # Apply adaptive scaling\n        return x_norm * (self.weight + scale * importance)\n```\n\n### Key Components\n\n1. **Dynamic Scale Generation**:\n```python\ndef generate_scale(self, x):\n    # Generate content-dependent scaling\n    base_scale = self.scale_generator(x)\n    importance = torch.sigmoid(self.gate(x))\n    return base_scale * importance\n```\n\n2. **Memory-Efficient Implementation**:\n```python\ndef efficient_norm(self, x):\n    # Compute variance with optimized memory access\n    x_squared = x.pow(2)\n    variance = x_squared.mean(dim=-1, keepdim=True)\n    \n    # Normalize with fused operations\n    x_norm = x * torch.rsqrt(variance + self.variance_epsilon)\n    return x_norm\n```\n\n3. **Gradient Stabilization**:\n```python\ndef stabilized_backward(self, grad_output, x):\n    # Compute stable gradients\n    with torch.cuda.amp.autocast():\n        scale = self.generate_scale(x)\n        grad_input = grad_output * scale\n    return grad_input\n```\n\n### Mathematical Formulation\n\n1. **Adaptive Normalization**:\n   \\[\n   \\text{AdaptiveRMSNorm}(x) = \\frac{x}{\\sqrt{\\mathbb{E}[x^2] + \\epsilon}} \\cdot (w + s(x) \\cdot g(x))\n   \\]\n   where:\n   - \\(w\\) is the learnable weight\n   - \\(s(x)\\) is the content-dependent scale\n   - \\(g(x)\\) is the importance gate\n\n2. **Dynamic Scaling**:\n   \\[\n   s(x) = W_s x + b_s\n   \\]\n   \\[\n   g(x) = \\sigma(W_g x + b_g)\n   \\]\n\n3. **Gradient Flow**:\n   \\[\n   \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} \\cdot (w + s(x) \\cdot g(x))\n   \\]\n\n### Implementation Guidelines\n\n1. **Initialization**:\n```python\ndef initialize_parameters(self):\n    nn.init.ones_(self.weight)\n    nn.init.xavier_normal_(self.scale_generator.weight)\n    nn.init.zeros_(self.scale_generator.bias)\n    nn.init.xavier_normal_(self.gate.weight)\n    nn.init.zeros_(self.gate.bias)\n```\n\n2. **Memory Management**:\n```python\ndef optimize_memory(self):\n    # Use memory-efficient computation\n    torch.cuda.empty_cache()\n    x = x.contiguous()\n    return x\n```\n\n3. **Hardware Optimization**:\n```python\ndef hardware_optimize(self):\n    # Fuse operations for better hardware utilization\n    with torch.cuda.amp.autocast():\n        variance = x.pow(2).mean(-1, keepdim=True)\n        x_norm = x * torch.rsqrt(variance + self.variance_epsilon)\n    return x_norm\n```\n\n## Theoretical Analysis\n\n1. **Complexity Analysis**:\n   - Time Complexity: O(n) for sequence length n\n   - Space Complexity: O(d) for embedding dimension d\n   - Memory Access: O(1) per element\n\n2. **Stability Analysis**:\n   - Bounded gradients through importance gating\n   - Stable variance through epsilon term\n   - Consistent optimization through adaptive scaling\n\n3. **Convergence Properties**:\n   - Faster convergence through adaptive normalization\n   - Improved gradient flow through dynamic scaling\n   - Better stability through importance gating\n\n## Advantages\n\n1. **Improved Adaptability**:\n   - Content-dependent normalization\n   - Dynamic parameter adjustment\n   - Better handling of varying complexities\n\n2. **Enhanced Efficiency**:\n   - Optimized memory usage\n   - Efficient computation\n   - Better hardware utilization\n\n3. **Training Stability**:\n   - Stable gradient flow\n   - Consistent optimization\n   - Improved convergence\n\n## References\n\n1. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n2. Lu, J., Yao, J., Zhang, J., et al. (2021). SOFT: Softmax-free Transformer with Linear Complexity. Neural Information Processing Systems.\n\n3. Kreisler, I., Nacson, M. S., Soudry, D., & Carmon, Y. (2023). Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions in Scalar Networks and Beyond. International Conference on Machine Learning.\n\n4. Yang, S., Wang, B., Shen, Y., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n5. Fu, Y., Yang, H., Yuan, J., et al. (2022). DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. International Conference on Machine Learning.\n",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel language model architecture that enhances RMSNorm with adaptive normalization and dynamic routing mechanisms, using hardware-efficient implementations to improve model performance while maintaining computational efficiency. The design integrates content-dependent normalization parameters with efficient memory management to enable better handling of varying input complexities.",
    "ideation": null,
    "modelname": "AdaptiveNormTransformer",
    "suggestions": "1. Implementation Strategy:\n- Provide more detailed hardware optimization guidelines\n- Include specific memory access patterns\n- Add benchmarking methodology\n- Consider different hardware architectures\n\n2. Training Stability:\n- Elaborate on gradient stabilization mechanisms\n- Include theoretical bounds on parameter updates\n- Add regularization techniques\n- Consider adaptive learning rate strategies\n\n3. Memory Management:\n- Detail memory allocation strategies\n- Include cache optimization techniques\n- Consider different memory hierarchies\n- Add memory bandwidth analysis\n\n4. Parameter Updates:\n- Specify update frequency\n- Include parameter initialization strategies\n- Consider different update schemes\n- Add stability analysis\n\n5. Hardware Optimization:\n- Provide specific hardware-aware optimizations\n- Include CUDA kernel implementations\n- Consider different accelerator types\n- Add resource utilization analysis\n\n6. Empirical Validation:\n- Define specific benchmarks\n- Include comparison methodology\n- Add ablation studies\n- Consider different model sizes\n\n7. Integration Guidelines:\n- Provide detailed integration steps\n- Include compatibility considerations\n- Add testing strategies\n- Consider different frameworks",
    "user_input": ""
}