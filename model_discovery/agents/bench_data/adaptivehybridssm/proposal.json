{
    "variantname": null,
    "review": "The AdaptiveHybridSSM proposal presents a novel approach to language model block design by combining adaptive state space models with hierarchical local-global processing and efficient memory management. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Theoretical Foundation:\n- Strong theoretical grounding in state space models and structured matrices\n- Clear mathematical formulation of the adaptive mechanisms\n- Well-justified hierarchical processing approach\n\n2. Innovation:\n- Novel integration of adaptive SSMs with hierarchical processing\n- Unique approach to memory optimization through selective compression\n- Original combination of local-global context processing\n- Distinct from existing approaches like HyperLoRA and traditional SSMs\n\n3. Efficiency:\n- Linear-time complexity O(n log n) for sequence processing\n- Memory-efficient design through selective compression\n- Hardware-aware implementation considerations\n- Efficient gradient computation through structured matrices\n\n4. Scalability:\n- Linear memory scaling with sequence length\n- Potential for handling longer sequences effectively\n- Structured approach to state space management\n- Efficient parallel processing capabilities\n\n5. Implementation Detail:\n- Clear architectural components and their interactions\n- Well-defined mathematical formulations\n- Detailed pseudo-code for key components\n- Comprehensive implementation guidelines\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex integration of multiple architectural components\n- Potential challenges in training stability\n- Need for careful parameter initialization\n- Complex interaction between adaptive and hierarchical components\n\n2. Training Considerations:\n- Potential instability with long sequences\n- Need for careful gradient flow management\n- Complex optimization requirements\n- Initialization sensitivity\n\n3. Memory Management:\n- Trade-offs between compression and information preservation\n- Complex state management requirements\n- Potential memory fragmentation issues\n- Need for careful balance in selective compression\n\n4. Hardware Optimization:\n- Complex requirements for efficient parallel processing\n- Need for IO-aware algorithm design\n- Potential challenges in hardware utilization\n- Complex memory hierarchy optimization\n\n5. Validation:\n- Limited discussion of empirical validation strategies\n- Need for more detailed performance benchmarks\n- Lack of specific comparison metrics\n- Need for more detailed ablation studies\n\nSUGGESTIONS FOR IMPROVEMENT:\n\n1. Implementation Details:\n- Provide more specific initialization strategies for the adaptive components\n- Include detailed gradient flow management techniques\n- Add specific hardware optimization guidelines\n- Elaborate on parallel processing strategies\n\n2. Training Stability:\n- Add specific techniques for handling long sequences\n- Include detailed optimization strategies\n- Provide initialization guidelines\n- Add gradient clipping and normalization details\n\n3. Memory Optimization:\n- Elaborate on compression threshold selection\n- Add specific state management strategies\n- Include memory fragmentation mitigation techniques\n- Provide detailed IO optimization guidelines\n\n4. Validation Framework:\n- Add specific performance benchmarks\n- Include detailed ablation study plans\n- Provide comparison metrics with existing approaches\n- Add specific evaluation criteria\n\n5. Hardware Considerations:\n- Include specific hardware optimization techniques\n- Add parallel processing guidelines\n- Provide IO optimization strategies\n- Include memory hierarchy optimization details",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient attention mechanisms, structured matrices neural networks, parameter sharing language models",
            "detail": "Find specific implementations and performance comparisons of:\n1. Efficient attention mechanisms that achieve linear complexity\n2. Methods for structuring parameter matrices in neural networks\n3. Successful approaches to parameter sharing in language models\nFocus on technical details, mathematical formulations, and empirical results.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and performance comparisons of:\n1. Efficient attention mechanisms that achieve linear complexity\n2. Methods for structuring parameter matrices in neural networks\n3. Successful approaches to parameter sharing in language models\nFocus on technical details, mathematical formulations, and empirical results.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention (Avg. Score: 0.94)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n##### *Relevant Chunk: No. 2/39 (Score: 0.94)*\n\n```\nDue to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intrablocks and linear attention kernel tricks for interblocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM. ## 1. Introduction\n\nLinear attention has emerged as a potentially viable alternative to conventional softmax attention over the last five years (Bahdanau et al., 2016; de Br\u00e9bisson \\& Vincent, 2016). [^0]However, despite its promise, none of the current leading large language models (Touvron et al., 2023a;b; Zeng et al., 2022; Black et al., 2022; Almazrouei et al., 2023; Team et al., 2023; Wang \\& Komatsuzaki, 2021; Baichuan, 2023; Jiang et al., 2023) have adopted linear attention mechanisms. There are two possible reasons for that: 1). Inferior performance: There is a notable performance gap between existing linear attention-based models (Katharopoulos et al., 2020; Qin et al., 2022b) and state-of-the-art softmax attentionbased models (Touvron et al., 2023a;b) in language modeling. 2). Slow training speed: Existing linear attention models frequently struggle with slow training speeds due to the use of cumulative summation operations (cumsum) (Hua et al., 2022). As a result, these models (Hua et al., 2022) often adopt conventional attention computation during practical use, losing the theoretical advantages of linear attention. In this paper, we address the aforementioned issues of linear attention and propose a new linear attention-based model that outperforms softmax attention-based models in terms of accuracy and efficiency in language modeling. Training speed. We introduce Lightning Attention, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve the linear computational complexities, the core idea is to leverage the \"kernel trick\" to accelerate the attention matrix computation, i.e., compute the product of keys and values first to circumvent the $n \\times n$ query-key matrix multiplication. The slow operation cumsum is needed during the calculation in causal language modeling. To solve this dilemma, we apply the concept of \"divide and conquer\" to perform the calculation. Specifically, our attention calculation is divided into intra-blocks and inter-blocks. The conventional attention calculation is applied to intra-blocks, while the \"kernel trick\" is utilized for inter-blocks. We also leverage tiling techniques in both forward and backward processes to maximize GPU hardware performance and tailor the technique used in FlashAttention (Dao et al., 2022a; Dao, 2023) to our Lightning Attention to make it IO-friendly. As a result, Lightning Attention maintains a constant training speed with increasing sequence length under fixed memory consumption, as shown in Fig.\n```\n\n#### 2. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.71)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.71)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n#### 3. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention (Avg. Score: 0.71)\n\n*Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, G. Fung, Yin Li, Vikas Singh*\n\n**Published in:** AAAI Conference on Artificial Intelligence (2021)\t**Cited by** 375  (*Influential: 62*)\n\n**TL;DR:** This work proposes Nystr\u00f6mformer - a model that exhibits favorable scalability as a function of sequence length and performs favorably relative to other efficient self-attention methods.\n\n**Abstract:** Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.\n\n##### *Relevant Chunk: No. 31/36 (Score: 0.71)*\n\n```\nR.; Su, Q.; Zhang, Y.; Li, C.; Henao, R.; and Carin, L. 2018a. Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), 440-450. Shen, Z.; Zhang, M.; Zhao, H.; Yi, S.; and Li, H. 2018b. Efficient Attention: Attention with Linear Complexities. arXiv preprint arXiv:1812.01243. Si, S.; Hsieh, C.-J.; and Dhillon, I. 2016. Computationally efficient Nystr\u00f6m approximation using fast transforms. In Proceedings of the International Conference on Machine Learning (ICML), 26552663. Si, S.; Hsieh, C.-J.; and Dhillon, I. S. 2017. Memory efficient kernel approximation. Journal of Machine Learning Research (JMLR) 18(1): 682-713. Socher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning, C. D.; Ng, A. Y.; and Potts, C. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 1631-1642. Tay, Y.; Dehghani, M.; Abnar, S.; Shen, Y.; Bahri, D.; Pham, P.; Rao, J.; Yang, L.; Ruder, S.; and Metzler, D. 2020. Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 5998-6008. Vyas, A.; Katharopoulos, A.; and Fleuret, F. 2020. Fast transformers with clustered attention. Advances in Neural Information Processing Systems 33. Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and Bowman, S.\n```\n\n#### 4. Luna: Linear unified nested attention (Avg. Score: 0.54)\n\n*Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, Luke Zettlemoyer*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 94  (*Influential: 17*)\n\n**TL;DR:** Luna is proposed, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear time and space complexity.\n\n**Abstract:** The quadratic computational and memory complexities of the Transformer's attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length. Then, the packed sequence is unpacked using the second attention function. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety\n\n##### *Relevant Chunk: No. 13/28 (Score: 0.54)*\n\n```\nFor a detailed overview we refer the readers to Tay et al. (2020b). Sparse Attention The general idea of these methods is that, instead of attending to the whole sequence, each token only access to a fixed, predefined range such as local neighborhoods and strided or \"dilated\" windows. Popular methods include local attention (Parmar et al., 2018), blockwise attention (Qiu et al., 2019), strided attention patterns (Child et al., 2019; Beltagy et al., 2020), and compressed attention (Liu et al., 2018). To make this range more flexible, Reformer (Kitaev et al., 2020) employs a hash-based similarity measure to efficiently cluster tokens into chunks and Routing Transformer(Roy et al., 2021) employ online k-means clustering on the tokens. The Sinkhorn sorting Network (Tay et al., 2020a) exposes the sparsity in attention weights by learning to sort blocks of the input sequence. Kernel Methods. A recently popular method to improve the efficiency of Transformers is to avoid explicitly computing the $m \\times n$ attention matrix $A$ in (1) by re-writing it with kernels. Typical models leveraging kernelization are Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020) and Random Feature Attention (Peng et al., 2021). Since kernels are a form of approximation of the attention matrix, they can be also viewed as a form of low-rank method (Choromanski et al., 2020) that compresses the context to a shorter length, such as Linformer (Wang et al., 2019) and the proposed Luna model. Recurrence. The simplest technique to reduce the complexity of Transformer is to chunk input sequences into fixed blocks, with the obvious disadvantage of losing contextual information from past chunks. As discussed in Tay et al. (2020b), these models can be regarded as fixed pattern models. Transformer-XL (Dai et al., 2019) proposed a natural extension to the blockwise method to connect these blocks via a recurrence mechanism. Compressive Transformer (Rae et al., 2020) further extends Transformer-XL by maintaining a fine-grained memory of past chunk activations, which are discarded in Transformer-XL. Technically, Luna can be adapted to a recurrence method, by simply using $P$ as an inherent memory module to maintain the recurrence across segments. ## 6 Conclusion\n\nWe have introduced Luna, a simple, efficient and effective linear attention mechanism used as a drop-in substitute for regular softmax attention. By introducing an extra input with the fixed length, Luna is capable of capturing adequate contextual information while performing attention operations linearly. On three sequence modeling tasks, i.e., long-context sequence modeling, neural machine translation, and large-scale pretraining and finetuning, Luna achieves comparable or even better performance than a variety of strong baselines, while acquiring prominent gains of efficiency in both speed and memory. In future work, we are interested in combining Luna with recurrence methods where $P$ can be used as a running memory across segments of inputs. Another interesting direction would be to apply Luna to other tasks with long input sequences, such as document-level summarization and translation. ## Acknowledgments and Disclosure of Funding\n\nThis material is based on research sponsored by Air Force Research Laboratory (AFRL) under agreement number FA8750-19-1-1000.\n```\n\n#### 5. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.50)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.50)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient attention mechanisms, structured matrices neural networks, parameter sharing language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Efficient Attention via Control Variates\n\n*From Search Query: efficient attention mechanisms*\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 2. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: efficient attention mechanisms*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 3. Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning\n\n*From Search Query: efficient attention mechanisms*\n\n*Mattia Atzeni, Mrinmaya Sachan, Andreas Loukas*\n\n**TL;DR:** This work focuses on geometry priors and introduces LatFormer, a model that incorporates lattice symmetry priors in attention masks that provides preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Abstract:** The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer requires 2 orders of magnitude fewer data than standard attention and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric priors provide preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Efficient Recurrent Neural Networks using Structured Matrices in FPGAs\n\n*From Search Query: structured matrices neural networks*\n\n*Zhe Li, Shuo Wang, Caiwen Ding, Qinru Qiu, Yanzhi Wang, Yun Liang*\n\n**TL;DR:** This work proposes block-circulant matrices for weight matrix representation in RNNs, thereby achieving simultaneous model compression and acceleration in FPGA and achieves a maximum energy efficiency improvement compared with ESE.\n\n**Abstract:** Recurrent Neural Networks (RNNs) are becoming increasingly important for time series-related applications which require efficient and real-time implementations. The recent pruning based work ESE suffers from degradation of performance/energy efficiency due to the irregular network structure after pruning. We propose block-circulant matrices for weight matrix representation in RNNs, thereby achieving simultaneous model compression and acceleration. We aim to implement RNNs in FPGA with highest performance and energy efficiency, with certain accuracy requirement (negligible accuracy degradation). Experimental results on actual FPGA deployments shows that the proposed framework achieves a maximum energy efficiency improvement of 35.7$\\times$ compared with ESE.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 5. Monarch: Expressive Structured Matrices for Efficient and Accurate Training\n\n*From Search Query: structured matrices neural networks*\n\n*Tri Dao, Beidi Chen, N. Sohoni, Arjun D Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution and can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications.\n\n**Abstract:** Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called\"reverse sparsification,\"Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 71  (*Influential: 16*)\n\n#### 6. Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing\n\n*From Search Query: parameter sharing language models*\n\n*Peiyu Liu, Ze-Feng Gao, Yushuo Chen, Xin Zhao, Ji-Rong Wen*\n\n**Abstract:** ,\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 7. Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models\n\n*From Search Query: parameter sharing language models*\n\n*Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou*\n\n**TL;DR:** This work introduces a straightforward technique to enhance the inference efficiency of parameter-shared PLMs and proposes a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration.\n\n**Abstract:** Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of parameter-shared models in resource-constrained settings.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 8. XLM-V: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models\n\n*From Search Query: parameter sharing language models*\n\n*Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, Madian Khabsa*\n\n**TL;DR:** A new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language is introduced.\n\n**Abstract:** Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This \\textit{vocabulary bottleneck} limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), to named entity recognition (WikiAnn). XLM-V is particularly effective on low-resource language tasks and outperforms XLM-R by 11.2% and 5.8% absolute on MasakhaNER and Americas NLI, respectively.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 57  (*Influential: 6*)\n\n### 3 related papers from ArXiv\n\n#### 1. LSTM-based Deep Learning Models for Non-factoid Answer Selection\n\n*From Search Query: efficient attention mechanisms*\n\n*Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou*\n\n**Abstract:** In this paper, we apply a general deep learning (DL) framework for the answer\nselection task, which does not depend on manually defined features or\nlinguistic tools. The basic framework is to build the embeddings of questions\nand answers based on bidirectional long short-term memory (biLSTM) models, and\nmeasure their closeness by cosine similarity. We further extend this basic\nmodel in two directions. One direction is to define a more composite\nrepresentation for questions and answers by combining convolutional neural\nnetwork with the basic framework. The other direction is to utilize a simple\nbut efficient attention mechanism in order to generate the answer\nrepresentation according to the question context. Several variations of models\nare provided. The models are examined by two datasets, including TREC-QA and\nInsuranceQA. Experimental results demonstrate that the proposed models\nsubstantially outperform several strong baselines.\n\n**Published:** 2015-11-12T22:01:54Z  (*Updated: 2016-03-28T04:12:45Z*)\n\n\n\n#### 2. Efficient Transformer Knowledge Distillation: A Performance Review\n\n*From Search Query: efficient attention mechanisms*\n\n*Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence*\n\n**Abstract:** As pretrained transformer language models continue to achieve\nstate-of-the-art performance, the Natural Language Processing community has\npushed for advances in model compression and efficient attention mechanisms to\naddress high computational requirements and limited input sequence length.\nDespite these separate efforts, no investigation has been done into the\nintersection of these two fields. In this work, we provide an evaluation of\nmodel compression via knowledge distillation on efficient attention\ntransformers. We provide cost-performance trade-offs for the compression of\nstate-of-the-art efficient attention architectures and the gains made in\nperformance in comparison to their full attention counterparts. Furthermore, we\nintroduce a new long-context Named Entity Recognition dataset, GONERD, to train\nand test the performance of NER models on long sequences. We find that\ndistilled efficient attention transformers can preserve a significant amount of\noriginal model performance, preserving up to 98.6% across short-context tasks\n(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context\nQuestion-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on\nlong-context Named Entity Recognition (GONERD), while decreasing inference\ntimes by up to 57.8%. We find that, for most models on most tasks, performing\nknowledge distillation is an effective method to yield high-performing\nefficient attention models with low costs.\n\n**Published:** 2023-11-22T19:19:37Z  (*Updated: 2023-11-22T19:19:37Z*)\n\n\n\n#### 3. When to Fold'em: How to answer Unanswerable questions\n\n*From Search Query: parameter sharing language models*\n\n*Marshall Ho, Zhipeng Zhou, Judith He*\n\n**Abstract:** We present 3 different question-answering models trained on the SQuAD2.0\ndataset -- BIDAF, DocumentQA and ALBERT Retro-Reader -- demonstrating the\nimprovement of language models in the past three years. Through our research in\nfine-tuning pre-trained models for question-answering, we developed a novel\napproach capable of achieving a 2% point improvement in SQuAD2.0 F1 in reduced\ntraining time. Our method of re-initializing select layers of a\nparameter-shared language model is simple yet empirically powerful.\n\n**Published:** 2021-05-01T19:08:40Z  (*Updated: 2021-05-01T19:08:40Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation\n\n*From Search Query: efficient attention mechanisms*\n\n*Jianlin Su, Rui Li, Shunyi Zheng, Chenxi Duan*\n\n**Abstract:** In this paper, to remedy this deficiency, we propose a Linear Attention Mechanism which is approximate to dot-product attention with much less memory and computational costs. The efficient design makes the incorporation between attention mechanisms and neural networks more flexible and versatile. Experiments conducted on semantic segmentation demonstrated the effectiveness of linear attention mechanism. Code is available at https://github.com/lironui/Linear-Attention-Mechanism.\n\n**Published:** 2020-07-29\n\n\n\n#### 2. BAM: A Balanced Attention Mechanism for Single Image Super Resolution\n\n*From Search Query: efficient attention mechanisms*\n\n*Cheng Shen, Haotian Hu, Fanyi Wang*\n\n**Abstract:** Recovering texture information from the aliasing regions has always been a major challenge for Single Image Super Resolution (SISR) task. These regions are often submerged in noise so that we have to restore texture details while suppressing noise. To address this issue, we propose a Balanced Attention Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to suppress extreme noise in the large scale feature maps while MSAM preserves high-frequency texture details. Thanks to the parallel structure, these two modules not only conduct self-optimization, but also mutual optimization to obtain the balance of noise reduction and high-frequency texture restoration during the back propagation process, and the parallel structure makes the inference faster. To verify the effectiveness and robustness of BAM, we applied it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently improve the networks performance, and for those originally with attention mechanism, the substitution with BAM further reduces the amount of parameters and increases the inference speed. Moreover, we present a dataset with rich texture aliasing regions in real scenes, named realSR7. Experiments prove that BAM achieves better super-resolution results on the aliasing area.\n\n**Published:** 2021-04-15\n\n\n\n#### 3. Learning Compressed Transforms with Low Displacement Rank\n\n*From Search Query: structured matrices neural networks*\n\n*Christopher R\u00e9, Albert Gu, Atri Rudra, Tri Dao, Anna T. Thomas*\n\n**Abstract:** The low displacement rank (LDR) framework for structured matrices represents\na matrix through two displacement operators and a low-rank residual. Existing\nuse of LDR matrices in deep learning has applied fixed displacement operators\nencoding forms of shift invariance akin to convolutions. We introduce a class\nof LDR matrices with more general displacement operators, and explicitly learn\nover both the operators and the low-rank component. This class generalizes\nseveral previous constructions while preserving compression and efficient\ncomputation. We prove bounds on the VC dimension of multi-layer neural networks\nwith structured weight matrices and show empirically that our compact\nparameterization can reduce the sample complexity of learning. When replacing\nweight layers in fully-connected, convolutional, and recurrent neural networks\nfor image classification and language modeling tasks, our new classes exceed\nthe accuracy of existing compression approaches, and on some tasks also\noutperform general unstructured layers while using more than 20x fewer\nparameters.\n\n**Conference:** learning-compressed-transforms-with-low-1\n\n**Published:** 2018-10-04\n\n\n\n#### 4. Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\n\n*From Search Query: structured matrices neural networks*\n\n*Hun-Seok Kim, Changwoo Lee*\n\n**Abstract:** This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. On the image and language tasks, our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.\n\n**Published:** 2023-10-29\n\n\n\n#### 5. Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression\n\n*From Search Query: parameter sharing language models*\n\n*Grace Li Zhang, Bing Li, Ing-Chao Lin, Yu-Guang Chen, Jingcun Wang*\n\n**Abstract:** Large Language Models (LLMs) have achieved remarkable breakthroughs. However, the huge number of parameters in LLMs require significant amount of memory storage in inference, which prevents their practical deployment in many applications. To reduce memory storage of LLMs, singular value decomposition (SVD) provides a promising solution to approximate weight matrices for compressing LLMs. In this paper, we take a step further to explore parameter sharing across different layers with SVD to achieve more effective compression for LLMs. Specifically, weight matrices in different layers are decomposed and represented as a linear combination of a set of shared basis vectors and unique coefficients. The types of weight matrices and the layer selection for basis sharing are examined when compressing LLMs to maintain the performance. Comprehensive experiments demonstrate that Basis Sharing outperforms state-of-the-art SVD-based compression approaches and parameter sharing techniques, especially under large compression ratios. Code is available at: https://github.com/TUDa-HWAI/Basis_Sharing\n\n**Published:** 2024-10-02\n\n\n\n#### 6. HyperNetworks\n\n*From Search Query: parameter sharing language models*\n\n*David Ha, Andrew Dai, Quoc V. Le*\n\n**Abstract:** This work explores hypernetworks: an approach of using a one network, also\nknown as a hypernetwork, to generate the weights for another network.\nHypernetworks provide an abstraction that is similar to what is found in\nnature: the relationship between a genotype - the hypernetwork - and a\nphenotype - the main network. Though they are also reminiscent of HyperNEAT in\nevolution, our hypernetworks are trained end-to-end with backpropagation and\nthus are usually faster. The focus of this work is to make hypernetworks useful\nfor deep convolutional networks and long recurrent networks, where\nhypernetworks can be viewed as relaxed form of weight-sharing across layers.\nOur main result is that hypernetworks can generate non-shared weights for LSTM\nand achieve near state-of-the-art results on a variety of sequence modelling\ntasks including character-level language modelling, handwriting generation and\nneural machine translation, challenging the weight-sharing paradigm for\nrecurrent networks. Our results also show that hypernetworks applied to\nconvolutional networks still achieve respectable results for image recognition\ntasks compared to state-of-the-art baseline models while requiring fewer\nlearnable parameters.\n\n**Published:** 2016-09-27\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on efficient attention mechanisms, structured parameter matrices, and effective parameter sharing, here are some specific implementations and performance comparisons based on the provided keywords and analysis:\n\n## Efficient Attention Mechanisms with Linear Complexity\n\n### SAMSA: Sampling-Self-Attention\nThe SAMSA (Sampling-Self-Attention) mechanism proposes a context-aware linear complexity self-attention method that works well on multiple data modalities. This approach uses differentiable sampling without replacement to attend to the most important token set, defined by the data. SAMSA achieves competitive or even state-of-the-art results on many benchmarks while being faster in inference compared to full self-attention mechanisms.\n\n### Linear-Complexity Multiplication (L-Mul)\nThe L-Mul algorithm approximates floating-point multiplication with integer addition operations, which can be integrated into the attention mechanism of transformer-based models. This method reduces the computational complexity of matrix multiplications in the attention layers, resulting in significant efficiency gains without a substantial performance loss. The L-Mul-based attention mechanism showed minimal performance gaps compared to higher precision formats like bf16.\n\n### FlashAttention\nFlashAttention is another efficient algorithm that computes exact attention in neural networks. It employs tiling and recomputation techniques to reduce memory usage and computational time. By dividing large matrices into smaller blocks, FlashAttention minimizes the memory footprint and speeds up computations, making it suitable for models with limited GPU memory.\n\n## Methods for Structuring Parameter Matrices in Neural Networks\n\n### Butterfly Factorizations\nButterfly factorizations offer a structured approach to learning efficient transformations. This method parameterizes divide-and-conquer techniques to represent a large class of transforms efficiently. By using butterfly patterns, parameter matrices can be structured in a way that reduces computational complexity, making it beneficial for long sequence processing[From Reference 2 in the analysis].\n\n### Vector Quantization\nVector quantization is used in models like Transformer-VQ to improve efficiency. By quantizing keys and using a novel caching mechanism, these models can process long sequences much faster than traditional quadratic-time transformers. This approach significantly reduces the computational complexity associated with long sequences[From Reference 2 in the analysis].\n\n## Successful Approaches to Parameter Sharing in Language Models\n\n### Hierarchical State Space Models (SSMs) with Gating Mechanisms\nCombining hierarchical state space models with selective gating mechanisms can enhance both expressiveness and efficiency in language modeling. This approach, as seen in models like GatedHierarchicalMamba, allows for better control over information flow and can handle long-range dependencies more effectively. The integration of gating mechanisms with SSMs helps in managing complex architectures efficiently[From Reference 0 in the analysis].\n\n### Parameter Sharing through Structured Matrices\nUsing structured matrices, such as those enabled by butterfly factorizations, can facilitate efficient parameter sharing. These structured matrices reduce the number of parameters needed, thereby improving efficiency without sacrificing performance. This approach is particularly useful when combined with other efficient attention mechanisms and architectural components[From Reference 3 in the analysis].\n\n## Integration Strategies and Empirical Results\n\n### Combining Efficient Attention Mechanisms\nIntegrating SAMSA or L-Mul with other efficient architectures, such as those using vector quantization or butterfly factorizations, can further enhance the performance and efficiency of language models. For example, SAMSA's linear complexity attention can be combined with vector-quantized keys to achieve both high accuracy and low computational complexity.\n\n### Empirical Performance\nThe empirical results from these studies show significant improvements in efficiency and performance. For instance, SAMSA achieved competitive or better results on various benchmarks while reducing inference time. The L-Mul algorithm showed minimal performance gaps compared to higher precision formats, and models using vector quantization and butterfly factorizations demonstrated substantial speedups over traditional transformers[From Reference 2 in the analysis].\n\nBy leveraging these efficient attention mechanisms, structured parameter matrices, and effective parameter sharing strategies, researchers can design autoregressive language models that are more efficient, scalable, and performant.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design Innovation\n\n1. Summary of Analysis\n\nThe initial research reveals several key trends and opportunities in LM block design:\n\na) State Space Models (SSMs) are emerging as a powerful alternative to traditional attention mechanisms, offering linear computational complexity and efficient processing capabilities.\n\nb) Vector quantization and butterfly factorizations present novel approaches for improving efficiency in linear transformations.\n\nc) Position interpolation techniques show promise for extending context windows while maintaining stability.\n\nd) There's a trend toward combining different architectural elements (e.g., gating mechanisms with state space models) to leverage complementary strengths.\n\n2. Useful References with Excerpts\n\nFrom Reference 0 (GatedHierarchicalMamba):\n```\nIntegrating selective gating mechanisms and Gated Linear Attention into HierarchicalMamba to enhance expressiveness, efficiency, and scalability in modeling long-range dependencies in language modeling.\n```\nKey insight: Combining gating mechanisms with hierarchical state space models can enhance both expressiveness and efficiency.\n\nFrom Reference 2 (Linear-Time Transformers via Vector Quantization):\n```\nTransformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism... over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k\n```\nKey insight: Vector quantization can significantly improve efficiency for long sequences.\n\nFrom Reference 3 (Butterfly Factorizations):\n```\nFast linear transforms are ubiquitous in machine learning... introduces a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms\n```\nKey insight: Butterfly factorizations offer a structured approach to learning efficient transformations.\n\nFrom Reference 4 (Position Interpolation):\n```\nPosition Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length\n```\nKey insight: Careful handling of positional information is crucial for stable long-range modeling.\n\nFrom Reference 5 (Structured State Space Models):\n```\nSSMs appear as an effective way to learn deep Wiener models... they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems\n```\nKey insight: SSMs provide a theoretically grounded approach to sequence modeling with good scalability properties.\n\n3. Key Insights and Detailed Analysis\n\na) Architectural Integration Opportunities:\n- Combining SSMs with vector quantization could provide both efficiency and expressiveness\n- Gating mechanisms can help control information flow in complex architectures\n- Butterfly patterns could be used to structure parameter matrices efficiently\n\nb) Efficiency Considerations:\n- Linear-time attention mechanisms are crucial for scaling\n- Caching strategies can significantly improve performance\n- Structured parameter matrices can reduce computational complexity\n\nc) Stability and Scalability:\n- Position handling is critical for extending context windows\n- Hierarchical structures help manage long-range dependencies\n- Careful parameter sharing can improve efficiency without sacrificing performance\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in efficient attention mechanisms\n2. Novel approaches to parameter sharing and structured matrices\n3. Integration strategies for combining multiple architectural components\n\n5. References\n\n[1] Chen, S., et al. (2023). \"Extending Context Window of Large Language Models via Positional Interpolation.\" arXiv:2306.15595.\n\n[2] Dao, T., et al. (2019). \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations.\" International Conference on Machine Learning.\n\n[3] Lingle, L.D. (2023). \"Linear-Time Transformers via Vector Quantization.\" arXiv preprint.\n\n[4] Bonassi, F., et al. (2023). \"Structured state-space models are deep Wiener models.\" arXiv preprint.\n\n[5] [Design Artifact] \"GatedHierarchicalMamba: Enhancing Hierarchical State Space Models with Selective Gating and Gated Linear Attention for Efficient Language Modeling.\""
        },
        {
            "ready": false,
            "query": "state space models neural networks, adaptive computation attention, efficient gradient computation",
            "detail": "Find specific implementations and performance analysis of:\n1. State space models integration with attention mechanisms\n2. Adaptive computation techniques in neural networks\n3. Methods for efficient gradient propagation and training stability\nFocus on mathematical formulations, architectural details, and empirical results.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and performance analysis of:\n1. State space models integration with attention mechanisms\n2. Adaptive computation techniques in neural networks\n3. Methods for efficient gradient propagation and training stability\nFocus on mathematical formulations, architectural details, and empirical results.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.90)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.96)*\n\n```\nFinally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code ${ }^{1}$ and pre-trained model checkpoints ${ }^{2}$ are publicly available. ## 2 Background\n\n### 2.1 Attention Mechanism\n\nSuppose the input to the layer is $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, where $L$ is the sequence length and $d$ is the embedding dimension, then the attention mechanism outputs\n\n$$\n\\operatorname{Attn}(\\mathbf{X})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}$. Here $\\mathbf{W}_{q}, \\mathbf{W}_{k}, \\mathbf{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix $\\mathbf{A}=$ $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\in \\mathbb{R}^{L \\times L}$. Then, $\\mathbf{A}_{i j}$ captures the alignment between the $i$-th and the $j$-th input tokens. ### 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a 1-dimensional input signal $u(t)$ to a $d_{s}$-dimensional latent state $x(t)$, after which $x(t)$ is mapped to a 1-dimensional output signal $y(t)$. Concretely,\n\n$$\nx^{\\prime}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nHere, $\\mathbf{A} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{B} \\in \\mathbb{R}^{d_{s}}$ and $\\mathbf{C} \\in \\mathbb{R}^{d_{s}}$. Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$\n\n[^1]cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, highorder polynomial projection operators) are proposed to initialize A. The HiPPO matrices are designed such that the state $x(t)$ at time $t$ can memorize the history of the input $u(t)$ up to time $t$. Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, where $L$ is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size $\\Delta$, such that\n\n$$\n\\begin{aligned}\n& x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k} \\\\\n& \\text { where } \\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1}(\\mathbf{I}+\\Delta / 2 \\cdot \\mathbf{A}) \\\\\n& \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1} \\Delta \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n\\end{aligned}\n$$\n\nWe unroll the above recurrent representation, after which we have\n\n$$\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\cdots+\\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C B}} u_{k}\n$$\n\nThis can be written as a convolutional representation $y=\\overline{\\mathbf{K}} * u$, where the convolution kernel\n\n$$\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\cdots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nHere, \" $*$ \" is the discrete convolution operator, $u$ represents the input sequence $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, and $y$ represents the corresponding output sequence $\\left(y_{0}, y_{1}, \\cdots, y_{L}\\right)$.\n```\n\n##### *Relevant Chunk: No. 2/35 (Score: 0.83)*\n\n```\nHowever, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State space $\\underline{A} u g m e n t e \\underline{D}$ TransformEr. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks. ## 1 Introduction\n\nTransformer models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019; He et al., 2021). These models leverage the attention mechanism (Vaswani et al., 2017), which computes a dependency score for every pair\n\n[^0]of tokens in an input sequence. Therefore, full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019). For example, empirically we find that a Transformer model ( 250 M parameters) consumes over 80 G of GPU memory when the sequence length is 8 k . Additionally, Transformer models equipped with the full attention are easy to overfit because of the lack of structural biases (Lin et al., 2022). That is, the attention mechanism does not assume any structural prior over the inputs. For example, we even need order information (e.g., through sinusoidal encoding) to train a Transformer model. Therefore, the full attention is too flexible such that Transformer models may easily overfit to the noise. This significantly limits the models' practicality in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is often low. Empirically, we find that on a two-way classification task, Transformer with the full attention has a $57.5 \\%$ accuracy, nearly $30 \\%$ less than stateof-the-art methods with powerful structural biases (see Section 4.1 for details). Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases. In approximation methods, we approximate the full attention using fast algorithms with linear complexity. For example, we can approximate and speedup the computation of the attention score matrix (i.e., $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right)$ in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021). However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue. To incorporate structural biases to the Transformer model, partial attention methods are pro-\nposed. Such methods can be further categorized into sparse attention and clustering methods. In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns. In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed. However, the introduced structural biases restrict the models' ability to capture global information. For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information. Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation. These models can be seen as linear recurrent neural networks with specifically designed fixed weights. Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information. This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens. We propose SPADE, short for State space $\\underline{\\text { Augmente }} \\underline{\\mathbf{D}}$ TransformEr. The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information. In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods. We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and finetuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.88)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.88)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.83)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 3/29 (Score: 0.83)*\n\n```\nWe use superscripts, e.g. . ${ }^{d}$, to denote the elements or block-elements of a matrix and a block-matrix. We use subscripts, e.g. ${ }_{i}$, to denote the time index (or input dependency). Specifically, $v_{i}$ represents the value of vector $v$ at time $i$. We use bold notation to indicate sequences, i.e., $\\mathbf{v}_{i}=\\left[v_{1}, \\ldots, v_{i}\\right]$. We use $\\sigma(\\cdot)$ to denote is the sigmoid function. The products $\\odot$ and $\\otimes$ denote the Hadamard (element-wise) product and the Kronecker (block-wise) product, respectively. $\\mathbb{I}_{n}$ denotes the identity matrix of size $\\mathbb{R}^{n \\times n}$. Generally, we omit stating the bias term for weight matrices unless stating the bias term helps with clarity. ## 2 Preliminaries\n\nIn this section, we introduce the key architectural components studied in this work: attention, SSMs, and RNNs. We remark that these components are often the central block - considered to be the backbone - within a complex architecture composed of other blocks and skip connections (see for instance Touvron et al., 2023]). In what follows, we review exclusively the backbone block, which\nwe denote as $f(\\cdot)$ in $\\mathbf{y}=f(\\mathbf{u})$, where $\\mathbf{u} \\in \\mathbb{R}^{L \\times d}$ and $\\mathbf{y} \\in \\mathbb{R}^{L \\times d}$ are the input and output sequences, respectively. ### 2.1 Attention\n\nThe standard self-attention block [Vaswani et al. 2017] consists of three matrices: $W_{Q}, W_{K}$, and $W_{V}$, which are the learnt parameters of the model. These matrices, when multiplied with the input $\\mathbf{u}$, yield the queries $\\mathbf{q} \\in \\mathbb{R}^{d_{k}}$, keys $\\mathbf{k} \\in \\mathbb{R}^{d_{k}}$, and values $\\mathbf{v} \\in \\mathbb{R}^{d_{v}}$, respectively:\n\n$$\n\\mathbf{q}=\\mathbf{u} W_{Q}, \\quad \\mathbf{k}=\\mathbf{u} W_{K}, \\quad \\mathbf{v}=\\mathbf{u} W_{V}\n$$\n\nKeys, queries, and values are then combined in the attention block to produce the output\n\n$$\n\\mathbf{y}=\\zeta\\left(\\frac{\\mathbf{q} \\mathbf{k}^{\\top}}{\\sqrt{d_{k}}}\\right) \\mathbf{v}\n$$\n\nwhere $\\zeta(\\cdot)$ is a map $\\mathbb{R}^{L} \\rightarrow \\mathbb{R}^{L}$ and is applied row-wise. In the standard version of attention - softmax attention $-\\zeta(\\cdot):=\\operatorname{softmax}(\\cdot)$. However, given the limitations of the softmax function, alternative formulations have been proposed. In this work, we consider two formulations of attention: softmax attention (2) and linear attention [Katharopoulos et al. 2020]. We focus on the masked attention formulations, i.e., the attention matrix $\\zeta\\left(\\mathbf{q k}^{\\top}\\right)$ has a lower-triangular structure. Furthermore, in order to simplify the derivations, we drop the scaling factor $\\sqrt{d_{k}}$. ### 2.2 State Space Models\n\nArchitectures based on a state space parametrization compute the output y through a dynamic recurrence of input signals at each time step $i$,\n\n$$\n\\begin{aligned}\nh_{i} & =A_{i} h_{i-1}+B_{i} u_{i} \\\\\ny_{i} & =C_{i} h_{i}+D_{i} u_{i}\n\\end{aligned}\n$$\n\nwhere $h_{i}$ is the hidden state of the system, and the dynamic matrices of appropriate dimensions $A_{i}, B_{i}, C_{i}, D_{i}$ are the learnt parameters of the model. Different time-varying and time-invariant parameterizations for $A_{i}, B_{i}, C_{i}, D_{i}$ have been proposed in the literature (an overview is given in (Amo Alonso et al., 2024]).\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.67)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.67)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: state space models neural networks, adaptive computation attention, efficient gradient computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: state space models neural networks*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. Liquid Structural State-Space Models\n\n*From Search Query: state space models neural networks*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 3. Resurrecting Recurrent Neural Networks for Long Sequences\n\n*From Search Query: state space models neural networks*\n\n*Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De*\n\n**TL;DR:** This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Abstract:** Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 190  (*Influential: 33*)\n\n#### 4. Scalable Adaptive Computation for Iterative Generation\n\n*From Search Query: adaptive computation attention*\n\n*A. Jabri, David J. Fleet, Ting Chen*\n\n**TL;DR:** The Recurrent Interface Networks (RINs) are proposed, an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data.\n\n**Abstract:** Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024X1024 images without cascades or guidance, while being domain-agnostic and up to 10X more efficient than 2D and 3D U-Nets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 85  (*Influential: 13*)\n\n#### 5. AdapLeR: Speeding up Inference by Adaptive Length Reduction\n\n*From Search Query: adaptive computation attention*\n\n*Ali Modarressi, Hosein Mohebbi, Mohammad Taher Pilehvar*\n\n**TL;DR:** This work proposes a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance, and dynamically eliminates less contributing tokens through layers, resulting in shorter lengths and consequently lower computational cost.\n\n**Abstract:** Pre-trained language models have shown stellar performance in various downstream tasks. But, this usually comes at the cost of high latency and computation, hindering their usage in resource-limited settings. In this work, we propose a novel approach for reducing the computational cost of BERT with minimal loss in downstream performance. Our method dynamically eliminates less contributing tokens through layers, resulting in shorter lengths and consequently lower computational cost. To determine the importance of each token representation, we train a Contribution Predictor for each layer using a gradient-based saliency method. Our experiments on several diverse classification tasks show speedups up to 22x during inference time without much sacrifice in performance. We also validate the quality of the selected tokens in our method using human annotations in the ERASER benchmark. In comparison to other widely used strategies for selecting important tokens, such as saliency and attention, our proposed method has a significantly lower false positive rate in generating rationales. Our code is freely available at https://github.com/amodaresi/AdapLeR.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 26  (*Influential: 2*)\n\n#### 6. Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion\n\n*From Search Query: adaptive computation attention*\n\n*Shangyu Wu, Ying Xiong, Yufei Cui, Xue Liu, Buzhou Tang, Tei-Wei Kuo, Chun Jason Xue*\n\n**TL;DR:** A new paradigm of RA named ReFusion is proposed, a computation-efficient Retrieval representation Fusion with bi-level optimization, which directly fuses the retrieval representations into the hidden states of models.\n\n**Abstract:** Retrieval-based augmentations (RA) incorporating knowledge from an external database into language models have greatly succeeded in various knowledge-intensive (KI) tasks. However, integrating retrievals in non-knowledge-intensive (NKI) tasks is still challenging. Existing works focus on concatenating retrievals with inputs to improve model performance. Unfortunately, the use of retrieval concatenation-based augmentations causes an increase in the input length, substantially raising the computational demands of attention mechanisms. This paper proposes a new paradigm of RA named \\textbf{ReFusion}, a computation-efficient Retrieval representation Fusion with bi-level optimization. Unlike previous works, ReFusion directly fuses the retrieval representations into the hidden states of models. Specifically, ReFusion leverages an adaptive retrieval integrator to seek the optimal combination of the proposed ranking schemes across different model layers. Experimental results demonstrate that the proposed ReFusion can achieve superior and robust performance in various NKI tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 7. LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n\n*From Search Query: efficient gradient computation*\n\n*Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu*\n\n**TL;DR:** It is proved that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size.\n\n**Abstract:** Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: \\emph{Gradient-based One-Side Sampling} (GOSS) and \\emph{Exclusive Feature Bundling} (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB \\emph{LightGBM}. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 8744  (*Influential: 936*)\n\n#### 8. Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses\n\n*From Search Query: efficient gradient computation*\n\n*Corinna Cortes, Vitaly Kuznetsov, M. Mohri, Dmitry Storcheus, Scott Yang*\n\n**TL;DR:** Efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses are designed that facilitate efficient gradient computation and enable one to train learning models such as neural networks with complex structured losses.\n\n**Abstract:** Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or $n$-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\\\"{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the $n$-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 9. Understanding Gradient Regularization in Deep Learning: Efficient Finite-Difference Computation and Implicit Bias\n\n*From Search Query: efficient gradient computation*\n\n*Ryo Karakida, Tomoumi Takase, Tomohiro Hayase, Kazuki Osawa*\n\n**TL;DR:** This study reveals that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational cost of GR and shows that the finite-Difference computation also works better in the sense of generalization performance.\n\n**Abstract:** Gradient regularization (GR) is a method that penalizes the gradient norm of the training loss during training. While some studies have reported that GR can improve generalization performance, little attention has been paid to it from the algorithmic perspective, that is, the algorithms of GR that efficiently improve the performance. In this study, we first reveal that a specific finite-difference computation, composed of both gradient ascent and descent steps, reduces the computational cost of GR. Next, we show that the finite-difference computation also works better in the sense of generalization performance. We theoretically analyze a solvable model, a diagonal linear network, and clarify that GR has a desirable implicit bias to so-called rich regime and finite-difference computation strengthens this bias. Furthermore, finite-difference GR is closely related to some other algorithms based on iterative ascent and descent steps for exploring flat minima. In particular, we reveal that the flooding method can perform finite-difference GR in an implicit way. Thus, this work broadens our understanding of GR for both practice and theory.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 11  (*Influential: 4*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Thermal Neural Networks: Lumped-Parameter Thermal Modeling With State-Space Machine Learning\n\n*From Search Query: state space models neural networks*\n\n*Joachim B\u00f6cker, Oliver Wallscheid, Wilhelm Kirchg\u00e4ssner*\n\n**Abstract:** With electric power systems becoming more compact and increasingly powerful, the relevance of thermal stress especially during overload operation is expected to increase ceaselessly. Whenever critical temperatures cannot be measured economically on a sensor base, a thermal model lends itself to estimate those unknown quantities. Thermal models for electric power systems are usually required to be both, real-time capable and of high estimation accuracy. Moreover, ease of implementation and time to production play an increasingly important role. In this work, the thermal neural network (TNN) is introduced, which unifies both, consolidated knowledge in the form of heat-transfer-based lumped-parameter models, and data-driven nonlinear function approximation with supervised machine learning. A quasi-linear parameter-varying system is identified solely from empirical data, where relationships between scheduling variables and system matrices are inferred statistically and automatically. At the same time, a TNN has physically interpretable states through its state-space representation, is end-to-end trainable -- similar to deep learning models -- with automatic differentiation, and requires no material, geometry, nor expert knowledge for its design. Experiments on an electric motor data set show that a TNN achieves higher temperature estimation accuracies than previous white-/grey- or black-box models with a mean squared error of $3.18~\\text{K}^2$ and a worst-case error of $5.84~\\text{K}$ at 64 model parameters.\n\n**Published:** 2021-03-30\n\n\n\n#### 2. Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models\n\n*From Search Query: state space models neural networks*\n\n*Tina Baykaner, Daniel L. Rubin, Christopher Lee-Messer, Khaled K. Saab, Liangqiong Qu, Jared A. Dunnmon, Siyi Tang*\n\n**Abstract:** Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show that GraphS4mer consistently improves over existing models, including (1) seizure detection from electroencephalographic signals, outperforming a previous GNN with self-supervised pre-training by 3.1 points in AUROC; (2) sleep staging from polysomnographic signals, a 4.1 points improvement in macro-F1 score compared to existing sleep staging models; and (3) 12-lead electrocardiogram classification, outperforming previous state-of-the-art models by 2.7 points in macro-F1 score.\n\n**Published:** 2022-11-21\n\n\n\n#### 3. Adaptive Attention Span in Computer Vision\n\n*From Search Query: adaptive computation attention*\n\n*Shakti Kumar, Jerrod Parker, Joe Roussy*\n\n**Abstract:** Recent developments in Transformers for language modeling have opened new areas of research in computer vision. Results from late 2019 showed vast performance increases in both object detection and recognition when convolutions are replaced by local self-attention kernels. Models using local self-attention kernels were also shown to have less parameters and FLOPS compared to equivalent architectures that only use convolutions. In this work we propose a novel method for learning the local self-attention kernel size. We then compare its performance to fixed-size local attention and convolution kernels. The code for all our experiments and models is available at https://github.com/JoeRoussy/adaptive-attention-in-cv\n\n**Published:** 2020-04-18\n\n\n\n#### 4. Adaptive Attention Span in Transformers\n\n*From Search Query: adaptive computation attention*\n\n*Edouard Grave, Armand Joulin, Piotr Bojanowski, Sainbayar Sukhbaatar*\n\n**Abstract:** We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.\n\n**Conference:** adaptive-attention-span-in-transformers-1\n\n**Published:** 2019-05-19\n\n\n\n#### 5. Efficient Per-Example Gradient Computations\n\n*From Search Query: efficient gradient computation*\n\n*Ian Goodfellow*\n\n**Abstract:** This technical report describes an efficient technique for computing the norm\nof the gradient of the loss function for a neural network with respect to its\nparameters. This gradient norm can be computed efficiently for every example.\n\n**Published:** 2015-10-07\n\n\n\n#### 6. AUTOMATA: Gradient Based Data Subset Selection for Compute-Efficient Hyper-parameter Tuning\n\n*From Search Query: efficient gradient computation*\n\n*Rishabh Iyer, Ganesh Ramakrishnan, Lucian Popa, Alexandre V. Evfimievski, Aakriti, Guttu Sai Abhishek, KrishnaTeja Killamsetty*\n\n**Abstract:** Deep neural networks have seen great success in recent years; however, training a deep model is often challenging as its performance heavily depends on the hyper-parameters used. In addition, finding the optimal hyper-parameter configuration, even with state-of-the-art (SOTA) hyper-parameter optimization (HPO) algorithms, can be time-consuming, requiring multiple training runs over the entire dataset for different possible sets of hyper-parameters. Our central insight is that using an informative subset of the dataset for model training runs involved in hyper-parameter optimization, allows us to find the optimal hyper-parameter configuration significantly faster. In this work, we propose AUTOMATA, a gradient-based subset selection framework for hyper-parameter tuning. We empirically evaluate the effectiveness of AUTOMATA in hyper-parameter tuning through several experiments on real-world datasets in the text, vision, and tabular domains. Our experiments show that using gradient-based data subsets for hyper-parameter tuning achieves significantly faster turnaround times and speedups of 3$\\times$-30$\\times$ while achieving comparable performance to the hyper-parameters found using the entire dataset.\n\n**Published:** 2022-03-15\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, focusing on the integration of state space models with attention mechanisms, adaptive computation techniques, and efficient gradient propagation, here are some key findings and insights from the provided sources and additional relevant information:\n\n## State Space Models Integration with Attention Mechanisms\n\n- Recent research has shown that state space models (SSMs) can be integrated with attention mechanisms to achieve efficient and expressive models. For instance, the paper \"State-space models can learn in-context by gradient descent\" demonstrates that a single structured state-space model layer, augmented with local self-attention, can perform in-context learning similar to transformers. This integration allows SSMs to reproduce the outputs of implicit linear models with least squares loss after one step of gradient descent, highlighting the potential for scalable training and effectiveness in general tasks.\n\n- The use of local self-attention in SSMs enables them to capture contextual dependencies efficiently, which is crucial for autoregressive language models. This approach combines the strengths of SSMs in handling temporal dynamics with the expressive power of attention mechanisms.\n\n## Adaptive Computation Techniques in Neural Networks\n\n- Adaptive computation techniques are essential for improving the efficiency and scalability of neural networks. One promising direction is the use of structured matrices and parameter sharing techniques. For example, \"Monarch: Expressive Structured Matrices\" introduces matrices parameterized as products of two block-diagonal matrices, which are hardware-efficient and expressive. This approach can be adapted to reduce computational complexity in autoregressive models while maintaining expressiveness[Analysis Note].\n\n- Another technique is the block-wise processing of attention, as seen in \"Lightning Attention-2\" and \"Luna: Linear Unified Nested Attention\". These methods handle intra-block and inter-block components differently, allowing for efficient attention computation without sacrificing model capacity[Analysis Note].\n\n## Methods for Efficient Gradient Propagation and Training Stability\n\n- Efficient gradient propagation is critical for stable and fast training of deep models. The integration of state space models with attention mechanisms, as mentioned earlier, can facilitate this. The diagonal linear recurrent layer in SSMs can act as a gradient accumulator, which helps in efficient gradient computation and stability during training.\n\n- The use of structured matrices and block-wise attention also contributes to efficient gradient propagation. For instance, \"Monarch\" matrices are designed for better hardware utilization, which can lead to faster and more stable training processes[Analysis Note].\n\n- Additionally, techniques like the Nystr\u00f6m method for approximating self-attention, as in \"Nystr\u00f6mformer\", reduce the computational complexity of attention mechanisms from quadratic to linear, which can significantly improve training efficiency and stability[Analysis Note].\n\n## Mathematical Formulations and Architectural Details\n\n- The mathematical formulation of integrating SSMs with attention involves representing the state of the model as a set of latent variables that evolve according to state transition equations. This is combined with local self-attention mechanisms to capture contextual dependencies. The key insight is that the diagonal linear recurrent layer can act as a gradient accumulator, enabling efficient gradient-based learning.\n\n- Architecturally, this involves designing layers that explicitly model the temporal evolution of the input sequence, using state variables to capture the internal representation of the data. This approach aligns with the internal workings of graph neural networks and can be extended to autoregressive language models.\n\n## Empirical Results\n\n- Empirical results from the integration of SSMs with attention mechanisms show promising performance. For example, the study on SSMs with local self-attention demonstrates that the empirically optimized parameters match the theoretical ones, obtained analytically from the implicit model construction. This indicates that such models can be effectively trained and generalized to various tasks.\n\n- The performance of models like \"Nystr\u00f6mformer\" and \"Luna\" also highlights the efficiency and expressiveness of these approaches. These models have been shown to handle longer sequences and maintain performance comparable to traditional self-attention mechanisms but with reduced computational complexity[Analysis Note].\n\nBy combining these insights, researchers can design innovative autoregressive language model blocks that integrate state space models with attention mechanisms, employ adaptive computation techniques, and ensure efficient gradient propagation and training stability. This approach has the potential to improve the overall performance, scalability, and efficiency of large language models.\n",
            "analysis": "Analysis Note: First Round Research on LM Block Design Innovation\n\n1. Summary of Analysis\n\nThe initial research reveals several promising directions for innovating LM block design:\n\na) Linear-time attention mechanisms are becoming more sophisticated, with approaches like Lightning Attention and Nystr\u00f6mformer offering efficient alternatives to traditional quadratic attention.\n\nb) Structured matrices and parameter sharing techniques show potential for reducing computational complexity while maintaining model expressiveness.\n\nc) The combination of state space models with adaptive mechanisms appears to be a particularly promising direction for innovation.\n\n2. Useful References with Excerpts\n\nFrom \"Lightning Attention-2\":\n```\nWe present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation.\n```\nKey insight: Block-wise computation with different strategies for intra and inter-block attention can maintain efficiency while preserving expressiveness.\n\nFrom \"Nystr\u00f6mformer\":\n```\nOur idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens.\n```\nKey insight: Matrix approximation techniques can effectively reduce attention complexity while maintaining performance.\n\nFrom \"Monarch: Expressive Structured Matrices\":\n```\nMonarch matrices are hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms).\n```\nKey insight: Structured matrices can provide both efficiency and expressiveness through careful parameterization.\n\nFrom \"Luna: Linear Unified Nested Attention\":\n```\nLuna approximates softmax attention with two nested linear attention functions, yielding only linear time and space complexity. Specifically, with the first attention function, Luna packs the input sequence into a sequence of fixed length.\n```\nKey insight: Nested attention structures can provide efficient sequence processing while maintaining model capacity.\n\n3. Key Insights and Detailed Analysis\n\na) Attention Mechanism Innovation:\n- Block-wise processing with different strategies for local and global attention\n- Matrix approximation techniques for reducing complexity\n- Nested attention structures for efficient sequence processing\n\nb) Structured Parameter Design:\n- Block-diagonal matrix products for hardware efficiency\n- Expressive parameterizations that can represent various transforms\n- Balance between structure and flexibility\n\nc) Integration Opportunities:\n- Combining block-wise processing with structured matrices\n- Integrating nested attention with state space models\n- Leveraging hardware-efficient matrix structures\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in state space models and their integration with attention mechanisms\n2. Techniques for adaptive computation in neural networks\n3. Methods for efficient gradient computation and training stability\n\n5. References\n\n[1] Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). \"Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models.\" arXiv preprint.\n\n[2] Xiong, Y., et al. (2021). \"Nystr\u00f6mformer: A Nystr\u00f6m-based Algorithm for Approximating Self-attention.\" AAAI Conference on Artificial Intelligence.\n\n[3] Dao, T., et al. (2022). \"Monarch: Expressive Structured Matrices for Efficient and Accurate Training.\" International Conference on Machine Learning.\n\n[4] Ma, X., et al. (2021). \"Luna: Linear Unified Nested Attention.\" Neural Information Processing Systems.\n\n[5] Liu, H., & Abbeel, P. (2023). \"Blockwise Parallel Transformer for Large Context Models.\""
        },
        {
            "ready": false,
            "query": "selective state space models, hybrid attention mechanisms, parallel implementation neural networks",
            "detail": "Find specific implementations and performance analysis of:\n1. Methods for combining selective state space models with attention mechanisms\n2. Hybrid architectures that integrate multiple attention types\n3. Efficient parallel implementations for neural network blocks\nFocus on architectural details, implementation strategies, and performance metrics.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and performance analysis of:\n1. Methods for combining selective state space models with attention mechanisms\n2. Hybrid architectures that integrate multiple attention types\n3. Efficient parallel implementations for neural network blocks\nFocus on architectural details, implementation strategies, and performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.96)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.96)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.94)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.94)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.91)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.96)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n##### *Relevant Chunk: No. 3/29 (Score: 0.86)*\n\n```\nWe use superscripts, e.g. . ${ }^{d}$, to denote the elements or block-elements of a matrix and a block-matrix. We use subscripts, e.g. ${ }_{i}$, to denote the time index (or input dependency). Specifically, $v_{i}$ represents the value of vector $v$ at time $i$. We use bold notation to indicate sequences, i.e., $\\mathbf{v}_{i}=\\left[v_{1}, \\ldots, v_{i}\\right]$. We use $\\sigma(\\cdot)$ to denote is the sigmoid function. The products $\\odot$ and $\\otimes$ denote the Hadamard (element-wise) product and the Kronecker (block-wise) product, respectively. $\\mathbb{I}_{n}$ denotes the identity matrix of size $\\mathbb{R}^{n \\times n}$. Generally, we omit stating the bias term for weight matrices unless stating the bias term helps with clarity. ## 2 Preliminaries\n\nIn this section, we introduce the key architectural components studied in this work: attention, SSMs, and RNNs. We remark that these components are often the central block - considered to be the backbone - within a complex architecture composed of other blocks and skip connections (see for instance Touvron et al., 2023]). In what follows, we review exclusively the backbone block, which\nwe denote as $f(\\cdot)$ in $\\mathbf{y}=f(\\mathbf{u})$, where $\\mathbf{u} \\in \\mathbb{R}^{L \\times d}$ and $\\mathbf{y} \\in \\mathbb{R}^{L \\times d}$ are the input and output sequences, respectively. ### 2.1 Attention\n\nThe standard self-attention block [Vaswani et al. 2017] consists of three matrices: $W_{Q}, W_{K}$, and $W_{V}$, which are the learnt parameters of the model. These matrices, when multiplied with the input $\\mathbf{u}$, yield the queries $\\mathbf{q} \\in \\mathbb{R}^{d_{k}}$, keys $\\mathbf{k} \\in \\mathbb{R}^{d_{k}}$, and values $\\mathbf{v} \\in \\mathbb{R}^{d_{v}}$, respectively:\n\n$$\n\\mathbf{q}=\\mathbf{u} W_{Q}, \\quad \\mathbf{k}=\\mathbf{u} W_{K}, \\quad \\mathbf{v}=\\mathbf{u} W_{V}\n$$\n\nKeys, queries, and values are then combined in the attention block to produce the output\n\n$$\n\\mathbf{y}=\\zeta\\left(\\frac{\\mathbf{q} \\mathbf{k}^{\\top}}{\\sqrt{d_{k}}}\\right) \\mathbf{v}\n$$\n\nwhere $\\zeta(\\cdot)$ is a map $\\mathbb{R}^{L} \\rightarrow \\mathbb{R}^{L}$ and is applied row-wise. In the standard version of attention - softmax attention $-\\zeta(\\cdot):=\\operatorname{softmax}(\\cdot)$. However, given the limitations of the softmax function, alternative formulations have been proposed. In this work, we consider two formulations of attention: softmax attention (2) and linear attention [Katharopoulos et al. 2020]. We focus on the masked attention formulations, i.e., the attention matrix $\\zeta\\left(\\mathbf{q k}^{\\top}\\right)$ has a lower-triangular structure. Furthermore, in order to simplify the derivations, we drop the scaling factor $\\sqrt{d_{k}}$. ### 2.2 State Space Models\n\nArchitectures based on a state space parametrization compute the output y through a dynamic recurrence of input signals at each time step $i$,\n\n$$\n\\begin{aligned}\nh_{i} & =A_{i} h_{i-1}+B_{i} u_{i} \\\\\ny_{i} & =C_{i} h_{i}+D_{i} u_{i}\n\\end{aligned}\n$$\n\nwhere $h_{i}$ is the hidden state of the system, and the dynamic matrices of appropriate dimensions $A_{i}, B_{i}, C_{i}, D_{i}$ are the learnt parameters of the model. Different time-varying and time-invariant parameterizations for $A_{i}, B_{i}, C_{i}, D_{i}$ have been proposed in the literature (an overview is given in (Amo Alonso et al., 2024]).\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: selective state space models, hybrid attention mechanisms, parallel implementation neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: selective state space models*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n#### 2. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space models*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: selective state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 4. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid attention mechanisms*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios\n\n*From Search Query: hybrid attention mechanisms*\n\n*Zhongzhan Huang, Mingfu Liang, Shan Zhong, Liang Lin*\n\n**Abstract:** We propose the attention-inspired numerical solver (AttNS), a concise method that helps the generalization and robustness issues faced by the AI-Hybrid numerical solver in solving differential equations due to limited data. AttNS is inspired by the effectiveness of attention modules in Residual Neural Networks (ResNet) in enhancing model generalization and robustness for conventional deep learning tasks. Drawing from the dynamical system perspective of ResNet, We seamlessly incorporate attention mechanisms into the design of numerical methods tailored for the characteristics of solving differential equations. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, show-case AttNS consistently enhancing various numerical solvers without any intricate model crafting. Finally, we analyze AttNS experimentally and theoretically, demonstrating its ability to achieve strong generalization and robustness while ensuring the convergence of the solver. This includes requiring less data compared to other advanced methods to achieve comparable generalization errors and better prevention of numerical explosion issues when solving differential equations.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention\n\n*From Search Query: hybrid attention mechanisms*\n\n*Ekta Sood, Simon Tannert, Philipp Mueller, A. Bulling*\n\n**TL;DR:** This work proposes a novel hybrid text saliency model that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Abstract:** A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing(NLP). We propose a novel hybrid text saliency model(TSM) that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 65  (*Influential: 4*)\n\n#### 7. Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions\n\n*From Search Query: parallel implementation neural networks*\n\n*Eunsang Lee, Joon-Woo Lee, Junghyun Lee, Young-Sik Kim, Yongjune Kim, Jong-Seon, W. Choi*\n\n**TL;DR:** This work proposes the imaginary-removing bootstrapping to prevent the deep neural networks from catastrophic divergence during approximate ReLU operations and successfully implements ResNet-110 with high accuracy on the RNS-CKKS scheme for the first time.\n\n**Abstract:** Recently, the standard ResNet-20 network was successfully implemented on the fully homomorphic encryption scheme, residue number system variant Cheon-Kim-Kim-Song (RNS-CKKS) scheme using bootstrapping, but the implementation lacks practicality due to high latency and low security level. To improve the performance, we first minimize total bootstrapping runtime using multiplexed parallel convolution that collects sparse output data for multiple channels com-pactly. We also propose the imaginary-removing bootstrapping to prevent the deep neural networks from catastrophic divergence during approximate ReLU operations. In addition, we optimize level consumptions and use lighter and tighter parameters. Simulation results show that we have 4.67 \u00d7 lower inference latency and 134 \u00d7 less amortized runtime (runtime per image) for ResNet-20 compared to the state-of-the-art previous work, and we achieve standard 128-bit security. Furthermore, we successfully implement ResNet-110 with high accuracy on the RNS-CKKS scheme for the first time.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 70  (*Influential: 22*)\n\n#### 8. EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression\n\n*From Search Query: parallel implementation neural networks*\n\n*Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu*\n\n**TL;DR:** An optimized GPU implementation which supports training GNNs with compressed activations for Pytorch Geometric and Pytorch, and a memory-ef\ufb01cient framework called EXACT, which for the first time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations.\n\n**Abstract:** Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously dif\ufb01cult to implement due to the lack of necessary tools in common graph learning packages. To un-leash the potential of this direction, we provide an optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-ef\ufb01cient framework called \u201cEXACT\u201d, which for the \ufb01rst time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to 32 \u00d7 with 0 . 2 - 0 . 5% accuracy drop and 10 - 25% time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on ogbn-products from a 48GB GPU to a 12GB GPU. The code is available at https://github.com/warai-0toko\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 55  (*Influential: 7*)\n\n#### 9. Parallel training of Deep Neural Networks with Natural Gradient and Parameter Averaging\n\n*From Search Query: parallel implementation neural networks*\n\n*Daniel Povey, Xiaohui Zhang, S. Khudanpur*\n\n**TL;DR:** Another method is described, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow the periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.\n\n**Abstract:** We describe the neural-network training framework used in the Kaldi speech recognition toolkit, which is geared towards training DNNs with large amounts of training data using multiple GPU-equipped or multicore machines. In order to be as hardwareagnostic as possible, we needed a way to use multiple machines without generating excessive network traffic. Our method is to average the neural network parameters periodically (typically every minute or two), and redistribute the averaged parameters to the machines for further training. Each machine sees different data. By itself, this method does not work very well. However, we have another method, an approximate and efficient implementation of Natural Gradient for Stochastic Gradient Descent (NG-SGD), which seems to allow our periodic-averaging method to work well, as well as substantially improving the convergence of SGD on a single machine.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2014\n\n**Citations:** 255  (*Influential: 18*)\n\n### 4 related papers from ArXiv\n\n#### 1. MambaByte: Token-free Selective State Space Model\n\n*From Search Query: selective state space models*\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.\n\n**Published:** 2024-01-24T18:53:53Z  (*Updated: 2024-08-09T20:18:57Z*)\n\n\n\n#### 2. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling\n\n*From Search Query: selective state space models*\n\n*Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11T17:50:51Z  (*Updated: 2024-06-11T17:50:51Z*)\n\n\n\n#### 3. Semantic-Unit-Based Dilated Convolution for Multi-Label Text\n  Classification\n\n*From Search Query: hybrid attention mechanisms*\n\n*Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma, Xu Sun*\n\n**Abstract:** We propose a novel model for multi-label text classification, which is based\non sequence-to-sequence learning. The model generates higher-level semantic\nunit representations with multi-level dilated convolution as well as a\ncorresponding hybrid attention mechanism that extracts both the information at\nthe word-level and the level of the semantic unit. Our designed dilated\nconvolution effectively reduces dimension and supports an exponential expansion\nof receptive fields without loss of local information, and the\nattention-over-attention mechanism is able to capture more summary relevant\ninformation from the source context. Results of our experiments show that the\nproposed model has significant advantages over the baseline models on the\ndataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is\ncompetitive to the deterministic hierarchical models and it is more robust to\nclassifying low-frequency labels.\n\n**Published:** 2018-08-26T14:36:22Z  (*Updated: 2018-11-11T19:12:35Z*)\n\n\n\n#### 4. MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation\n\n*From Search Query: hybrid attention mechanisms*\n\n*Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel*\n\n**Abstract:** Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs\n\n**Published:** 2024-10-03T14:50:33Z  (*Updated: 2024-10-04T14:19:33Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: selective state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model for Semantic Segmentation of Remote Sensing Imagery\n\n*From Search Query: selective state space models*\n\n*Danfeng Hong, Xiaokang Zhang, Xiaoliang Meng, Sijun Dong, Dongxu Li, Libo Wang*\n\n**Abstract:** Semantic segmentation, as a basic tool for intelligent interpretation of remote sensing images, plays a vital role in many Earth Observation (EO) applications. Nowadays, accurate semantic segmentation of remote sensing images remains a challenge due to the complex spatial-temporal scenes and multi-scale geo-objects. Driven by the wave of deep learning (DL), CNN- and Transformer-based semantic segmentation methods have been explored widely, and these two architectures both revealed the importance of multi-scale feature representation for strengthening semantic information of geo-objects. However, the actual multi-scale feature fusion often comes with the semantic redundancy issue due to homogeneous semantic contents in pyramid features. To handle this issue, we propose a novel Mamba-based segmentation network, namely PyramidMamba. Specifically, we design a plug-and-play decoder, which develops a dense spatial pyramid pooling (DSPP) to encode rich multi-scale semantic features and a pyramid fusion Mamba (PFM) to reduce semantic redundancy in multi-scale feature fusion. Comprehensive ablation experiments illustrate the effectiveness and superiority of the proposed method in enhancing multi-scale feature representation as well as the great potential for real-time semantic segmentation. Moreover, our PyramidMamba yields state-of-the-art performance on three publicly available datasets, i.e. the OpenEarthMap (70.8% mIoU), ISPRS Vaihingen (84.8% mIoU) and Potsdam (88.0% mIoU) datasets. The code will be available at https://github.com/WangLibo1995/GeoSeg.\n\n**Published:** 2024-06-16\n\n\n\n#### 3. Hybrid intelligence for dynamic job-shop scheduling with deep reinforcement learning and attention mechanism\n\n*From Search Query: hybrid attention mechanisms*\n\n*Bo Yuan, Xiu Li, Rong Wang, Yuanzhi Dai, Zijun Liao, Yunhui Zeng*\n\n**Abstract:** The dynamic job-shop scheduling problem (DJSP) is a class of scheduling tasks that specifically consider the inherent uncertainties such as changing order requirements and possible machine breakdown in realistic smart manufacturing settings. Since traditional methods cannot dynamically generate effective scheduling strategies in face of the disturbance of environments, we formulate the DJSP as a Markov decision process (MDP) to be tackled by reinforcement learning (RL). For this purpose, we propose a flexible hybrid framework that takes disjunctive graphs as states and a set of general dispatching rules as the action space with minimum prior domain knowledge. The attention mechanism is used as the graph representation learning (GRL) module for the feature extraction of states, and the double dueling deep Q-network with prioritized replay and noisy networks (D3QPN) is employed to map each state to the most appropriate dispatching rule. Furthermore, we present Gymjsp, a public benchmark based on the well-known OR-Library, to provide a standardized off-the-shelf facility for RL and DJSP research communities. Comprehensive experiments on various DJSP instances confirm that our proposed framework is superior to baseline algorithms with smaller makespan across all instances and provide empirical justification for the validity of the various components in the hybrid framework.\n\n**Published:** 2022-01-03\n\n\n\n#### 4. A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization\n\n*From Search Query: hybrid attention mechanisms*\n\n*Richard Radke, Chengjiang Long, Ashraful Islam*\n\n**Abstract:** Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an \"action-ness\" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.\n\n**Published:** 2021-01-03\n\n\n\n#### 5. A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale\n\n*From Search Query: parallel implementation neural networks*\n\n*Michael Rabbat, Dheevatsa Mudigere, Kaushik Rangadurai, Zhijing Li, Jose Gallego-Posada, Shintaro Iwasaki, Tsung-Hsien Lee, Hao-Jun Michael Shi*\n\n**Abstract:** Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks. It constructs a block-diagonal preconditioner where each block consists of a coarse Kronecker product approximation to full-matrix AdaGrad for each parameter of the neural network. In this work, we provide a complete description of the algorithm as well as the performance optimizations that our implementation leverages to train deep networks at-scale in PyTorch. Our implementation enables fast multi-GPU distributed data-parallel training by distributing the memory and computation associated with blocks of each parameter via PyTorch's DTensor data structure and performing an AllGather primitive on the computed search directions at each iteration. This major performance enhancement enables us to achieve at most a 10% performance reduction in per-step wall-clock time compared against standard diagonal-scaling-based adaptive gradient methods. We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo's superiority over standard training recipes with minimal hyperparameter tuning.\n\n**Published:** 2023-09-12\n\n\n\n#### 6. A Highly Parallel FPGA Implementation of Sparse Neural Network Training\n\n*From Search Query: parallel implementation neural networks*\n\n*Kuan-Wen Huang, Zongyang Li, Souvik Kundu, Keith M. Chugg, Sourya Dey, Peter A. Beerel, Diandian Chen*\n\n**Abstract:** We demonstrate an FPGA implementation of a parallel and reconfigurable\narchitecture for sparse neural networks, capable of on-chip training and\ninference. The network connectivity uses pre-determined, structured sparsity to\nsignificantly reduce complexity by lowering memory and computational\nrequirements. The architecture uses a notion of edge-processing, leading to\nefficient pipelining and parallelization. Moreover, the device can be\nreconfigured to trade off resource utilization with training time to fit\nnetworks and datasets of varying sizes. The combined effects of complexity\nreduction and easy reconfigurability enable significantly greater exploration\nof network hyperparameters and structures on-chip. As proof of concept, we show\nimplementation results on an Artix-7 FPGA.\n\n**Published:** 2018-05-31\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by integrating selective state space models (SSMs) with attention mechanisms, implementing hybrid architectures, and ensuring efficient parallel implementations, here are some key findings and strategies based on the provided sources and analysis:\n\n## Combining SSMs with Attention Mechanisms\n\n- The Mamba model introduces a novel approach by integrating selective state spaces within a simplified neural network architecture. It addresses the weakness of traditional SSMs in performing content-based reasoning by making SSM parameters functions of the input. This approach allows for selective propagation or forgetting of information based on the current token, which complements local self-attention mechanisms.\n- Theoretical foundations of deep selective state-space models suggest that incorporating a selectivity mechanism leveraging multiplicative interactions between inputs and hidden states can surpass attention-powered foundation models in both accuracy and efficiency. This is particularly relevant when combined with local self-attention, as it enhances in-context learning and content-based reasoning.\n\n## Hybrid Architectures Integrating Multiple Attention Types\n\n- The Zamba model, discussed in the podcast, combines selective state space and attention mechanisms. This hybrid architecture leverages the strengths of both approaches, where selective state spaces handle long-range dependencies and attention mechanisms focus on local and global patterns. This combination is crucial for efficient and accurate processing of sequential data.\n- The integration of local self-attention with global SSM processing can be seen as a block-wise attention strategy, reducing computational complexity while maintaining performance. This hybrid approach allows for adaptive attention spans and improves overall efficiency[Analysis Note].\n\n## Efficient Parallel Implementations for Neural Network Blocks\n\n- Mamba achieves fast inference with a 5 times higher throughput than Transformers by incorporating a hardware-aware parallel algorithm. This algorithm is designed for recurrent mode, ensuring linear scaling in sequence length and efficient computation even on long sequences.\n- The use of structured matrices and convolutional implementations in SSMs can significantly reduce computational complexity. For example, the discretization process in SSMs involves transforming continuous matrices into discrete counterparts, which can be implemented globally in a convolutional manner, enhancing parallelizability and efficiency.\n\n## Implementation Strategies and Performance Metrics\n\n- **Hardware-Aware Algorithms**: Designing algorithms that are aware of the underlying hardware can significantly improve training and inference efficiency. Mamba's approach to using hardware-aware parallel algorithms in recurrent mode is a prime example of this strategy.\n- **Structured Matrices**: Utilizing structured matrices can reduce computational complexity. This is evident in the mathematical formulation of SSMs, where discretization and convolutional implementations play a crucial role.\n- **Gradient Propagation**: Ensuring stable and efficient gradient propagation is critical. Theoretical foundations of SSMs provide insights into the expressive power and the importance of the gating mechanism, which can guide the design of stable and efficient gradient propagation techniques.\n\n## Key Insights and Future Directions\n\n- **Input-Dependent State Transitions**: Making SSM parameters input-dependent enhances adaptability and performance, allowing for content-based reasoning[Analysis Note, 2].\n- **Dense Connections**: Dense connections between layers in SSMs can improve information flow without sacrificing efficiency, which is crucial for maintaining training parallelizability and inference efficiency[Analysis Note, 2].\n- **Theoretical Understanding**: A rigorous theoretical understanding of different architectures can guide the design of hybrid models. Comparing linear attention and selective SSMs can provide insights into the conditions under which both are equivalent or superior[Analysis Note, 3].\n\nBy focusing on these strategies and insights, researchers can develop innovative autoregressive language models that integrate the strengths of selective state space models and attention mechanisms, while ensuring efficient parallel implementations and optimal performance metrics.\n",
            "analysis": "Analysis Note: Second Round Research on LM Block Design Innovation\n\n1. Summary of Analysis\n\nThe second round of research reveals several promising directions for innovating LM block design:\n\na) Integration of SSMs with Attention:\n- SSMs can be effectively combined with local self-attention for improved performance\n- Hierarchical and dense connections in SSMs show promise for enhancing information flow\n- Selective mechanisms in SSMs enable content-based reasoning\n\nb) Adaptive Computation Strategies:\n- Block-wise processing with different strategies for local and global patterns\n- Dynamic parameter adjustment based on input complexity\n- Efficient parallel algorithms for recurrent computations\n\nc) Efficient Training and Implementation:\n- Hardware-aware parallel algorithms for selective SSMs\n- Structured matrices for efficient computation\n- Novel gradient propagation techniques\n\n2. Useful References with Excerpts\n\nFrom \"Liquid Structural State-Space Models\":\n```\nLTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference.\n```\nKey insight: Input-dependent state transitions can enhance adaptability and performance.\n\nFrom \"DenseMamba\":\n```\nDenseSSM retains fine-grained information crucial for the final output... Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\n```\nKey insight: Dense connections between layers can improve information flow without sacrificing efficiency.\n\nFrom \"Understanding the differences in Foundation Models\":\n```\nOur framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class... compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent.\n```\nKey insight: Theoretical understanding of different architectures can guide hybrid design.\n\nFrom \"Mamba\":\n```\nA key weakness of such models is their inability to perform content-based reasoning... letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\n```\nKey insight: Making SSM parameters input-dependent enables content-based processing.\n\n3. Key Insights and Detailed Analysis\n\na) State Space Model Innovations:\n- Input-dependent state transitions improve adaptability\n- Dense connections enhance information flow\n- Selective mechanisms enable content-based processing\n- Hierarchical structures capture multi-scale patterns\n\nb) Attention Integration:\n- Local self-attention complements global SSM processing\n- Block-wise attention reduces computational complexity\n- Adaptive attention spans improve efficiency\n- Hybrid architectures combine strengths of both approaches\n\nc) Implementation Considerations:\n- Hardware-aware algorithm design crucial for efficiency\n- Structured matrices reduce computational complexity\n- Parallel processing important for training speed\n- Gradient stability needs careful consideration\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for combining SSMs with attention mechanisms\n2. Methods for efficient parallel implementation\n3. Novel approaches to parameter sharing and structured matrices\n\n5. References\n\n[1] Hasani, R.M., et al. (2022). \"Liquid Structural State-Space Models.\" International Conference on Learning Representations.\n\n[2] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models.\" arXiv preprint.\n\n[3] Sieber, J., et al. (2024). \"Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks.\" arXiv preprint.\n\n[4] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv preprint.\n\n[5] Smith, J.T.H., et al. (2023). \"Convolutional State Space Models for Long-Range Spatiotemporal Modeling.\" Neural Information Processing Systems.\n\n[6] Orvieto, A., et al. (2023). \"Resurrecting Recurrent Neural Networks for Long Sequences.\" International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "efficient gradient computation neural networks, memory optimization sequence models, local global processing architectures",
            "detail": "Find specific implementations and analysis of:\n1. Efficient gradient computation techniques in hybrid neural architectures\n2. Memory optimization strategies for processing long sequences\n3. Methods for combining local and global processing in neural networks\nFocus on implementation details, optimization techniques, and performance metrics.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and analysis of:\n1. Efficient gradient computation techniques in hybrid neural architectures\n2. Memory optimization strategies for processing long sequences\n3. Methods for combining local and global processing in neural networks\nFocus on implementation details, optimization techniques, and performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Scalable MatMul-free Language Modeling (Avg. Score: 0.90)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.90)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 2. ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths (Avg. Score: 0.68)\n\n*Ruslan Khalitov, Tong Yu, Lei Cheng, Zhirong Yang*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** A simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths, and substantially outperforms other neural attention models.\n\n**Abstract:** Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.\n\n##### *Relevant Chunk: No. 17/29 (Score: 0.68)*\n\n```\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR), 2022. Jun He, Liqun Wang, Liu Liu, Jiao Feng, and Hao Wu. Long document classification from local word glimpses via recurrent attention learning. IEEE Access, 7:40707-40718, 2019. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Andrew Jaegle, Felix Axel Gimeno Gil, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning (ICML), 2021. Ruslan Khalitov, Tong Yu, Lei Cheng, and Zhirong Yang. Sparse factorization of square matrices with application to neural attention modeling. Neural Networks, 152:160-168, 2022. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv, 2001.04451, 2020. Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks: A unified approach to action segmentation.\n```\n\n#### 3. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization  (Avg. Score: 0.46)\n\n*R. Csord\u00e1s, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 42  (*Influential: 7*)\n\n**TL;DR:** This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.\n\n**Abstract:** Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.\n\n##### *Relevant Chunk: No. 28/47 (Score: 0.46)*\n\n```\nIn Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. In Proc. Association for Computational Linguistics (ACL), pp. 403-413, Virtual only, July 2020. Jerry Fodor and Brian P McLaughlin. Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work. Cognition, 35(2):183-204, 1990. Jerry A Fodor, Zenon W Pylyshyn, et al. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch\u00e4rli. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. Preprint arXiv:2007.08970, 2020. Alex Graves. Adaptive computation time for recurrent neural networks. In Int. Conf. on Learning Representations (ICLR) Workshop Track, Vancouver, Canada, April 2016. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John P. Agapiou, Adri\u00e0 Puigdom\u00e8nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): $471-476,2016$. Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. Preprint arXiv:2012.05208, 2020. Stephen Jos\u00e9 Hanson. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena, 42 $(1-3): 265-272,1990$. Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proc. North American Chapter of the Association for Computational Linguistics on Human Language Technologies (NAACL-HLT), pp. 1118-1128, Minneapolis, USA, June 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, Las Vegas, NV, USA, June 2016. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, pp. $1735-1780,1997$. Dieuwke Hupkes, Anand Singh, Kris Korrel, German Kruszewski, and Elia Bruni. Learning compositionally through attentive guidance.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.32)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.32)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 5. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.29)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 31/54 (Score: 0.29)*\n\n```\n## A. 2 Memory in Machine Learning\n\nMemory in sequence models Sequential or temporal data in areas such as language, reinforcement learning, and continual learning can involve increasingly long dependencies. However, direct parametric modeling cannot handle inputs of unknown and potentially unbounded lengths. Many modern solutions such as attention [70] and dilated convolutions [5], are functions on finite windows, thus sidestepping the need for an explicit memory representation. While this suffices for certain tasks, these approaches can only process a finite context window instead of an entire sequence. Naively increasing the window length poses significant compute and memory challenges. This has spurred various approaches to extend this fixed context window subjected to compute and storage constraints [6, 15, 18, 42, 59, 60, 64, 74]. We instead focus on the core problem of online processing and memorization of continuous and discrete signals, and anticipate that the study of this foundational problem will be useful in improving a variety of models. Recurrent memory Recurrent neural networks are a natural tool for modeling sequential data online, with the appealing property of having unbounded context; in other words they can summarize history indefinitely. However, due to difficulties in the optimization process (vanishing/exploding gradients [56]), particular care must be paid to endow them with longer memory. The ubiquitous LSTM 34 and simplifications such as the GRU [17] control the update with gates to smooth the optimization process. With more careful parametrization, the addition of gates alone make RNNs significantly more robust and able to address long-term dependencies [31. Tallec and Ollivier [66] show that gates are in fact fundamental for recurrent dynamics by allowing time dilations. Many other approaches to endowing RNNs with better memory exist, such as noise injection [32] or non-saturating gates [9], which can suffer from instability issues.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient gradient computation neural networks, memory optimization sequence models, local global processing architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses\n\n*From Search Query: efficient gradient computation neural networks*\n\n*Corinna Cortes, Vitaly Kuznetsov, M. Mohri, Dmitry Storcheus, Scott Yang*\n\n**TL;DR:** Efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses are designed that facilitate efficient gradient computation and enable one to train learning models such as neural networks with complex structured losses.\n\n**Abstract:** Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or $n$-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\\\"{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the $n$-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 2. Gradient Descent for Spiking Neural Networks\n\n*From Search Query: efficient gradient computation neural networks*\n\n*Dongsung Huh, T. Sejnowski*\n\n**TL;DR:** A gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.\n\n**Abstract:** Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (~millisecond) spike-based interactions for efficient encoding of information, and a delayed memory XOR task over extended duration (~second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 250  (*Influential: 7*)\n\n#### 3. Efficient Neural Network Training via Forward and Backward Propagation Sparsification\n\n*From Search Query: efficient gradient computation neural networks*\n\n*Xiao Zhou, Weizhong Zhang, Zonghao Chen, Shizhe Diao, Tong Zhang*\n\n**TL;DR:** This paper proposes an efficient sparse training method with completely sparse forward and backward passes, and proposes a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training.\n\n**Abstract:** Sparse training is a natural idea to accelerate the training speed of deep neural networks and save the memory usage, especially since large modern neural networks are significantly over-parameterized. However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step. This paper solves this problem by proposing an efficient sparse training method with completely sparse forward and backward passes. We first formulate the training process as a continuous minimization problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure. For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algorithm is much more effective in accelerating the training process, up to an order of magnitude faster.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 41  (*Influential: 1*)\n\n#### 4. GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based Collaborative Filtering\n\n*From Search Query: memory optimization sequence models*\n\n*Yoonseok Yang, Kyu Seok Kim, Minsam Kim, Juneyoung Park*\n\n**TL;DR:** This work proposes GRAM (GRadient Accumulation for Multi-modality in CCF), which exploits the fact that a given item often appears multiple times within a batch of interaction histories, and significantly improves training efficiency.\n\n**Abstract:** Content-based collaborative filtering (CCF) predicts user-item interactions based on both users\u2019 interaction history and items\u2019 content information. Recently, pre-trained language models (PLM) have been used to extract high-quality item encodings for CCF. However, it is resource-intensive to train a PLM-based CCF model in an end-to-end (E2E) manner, since optimization involves back-propagating through every content encoding within a given user interaction sequence. To tackle this issue, we propose GRAM (GRadient Accumulation for Multi-modality in CCF), which exploits the fact that a given item often appears multiple times within a batch of interaction histories. Specifically, Single-step GRAM aggregates each item encoding\u2019s gradients for back-propagation, with theoretic equivalence to the standard E2E training. As an extension of Single-step GRAM, we propose Multi-step GRAM, which increases the gradient update latency, achieving a further speedup with drastically less GPU memory. GRAM significantly improves training efficiency (up to 146x) on five datasets from two task domains of Knowledge Tracing and News Recommendation. Our code is available at https://github.com/yoonseok312/GRAM.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 5. Learning Recurrent Neural Networks with Hessian-Free Optimization\n\n*From Search Query: memory optimization sequence models*\n\n*James Martens, I. Sutskever*\n\n**TL;DR:** This work solves the long-outstanding problem of how to effectively train recurrent neural networks on complex and difficult sequence modeling problems which may contain long-term data dependencies and offers a new interpretation of the generalized Gauss-Newton matrix of Schraudolph which is used within the HF approach of Martens.\n\n**Abstract:** In this work we resolve the long-outstanding problem of how to effectively train recurrent neural networks (RNNs) on complex and difficult sequence modeling problems which may contain long-term data dependencies. Utilizing recent advances in the Hessian-free optimization approach (Martens, 2010), together with a novel damping scheme, we successfully train RNNs on two sets of challenging problems. First, a collection of pathological synthetic datasets which are known to be impossible for standard optimization approaches (due to their extremely long-term dependencies), and second, on three natural and highly complex real-world sequence datasets where we find that our method significantly outperforms the previous state-of-the-art method for training neural sequence models: the Long Short-term Memory approach of Hochreiter and Schmidhuber (1997). Additionally, we offer a new interpretation of the generalized Gauss-Newton matrix of Schraudolph (2002) which is used within the HF approach of Martens.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2011\n\n**Citations:** 624  (*Influential: 58*)\n\n#### 6. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: memory optimization sequence models*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**TL;DR:** The low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter, is introduced and achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 3*)\n\n#### 7. Taxonomizing local versus global structure in neural network loss landscapes\n\n*From Search Query: local global processing architectures*\n\n*Yaoqing Yang, Liam Hodgkinson, Ryan Theisen, Joe Zou, Joseph E. Gonzalez, K. Ramchandran, Michael W. Mahoney*\n\n**TL;DR:** A detailed empirical analysis of the loss landscape structure of thousands of neural network models, systematically varying learning tasks, model architectures, and/or quantity/quality of data sheds light on phases of learning, fundamental versus incidental determinants of good generalization, the role of load-like and temperature-like parameters in the learning process, different influences on the lost landscape from model and data, and the relationships between local and global metrics.\n\n**Abstract:** Viewing neural network models in terms of their loss landscapes has a long history in the statistical mechanics approach to learning, and in recent years it has received attention within machine learning proper. Among other things, local metrics (such as the smoothness of the loss landscape) have been shown to correlate with global properties of the model (such as good generalization performance). Here, we perform a detailed empirical analysis of the loss landscape structure of thousands of neural network models, systematically varying learning tasks, model architectures, and/or quantity/quality of data. By considering a range of metrics that attempt to capture different aspects of the loss landscape, we demonstrate that the best test accuracy is obtained when: the loss landscape is globally well-connected; ensembles of trained models are more similar to each other; and models converge to locally smooth regions. We also show that globally poorly-connected landscapes can arise when models are small or when they are trained to lower quality data; and that, if the loss landscape is globally poorly-connected, then training to zero loss can actually lead to worse test accuracy. Our detailed empirical results shed light on phases of learning (and consequent double descent behavior), fundamental versus incidental determinants of good generalization, the role of load-like and temperature-like parameters in the learning process, different influences on the loss landscape from model and data, and the relationships between local and global metrics, all topics of recent interest.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 30  (*Influential: 6*)\n\n#### 8. Taming Local Effects in Graph-based Spatiotemporal Forecasting\n\n*From Search Query: local global processing architectures*\n\n*Andrea Cini, Ivan Marisca, Daniele Zambon, C. Alippi*\n\n**TL;DR:** The main objective of this paper is to understand the interplay between globality and locality in graph-based spatiotemporal forecasting, while contextually proposing a methodological framework to rationalize the practice of including trainable node embeddings in such architectures.\n\n**Abstract:** Spatiotemporal graph neural networks have shown to be effective in time series forecasting applications, achieving better performance than standard univariate predictors in several settings. These architectures take advantage of a graph structure and relational inductive biases to learn a single (global) inductive model to predict any number of the input time series, each associated with a graph node. Despite the gain achieved in computational and data efficiency w.r.t. fitting a set of local models, relying on a single global model can be a limitation whenever some of the time series are generated by a different spatiotemporal stochastic process. The main objective of this paper is to understand the interplay between globality and locality in graph-based spatiotemporal forecasting, while contextually proposing a methodological framework to rationalize the practice of including trainable node embeddings in such architectures. We ascribe to trainable node embeddings the role of amortizing the learning of specialized components. Moreover, embeddings allow for 1) effectively combining the advantages of shared message-passing layers with node-specific parameters and 2) efficiently transferring the learned model to new node sets. Supported by strong empirical evidence, we provide insights and guidelines for specializing graph-based models to the dynamics of each time series and show how this aspect plays a crucial role in obtaining accurate predictions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 21  (*Influential: 3*)\n\n#### 9. Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis\n\n*From Search Query: local global processing architectures*\n\n*Mengwei Ren, Neel Dey, M. Styner, K. Botteron, G. Gerig*\n\n**TL;DR:** This paper presents a local and multi-scale spatiotemporal representation learning method for image-to-image architectures trained on longitudinal images that exploits the spatiotsemporal self-similarity of learned multi- scale intra-subject image features for pretraining and develops several feature-wise regularizations that avoid degenerate representations.\n\n**Abstract:** Recent self-supervised advances in medical computer vision exploit the global and local anatomical self-similarity for pretraining prior to downstream tasks such as segmentation. However, current methods assume i.i.d. image acquisition, which is invalid in clinical study designs where follow-up longitudinal scans track subject-specific temporal changes. Further, existing self-supervised methods for medically-relevant image-to-image architectures exploit only spatial or temporal self-similarity and do so via a loss applied only at a single image-scale, with naive multi-scale spatiotemporal extensions collapsing to degenerate solutions. To these ends, this paper makes two contributions: (1) It presents a local and multi-scale spatiotemporal representation learning method for image-to-image architectures trained on longitudinal images. It exploits the spatiotemporal self-similarity of learned multi-scale intra-subject image features for pretraining and develops several feature-wise regularizations that avoid degenerate representations; (2) During finetuning, it proposes a surprisingly simple self-supervised segmentation consistency regularization to exploit intra-subject correlation. Benchmarked across various segmentation tasks, the proposed framework outperforms both well-tuned randomly-initialized baselines and current self-supervised techniques designed for both i.i.d. and longitudinal datasets. These improvements are demonstrated across both longitudinal neurodegenerative adult MRI and developing infant brain MRI and yield both higher performance and longitudinal consistency.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Efficient Per-Example Gradient Computations in Convolutional Neural Networks\n\n*From Search Query: efficient gradient computation neural networks*\n\n*Eric W. Tramel, Gaspar Rochette, Andre Manoel*\n\n**Abstract:** Deep learning frameworks leverage GPUs to perform massively-parallel computations over batches of many training examples efficiently. However, for certain tasks, one may be interested in performing per-example computations, for instance using per-example gradients to evaluate a quantity of interest unique to each example. One notable application comes from the field of differential privacy, where per-example gradients must be norm-bounded in order to limit the impact of each example on the aggregated batch gradient. In this work, we discuss how per-example gradients can be efficiently computed in convolutional neural networks (CNNs). We compare existing strategies by performing a few steps of differentially-private training on CNNs of varying sizes. We also introduce a new strategy for per-example gradient calculation, which is shown to be advantageous depending on the model architecture and how the model is trained. This is a first step in making differentially-private training of CNNs practical.\n\n**Published:** 2019-12-12\n\n\n\n#### 2. Deep Learning with Differential Privacy\n\n*From Search Query: efficient gradient computation neural networks*\n\n*Kunal Talwar, H. Brendan McMahan, Ian Goodfellow, Andy Chu, Mart\u00edn Abadi, Li Zhang, Ilya Mironov*\n\n**Abstract:** Machine learning techniques based on neural networks are achieving remarkable\nresults in a wide variety of domains. Often, the training of models requires\nlarge, representative datasets, which may be crowdsourced and contain sensitive\ninformation. The models should not expose private information in these\ndatasets. Addressing this goal, we develop new algorithmic techniques for\nlearning and a refined analysis of privacy costs within the framework of\ndifferential privacy. Our implementation and experiments demonstrate that we\ncan train deep neural networks with non-convex objectives, under a modest\nprivacy budget, and at a manageable cost in software complexity, training\nefficiency, and model quality.\n\n**Published:** 2016-07-01\n\n\n\n#### 3. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n\n*From Search Query: memory optimization sequence models*\n\n*Tri Dao*\n\n**Abstract:** Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).\n\n**Published:** 2023-07-17\n\n\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n*From Search Query: memory optimization sequence models*\n\n*Christopher R\u00e9, Atri Rudra, Stefano Ermon, Daniel Y. Fu, Tri Dao*\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n**Published:** 2022-05-27\n\n\n\n#### 5. ETC: Encoding Long and Structured Inputs in Transformers\n\n*From Search Query: local global processing architectures*\n\n*Zachary Fisher, Santiago Ontanon, Vaclav Cvicek, Chris Alberti, Sumit Sanghai, Philip Pham, Joshua Ainslie, Anirudh Ravula, Qifan Wang, Li Yang*\n\n**Abstract:** Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.\n\n**Proceeding:** emnlp-2020-11\n\n**Published:** 2020-04-17\n\n\n\n#### 6. XCiT: Cross-Covariance Image Transformers\n\n*From Search Query: local global processing architectures*\n\n*Herv\u00e9 Jegou, Jakob Verbeek, Gabriel Synnaeve, Natalia Neverova, Ivan Laptev, Armand Joulin, Matthijs Douze, Piotr Bojanowski, Mathilde Caron, Hugo Touvron, Alaaeldin El-Nouby*\n\n**Abstract:** Following their success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens ,i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a \"transposed\" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) is built upon XCA. It combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including image classification and self-supervised feature learning on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-17\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, focusing on efficient gradient computation, memory optimization for long sequences, and combining local and global processing, here are some specific implementations and analysis that can be helpful:\n\n### Efficient Gradient Computation Techniques in Hybrid Architectures\n\n1. **Selective State Space Models (SSMs) with Attention Mechanisms:**\n   - The integration of SSMs with attention mechanisms, as seen in models like Samba, allows for efficient gradient computation. Samba selectively compresses sequences into recurrent hidden states while maintaining the precision of attention mechanisms, which can reduce the computational overhead associated with pure attention models.\n   - The state space duality (SSD) framework, mentioned in \"Transformers are SSMs,\" enables the design of architectures that are 2-8X faster than traditional Transformers while maintaining competitive performance. This framework can help in optimizing gradient computations by leveraging the structured state space duality.\n\n2. **Hardware-Aware Implementations:**\n   - Hardware-aware algorithm design is crucial for efficient gradient computation. For instance, highly parallel implementations on FPGA or distributed data-parallel implementations can significantly speed up gradient computations. The work on \"A Highly Parallel FPGA Implementation of Sparse Neural Network Training\" and \"A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer\" provides insights into how to optimize gradient computations using parallel processing.\n\n### Memory Optimization Strategies for Processing Long Sequences\n\n1. **Selective State Space Models (SSMs):**\n   - SSMs, such as those used in MambaByte, operate with a fixed-sized memory state, which is particularly beneficial for handling very long sequences. This approach ensures that memory usage does not scale linearly with sequence length, making it more memory-efficient.\n\n2. **Sequence Compression and State Compression:**\n   - Models like Samba and MambaByte use sequence compression techniques to reduce the memory footprint. By selectively compressing sequences into recurrent hidden states, these models can handle long sequences without significant memory increases.\n\n3. **Pruning and Sparse Neural Networks:**\n   - Techniques like SNIP and SynFlow, which involve pruning \"unimportant\" weights before or during training, can reduce the memory load of the model. These methods can be particularly effective in reducing the memory requirements for long sequence processing.\n\n### Methods for Combining Local and Global Processing\n\n1. **Hybrid Architectures:**\n   - Combining local processing (e.g., using SSMs) with global processing (e.g., using attention mechanisms) is a promising approach. Models like Samba and PyramidMamba demonstrate how this combination can enhance performance. Samba integrates SSMs with attention to achieve superior performance, while PyramidMamba uses a dense spatial pyramid pooling to encode multi-scale semantic features and a pyramid fusion Mamba to reduce semantic redundancy.\n\n2. **Multi-Scale Feature Representation:**\n   - The PyramidMamba model uses a dense spatial pyramid pooling to encode rich multi-scale semantic features. This multi-scale processing approach allows the model to capture both local and global context effectively, enhancing the overall performance of the model.\n\n3. **Residual Connections:**\n   - Residual connections, or skip connections, can help in combining local and global processing by allowing gradients to flow more freely through the network. This architectural motif is particularly useful in deep networks, as it mitigates the vanishing gradient problem and allows for more efficient training.\n\n### Additional Considerations\n\n- **Vanishing Gradient Problem:**\n  - Techniques such as residual connections, batch normalization, and careful weight initialization can help mitigate the vanishing gradient problem, which is particularly relevant when dealing with deep networks and long sequences.\n\n- **Parallel Processing:**\n  - Efficient parallel implementations, such as those described in distributed data-parallel PyTorch implementations, can significantly speed up training and inference times, making the model more scalable and efficient.\n\nBy incorporating these techniques, researchers can develop autoregressive language models that are more efficient in gradient computation, optimized for memory usage, and capable of combining local and global processing effectively.\n",
            "analysis": "Analysis Note: Third Round Research on LM Block Design Innovation\n\n1. Summary of Analysis\n\nThe third round of research reveals several promising directions for innovating LM block design, particularly around:\n\na) Novel Hybrid Architectures:\n- Integration of selective SSMs with attention mechanisms\n- Combination of local and global processing strategies\n- Hardware-aware implementation techniques\n\nb) Performance Optimization:\n- Linear-time sequence modeling with selective state spaces\n- Efficient parallel implementations\n- Memory and computation optimization strategies\n\nc) Theoretical Foundations:\n- Understanding the relationships between different architectures\n- Mathematical formulations for hybrid models\n- Conditions for equivalence between different approaches\n\n2. Useful References with Excerpts\n\nFrom \"Transformers are SSMs\":\n```\nThe state space duality (SSD) framework allows us to design a new architecture whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n```\nKey insight: Theoretical connections between SSMs and attention variants can lead to more efficient architectures.\n\nFrom \"Samba\":\n```\nSamba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism... substantially outperforms state-of-the-art models based on pure attention or SSMs.\n```\nKey insight: Hybrid architectures combining SSMs with attention can achieve superior performance.\n\nFrom \"MambaByte\":\n```\nOperating on bytes results in significantly longer sequences... Mamba SSM offers an appealing alternative approach with a fixed-sized memory state and efficient decoding.\n```\nKey insight: SSMs can effectively handle very long sequences with fixed memory requirements.\n\nFrom \"PyramidMamba\":\n```\nDesign a plug-and-play decoder, which develops a dense spatial pyramid pooling (DSPP) to encode rich multi-scale semantic features and a pyramid fusion Mamba (PFM) to reduce semantic redundancy.\n```\nKey insight: Multi-scale feature representation and fusion strategies can enhance model performance.\n\n3. Key Insights and Detailed Analysis\n\na) Architectural Integration:\n- SSMs and attention mechanisms can be complementary\n- Multi-scale processing improves feature representation\n- Hybrid architectures can leverage strengths of both approaches\n\nb) Implementation Strategies:\n- Hardware-aware algorithm design crucial for efficiency\n- Parallel processing can be optimized through careful structuring\n- Memory management key for handling long sequences\n\nc) Performance Considerations:\n- Linear-time processing achievable with selective SSMs\n- Memory-efficient implementations possible through state compression\n- Trade-offs between efficiency and expressiveness can be managed\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient gradient computation in hybrid architectures\n2. Memory optimization strategies for long sequence processing\n3. Novel approaches to combining local and global processing\n\n5. References\n\n[1] Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" International Conference on Machine Learning.\n\n[2] Ren, L., et al. (2024). \"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling.\" arXiv preprint.\n\n[3] Wang, J., et al. (2024). \"MambaByte: Token-free Selective State Space Model.\" arXiv preprint.\n\n[4] Hong, D., et al. (2024). \"PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model.\" arXiv preprint.\n\n[5] Huang, K.W., et al. (2018). \"A Highly Parallel FPGA Implementation of Sparse Neural Network Training.\" IEEE International Symposium on Circuits and Systems.\n\n[6] Rabbat, M., et al. (2023). \"A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"SSM initialization techniques\", \"gradient optimization methods\", \"performance benchmarks\"",
            "detail": "Find papers discussing initialization techniques and optimization methods for State Space Models, particularly focusing on training stability, gradient flow management, and performance benchmarks. Include information about specific initialization strategies, optimization techniques, and comparative performance analysis.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing initialization techniques and optimization methods for State Space Models, particularly focusing on training stability, gradient flow management, and performance benchmarks. Include information about specific initialization strategies, optimization techniques, and comparative performance analysis.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. From generalization analysis to optimization designs for state space models (Avg. Score: 1.00)\n\n*Fusheng Liu, Qianxiao Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n##### *Relevant Chunk: No. 1/32 (Score: 1.00)*\n\n```\n# From Generalization Analysis to Optimization Designs for State Space Models \n\nFusheng Liu<br>National University of Singapore<br>fusheng@u.nus.edu\n\nQianxiao Li<br>National University of Singapore<br>qianxiao@nus.edu.sg\n\n\n#### Abstract\n\nA State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling.\n```\n\n#### 2. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.99)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 9/20 (Score: 0.99)*\n\n```\nIn International Conference on Learning Representations, January 2021. [7] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models. Advances in Neural Information Processing Systems, 35:35971-35983, December 2022. [8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [9] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections. In International Conference on Learning Representations, February 2023. [10] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [11] Eric Martin and Chris Cundy. Parallelizing Linear Recurrent Neural Nets Over Sequence Length. In International Conference on Learning Representations, February 2018. [12] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards Larger Convolutional Language Models. In International Conference on Machine Learning, June 2023. [13] Joshua Hanson, Maxim Raginsky, and Eduardo Sontag. Learning Recurrent Neural Net Models of Nonlinear Systems. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, pages 425-435. PMLR, May 2021. [14] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks. Journal of Machine Learning Research, 23(42):1-85, 2022. ISSN 1533-7928. [15] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, March 1994.\n```\n\n#### 3. Resurrecting Recurrent Neural Networks for Long Sequences  (Avg. Score: 0.98)\n\n*Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 146  (*Influential: 26*)\n\n**TL;DR:** This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Abstract:** Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n##### *Relevant Chunk: No. 29/71 (Score: 0.98)*\n\n```\nGoel, and C. R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022a. A. Gu, I. Johnson, A. Timalsina, A. Rudra, and C. R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections. arXiv preprint arXiv:2206.12037, 2022b. A. Gupta, A. Gu, and J. Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022a.\n```\n\n#### 4. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.98)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 21/27 (Score: 0.98)*\n\n```\n[17] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" 2014. [Online]. Available: https://arxiv.org/abs/1412. 6980\n[18] A. Gu, A. Gupta, K. Goel, and C. R\u00e9, \"On the Parameterization and Initialization of Diagonal State Space Models,\" 2022. [Online]. Available: https://arxiv.org/abs/2206.11893\n[19] J. T. Smith, A. Warrington, and S. Linderman, \"Simplified State Space Layers for Sequence Modeling,\" in The Eleventh International Conference on Learning Representations, 2023.\n```\n\n#### 5. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.98)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 18/56 (Score: 0.98)*\n\n```\nGoel, and C. R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus).\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"SSM initialization techniques\", \"gradient optimization methods\", \"performance benchmarks\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: \"SSM initialization techniques\"*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 216  (*Influential: 38*)\n\n#### 2. Stability-Informed Initialization of Neural Ordinary Differential Equations\n\n*From Search Query: \"SSM initialization techniques\"*\n\n*Theodor Westny, Arman Mohammadi, Daniel Jung, Erik Frisk*\n\n**Abstract:** This paper addresses the training of Neural Ordinary Differential Equations (neural ODEs), and in particular explores the interplay between numerical integration techniques, stability regions, step size, and initialization techniques. It is shown how the choice of integration technique implicitly regularizes the learned model, and how the solver's corresponding stability region affects training and prediction performance. From this analysis, a stability-informed parameter initialization technique is introduced. The effectiveness of the initialization method is displayed across several learning benchmarks and industrial applications.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search\n\n*From Search Query: \"gradient optimization methods\"*\n\n*Reid Pryzant, Dan Iter, Jerry Li, Y. Lee, Chenguang Zhu, Michael Zeng*\n\n**TL;DR:** Preliminary results suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.\n\n**Abstract:** Large Language Models (LLMs) have shown impressive performance as general purpose agents, but their abilities remain highly dependent on prompts which are hand written with onerous trial-and-error effort. We propose a simple and nonparametric solution to this problem, Automatic Prompt Optimization (APO), which is inspired by numerical gradient descent to automatically improve prompts, assuming access to training data and an LLM API. The algorithm uses minibatches of data to form natural language\"gradients\"that criticize the current prompt. The gradients are then\"propagated\"into the prompt by editing the prompt in the opposite semantic direction of the gradient. These gradient descent steps are guided by a beam search and bandit selection procedure which significantly improves algorithmic efficiency. Preliminary results across three benchmark NLP tasks and the novel problem of LLM jailbreak detection suggest that Automatic Prompt Optimization can outperform prior prompt editing techniques and improve an initial prompt's performance by up to 31%, by using data to rewrite vague task descriptions into more precise annotation instructions.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 203  (*Influential: 36*)\n\n#### 4. The Role of Baselines in Policy Gradient Optimization\n\n*From Search Query: \"gradient optimization methods\"*\n\n*Jincheng Mei, Wesley Chung, Valentin Thomas, Bo Dai, Csaba Szepesvari, D. Schuurmans*\n\n**TL;DR:** It is demonstrated that a finite variance is not necessary for almost sure convergence of stochastic NPG, while controlling update aggressiveness is both necessary and sufficient.\n\n**Abstract:** We study the effect of baselines in on-policy stochastic policy gradient optimization, and close the gap between the theory and practice of policy optimization methods. Our first contribution is to show that the \\emph{state value} baseline allows on-policy stochastic \\emph{natural} policy gradient (NPG) to converge to a globally optimal policy at an $O(1/t)$ rate, which was not previously known. The analysis relies on two novel findings: the expected progress of the NPG update satisfies a stochastic version of the non-uniform \\L{}ojasiewicz (N\\L{}) inequality, and with probability 1 the state value baseline prevents the optimal action's probability from vanishing, thus ensuring sufficient exploration. Importantly, these results provide a new understanding of the role of baselines in stochastic policy gradient: by showing that the variance of natural policy gradient estimates remains unbounded with or without a baseline, we find that variance reduction \\emph{cannot} explain their utility in this setting. Instead, the analysis reveals that the primary effect of the value baseline is to \\textbf{reduce the aggressiveness of the updates} rather than their variance. That is, we demonstrate that a finite variance is \\emph{not necessary} for almost sure convergence of stochastic NPG, while controlling update aggressiveness is both necessary and sufficient. Additional experimental results verify these theoretical findings.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 5. Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery\n\n*From Search Query: \"gradient optimization methods\"*\n\n*Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, T. Goldstein*\n\n**TL;DR:** This work describes an approach to robustly optimize hard text prompts through efficient gradient-based optimization and shows that hard prompts can be automatically discovered that are effective in tuning LMs for classification.\n\n**Abstract:** The strength of modern generative models lies in their ability to be controlled through text-based prompts. Typical\"hard\"prompts are made from interpretable words and tokens, and must be hand-crafted by humans. There are also\"soft\"prompts, which consist of continuous feature vectors. These can be discovered using powerful optimization methods, but they cannot be easily interpreted, re-used across models, or plugged into a text-based interface. We describe an approach to robustly optimize hard text prompts through efficient gradient-based optimization. Our approach automatically generates hard text-based prompts for both text-to-image and text-to-text applications. In the text-to-image setting, the method creates hard prompts for diffusion models, allowing API users to easily generate, discover, and mix and match image concepts without prior knowledge on how to prompt the model. In the text-to-text setting, we show that hard prompts can be automatically discovered that are effective in tuning LMs for classification.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 172  (*Influential: 24*)\n\n#### 6. PyTorch: An Imperative Style, High-Performance Deep Learning Library\n\n*From Search Query: \"performance benchmarks\"*\n\n*Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas K\u00f6pf, E. Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala*\n\n**TL;DR:** This paper details the principles that drove the implementation of PyTorch and how they are reflected in its architecture, and explains how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance.\n\n**Abstract:** Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 37109  (*Influential: 3810*)\n\n#### 7. Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods\n\n*From Search Query: \"performance benchmarks\"*\n\n*Derek Lim, Felix Hohne, Xiuyu Li, Sijia Huang, Vaishnavi Gupta, Omkar Bhalerao, Ser-Nam Lim*\n\n**TL;DR:** This work collects and introduces diverse non-homophilous datasets from a variety of application areas that have up to 384x more nodes and 1398x more edges than prior datasets and introduces LINKX -- a strong simple method that admits straightforward minibatch training and inference.\n\n**Abstract:** Many widely used datasets for graph machine learning tasks have generally been homophilous, where nodes with similar labels connect to each other. Recently, new Graph Neural Networks (GNNs) have been developed that move beyond the homophily regime; however, their evaluation has often been conducted on small graphs with limited application domains. We collect and introduce diverse non-homophilous datasets from a variety of application areas that have up to 384x more nodes and 1398x more edges than prior datasets. We further show that existing scalable graph learning and graph minibatching techniques lead to performance degradation on these non-homophilous datasets, thus highlighting the need for further work on scalable non-homophilous methods. To address these concerns, we introduce LINKX -- a strong simple method that admits straightforward minibatch training and inference. Extensive experimental results with representative simple methods and GNNs across our proposed datasets show that LINKX achieves state-of-the-art performance for learning on non-homophilous graphs. Our codes and data are available at https://github.com/CUAI/Non-Homophily-Large-Scale.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 274  (*Influential: 59*)\n\n#### 8. Investigating Data Contamination in Modern Benchmarks for Large Language Models\n\n*From Search Query: \"performance benchmarks\"*\n\n*Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark B. Gerstein, Arman Cohan*\n\n**TL;DR:** It is found that certain commercial LLMs could surprisingly guess the missing option in various test sets, and in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data.\n\n**Abstract:** Recent observations have underscored a disparity between the inflated benchmark scores and the actual performance of LLMs, raising concerns about potential contamination of evaluation benchmarks. This issue is especially critical for closed-source models and certain open-source models where training data transparency is lacking. In this paper we study data contamination by proposing two methods tailored for both open-source and proprietary LLMs. We first introduce a retrieval-based system to explore potential overlaps between evaluation benchmarks and pretraining corpora. We further present a novel investigation protocol named Testset Slot Guessing (TS-Guessing), applicable to both open and proprietary models. This approach entails masking a wrong answer in a multiple-choice question and prompting the model to fill in the gap. Additionally, it involves obscuring an unlikely word in an evaluation example and asking the model to produce it. We find that certain commercial LLMs could surprisingly guess the missing option in various test sets. Specifically, in the MMLU benchmark, ChatGPT and GPT-4 demonstrated an exact match rate of 52% and 57%, respectively, in guessing the missing options in benchmark test data. We hope these results underscore the need for more robust evaluation methodologies and benchmarks in the field.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 21  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"SSM initialization techniques\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n#### 2. Rethinking Token Reduction for State Space Models\n\n*From Search Query: \"SSM initialization techniques\"*\n\n*Zheng Zhan, Yushu Wu, Zhenglun Kong, Changdi Yang, Yifan Gong, Xuan Shen, Xue Lin, Pu Zhao, Yanzhi Wang*\n\n**Abstract:** Recent advancements in State Space Models (SSMs) have attracted significant\ninterest, particularly in models optimized for parallel training and handling\nlong-range dependencies. Architectures like Mamba have scaled to billions of\nparameters with selective SSM. To facilitate broader applications using Mamba,\nexploring its efficiency is crucial. While token reduction techniques offer a\nstraightforward post-training strategy, we find that applying existing methods\ndirectly to SSMs leads to substantial performance drops. Through insightful\nanalysis, we identify the reasons for this failure and the limitations of\ncurrent techniques. In response, we propose a tailored, unified post-training\ntoken reduction method for SSMs. Our approach integrates token importance and\nsimilarity, thus taking advantage of both pruning and merging, to devise a\nfine-grained intra-layer token reduction strategy. Extensive experiments show\nthat our method improves the average accuracy by 5.7% to 13.1% on six\nbenchmarks with Mamba-2 compared to existing methods, while significantly\nreducing computational demands and memory requirements.\n\n**Published:** 2024-10-16T00:06:13Z  (*Updated: 2024-10-16T00:06:13Z*)\n\n\n\n#### 3. Gradient Vaccine: Investigating and Improving Multi-task Optimization in\n  Massively Multilingual Models\n\n*From Search Query: \"gradient optimization methods\"*\n\n*Zirui Wang, Yulia Tsvetkov, Orhan Firat, Yuan Cao*\n\n**Abstract:** Massively multilingual models subsuming tens or even hundreds of languages\npose great challenges to multi-task optimization. While it is a common practice\nto apply a language-agnostic procedure optimizing a joint multilingual task\nobjective, how to properly characterize and take advantage of its underlying\nproblem structure for improving optimization efficiency remains under-explored.\nIn this paper, we attempt to peek into the black-box of multilingual\noptimization through the lens of loss function geometry. We find that gradient\nsimilarity measured along the optimization trajectory is an important signal,\nwhich correlates well with not only language proximity but also the overall\nmodel performance. Such observation helps us to identify a critical limitation\nof existing gradient-based multi-task learning methods, and thus we derive a\nsimple and scalable optimization procedure, named Gradient Vaccine, which\nencourages more geometrically aligned parameter updates for close tasks.\nEmpirically, our method obtains significant model performance gains on\nmultilingual machine translation and XTREME benchmark tasks for multilingual\nlanguage models. Our work reveals the importance of properly measuring and\nutilizing language proximity in multilingual optimization, and has broader\nimplications for multi-task learning beyond multilingual modeling.\n\n**Published:** 2020-10-12T17:26:34Z  (*Updated: 2020-10-12T17:26:34Z*)\n\n\n\n#### 4. Dynamic Gradient Alignment for Online Data Mixing\n\n*From Search Query: \"gradient optimization methods\"*\n\n*Simin Fan, David Grangier, Pierre Ablin*\n\n**Abstract:** The composition of training data mixtures is critical for effectively\ntraining large language models (LLMs), as it directly impacts their performance\non downstream tasks. Our goal is to identify an optimal data mixture to\nspecialize an LLM for a specific task with access to only a few examples.\nTraditional approaches to this problem include ad-hoc reweighting methods,\nimportance sampling, and gradient alignment techniques. This paper focuses on\ngradient alignment and introduces Dynamic Gradient Alignment (DGA), a scalable\nonline gradient alignment algorithm. DGA dynamically estimates the pre-training\ndata mixture on which the models' gradients align as well as possible with\nthose of the model on the specific task. DGA is the first gradient alignment\napproach that incurs minimal overhead compared to standard pre-training and\noutputs a competitive model, eliminating the need for retraining the model.\nExperimentally, we demonstrate significant improvements over importance\nsampling in two key scenarios: (i) when the pre-training set is small and\nimportance sampling overfits due to limited data; and (ii) when there is\ninsufficient specialized data, trapping importance sampling on narrow pockets\nof data. Our findings underscore the effectiveness of gradient alignment\nmethods in optimizing training data mixtures, particularly in data-constrained\nenvironments, and offer a practical solution for enhancing LLM performance on\nspecific tasks with limited data availability.\n\n**Published:** 2024-10-03T14:00:44Z  (*Updated: 2024-10-03T14:00:44Z*)\n\n\n\n#### 5. MedFuzz: Exploring the Robustness of Large Language Models in Medical\n  Question Answering\n\n*From Search Query: \"performance benchmarks\"*\n\n*Robert Osazuwa Ness, Katie Matton, Hayden Helm, Sheng Zhang, Junaid Bajwa, Carey E. Priebe, Eric Horvitz*\n\n**Abstract:** Large language models (LLM) have achieved impressive performance on medical\nquestion-answering benchmarks. However, high benchmark accuracy does not imply\nthat the performance generalizes to real-world clinical settings. Medical\nquestion-answering benchmarks rely on assumptions consistent with quantifying\nLLM performance but that may not hold in the open world of the clinic. Yet LLMs\nlearn broad knowledge that can help the LLM generalize to practical conditions\nregardless of unrealistic assumptions in celebrated benchmarks. We seek to\nquantify how well LLM medical question-answering benchmark performance\ngeneralizes when benchmark assumptions are violated. Specifically, we present\nan adversarial method that we call MedFuzz (for medical fuzzing). MedFuzz\nattempts to modify benchmark questions in ways aimed at confounding the LLM. We\ndemonstrate the approach by targeting strong assumptions about patient\ncharacteristics presented in the MedQA benchmark. Successful \"attacks\" modify a\nbenchmark item in ways that would be unlikely to fool a medical expert but\nnonetheless \"trick\" the LLM into changing from a correct to an incorrect\nanswer. Further, we present a permutation test technique that can ensure a\nsuccessful attack is statistically significant. We show how to use performance\non a \"MedFuzzed\" benchmark, as well as individual successful attacks. The\nmethods show promise at providing insights into the ability of an LLM to\noperate robustly in more realistic settings.\n\n**Published:** 2024-06-03T18:15:56Z  (*Updated: 2024-09-01T19:38:02Z*)\n\n\n\n#### 6. ARB: Advanced Reasoning Benchmark for Large Language Models\n\n*From Search Query: \"performance benchmarks\"*\n\n*Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kranias, John J. Nay, Kshitij Gupta, Aran Komatsuzaki*\n\n**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious quantitative reasoning and knowledge benchmarks. However, many of these\nbenchmarks are losing utility as LLMs get increasingly high scores, despite not\nyet reaching expert performance in these domains. We introduce ARB, a novel\nbenchmark composed of advanced reasoning problems in multiple fields. ARB\npresents a more challenging test than prior benchmarks, featuring problems in\nmathematics, physics, biology, chemistry, and law. As a subset of ARB, we\nintroduce a challenging set of math and physics problems which require advanced\nsymbolic reasoning and domain knowledge. We evaluate recent models such as\nGPT-4 and Claude on ARB and demonstrate that current models score well below\n50% on more demanding tasks. In order to improve both automatic and assisted\nevaluation capabilities, we introduce a rubric-based evaluation approach,\nallowing GPT-4 to score its own intermediate reasoning steps. Further, we\nconduct a human evaluation of the symbolic subset of ARB, finding promising\nagreement between annotators and GPT-4 rubric evaluation scores.\n\n**Published:** 2023-07-25T17:55:19Z  (*Updated: 2023-07-28T03:31:08Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: \"SSM initialization techniques\"*\n\n*Christopher R\u00e9, Atri Rudra, Tri Dao, Michael Zhang, Armin W. Thomas, Eric Nguyen, Elliot L. Epstein, Daniel Y. Fu*\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Published:** 2023-02-13\n\n\n\n#### 2. Investigating the Indirect Object Identification circuit in Mamba\n\n*From Search Query: \"SSM initialization techniques\"*\n\n*Adri\u00e0 Garriga-Alonso, Danielle Ensign*\n\n**Abstract:** How well will current interpretability techniques generalize to future models? A relevant case study is Mamba, a recent recurrent architecture with scaling comparable to Transformers. We adapt pre-Mamba techniques to Mamba and partially reverse-engineer the circuit responsible for the Indirect Object Identification (IOI) task. Our techniques provide evidence that 1) Layer 39 is a key bottleneck, 2) Convolutions in layer 39 shift names one position forward, and 3) The name entities are stored linearly in Layer 39's SSM. Finally, we adapt an automatic circuit discovery tool, positional Edge Attribution Patching, to identify a Mamba IOI circuit. Our contributions provide initial evidence that circuit-based mechanistic interpretability tools work well for the Mamba architecture.\n\n**Published:** 2024-07-19\n\n\n\n#### 3. Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization\n\n*From Search Query: \"gradient optimization methods\"*\n\n*Samy Jelassi, Aaron Defazio*\n\n**Abstract:** We introduce MADGRAD, a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple fields, including classification and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing. For each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly.\n\n**Published:** 2021-01-26\n\n\n\n#### 4. The Parallel Knowledge Gradient Method for Batch Bayesian Optimization\n\n*From Search Query: \"gradient optimization methods\"*\n\n*Jian Wu, Peter I. Frazier*\n\n**Abstract:** In many applications of black-box optimization, one can evaluate multiple\npoints simultaneously, e.g. when evaluating the performances of several\ndifferent neural network architectures in a parallel computing environment. In\nthis paper, we develop a novel batch Bayesian optimization algorithm --- the\nparallel knowledge gradient method. By construction, this method provides the\none-step Bayes-optimal batch of points to sample. We provide an efficient\nstrategy for computing this Bayes-optimal batch of points, and we demonstrate\nthat the parallel knowledge gradient method finds global optima significantly\nfaster than previous batch Bayesian optimization algorithms on both synthetic\ntest functions and when tuning hyperparameters of practical machine learning\nalgorithms, especially when function evaluations are noisy.\n\n**Conference:** the-parallel-knowledge-gradient-method-for-1\n\n**Published:** 2016-06-14\n\n\n\n#### 5. Revisiting the Performance of iALS on Item Recommendation Benchmarks\n\n*From Search Query: \"performance benchmarks\"*\n\n*Yehuda Koren, Li Zhang, Walid Krichene, Steffen Rendle*\n\n**Abstract:** Matrix factorization learned by implicit alternating least squares (iALS) is a popular baseline in recommender system research publications. iALS is known to be one of the most computationally efficient and scalable collaborative filtering methods. However, recent studies suggest that its prediction quality is not competitive with the current state of the art, in particular autoencoders and other item-based collaborative filtering methods. In this work, we revisit the iALS algorithm and present a bag of tricks that we found useful when applying iALS. We revisit four well-studied benchmarks where iALS was reported to perform poorly and show that with proper tuning, iALS is highly competitive and outperforms any method on at least half of the comparisons. We hope that these high quality results together with iALS's known scalability spark new interest in applying and further improving this decade old technique.\n\n**Published:** 2021-10-26\n\n\n\n#### 6. SUPERB: Speech processing Universal PERformance Benchmark\n\n*From Search Query: \"performance benchmarks\"*\n\n*Hung-Yi Lee, Abdelrahman Mohamed, Shinji Watanabe, Shang-Wen Li, Shuyan Dong, Zili Huang, Da-Rong Liu, Ko-tik Lee, Wei-Cheng Tseng, Tzu-Hsien Huang, Guan-Ting Lin, Xuankai Chang, Jiatong Shi, Andy T. Liu, Yist Y. Lin, Kushal Lakhotia, Cheng-I Jeff Lai, Yung-Sung Chuang, Po-Han Chi, Shu-wen Yang*\n\n**Abstract:** Self-supervised learning (SSL) has proven vital for advancing research in natural language processing (NLP) and computer vision (CV). The paradigm pretrains a shared model on large volumes of unlabeled data and achieves state-of-the-art (SOTA) for various tasks with minimal adaptation. However, the speech processing community lacks a similar setup to systematically explore the paradigm. To bridge this gap, we introduce Speech processing Universal PERformance Benchmark (SUPERB). SUPERB is a leaderboard to benchmark the performance of a shared model across a wide range of speech processing tasks with minimal architecture changes and labeled data. Among multiple usages of the shared model, we especially focus on extracting the representation learned from SSL due to its preferable re-usability. We present a simple framework to solve SUPERB tasks by learning task-specialized lightweight prediction heads on top of the frozen shared model. Our results demonstrate that the framework is promising as SSL representations show competitive generalizability and accessibility across SUPERB tasks. We release SUPERB as a challenge with a leaderboard and a benchmark toolkit to fuel the research in representation learning and general speech processing.\n\n**Published:** 2021-05-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using State Space Models (SSMs), here are some key findings and techniques related to initialization, optimization, and performance benchmarks:\n\n## SSM Initialization Techniques\n\n### Mimetic Initialization\nA recent paper titled \"Mimetic Initialization Helps State Space Models Learn to Recall\" introduces a structured initialization technique that allows SSMs to mimic self-attention mechanisms. This technique enhances the ability of SSMs to perform recall tasks by initializing the state space layers in a way that resembles the attention maps of self-attention layers. This approach significantly improves the performance of SSMs on tasks that require recalling long sequences.\n\n### SDLoRA Initialization\nThe paper \"Parameter-Efficient Fine-Tuning of State Space Models\" discusses the use of SDLoRA (Selectively updating the channel and state Dimensions of SSM modules with Low-Rank Adaptation) for fine-tuning SSM-based models. SDLoRA involves applying Low-Rank Adaptation (LoRA) to linear projection matrices and selectively updating the channel and state dimensions of SSM modules. This method has been shown to enhance the performance of SSM-based models compared to using LoRA alone.\n\n## Gradient Optimization Methods\n\n### Parameter-Efficient Fine-Tuning\nThe same paper on \"Parameter-Efficient Fine-Tuning of State Space Models\" evaluates various parameter-efficient fine-tuning (PEFT) methods for SSM-based models. It compares methods such as BitFit, LoRA, prefix-tuning, and prompt tuning. The study highlights the importance of initial parameter settings and the use of optimization methods like the modified Powell method followed by LBFGS to mitigate the issue of local minima in multivariate models.\n\n### Gradient Flow Management\nThe analysis note from the researcher emphasizes the importance of gradient flow management, particularly in managing extreme values of gradient variance that can arise from long sequence lengths. This is crucial for maintaining training stability. Papers like \"The Stability-Efficiency Dilemma\" suggest that sequence length warmup strategies can help in stabilizing the training process by gradually increasing the sequence length to reduce extreme gradient variance values.\n\n## Performance Benchmarks\n\n### Comparative Performance Analysis\nThe paper on \"Mimetic Initialization\" provides a comparative analysis of SSMs with self-attention models, showing that with the proposed initialization technique, SSMs can achieve better performance on recall tasks. Specifically, it demonstrates that SSMs can generalize to sequences up to 4 times longer than those seen during training, a significant improvement over previous results.\n\n### Hybrid Architectures\nThe work on \"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\" explores the integration of SSMs with other architectures to leverage their strengths. This hybrid approach can offer better performance benchmarks by combining the efficiency of SSMs with the expressive power of other models like Transformers.\n\nIn summary, the key strategies for improving autoregressive language models using SSMs include:\n\n- **Mimetic Initialization**: To enhance recall performance by mimicking self-attention mechanisms.\n- **SDLoRA and Other PEFT Methods**: To fine-tune SSM-based models efficiently and improve their performance.\n- **Gradient Flow Management**: To stabilize training by managing gradient variance, especially with long sequence lengths.\n- **Hybrid Architectures**: To combine the strengths of SSMs with other models to achieve better performance benchmarks.\n\nThese techniques and benchmarks can help in designing more efficient, stable, and high-performing autoregressive language models.\n",
            "analysis": "Analysis Note - Round 3:\n\n1. Summary of Analysis:\nThe third round of searches reveals important insights about hardware optimization, implementation challenges, and training stability for the proposed AdaptiveHybridSSM design:\n\na) Hardware Optimization:\n- Need for IO-aware algorithms and memory hierarchy optimization\n- Importance of efficient parallel processing\n- Trade-offs between memory usage and computational efficiency\n\nb) Implementation Considerations:\n- Selective compression mechanisms require careful design\n- Balance between compression and information preservation\n- Hardware-aware algorithm design crucial for performance\n\nc) Training Stability:\n- Sequence length and batch size affect stability\n- Need for careful initialization and optimization strategies\n- Importance of gradient flow management\n\n2. Useful References with Excerpts:\n\nFrom \"Simple Hardware-Efficient Long Convolutions\":\n\"FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2\u00d7, and allows us to train on Path256, a challenging task with sequence length 64K.\"\n\nFrom \"Gated Linear Attention Transformers\":\n\"We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates... GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\"\n\nFrom \"The Stability-Efficiency Dilemma\":\n\"We find that there is a strong correlation between training instability and extreme values of gradient variance, and that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training.\"\n\n3. Key Insights:\n\na) Hardware Optimization:\n- IO-aware algorithms crucial for performance\n- Memory hierarchy optimization essential\n- Need for efficient parallel processing strategies\n- Trade-offs between memory usage and speed\n\nb) Implementation Strategies:\n- Selective compression requires careful balance\n- Hardware-aware algorithm design important\n- Need for efficient state management\n- Importance of parallel processing capabilities\n\nc) Training Considerations:\n- Sequence length affects stability\n- Need for careful initialization\n- Gradient flow management crucial\n- Importance of optimization strategy\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific initialization techniques for SSMs\n2. Gradient flow optimization methods\n3. Detailed performance comparisons with existing models\n4. Integration strategies for hybrid architectures\n\nNext search queries should focus on:\n- SSM initialization techniques\n- Gradient optimization methods\n- Performance benchmarks\n- Integration strategies\n\n5. References:\n\n[1] Fu, D. Y., et al. (2023). \"Simple Hardware-Efficient Long Convolutions for Sequence Modeling.\" International Conference on Machine Learning.\n\n[2] Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\" International Conference on Machine Learning.\n\n[3] Li, C., et al. (2021). \"The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models.\" Neural Information Processing Systems.\n\n[4] Ren, L., et al. (2024). \"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling.\" arXiv preprint.\n\n[5] Tang, H., et al. (2024). \"RazorAttention: Efficient KV Cache Compression Through Retrieval Heads.\" arXiv preprint.\n\n[6] Galim, K., et al. (2024). \"Parameter-Efficient Fine-Tuning of State Space Models.\" arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"adaptive state space models\", \"hierarchical sequence processing\", \"memory efficient language models\"",
            "detail": "Find papers discussing adaptive state space models in language modeling, focusing on parameter generation, hierarchical processing, and memory optimization. Include details about implementation approaches, efficiency improvements, and theoretical foundations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing adaptive state space models in language modeling, focusing on parameter generation, hierarchical processing, and memory optimization. Include details about implementation approaches, efficiency improvements, and theoretical foundations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.99)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.99)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 2. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.98)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.98)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 3. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry (Avg. Score: 0.98)\n\n*S. Aasi*\n\n**Published in:** Asylum (2019)\t**Cited by** 13  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** N/A\n\n##### *Relevant Chunk: No. 17/43 (Score: 0.98)*\n\n```\narXiv preprint arXiv:2010.11929, 2020. Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=COZDyOWYGg. Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu (eds.), Proceedings of the 2nd Workshop on New Frontiers in Summarization, pp. 70-79, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-5409. URL https://aclanthology.org/D19-5409. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790-2799. PMLR, 2019. Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021 . Kazuki Irie, Imanol Schlag, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. Going beyond linear transformers with recurrent fast weight programmers. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview. net/forum?id=ot20RiBqTa1. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into RNNs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10630-10643, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/ v1/2021.emnlp-main.830. URL https://aclanthology.org/2021.emnlp-main. 830. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Associative Recurrent Memory Transformer (Avg. Score: 0.98)\n\n*Ivan Rodkin, Yuri Kuratov, Aydar Bulatov, Mikhail Burtsev*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is demonstrated that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%.\n\n**Abstract:** This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.\n\n##### *Relevant Chunk: No. 6/16 (Score: 0.98)*\n\n```\n[8] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models, 2023. [9] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling, 2020. [10] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023. [11] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. [12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training.\n```\n\n#### 5. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.98)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.98)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive state space models\", \"hierarchical sequence processing\", \"memory efficient language models\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models\n\n*From Search Query: \"adaptive state space models\"*\n\n*Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu*\n\n**TL;DR:** This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass, and introduces a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre- trained knowledge during the pruning process.\n\n**Abstract:** The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. TADAM: Task dependent adaptive metric for improved few-shot learning\n\n*From Search Query: \"adaptive state space models\"*\n\n*Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, Alexandre Lacoste*\n\n**TL;DR:** This work identifies that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms and proposes and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space.\n\n**Abstract:** Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100. Our code is publicly available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1236  (*Influential: 202*)\n\n#### 3. DMix: Adaptive Distance-aware Interpolative Mixup\n\n*From Search Query: \"adaptive state space models\"*\n\n*Ramit Sawhney, Megh Thakkar, Shrey Pandit, Ritesh Soun, Di Jin, Diyi Yang, Lucie Flek*\n\n**TL;DR:** DMix is proposed, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space that leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.\n\n**Abstract:** Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities.We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations.We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components.DMix being generalizable, can be applied to various tasks, models and modalities.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 2*)\n\n#### 4. Scalable and Accurate Dialogue State Tracking via Hierarchical Sequence Generation\n\n*From Search Query: \"hierarchical sequence processing\"*\n\n*Liliang Ren, Jianmo Ni, Julian McAuley*\n\n**TL;DR:** This paper investigates how to approach DST using a generation framework without the pre-defined ontology list, where each turn of user utterance and system response is directly generated by applying a hierarchical encoder-decoder structure.\n\n**Abstract:** Existing approaches to dialogue state tracking rely on pre-defined ontologies consisting of a set of all possible slot types and values. Though such approaches exhibit promising performance on single-domain benchmarks, they suffer from computational complexity that increases proportionally to the number of pre-defined slots that need tracking. This issue becomes more severe when it comes to multi-domain dialogues which include larger numbers of slots. In this paper, we investigate how to approach DST using a generation framework without the pre-defined ontology list. Given each turn of user utterance and system response, we directly generate a sequence of belief states by applying a hierarchical encoder-decoder structure. In this way, the computational complexity of our model will be a constant regardless of the number of pre-defined slots. Experiments on both the multi-domain and the single domain dialogue state tracking dataset show that our model not only scales easily with the increasing number of pre-defined domains and slots but also reaches the state-of-the-art performance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 79  (*Influential: 22*)\n\n#### 5. A Hierarchical Training Paradigm for Antibody Structure-sequence Co-design\n\n*From Search Query: \"hierarchical sequence processing\"*\n\n*Fang Wu, Stan Z. Li*\n\n**TL;DR:** A hierarchical training paradigm (HTP) for the antibody sequence-structure co-design, which consists of four levels of training stages, each corresponding to a specific protein modality within a particular protein domain, and offers a hopeful path to unleash the potential of deep generative architectures.\n\n**Abstract:** Therapeutic antibodies are an essential and rapidly expanding drug modality. The binding specificity between antibodies and antigens is decided by complementarity-determining regions (CDRs) at the tips of these Y-shaped proteins. In this paper, we propose a hierarchical training paradigm (HTP) for the antibody sequence-structure co-design. HTP consists of four levels of training stages, each corresponding to a specific protein modality within a particular protein domain. Through carefully crafted tasks in different stages, HTP seamlessly and effectively integrates geometric graph neural networks (GNNs) with large-scale protein language models to excavate evolutionary information from not only geometric structures but also vast antibody and non-antibody sequence databases, which determines ligand binding pose and strength. Empirical experiments show that HTP sets the new state-of-the-art performance in the co-design problem as well as the fix-backbone design. Our research offers a hopeful path to unleash the potential of deep generative architectures and seeks to illuminate the way forward for the antibody sequence and structure co-design challenge.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 6. Hierarchical Phrase-Based Sequence-to-Sequence Learning\n\n*From Search Query: \"hierarchical sequence processing\"*\n\n*Bailin Wang, Ivan Titov, Jacob Andreas, Yoon Kim*\n\n**Abstract:** This paper describes a neural transducer that maintains the flexibility of standard sequence-to-sequence (seq2seq) models while incorporating hierarchical phrases as a source of inductive bias during training and as explicit constraints during inference. Our approach trains two models: a discriminative parser based on a bracketing transduction grammar whose derivation tree hierarchically aligns source and target phrases, and a neural seq2seq model that learns to translate the aligned phrases one-by-one. We use the same seq2seq model to translate at all phrase scales, which results in two inference modes: one mode in which the parser is discarded and only the seq2seq component is used at the sequence-level, and another in which the parser is combined with the seq2seq model. Decoding in the latter mode is done with the cube-pruned CKY algorithm, which is more involved but can make use of new translation rules during inference. We formalize our model as a source-conditioned synchronous grammar and develop an efficient variational inference algorithm for training. When applied on top of both randomly initialized and pretrained seq2seq models, we find that it performs well compared to baselines on small scale machine translation benchmarks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 7. Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\n\n*From Search Query: \"memory efficient language models\"*\n\n*Jeonghoon Kim, J. H. Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, S. Kwon, Dongsoo Lee*\n\n**TL;DR:** Parameter-Efficient and Quantization-aware Adaptation (PEQA) is presented - a simple yet effective method that combines the advantages of PEFT with quantized LLMs and significantly reduces the memory overhead associated with the optimizer state.\n\n**Abstract:** Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 66  (*Influential: 2*)\n\n#### 8. LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models\n\n*From Search Query: \"memory efficient language models\"*\n\n*Mojan Javaheripi, Gustavo de Rosa, Subhabrata Mukherjee, S. Shah, T. L. Religa, C. C. T. Mendes, S\u00e9bastien Bubeck, F. Koushanfar, Debadeepta Dey*\n\n**TL;DR:** The search phase of this training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs and effectively removes the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.\n\n**Abstract:** The Transformer architecture is ubiquitously used as the building block of large-scale autoregressive language models. However, finding architectures with the optimal trade-off between task performance (perplexity) and hardware constraints like peak memory utilization and latency is non-trivial. This is exacerbated by the proliferation of various hardware. We leverage the somewhat surprising empirical observation that the number of decoder parameters in autoregressive Transformers has a high rank correlation with task performance, irrespective of the architecture topology. This observation organically induces a simple Neural Architecture Search (NAS) algorithm that uses decoder parameters as a proxy for perplexity without need for any model training. The search phase of our training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of perplexity versus any hardware performance cost. We evaluate LTS on diverse devices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer backbones: GPT-2 and Transformer-XL. Results show that the perplexity of 16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster runtime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero and one-shot settings, LTS Pareto-frontier models achieve higher average accuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x lower latency. LTS extracts the Pareto-frontier in under 3 hours while running on a commodity laptop. We effectively remove the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 2*)\n\n#### 9. Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models\n\n*From Search Query: \"memory efficient language models\"*\n\n*Zhengxin Zhang, Dan Zhao, Xupeng Miao, Gabriele Oliaro, Qing Li, Yong Jiang, Zhihao Jia*\n\n**TL;DR:** Quantized Side Tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process, and leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states.\n\n**Abstract:** Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory and none can simultaneously mitigate memory footprint for all three sources. In this paper, we present Quantized Side Tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM's model weights into 4-bit to reduce the memory footprint of the LLM's original weights; QST also introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing backpropagation through the LLM, thus reducing the memory requirement of the intermediate activations. Furthermore, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3 $\\times$ and speed up the finetuning process by up to 3 $\\times$ while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7 $\\times$.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. MiDAS: Multi-integrated Domain Adaptive Supervision for Fake News\n  Detection\n\n*From Search Query: \"adaptive state space models\"*\n\n*Abhijit Suprem, Calton Pu*\n\n**Abstract:** COVID-19 related misinformation and fake news, coined an 'infodemic', has\ndramatically increased over the past few years. This misinformation exhibits\nconcept drift, where the distribution of fake news changes over time, reducing\neffectiveness of previously trained models for fake news detection. Given a set\nof fake news models trained on multiple domains, we propose an adaptive\ndecision module to select the best-fit model for a new sample. We propose\nMiDAS, a multi-domain adaptative approach for fake news detection that ranks\nrelevancy of existing models to new samples. MiDAS contains 2 components: a\ndoman-invariant encoder, and an adaptive model selector. MiDAS integrates\nmultiple pre-trained and fine-tuned models with their training data to create a\ndomain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of\nthe invariant embedding space to estimate each model's relevance to a new\nsample. Higher ranked models provide predictions, and lower ranked models\nabstain. We evaluate MiDAS on generalization to drifted data with 9 fake news\ndatasets, each obtained from different domains and modalities. MiDAS achieves\nnew state-of-the-art performance on multi-domain adaptation for\nout-of-distribution fake news classification.\n\n**Published:** 2022-05-19T19:36:08Z  (*Updated: 2022-05-19T19:36:08Z*)\n\n\n\n#### 2. DAN: Dual-View Representation Learning for Adapting Stance Classifiers\n  to New Domains\n\n*From Search Query: \"adaptive state space models\"*\n\n*Chang Xu, Cecile Paris, Surya Nepal, Ross Sparks, Chong Long, Yafang Wang*\n\n**Abstract:** We address the issue of having a limited number of annotations for stance\nclassification in a new domain, by adapting out-of-domain classifiers with\ndomain adaptation. Existing approaches often align different domains in a\nsingle, global feature space (or view), which may fail to fully capture the\nrichness of the languages used for expressing stances, leading to reduced\nadaptability on stance data. In this paper, we identify two major types of\nstance expressions that are linguistically distinct, and we propose a tailored\ndual-view adaptation network (DAN) to adapt these expressions across domains.\nThe proposed model first learns a separate view for domain transfer in each\nexpression channel and then selects the best adapted parts of both views for\noptimal transfer. We find that the learned view features can be more easily\naligned and more stance-discriminative in either or both views, leading to more\ntransferable overall features after combining the views. Results from extensive\nexperiments show that our method can enhance the state-of-the-art single-view\nmethods in matching stance data across different domains, and that it\nconsistently improves those methods on various adaptation tasks.\n\n**Published:** 2020-03-13T23:56:37Z  (*Updated: 2020-03-13T23:56:37Z*)\n\n\n\n#### 3. Tree-structured Attention with Hierarchical Accumulation\n\n*From Search Query: \"hierarchical sequence processing\"*\n\n*Xuan-Phi Nguyen, Shafiq Joty, Steven C. H. Hoi, Richard Socher*\n\n**Abstract:** Incorporating hierarchical structures like constituency trees has been shown\nto be effective for various natural language processing (NLP) tasks. However,\nit is evident that state-of-the-art (SOTA) sequence-based models like the\nTransformer struggle to encode such structures inherently. On the other hand,\ndedicated models like the Tree-LSTM, while explicitly modeling hierarchical\nstructures, do not perform as efficiently as the Transformer. In this paper, we\nattempt to bridge this gap with \"Hierarchical Accumulation\" to encode parse\ntree structures into self-attention at constant time complexity. Our approach\noutperforms SOTA methods in four IWSLT translation tasks and the WMT'14\nEnglish-German translation task. It also yields improvements over Transformer\nand Tree-LSTM on three text classification tasks. We further demonstrate that\nusing hierarchical priors can compensate for data shortage, and that our model\nprefers phrase-level attentions over token-level attentions.\n\n**Published:** 2020-02-19T08:17:00Z  (*Updated: 2020-02-19T08:17:00Z*)\n\n\n\n#### 4. Structured Prediction of Sequences and Trees using Infinite Contexts\n\n*From Search Query: \"hierarchical sequence processing\"*\n\n*Ehsan Shareghi, Gholamreza Haffari, Trevor Cohn, Ann Nicholson*\n\n**Abstract:** Linguistic structures exhibit a rich array of global phenomena, however\ncommonly used Markov models are unable to adequately describe these phenomena\ndue to their strong locality assumptions. We propose a novel hierarchical model\nfor structured prediction over sequences and trees which exploits global\ncontext by conditioning each generation decision on an unbounded context of\nprior decisions. This builds on the success of Markov models but without\nimposing a fixed bound in order to better represent global phenomena. To\nfacilitate learning of this large and unbounded model, we use a hierarchical\nPitman-Yor process prior which provides a recursive form of smoothing. We\npropose prediction algorithms based on A* and Markov Chain Monte Carlo\nsampling. Empirical results demonstrate the potential of our model compared to\nbaseline finite-context Markov models on part-of-speech tagging and syntactic\nparsing.\n\n**Published:** 2015-03-09T10:35:10Z  (*Updated: 2015-03-09T10:35:10Z*)\n\n\n\n#### 5. Improving Neural Language Models with a Continuous Cache\n\n*From Search Query: \"memory efficient language models\"*\n\n*Edouard Grave, Armand Joulin, Nicolas Usunier*\n\n**Abstract:** We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.\n\n**Published:** 2016-12-13T23:09:49Z  (*Updated: 2016-12-13T23:09:49Z*)\n\n\n\n#### 6. Task-oriented Memory-efficient Pruning-Adapter\n\n*From Search Query: \"memory efficient language models\"*\n\n*Guorun Wang, Jun Yang, Yaoru Sun*\n\n**Abstract:** The Outstanding performance and growing size of Large Language Models has led\nto increased attention in parameter efficient learning. The two predominant\napproaches are Adapters and Pruning. Adapters are to freeze the model and give\nit a new weight matrix on the side, which can significantly reduce the time and\nmemory of training, but the cost is that the evaluation and testing will\nincrease the time and memory consumption. Pruning is to cut off some weight and\nre-distribute the remaining weight, which sacrifices the complexity of training\nat the cost of extremely high memory and training time, making the cost of\nevaluation and testing relatively low. So efficiency of training and inference\ncan't be obtained in the same time. In this work, we propose a task-oriented\nPruning-Adapter method that achieve a high memory efficiency of training and\nmemory, and speeds up training time and ensures no significant decrease in\naccuracy in GLUE tasks, achieving training and inference efficiency at the same\ntime.\n\n**Published:** 2023-03-26T12:18:00Z  (*Updated: 2023-04-06T03:44:38Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class-Incremental Learning\n\n*From Search Query: \"adaptive state space models\"*\n\n*Min Zhang, Liqiang Nie, Bernard Ghanem, Jianlong Wu, Yibo Yang, Xiaojie Li*\n\n**Abstract:** Few-shot class-incremental learning (FSCIL) confronts the challenge of integrating new classes into a model with minimal training samples while preserving the knowledge of previously learned classes. Traditional methods widely adopt static adaptation relying on a fixed parameter space to learn from data that arrive sequentially, prone to overfitting to the current session. Existing dynamic strategies require the expansion of the parameter space continually, leading to increased complexity. In this study, we explore the potential of Selective State Space Models (SSMs) for FSCIL, leveraging its dynamic weights and strong ability in sequence modeling to address these challenges. Concretely, we propose a dual selective SSM projector that dynamically adjusts the projection parameters based on the intermediate features for dynamic adaptation. The dual design enables the model to maintain the robust features of base classes, while adaptively learning distinctive feature shifts for novel classes. Additionally, we develop a class-sensitive selective scan mechanism to guide dynamic adaptation. It minimizes the disruption to base-class representations caused by training on novel data, and meanwhile, forces the selective scan to perform in distinct patterns between base and novel classes. Experiments on miniImageNet, CUB-200, and CIFAR-100 demonstrate that our framework outperforms the existing state-of-the-art methods. The code is available at \\url{https://github.com/xiaojieli0903/Mamba-FSCIL}.\n\n**Published:** 2024-07-08\n\n\n\n#### 2. Adaptive Probabilistic ODE Solvers Without Adaptive Memory Requirements\n\n*From Search Query: \"adaptive state space models\"*\n\n*Nicholas Kr\u00e4mer*\n\n**Abstract:** Despite substantial progress in recent years, probabilistic solvers with adaptive step sizes can still not solve memory-demanding differential equations -- unless we care only about a single point in time (which is far too restrictive; we want the whole time series). Counterintuitively, the culprit is the adaptivity itself: Its unpredictable memory demands easily exceed our machine's capabilities, making our simulations fail unexpectedly and without warning. Still, dropping adaptivity would abandon years of progress, which can't be the answer. In this work, we solve this conundrum. We develop an adaptive probabilistic solver with fixed memory demands building on recent developments in robust state estimation. Switching to our method (i) eliminates memory issues for long time series, (ii) accelerates simulations by orders of magnitude through unlocking just-in-time compilation, and (iii) makes adaptive probabilistic solvers compatible with scientific computing in JAX.\n\n**Published:** 2024-10-14\n\n\n\n#### 3. MusicLM: Generating Music From Text\n\n*From Search Query: \"hierarchical sequence processing\"*\n\n*Christian Frank, Neil Zeghidour, Matt Sharifi, Marco Tagliasacchi, Adam Roberts, Aren Jansen, Qingqing Huang, Antoine Caillon, Mauro Verzetti, Jesse Engel, Zal\u00e1n Borsos, Timo I. Denk, Andrea Agostinelli*\n\n**Abstract:** We introduce MusicLM, a model generating high-fidelity music from text descriptions such as \"a calming violin melody backed by a distorted guitar riff\". MusicLM casts the process of conditional music generation as a hierarchical sequence-to-sequence modeling task, and it generates music at 24 kHz that remains consistent over several minutes. Our experiments show that MusicLM outperforms previous systems both in audio quality and adherence to the text description. Moreover, we demonstrate that MusicLM can be conditioned on both text and a melody in that it can transform whistled and hummed melodies according to the style described in a text caption. To support future research, we publicly release MusicCaps, a dataset composed of 5.5k music-text pairs, with rich text descriptions provided by human experts.\n\n**Published:** 2023-01-26\n\n\n\n#### 4. Real-Time Anomaly Detection for Streaming Analytics\n\n*From Search Query: \"hierarchical sequence processing\"*\n\n*Subutai Ahmad, Scott Purdy*\n\n**Abstract:** Much of the worlds data is streaming, time-series data, where anomalies give\nsignificant information in critical situations. Yet detecting anomalies in\nstreaming data is a difficult task, requiring detectors to process data in\nreal-time, and learn while simultaneously making predictions. We present a\nnovel anomaly detection technique based on an on-line sequence memory algorithm\ncalled Hierarchical Temporal Memory (HTM). We show results from a live\napplication that detects anomalies in financial metrics in real-time. We also\ntest the algorithm on NAB, a published benchmark for real-time anomaly\ndetection, where our algorithm achieves best-in-class results.\n\n**Published:** 2016-07-08\n\n\n\n#### 5. Efficient Memory Management for Large Language Model Serving with PagedAttention\n\n*From Search Query: \"memory efficient language models\"*\n\n*Ion Stoica, Hao Zhang, Joseph E. Gonzalez, Cody Hao Yu, Lianmin Zheng, Ying Sheng, Siyuan Zhuang, Zhuohan Li, Woosuk Kwon*\n\n**Abstract:** High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm\n\n**Published:** 2023-09-12\n\n\n\n#### 6. LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning\n\n*From Search Query: \"memory efficient language models\"*\n\n*Tong Zhang, Chi Han, Jipeng Zhang, Renjie Pi, Shizhe Diao, Xiang Liu, Rui Pan*\n\n**Abstract:** The machine learning community has witnessed impressive advancements since large language models (LLMs) first appeared. Yet, their massive memory consumption has become a significant roadblock to large-scale training. For instance, a 7B model typically requires at least 60 GB of GPU memory with full parameter training, which presents challenges for researchers without access to high-resource environments. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem. However, in most large-scale fine-tuning settings, their performance does not reach the level of full parameter training because they confine the parameter search to a low-rank subspace. Attempting to complement this deficiency, we investigate the layerwise properties of LoRA on fine-tuning tasks and observe an unexpected but consistent skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of importance sampling to different layers in LLMs and randomly freezes most middle layers during optimization. Experimental results show that with similar or less GPU memory consumption, LISA surpasses LoRA or even full parameter tuning in downstream fine-tuning tasks, where LISA consistently outperforms LoRA by over 10%-35% in terms of MT-Bench score while achieving on-par or better performance in MMLU, AGIEval and WinoGrande. On large models, specifically LLaMA-2-70B, LISA surpasses LoRA on MT-Bench, GSM8K, and PubMedQA, demonstrating its effectiveness across different domains.\n\n**Published:** 2024-03-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using adaptive state space models, hierarchical sequence processing, and memory optimization, here are some key findings and references that align with the researcher's goals:\n\n## Adaptive State Space Models\n\n- The work on Mamba and its successors, such as Mamba2, is particularly relevant. These models leverage state space models (SSMs) with input-dependent parameters, which can be adapted to language modeling. Mamba2 introduces the concept of State Space Duality (SSD), simplifying the matrix A into a scalar, which enhances adaptability and performance. This approach is discussed in detail in the context of single-cell data analysis but has implications for natural language processing as well.\n\n## Hierarchical Sequence Processing\n\n- Hierarchical local-global processing is a key component of the AdaptiveHybridSSM proposal. Research on hierarchical architectures in language models, such as those using transformer-based models, can provide insights. For instance, papers on in-context learning and contextual generalization often discuss hierarchical processing methods. The paper \"In-Context In-Context Learning with Transformer Neural Processes\" explores how transformers can learn hierarchical representations, which could be integrated with SSMs.\n\n## Memory Optimization\n\n- Memory optimization is crucial for efficient language models. The Mamba and Mamba2 models are designed to achieve near-linear scalability with respect to sequence length while retaining modeling capabilities comparable to transformers. This is done through selective state updates and efficient discretization techniques, which are essential for memory optimization.\n\n## Implementation Approaches and Efficiency Improvements\n\n- The implementation of adaptive state space models involves discretizing continuous SSMs and using techniques like the zero-order hold (ZOH) for efficient computation. This process is detailed in the Mamba2 paper, where the authors reformulate the continuous matrices into discrete counterparts, making them suitable for modern computational frameworks.\n\n- For hierarchical processing, the use of multiple attention heads and layers in transformer models can be adapted. For example, the GPT-2 model uses multiple attention heads to calculate relevance weights, which can be integrated with hierarchical SSMs to enhance local-global context integration.\n\n## Theoretical Foundations\n\n- The theoretical foundation of using SSMs in language models is rooted in their ability to model complex dependencies efficiently. The paper \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" provides a thorough theoretical analysis of how SSMs can be applied to sequence modeling, which is essential for understanding the adaptivity and efficiency of these models.\n\n## Comparison with Existing SSM Architectures\n\n- Comparing the proposed AdaptiveHybridSSM with existing SSM architectures, such as HyperLoRA and LowRankSelectiveRMSNorm, is crucial. The AdaptiveHybridSSM integrates adaptive SSMs with hierarchical processing and memory optimization more comprehensively than these previous designs. This is highlighted in the initial analysis, where it is noted that the new approach is more comprehensive and novel in its integration of multiple components[Initial Analysis].\n\n## Integration with Current State-of-the-Art Approaches\n\n- Integrating the AdaptiveHybridSSM with current state-of-the-art transformer models involves combining the strengths of both architectures. For instance, the use of transformer neural processes for in-context learning can be integrated with the adaptive parameter generation and hierarchical processing of SSMs. Papers like \"In-Context In-Context Learning with Transformer Neural Processes\" and \"Transformers Learn Higher-Order Optimization Methods for In-Context Learning\" provide insights into how such integrations can be achieved.\n\nBy focusing on these aspects, the researchers can develop a more efficient, scalable, and adaptive autoregressive language model that leverages the strengths of both state space models and hierarchical sequence processing.\n",
            "analysis": "Initial Analysis of AdaptiveHybridSSM Proposal:\n\n1. Summary of Key Components and Innovation Points:\n\nThe proposal introduces AdaptiveHybridSSM, which combines:\n- Adaptive state space models with input-dependent parameters\n- Hierarchical local-global processing with selective compression\n- Memory-optimized implementation with selective state updates\n- Efficient gradient computation through structured state spaces\n\nKey aspects requiring investigation:\na) Adaptive parameter generation mechanism and its efficiency\nb) Hierarchical processing architecture and its effectiveness\nc) Memory compression techniques and their impact\nd) Comparison with existing SSM architectures\ne) Integration with current state-of-the-art approaches\n\n2. Investigation Plan:\n\nPhase 1: State Space Models and Adaptivity\n- Research recent developments in state space models\n- Investigate adaptive parameter generation techniques\n- Study memory efficiency in SSMs\n\nPhase 2: Hierarchical Processing\n- Examine hierarchical architectures in language models\n- Research local-global context integration methods\n- Study selective compression techniques\n\nPhase 3: Memory and Gradient Optimization\n- Investigate efficient gradient computation methods\n- Research memory optimization techniques\n- Study selective state update mechanisms\n\n3. Initial Assessment:\n\nStrengths:\n- Novel combination of adaptive SSMs with hierarchical processing\n- Focus on memory efficiency and scalability\n- Structured approach to gradient computation\n- Clear theoretical foundation\n\nPotential Concerns:\n- Complexity of implementation\n- Training stability with adaptive parameters\n- Computational overhead of parameter generation\n- Integration challenges of multiple components\n\n4. Novelty Check:\n\nCompared to previous designs:\n- More comprehensive than HyperLoRA designs, which focus mainly on low-rank adaptations\n- Different approach from LowRankSelectiveRMSNorm, which focuses on normalization\n- Novel integration of SSMs with hierarchical processing\n\nThe proposal appears to be novel in its comprehensive integration of adaptive SSMs with hierarchical processing and memory optimization, though some individual components have precedents in existing research.\n\n5. Next Steps:\n\nNeed to investigate:\n1. Recent developments in SSM architectures\n2. Adaptive parameter generation techniques\n3. Hierarchical processing in language models\n4. Memory optimization methods\n5. Integration approaches for complex architectures\n\nReferences from initial review:\n1. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\"\n2. Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\"\n3. He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\""
        },
        {
            "ready": false,
            "query": "\"selective state space models\", \"hybrid attention mechanisms\", \"memory efficient training\"",
            "detail": "Find papers discussing implementations of selective state space models in language modeling, particularly focusing on hybrid architectures that combine SSMs with attention mechanisms, and memory-efficient training strategies for adaptive models.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing implementations of selective state space models in language modeling, particularly focusing on hybrid architectures that combine SSMs with attention mechanisms, and memory-efficient training strategies for adaptive models.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n##### *Relevant Chunk: No. 2/21 (Score: 1.00)*\n\n```\nWhile state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5\\% accuracy improvement on public benchmarks. code is avalaible at: https://github.com/ WailordHe/DenseSSM. ## 1. Introduction\n\nSince the release of ChatGPT (OpenAI, 2023), large language models have entered a new epoch, showcasing outstanding abilities in language comprehension, dialogue, and logical reasoning. Over the past year, the industry has witnessed the emergence of numerous large language models, such as LLaMA (Touvron et al., 2023) and ChatGLM (Zeng et al., 2023). These large language models have given rise to a plethora of practical applications, including conversational bots, code assistants, and AI agents. The foundation of large language models lies in the Transformer network\n\n[^0]structure (Vaswani et al., 2017), primarily utilizing a multihead self-attention module for modeling relationships between tokens and a Feed-forward network for non-linear feature transformations. The scaling law (Kaplan et al., 2020) based on the Transformer structure has propelled the continuous development and expansion of large language models. In the Transformer network, multi-head self-attention (MHSA) plays a crucial role, but it comes with significant computational demands and memory requirements during inference. In terms of computational complexity, for an input sentence of length $N$, the calculation of selfattention has a complexity of $O\\left(N^{2}\\right)$ during training and inference. Regarding memory usage, previously encountered keys and values are stored, leading to a memory occupation of $O(N D)$. As a result, recent efforts on network architectures have focused on simplifying Transformer by reducing its computation and space complexity. This includes various approaches, notably convolutional language models (Poli et al., 2023), recurrent unit (Lei, 2021), long context models (Ding et al., 2023), and state space models (SSMs) (Gu et al., 2021; Gu \\& Dao, 2023). These new models have provided strong alternatives to Transformer for building efficient LLMs. SSMs propose modeling sequences by introducing an appropriate design of hidden states for handling long-range dependencies with both training parallelizability and inference efficiency. Starting from the continuous mapping system, SSMs are discretized to process discrete inputs in deep learning such as language sequence. The discretized SSMs can be computed in both linear recurrence and global convolution modes. Commonly, convolution mode is used during training to achieve parallel acceleration, while recurrence mode is used during autoregressive inference because it has lower computational complexity. The core distinction of SSMs from other neural networks, such as fully-connected neural networks, lies in the design of hidden states. Hidden states enable information to be propagated along the temporal dimension, while avoiding the computation complexity of accessing historical tokens at each step. Through state transition parameters $A$, hidden states transfer the hidden information from the previous time\nsteps to the current time step, allowing for autoregressive prediction of the next token. Hidden states play a crucial role in SSMs, but have not received sufficient investigation in the past. Weights and hidden features in different layers contain information at various levels from fine-grained to coarsegrained (Gu et al., 2021). However, in previous versions of SSMs, hidden states only flowed within the current layer and could not transmit more information to deeper layers, thus failing to capture more hierarchical information. In this paper, we propose DenseSSM to facilitate a more comprehensive flow of hidden information between layers in state space models. We first analyze the hidden state degradation in conventional SSMs which will prevent hidden information flow from low levels to high levels. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information that is useful for the final output. The proposed method is applicable to different types of SSMs, such as RetNet (Sun et al., 2023) and Mamba (Gu \\& Dao, 2023). Our approach maintains the training parallelizability and inference efficiency of SSMs, while achieving a significant improvement with only a slight increase in the number of parameters. For instance, our DenseRetNet model outperforms traditional RetNet with up to 5\\% accuracy improvement on public benchmarks.\n```\n\n#### 3. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 1.00)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 1.00)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n#### 4. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 1.00)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 1.00)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"selective state space models\", \"hybrid attention mechanisms\", \"memory efficient training\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: \"selective state space models\"*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n#### 2. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: \"selective state space models\"*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: \"selective state space models\"*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 4. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: \"hybrid attention mechanisms\"*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios\n\n*From Search Query: \"hybrid attention mechanisms\"*\n\n*Zhongzhan Huang, Mingfu Liang, Shan Zhong, Liang Lin*\n\n**Abstract:** We propose the attention-inspired numerical solver (AttNS), a concise method that helps the generalization and robustness issues faced by the AI-Hybrid numerical solver in solving differential equations due to limited data. AttNS is inspired by the effectiveness of attention modules in Residual Neural Networks (ResNet) in enhancing model generalization and robustness for conventional deep learning tasks. Drawing from the dynamical system perspective of ResNet, We seamlessly incorporate attention mechanisms into the design of numerical methods tailored for the characteristics of solving differential equations. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, show-case AttNS consistently enhancing various numerical solvers without any intricate model crafting. Finally, we analyze AttNS experimentally and theoretically, demonstrating its ability to achieve strong generalization and robustness while ensuring the convergence of the solver. This includes requiring less data compared to other advanced methods to achieve comparable generalization errors and better prevention of numerical explosion issues when solving differential equations.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Learning with Auxiliary Activation for Memory-Efficient Training\n\n*From Search Query: \"memory efficient training\"*\n\n*Sunghyeon Woo, Dongsuk Jeon*\n\n**TL;DR:** This work proposes a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation, and combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation.\n\n**Abstract:** While deep learning has achieved great success in various fields, a large amount of memory is necessary to train deep neural networks, which hinders the development of massive state-of-the-art models. The reason is the conventional learning rule, backpropagation, should temporarily store input activations of all the layers in the network. To overcome this, recent studies suggested various memory-efficient implementations of backpropagation. However, those approaches incur computational overhead due to the recomputation of activations, slowing down neural network training. In this work, we propose a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation. The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored. We mathematically show that our learning rule can reliably train the networks if the auxiliary activation satisfies certain conditions. Based on this observation, we suggest candidates of auxiliary activation that satisfy those conditions. Experimental results confirm that the proposed learning rule achieves competitive performance compared to backpropagation in various models such as ResNet, Transformer, BERT, ViT, and MLP-Mixer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 7. Softmax Output Approximation for Activation Memory-Efficient Training of Attention-based Networks\n\n*From Search Query: \"memory efficient training\"*\n\n*Changhyeon Lee, Seulki Lee*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. DIVISION: Memory Efficient Training via Dual Activation Precision\n\n*From Search Query: \"memory efficient training\"*\n\n*Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Nannan Zou, Xia Hu*\n\n**TL;DR:** DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision during the training, which can significantly reduce the memory cost without negatively affecting the precision of backward propagation.\n\n**Abstract:** Activation compressed training provides a solution towards reducing the memory cost of training deep neural networks~(DNNs). However, state-of-the-art work combines a search of quantization bit-width with the training, which makes the procedure complicated and less transparent. To this end, we propose a simple and effective method to compress DNN training. Our method is motivated by an instructive observation: DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps, while the majority of memory is for caching the high-frequency component (HFC) during the training. This indicates the HFC of activation maps is highly redundant and compressible during DNN training, which inspires our proposed Dual Activation Precision (DIVISION). During the training, DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision. This can significantly reduce the memory cost without negatively affecting the precision of backward propagation such that DIVISION maintains competitive model accuracy. Experiment results show DIVISION has better comprehensive performance than state-of-the-art methods, including over 10x compression of activation maps and competitive training throughput, without loss of model accuracy.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 2  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling\n\n*From Search Query: \"selective state space models\"*\n\n*Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11T17:50:51Z  (*Updated: 2024-06-11T17:50:51Z*)\n\n\n\n#### 2. MambaByte: Token-free Selective State Space Model\n\n*From Search Query: \"selective state space models\"*\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.\n\n**Published:** 2024-01-24T18:53:53Z  (*Updated: 2024-08-09T20:18:57Z*)\n\n\n\n#### 3. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: \"hybrid attention mechanisms\"*\n\n*Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, ChengXiang Zhai*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with\nself-attention mechanisms have demonstrated impressive results across a range\nof sequence modeling tasks. However, current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. To address this limitation, we\nintroduce Sparse Modular Activation (SMA), a general mechanism enabling neural\nnetworks to sparsely and dynamically activate sub-modules for sequence elements\nin a differentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption of neural networks\nat both training and inference stages. To validate the effectiveness of SMA on\nsequence modeling, we design a novel neural architecture, SeqBoat, which\nemploys SMA to sparsely activate a Gated Attention Unit (GAU) based on the\nstate representations learned from an SSM. By constraining the GAU to only\nconduct local attention on the activated inputs, SeqBoat can achieve linear\ninference complexity with theoretically infinite attention span, and provide\nsubstantially better quality-efficiency trade-off than the chunking-based\nmodels. With experiments on a wide range of tasks, including long sequence\nmodeling, speech classification and language modeling, SeqBoat brings new\nstate-of-the-art results among hybrid models with linear complexity, and\nreveals the amount of attention needed for each task through the learned sparse\nactivation patterns. Our code is publicly available at\nhttps://github.com/renll/SeqBoat.\n\n**Published:** 2023-06-19T23:10:02Z  (*Updated: 2023-11-04T21:26:03Z*)\n\n\n\n#### 4. Improving Natural Language Processing Tasks with Human Gaze-Guided\n  Neural Attention\n\n*From Search Query: \"hybrid attention mechanisms\"*\n\n*Ekta Sood, Simon Tannert, Philipp Mueller, Andreas Bulling*\n\n**Abstract:** A lack of corpora has so far limited advances in integrating human gaze data\nas a supervisory signal in neural attention mechanisms for natural language\nprocessing(NLP). We propose a novel hybrid text saliency model(TSM) that, for\nthe first time, combines a cognitive model of reading with explicit human gaze\nsupervision in a single machine learning framework. On four different corpora\nwe demonstrate that our hybrid TSM duration predictions are highly correlated\nwith human gaze ground truth. We further propose a novel joint modeling\napproach to integrate TSM predictions into the attention layer of a network\ndesigned for a specific upstream NLP task without the need for any\ntask-specific human gaze data. We demonstrate that our joint model outperforms\nthe state of the art in paraphrase generation on the Quora Question Pairs\ncorpus by more than 10% in BLEU-4 and achieves state of the art performance for\nsentence compression on the challenging Google Sentence Compression corpus. As\nsuch, our work introduces a practical approach for bridging between data-driven\nand cognitive models and demonstrates a new way to integrate human gaze-guided\nneural attention into NLP tasks.\n\n**Published:** 2020-10-15T17:14:09Z  (*Updated: 2020-10-27T16:16:18Z*)\n\n\n\n#### 5. Task-oriented Memory-efficient Pruning-Adapter\n\n*From Search Query: \"memory efficient training\"*\n\n*Guorun Wang, Jun Yang, Yaoru Sun*\n\n**Abstract:** The Outstanding performance and growing size of Large Language Models has led\nto increased attention in parameter efficient learning. The two predominant\napproaches are Adapters and Pruning. Adapters are to freeze the model and give\nit a new weight matrix on the side, which can significantly reduce the time and\nmemory of training, but the cost is that the evaluation and testing will\nincrease the time and memory consumption. Pruning is to cut off some weight and\nre-distribute the remaining weight, which sacrifices the complexity of training\nat the cost of extremely high memory and training time, making the cost of\nevaluation and testing relatively low. So efficiency of training and inference\ncan't be obtained in the same time. In this work, we propose a task-oriented\nPruning-Adapter method that achieve a high memory efficiency of training and\nmemory, and speeds up training time and ensures no significant decrease in\naccuracy in GLUE tasks, achieving training and inference efficiency at the same\ntime.\n\n**Published:** 2023-03-26T12:18:00Z  (*Updated: 2023-04-06T03:44:38Z*)\n\n\n\n#### 6. CompAct: Compressed Activations for Memory-Efficient LLM Training\n\n*From Search Query: \"memory efficient training\"*\n\n*Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*\n\n**Abstract:** We introduce CompAct, a technique that reduces peak memory utilization on GPU\nby 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory\nis a major limiting factor in training LLMs, with various recent works aiming\nto reduce model memory. However most works don't target the largest component\nof allocated memory during training: the model's compute graph, which is stored\nfor the backward pass. By storing low-rank, compressed activations to be used\nin the backward pass we greatly reduce the required memory, unlike previous\nmethods which only reduce optimizer overheads or the number of trained\nparameters. Our compression uses random projection matrices, thus avoiding\nadditional memory overheads. Comparisons with previous techniques for either\npretraining or fine-tuning show that CompAct substantially improves existing\ncompute-performance tradeoffs. We expect CompAct's savings to scale even higher\nfor larger models.\n\n**Published:** 2024-10-20T10:24:38Z  (*Updated: 2024-10-20T10:24:38Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: \"selective state space models\"*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model for Semantic Segmentation of Remote Sensing Imagery\n\n*From Search Query: \"selective state space models\"*\n\n*Danfeng Hong, Xiaokang Zhang, Xiaoliang Meng, Sijun Dong, Dongxu Li, Libo Wang*\n\n**Abstract:** Semantic segmentation, as a basic tool for intelligent interpretation of remote sensing images, plays a vital role in many Earth Observation (EO) applications. Nowadays, accurate semantic segmentation of remote sensing images remains a challenge due to the complex spatial-temporal scenes and multi-scale geo-objects. Driven by the wave of deep learning (DL), CNN- and Transformer-based semantic segmentation methods have been explored widely, and these two architectures both revealed the importance of multi-scale feature representation for strengthening semantic information of geo-objects. However, the actual multi-scale feature fusion often comes with the semantic redundancy issue due to homogeneous semantic contents in pyramid features. To handle this issue, we propose a novel Mamba-based segmentation network, namely PyramidMamba. Specifically, we design a plug-and-play decoder, which develops a dense spatial pyramid pooling (DSPP) to encode rich multi-scale semantic features and a pyramid fusion Mamba (PFM) to reduce semantic redundancy in multi-scale feature fusion. Comprehensive ablation experiments illustrate the effectiveness and superiority of the proposed method in enhancing multi-scale feature representation as well as the great potential for real-time semantic segmentation. Moreover, our PyramidMamba yields state-of-the-art performance on three publicly available datasets, i.e. the OpenEarthMap (70.8% mIoU), ISPRS Vaihingen (84.8% mIoU) and Potsdam (88.0% mIoU) datasets. The code will be available at https://github.com/WangLibo1995/GeoSeg.\n\n**Published:** 2024-06-16\n\n\n\n#### 3. Hybrid intelligence for dynamic job-shop scheduling with deep reinforcement learning and attention mechanism\n\n*From Search Query: \"hybrid attention mechanisms\"*\n\n*Bo Yuan, Xiu Li, Rong Wang, Yuanzhi Dai, Zijun Liao, Yunhui Zeng*\n\n**Abstract:** The dynamic job-shop scheduling problem (DJSP) is a class of scheduling tasks that specifically consider the inherent uncertainties such as changing order requirements and possible machine breakdown in realistic smart manufacturing settings. Since traditional methods cannot dynamically generate effective scheduling strategies in face of the disturbance of environments, we formulate the DJSP as a Markov decision process (MDP) to be tackled by reinforcement learning (RL). For this purpose, we propose a flexible hybrid framework that takes disjunctive graphs as states and a set of general dispatching rules as the action space with minimum prior domain knowledge. The attention mechanism is used as the graph representation learning (GRL) module for the feature extraction of states, and the double dueling deep Q-network with prioritized replay and noisy networks (D3QPN) is employed to map each state to the most appropriate dispatching rule. Furthermore, we present Gymjsp, a public benchmark based on the well-known OR-Library, to provide a standardized off-the-shelf facility for RL and DJSP research communities. Comprehensive experiments on various DJSP instances confirm that our proposed framework is superior to baseline algorithms with smaller makespan across all instances and provide empirical justification for the validity of the various components in the hybrid framework.\n\n**Published:** 2022-01-03\n\n\n\n#### 4. A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization\n\n*From Search Query: \"hybrid attention mechanisms\"*\n\n*Richard Radke, Chengjiang Long, Ashraful Islam*\n\n**Abstract:** Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an \"action-ness\" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.\n\n**Published:** 2021-01-03\n\n\n\n#### 5. GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection\n\n*From Search Query: \"memory efficient training\"*\n\n*Yuandong Tian, Anima Anandkumar, Zhangyang Wang, Beidi Chen, Zhenyu Zhang, Jiawei Zhao*\n\n**Abstract:** Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies.\n\n**Published:** 2024-03-06\n\n\n\n#### 6. H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training\n\n*From Search Query: \"memory efficient training\"*\n\n*Anonymous*\n\n**Abstract:** In recent years, big models based on Transformers have achieved state-of-the-art performance on many artificial intelligence (AI) tasks.\nDespite the success of these Transformer-based models, their huge parameter size poses a serious challenge to their training, both from the storage and computation perspectives.\nTo this end, memory optimization (e.g., rematerialization and offloading) and parallelism (e.g., data parallelism and model parallelism) are widely explored to make training Transformers more efficient.\nIn this paper, we propose a framework to automatically find an efficient integration of memory optimization and parallelism for High-Throughput Transformer Training (named H3T), which is rarely considered by existing efforts for training big Transformer-based models.\nSpecifically, we design search algorithms to combine appropriate memory optimization strategies and parallelism schemes to achieve a balance between memory overhead and training efficiency.\nWe implement H3T based on an open-source toolkit BMTrain and then use H3T to train the Transformers of different sizes to evaluate the efficiency of H3T.\nThe experimental results show that H3T outperforms the most popular deep learning (DL) toolkit Megatron-DeepSpeed by $1.2\\times \\sim 4.3\\times$ training speed while reducing $34.6\\% \\sim 80.5\\%$ of memory overhead.\nMoreover, H3T can use only 64 NVIDIA A100 GPUs to train GPT-3-175B, which is very difficult for existing DL toolkits. The source code is available at https://github.com/OpenBMB/BMTrain/tree/h3t.Submission Number: 3012\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-09-21\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using selective state space models (SSMs), hybrid architectures, and memory-efficient training strategies, here are some key findings and references that align with your goals:\n\n## Hybrid Architectures Combining SSMs and Attention Mechanisms\n\n- The paper \"Taipan: Efficient and Expressive State Space Language Models with Selective Attention\" introduces a novel hybrid architecture that combines Mamba-2 (a type of SSM) with Selective Attention Layers (SALs). This approach balances the efficiency of Mamba with the performance of Transformers in memory-intensive tasks, allowing for accurate predictions up to 1 million tokens while maintaining computational efficiency.\n\n## Selective State Space Models in Language Modeling\n\n- The \"Mamba\" paper by Albert Gu and Tri Dao presents a selective state space model that addresses the computational inefficiency of Transformer-based models on long sequences. Mamba uses selective state spaces within a simplified neural network architecture to improve content-based reasoning and achieves linear scaling in sequence length, outperforming similarly sized Transformers and matching larger ones in pretraining and downstream tasks.\n\n## Memory-Efficient Training Strategies\n\n- The \"Quamba\" papers discuss post-training quantization methods for SSMs, which are crucial for memory-efficient training and deployment. Quamba introduces an 8-bit per-tensor quantization method that reduces model size and utilizes low bit-width acceleration features, resulting in lower generation latency and minimal accuracy drop. This approach is particularly beneficial for deploying SSM-based models on both cloud and edge platforms.\n\n## Integration and Efficiency\n\n- State Space Models (SSMs) are highlighted for their ability to capture long-range dependencies and process data efficiently, making them suitable for natural language processing tasks. The integration of SSMs with attention mechanisms, as seen in Taipan, enhances their performance in tasks requiring extensive in-context retrieval while maintaining linear complexity and context-aware selectivity.\n\n## Key Insights and Future Directions\n\n- The analysis note emphasizes the importance of hierarchical processing, multi-scale feature representation, and careful memory management. The Taipan and Mamba models address these by using selective state spaces and hybrid architectures, which balance local and global processing and optimize memory usage.\n- Future research should focus on refining these hybrid architectures, optimizing memory management, and exploring dynamic adaptation techniques to enhance the robustness and scalability of these models.\n\nBy combining these insights, researchers can develop more efficient, scalable, and adaptive autoregressive language models that leverage the strengths of both SSMs and attention mechanisms.\n",
            "analysis": "Analysis Note for AdaptiveHybridSSM Proposal:\n\n1. Summary of Analysis:\nThe initial search reveals several key aspects about the proposal's components:\n- State Space Models (SSMs) are emerging as a promising alternative to traditional attention mechanisms\n- Hierarchical processing in language models shows potential for improved efficiency\n- Memory optimization techniques are crucial for practical implementation\n- The proposal's integration of these components appears novel compared to existing approaches\n\n2. Useful References with Excerpts:\n\nFrom \"Understanding the differences in Foundation Models\":\n\"While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability.\"\n\nFrom \"Mamba-FSCIL\":\n\"We propose a dual selective SSM projector that dynamically adjusts the projection parameters based on the intermediate features for dynamic adaptation. The dual design enables the model to maintain the robust features of base classes, while adaptively learning distinctive feature shifts.\"\n\nFrom \"Efficient Memory Management for LLM Serving\":\n\"When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size.\"\n\n3. Key Insights:\n\na) State Space Models:\n- SSMs offer linear-time sequence modeling capabilities\n- Selective state spaces can provide adaptive behavior\n- Integration with traditional attention mechanisms needs careful consideration\n\nb) Hierarchical Processing:\n- Multi-scale feature representation is crucial\n- Balance needed between local and global processing\n- Memory efficiency depends on hierarchical structure design\n\nc) Memory Optimization:\n- Current approaches struggle with memory fragmentation\n- Dynamic adaptation requires efficient memory management\n- Trade-offs between adaptivity and memory usage need consideration\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of selective state spaces in language models\n2. Techniques for combining SSMs with attention mechanisms\n3. Detailed memory optimization strategies for adaptive models\n\nNext search queries should focus on:\n- Implementation details of selective state spaces\n- Hybrid architectures combining attention and SSMs\n- Memory-efficient training strategies\n\n5. References:\n\n[1] Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv:2402.19427.\n\n[2] Zhang, M., Nie, L., Ghanem, B., Wu, J., Yang, Y., & Li, X. (2024). Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class-Incremental Learning.\n\n[3] Stoica, I., Zhang, H., Gonzalez, J. E., Yu, C. H., Zheng, L., Sheng, Y., ... & Kwon, W. (2023). Efficient Memory Management for Large Language Model Serving with PagedAttention.\n\n[4] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\n\n[5] Fu, D. Y., et al. (2023). Hungry Hungry Hippos: Towards Language Modeling with State Space Models."
        },
        {
            "ready": false,
            "query": "\"selective compression mechanisms\", \"hardware optimization SSM\", \"training stability hybrid models\"",
            "detail": "Find papers discussing implementation details of selective compression mechanisms in SSMs and hybrid models, focusing on hardware optimization techniques, training stability, and convergence properties. Include information about specific architectures, optimization strategies, and performance metrics.",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing implementation details of selective compression mechanisms in SSMs and hybrid models, focusing on hardware optimization techniques, training stability, and convergence properties. Include information about specific architectures, optimization strategies, and performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.90)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.98)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.82)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n##### *Relevant Chunk: No. 57/74 (Score: 0.80)*\n\n```\n2019. [113] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. \"An Attention Free Transformer\". In: arXiv preprint arXiv:2105.14103 (2021). [114] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \"Effectively Modeling Time Series with Simple Discrete State Spaces\". In: The International Conference on Learning Representations (ICLR). 2023. [115] Lin Zheng, Chong Wang, and Lingpeng Kong. \"Linear complexity randomized self-attention mechanism\". In: International Conference on Machine Learning. PMLR. 2022, pp. 27011-27041. [116] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \"Efficient Long Sequence Modeling via State Space Augmented Transformer\". In: arXiv preprint arXiv:2212.08136 (2022). ## A Discussion: Selection Mechanism\n\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It can also be viewed as related to \"fast weights\" (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs with the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept that is worth clarifying. Gating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal through time and causes inputs to interact along the sequence length dimension. However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function). For example, elementwise multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta et al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of $R N N$ gating versus the popular usage of multiplicative gating actually have a very different semantic meaning. Hypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose recurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber 1992). Data-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data (Poli et al. 2023). Example: GLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear layer $y=D x$, where $D$ is a diagonal weight parameter. Now suppose that $D$ is itself generated from a linear transformation of $x$, with an optional nonlinearity: $D=\\sigma(\\boldsymbol{W} x)$. Since it is diagonal, the multiplication becomes an elementwise product: $y=\\sigma(W x) \\circ x$. This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative \"branch\"), hypernetworks (since the parameter $\\boldsymbol{D}$ is generated by another layer), and data-dependent (since $\\boldsymbol{D}$ depends on the data $x$ ). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer. Selection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructions-essentially anything with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as well-and we find it uninformative to think of them as such. Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of $\\Delta$ (Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term \"gating\" in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention. ## B Related Work\n\nWe overview several prior works related to our methods. We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV. ## B. 1 S4 Variants and Derivatives\n\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method. - S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020). - DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022). - S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the first S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO dimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism. Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where $\\bar{A}$ is manually set to 0 , instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries. - Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show that real-valued SSMs are empirically effective in certain settings or when combined with different architectural components. - Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI. - SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of S4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do fast autoregressive inference directly. Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant). ## B. 2 SSM Architectures\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer. - GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most importantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1. - Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient attention approximation. - H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures. - Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the input. While sharing the \"selection\" name, we consider this an architectural modification that is closer to architectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\nCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0 ).\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.83)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.83)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"selective compression mechanisms\", \"hardware optimization SSM\", \"training stability hybrid models\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Selective compression learning of latent representations for variable-rate image compression\n\n*From Search Query: \"selective compression mechanisms\"*\n\n*Jooyoung Lee, S. Jeong, Munchurl Kim*\n\n**TL;DR:** A selective compression method that partially encodes the latent representations in a fully generalized manner for deep learning-based variable-rate image compression and can achieve comparable compression efficiency as those of the separately trained reference compression models and can reduce decoding time.\n\n**Abstract:** Recently, many neural network-based image compression methods have shown promising results superior to the existing tool-based conventional codecs. However, most of them are often trained as separate models for different target bit rates, thus increasing the model complexity. Therefore, several studies have been conducted for learned compression that supports variable rates with single models, but they require additional network modules, layers, or inputs that often lead to complexity overhead, or do not provide sufficient coding efficiency. In this paper, we firstly propose a selective compression method that partially encodes the latent representations in a fully generalized manner for deep learning-based variable-rate image compression. The proposed method adaptively determines essential representation elements for compression of different target quality levels. For this, we first generate a 3D importance map as the nature of input content to represent the underlying importance of the representation elements. The 3D importance map is then adjusted for different target quality levels using importance adjustment curves. The adjusted 3D importance map is finally converted into a 3D binary mask to determine the essential representation elements for compression. The proposed method can be easily integrated with the existing compression models with a negligible amount of overhead increase. Our method can also enable continuously variable-rate compression via simple interpolation of the importance adjustment curves among different quality levels. The extensive experimental results show that the proposed method can achieve comparable compression efficiency as those of the separately trained reference compression models and can reduce decoding time owing to the selective compression. The sample codes are publicly available at https://github.com/JooyoungLeeETRI/SCR.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 2. EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization\n\n*From Search Query: \"selective compression mechanisms\"*\n\n*Laurent Condat, Kai Yi, Peter Richt'arik*\n\n**TL;DR:** This paper unifies two classes of compression operators into a single framework and proposes a new algorithm, recovering DIANA and EF21 as particular cases, and proves its linear convergence under certain conditions.\n\n**Abstract:** In distributed or federated optimization and learning, communication between the different computing units is often the bottleneck and gradient compression is widely used to reduce the number of bits sent within each communication round of iterative methods. There are two classes of compression operators and separate algorithms making use of them. In the case of unbiased random compressors with bounded variance (e.g., rand-k), the DIANA algorithm of Mishchenko et al. (2019), which implements a variance reduction technique for handling the variance introduced by compression, is the current state of the art. In the case of biased and contractive compressors (e.g., top-k), the EF21 algorithm of Richt\\'arik et al. (2021), which instead implements an error-feedback mechanism, is the current state of the art. These two classes of compression schemes and algorithms are distinct, with different analyses and proof techniques. In this paper, we unify them into a single framework and propose a new algorithm, recovering DIANA and EF21 as particular cases. Our general approach works with a new, larger class of compressors, which has two parameters, the bias and the variance, and includes unbiased and biased compressors as particular cases. This allows us to inherit the best of the two worlds: like EF21 and unlike DIANA, biased compressors, like top-k, whose good performance in practice is recognized, can be used. And like DIANA and unlike EF21, independent randomness at the compressors allows to mitigate the effects of compression, with the convergence rate improving when the number of parallel workers is large. This is the first time that an algorithm with all these features is proposed. We prove its linear convergence under certain conditions. Our approach takes a step towards better understanding of two so-far distinct worlds of communication-efficient distributed learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 17  (*Influential: 2*)\n\n#### 3. RECOMP: Improving Retrieval-Augmented LMs with Context Compression and Selective Augmentation\n\n*From Search Query: \"selective compression mechanisms\"*\n\n*Fangyuan Xu, Weijia Shi, Eunsol Choi*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 40  (*Influential: 7*)\n\n#### 4. Data-Driven Offline Optimization For Architecting Hardware Accelerators\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Aviral Kumar, A. Yazdanbakhsh, Milad Hashemi, Kevin Swersky, S. Levine*\n\n**TL;DR:** This paper develops a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization.\n\n**Abstract:** Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a\"simulation-driven\"approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a\"data-driven\", offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators -- tailored towards both single and multiple applications -- improving performance upon state-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 5. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 6. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 70  (*Influential: 13*)\n\n#### 7. Message Passing Neural PDE Solvers\n\n*From Search Query: \"training stability hybrid models\"*\n\n*Johannes Brandstetter, Daniel E. Worrall, M. Welling*\n\n**TL;DR:** This work builds a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators.\n\n**Abstract:** The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers, which piggy-backs the modern trend towards fully end-to-end learned systems. Most works so far can only generalize over a subset of properties to which a generic solver would be faced, including: resolution, topology, geometry, boundary conditions, domain discretization regularity, dimensionality, etc. In this work, we build a solver, satisfying these properties, where all the components are based on neural message passing, replacing all heuristically designed components in the computation graph with backprop-optimized neural function approximators. We show that neural message passing solvers representationally contain some classical methods, such as finite differences, finite volumes, and WENO schemes. In order to encourage stability in training autoregressive models, we put forward a method that is based on the principle of zero-stability, posing stability as a domain adaptation problem. We validate our method on various fluid-like flow problems, demonstrating fast, stable, and accurate performance across different domain topologies, equation parameters, discretizations, etc., in 1D and 2D.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 219  (*Influential: 45*)\n\n#### 8. The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models\n\n*From Search Query: \"training stability hybrid models\"*\n\n*Conglong Li, Minjia Zhang, Yuxiong He*\n\n**TL;DR:** An in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model finds that there is a strong correlation between training instability and extreme values of gradient variance, and that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training.\n\n**Abstract:** Recent works have demonstrated great success in pre-training large-scale autoregressive language models on massive GPUs. To reduce the wall-clock training time, a common practice is to increase the batch size and learning rate. However, such practice is often brittle and leads to a so-called stability-efficiency dilemma: increasing the batch sizes and learning rates leads to better training efficiency but can also result in training instability, leading to poor generalization accuracy or failed runs. To better understand this phenomenon, we conduct an in-depth analysis on large-scale pre-training experiments replicating the GPT-2 model. We find that there is a strong correlation between training instability and extreme values of gradient variance, and that samples with long sequence lengths contribute to these extreme gradient variance values, especially at the beginning of the training, indicating that long sequence length can be a main source of training instability. Based on the analysis, we present a Sequence Length Warmup method that aims to solve the training stability-efficiency dilemma. Experiments replicating GPT-2 models show that our approach enables stable training with 8x larger batch size and 4x larger learning rate, whereas the baseline approach struggles with training instability. To achieve the same or better zero-shot evaluation results, our method reduces the required number of training tokens and wall clock time by up to 2.2x and 3.7x, respectively. Experiments replicating GPT-3 model (125M) show that our approach enables stable training with 8x larger batch size and 40x larger learning rate, and retains 99% of the zero-shot accuracy on 11 tasks using 10x less data and 17x less time compared to the original GPT-3 training recipe, while the baseline diverges under the same settings and only retain 95% of accuracy under lower learning rate.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 29  (*Influential: 1*)\n\n### 6 related papers from ArXiv\n\n#### 1. RazorAttention: Efficient KV Cache Compression Through Retrieval Heads\n\n*From Search Query: \"selective compression mechanisms\"*\n\n*Hanlin Tang, Yang Lin, Jing Lin, Qingsen Han, Shikuan Hong, Yiwu Yao, Gongyi Wang*\n\n**Abstract:** The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.\n\n**Published:** 2024-07-22T01:12:23Z  (*Updated: 2024-07-22T01:12:23Z*)\n\n\n\n#### 2. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling\n\n*From Search Query: \"selective compression mechanisms\"*\n\n*Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11T17:50:51Z  (*Updated: 2024-06-11T17:50:51Z*)\n\n\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n#### 4. Parameter-Efficient Fine-Tuning of State Space Models\n\n*From Search Query: \"hardware optimization SSM\"*\n\n*Kevin Galim, Wonjun Kang, Yuchen Zeng, Hyung Il Koo, Kangwook Lee*\n\n**Abstract:** Deep State Space Models (SSMs), such as Mamba (Gu & Dao, 2024), have emerged\nas powerful tools for language modeling, offering high performance with\nefficient inference and linear scaling in sequence length. However, the\napplication of parameter-efficient fine-tuning (PEFT) methods to SSM-based\nmodels remains largely unexplored. This paper aims to systematically study two\nkey questions: (i) How do existing PEFT methods perform on SSM-based models?\n(ii) Which modules are most effective for fine-tuning? We conduct an empirical\nbenchmark of four basic PEFT methods on SSM-based models. Our findings reveal\nthat prompt-based methods (e.g., prefix-tuning) are no longer effective, an\nempirical result further supported by theoretical analysis. In contrast, LoRA\nremains effective for SSM-based models. We further investigate the optimal\napplication of LoRA within these models, demonstrating both theoretically and\nexperimentally that applying LoRA to linear projection matrices without\nmodifying SSM modules yields the best results, as LoRA is not effective at\ntuning SSM modules. To further improve performance, we introduce LoRA with\nSelective Dimension tuning (SDLoRA), which selectively updates certain channels\nand states on SSM modules while applying LoRA to linear projection matrices.\nExtensive experimental results show that this approach outperforms standard\nLoRA.\n\n**Published:** 2024-10-11T17:30:28Z  (*Updated: 2024-10-11T17:30:28Z*)\n\n\n\n#### 5. SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced\n  Token Detection\n\n*From Search Query: \"training stability hybrid models\"*\n\n*Ke Ye, Heinrich Jiang, Afshin Rostamizadeh, Ayan Chakrabarti, Giulia DeSalvo, Jean-Fran\u00e7ois Kagy, Lazaros Karydas, Gui Citovsky, Sanjiv Kumar*\n\n**Abstract:** Pre-training large language models is known to be extremely resource\nintensive and often times inefficient, under-utilizing the information\nencapsulated in the training text sequences. In this paper, we present SpacTor,\na new training procedure consisting of (1) a hybrid objective combining span\ncorruption (SC) and token replacement detection (RTD), and (2) a two-stage\ncurriculum that optimizes the hybrid objective over the initial $\\tau$\niterations, then transitions to standard SC loss. We show empirically that the\neffectiveness of the hybrid objective is tied to the two-stage pre-training\nschedule, and provide extensive analysis on why this is the case. In our\nexperiments with encoder-decoder architectures (T5) on a variety of NLP tasks,\nSpacTor-T5 yields the same downstream performance as standard SC pre-training,\nwhile enabling a 50% reduction in pre-training iterations and 40% reduction in\ntotal FLOPs. Alternatively, given the same amount of computing budget, we find\nthat SpacTor results in significantly improved downstream benchmark\nperformance.\n\n**Published:** 2024-01-24T00:36:13Z  (*Updated: 2024-01-24T00:36:13Z*)\n\n\n\n#### 6. LadaBERT: Lightweight Adaptation of BERT through Hybrid Model\n  Compression\n\n*From Search Query: \"training stability hybrid models\"*\n\n*Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai Tong, Jing Bai*\n\n**Abstract:** BERT is a cutting-edge language representation model pre-trained by a large\ncorpus, which achieves superior performances on various natural language\nunderstanding tasks. However, a major blocking issue of applying BERT to online\nservices is that it is memory-intensive and leads to unsatisfactory latency of\nuser requests, raising the necessity of model compression. Existing solutions\nleverage the knowledge distillation framework to learn a smaller model that\nimitates the behaviors of BERT. However, the training procedure of knowledge\ndistillation is expensive itself as it requires sufficient training data to\nimitate the teacher model. In this paper, we address this issue by proposing a\nhybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid\nmodel compression), which combines the advantages of different model\ncompression methods, including weight pruning, matrix factorization and\nknowledge distillation. LadaBERT achieves state-of-the-art accuracy on various\npublic datasets while the training overheads can be reduced by an order of\nmagnitude.\n\n**Published:** 2020-04-08T17:18:56Z  (*Updated: 2020-10-21T15:15:11Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid\n\n*From Search Query: \"selective compression mechanisms\"*\n\n*Xiang Bai, Lianwen Jin, Dingkang Liang, Yuliang Liu, Mingxin Huang*\n\n**Abstract:** Recently, scaling images to high resolution has received much attention in multimodal large language models (MLLMs). Most existing practices adopt a sliding-window-style cropping strategy to adapt to resolution increase. Such a cropping strategy, however, can easily cut off objects and connected regions, which introduces semantic discontinuity and therefore impedes MLLMs from recognizing small or irregularly shaped objects or text, leading to a phenomenon we call the semantic sawtooth effect. This effect is particularly evident in lightweight MLLMs. To address this issue, we introduce a Complementary Image Pyramid (CIP), a simple, effective, and plug-and-play solution designed to mitigate semantic discontinuity during high-resolution image processing. In particular, CIP dynamically constructs an image pyramid to provide complementary semantic information for the cropping-based MLLMs, enabling them to richly acquire semantics at all levels. Furthermore, we introduce a Scale Compression Mechanism (SCM) to reduce the additional computational overhead by compressing the redundant visual tokens. Our experiments demonstrate that CIP can consistently enhance the performance across diverse architectures (e.g., MiniCPM-V-2, InternVL2, and LLaVA-OneVision), various model capacity (1B$\\rightarrow$8B), and different usage configurations (training-free and fine-tuning). Leveraging the proposed CIP and SCM, we introduce a lightweight MLLM, Mini-Monkey, which achieves remarkable performance in both general multimodal understanding and document understanding. On the OCRBench, the 2B-version Mini-Monkey even surpasses the 8B model InternVL2-8B by 12 score. Additionally, training Mini-Monkey is cheap, requiring only eight RTX 3090 GPUs. The code is available at https://github.com/Yuliang-Liu/Monkey.\n\n**Published:** 2024-08-04\n\n\n\n#### 2. Stable and low-precision training for large-scale vision-language models\n\n*From Search Query: \"training stability hybrid models\"*\n\n*Ludwig Schmidt, Ali Farhadi, Ari Morcos, Luke Zettlemoyer, Tim Dettmers, Mitchell Wortsman*\n\n**Abstract:** We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-04-25\n\n\n\n#### 3. Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated Robot Response\n\n*From Search Query: \"training stability hybrid models\"*\n\n*Jiangmiao Pang, Liu Cao, Jiawei Gao, Quanyi Li, ZiRui Wang, Junfeng Long*\n\n**Abstract:** Robust locomotion control depends on accurate state estimations. However, the sensors of most legged robots can only provide partial and noisy observations, making the estimation particularly challenging, especially for external states like terrain frictions and elevation maps. Inspired by the classical Internal Model Control principle, we consider these external states as disturbances and introduce Hybrid Internal Model (HIM) to estimate them according to the response of the robot. The response, which we refer to as the hybrid internal embedding, contains the robot's explicit velocity and implicit stability representation, corresponding to two primary goals for locomotion tasks: explicitly tracking velocity and implicitly maintaining stability. We use contrastive learning to optimize the embedding to be close to the robot's successor state, in which the response is naturally embedded. HIM has several appealing benefits: It only needs the robot's proprioceptions, i.e., those from joint encoders and IMU as observations. It innovatively maintains consistent observations between simulation reference and reality that avoids information loss in mimicking learning. It exploits batch-level information that is more robust to noises and keeps better sample efficiency. It only requires 1 hour of training on an RTX 4090 to enable a quadruped robot to traverse any terrain under any disturbances. A wealth of real-world experiments demonstrates its agility, even in high-difficulty tasks and cases never occurred during the training process, revealing remarkable open-world generalizability.\n\n**Published:** 2023-12-18\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on selective compression mechanisms, hardware optimization for State Space Models (SSMs), and training stability in hybrid models, here are some key findings and references:\n\n## Selective Compression Mechanisms in SSMs\n\n- The **Sparse Mamba (S-Mamba)** model introduces a selective state space model that reinforces controllability and observability, which are fundamental concepts in control theory. This model modifies the vanilla Mamba architecture to be in the controller canonical form and observable canonical form, leading to reduced parameters and faster training times. S-Mamba uses a high-order polynomial projection operator (HiPPO) for continuous-time memorization, which efficiently compresses the history of the input sequence into a fixed-size state space.\n- The **Mamba** model itself employs selective parameters to update its hidden state dynamically based on the current input. This is achieved through specialized trainable linear layers that map the input to matrices, allowing for efficient and flexible sequence modeling. The introduction of an input-dependent global gating module in Mamba further improves its performance on complex tasks by mitigating local pattern shortcuts.\n\n## Hardware Optimization Techniques for Hybrid Models\n\n- **Distributed Training and Memory Optimization**: For large-scale models, distributed training with hybrid data+model-parallelism is crucial. Innovations in algorithms for tensor contractions and tensor decompositions can minimize costs and improve scalability. Libraries like Cyclops provide distributed tensor contraction kernels, enabling efficient training on heterogeneous memory architectures such as DRAM and Intel Optane DC Persistent Memory Module (PMM).\n- **Efficient Parallel Processing**: The use of hierarchical parallel tensor infrastructure (HiParTI) can significantly speed up sparse tensor contractions, which are common in SSMs. HiParTI employs multi-dimensional efficient hashtable representations and all-stage parallelization, achieving substantial speedups over traditional methods.\n\n## Training Stability and Convergence Properties\n\n- **Global Selection Mechanism**: Introducing a global selection mechanism in Mamba models can improve training stability and convergence. This mechanism involves an input-dependent global gating module that helps in mitigating local pattern shortcuts and improving performance on synthetic tasks and language modeling.\n- **Optimization Strategies**: Advanced optimization techniques such as those discussed in the context of deep learning efficiency can be applied. Optimizers like Adam, Shampoo, and SNIP can help in stabilizing training and improving convergence. For example, Shampoo has been shown to converge quicker than other optimizers by storing preconditioners at varying levels of granularity.\n- **Memory Efficiency and State Management**: Models like LOCOST, which bypass self-attention mechanisms, achieve O(L log L) complexity and are up to 50% more memory-efficient during training. This highlights the importance of efficient state management and the trade-offs between compression and performance[Analysis Note].\n\n## Specific Architectures and Performance Metrics\n\n- **Hybrid Architectures**: Combining SSMs with attention mechanisms, as seen in models like Samba, can leverage the strengths of both architectures. Samba selectively compresses sequences into recurrent hidden states while maintaining the ability to recall memories with attention, substantially outperforming pure attention or SSM models[Analysis Note].\n- **Performance Metrics**: Evaluations on synthetic tasks such as Multi-Query Associative Recall (Mqar) and language modeling tasks provide insights into the performance of these models. Metrics include perplexity, accuracy on downstream tasks, and efficiency in training and inference.\n\nBy integrating these insights, researchers can design more efficient, stable, and scalable autoregressive language models that leverage the strengths of SSMs and hybrid architectures.\n",
            "analysis": "Analysis Note for AdaptiveHybridSSM Proposal - Round 2:\n\n1. Summary of Analysis:\nThe second round of searches reveals important insights about:\n- The relationship between SSMs and attention mechanisms\n- Implementation challenges and solutions for hybrid architectures\n- Memory optimization techniques for efficient training\n- State-of-the-art approaches in combining different architectural components\n\n2. Useful References with Key Excerpts:\n\nFrom \"Transformers are SSMs\":\n\"We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices.\"\n\nFrom \"Samba\":\n\"Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism... substantially outperforms the state-of-the-art models based on pure attention or SSMs.\"\n\nFrom \"The Expressive Capacity of State Space Models\":\n\"SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly.\"\n\nFrom \"LOCOST\":\n\"By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of O(L log L) instead of O(L\u00b2) as in traditional transformers... achieves 93-96% of the best performance while being up to 50% more memory-efficient during training.\"\n\n3. Key Insights:\n\na) Architectural Integration:\n- SSMs and attention mechanisms are theoretically connected\n- Hybrid approaches can leverage strengths of both architectures\n- Selective compression with memory recall shows promise\n- Need for careful balance between components\n\nb) Memory Efficiency:\n- O(L log L) complexity achievable with proper design\n- Memory savings of up to 50% during training possible\n- Trade-offs between compression and performance\n- Importance of efficient state management\n\nc) Implementation Considerations:\n- Hardware-aware algorithm design crucial\n- Need for efficient parallel processing\n- Balance between training and inference efficiency\n- Importance of selective state updates\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementation details of selective compression mechanisms\n2. Hardware optimization techniques for hybrid architectures\n3. Performance metrics and benchmarks for similar models\n4. Training stability and convergence properties\n\nNext search queries should focus on:\n- Implementation details of selective compression\n- Hardware optimization for hybrid models\n- Training stability and convergence\n\n5. References:\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. International Conference on Machine Learning.\n\n[2] Ren, L., Liu, Y., Lu, Y., Shen, Y., Liang, C., & Chen, W. (2024). Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arXiv preprint.\n\n[3] Sarrof, Y., Veitsman, Y., & Hahn, M. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv preprint.\n\n[4] Le Bronnec, F., et al. (2024). LOCOST: State-Space Models for Long Document Abstractive Summarization. Conference of the European Chapter of the Association for Computational Linguistics.\n\n[5] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection. arXiv preprint."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.747327,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.446718,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": null,
    "proposal": "# AdaptiveHybridSSM: Combining Adaptive State Space Models with Hierarchical Processing for Efficient Language Modeling\n\n## Motivation\n\nCurrent language model architectures face several key challenges:\n1. Efficiently processing long sequences while maintaining model expressiveness\n2. Managing computational and memory requirements for large-scale models\n3. Effectively combining local and global context processing\n4. Ensuring stable gradient flow during training\n\nThe proposed AdaptiveHybridSSM addresses these challenges by introducing:\n- Adaptive state space models with input-dependent parameters\n- Hierarchical local-global processing with selective compression\n- Efficient gradient computation through structured state spaces\n- Memory-optimized implementation with selective state updates\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **Traditional Attention Mechanisms:**\n   - Quadratic complexity in sequence length\n   - High memory requirements\n   - Limited ability to handle long-range dependencies efficiently\n\n2. **Pure State Space Models:**\n   - Fixed parameters limit expressiveness\n   - Difficulty in capturing content-based relationships\n   - Challenge in balancing efficiency and model capacity\n\n3. **Existing Hybrid Approaches:**\n   - Complex integration of different architectural components\n   - Inefficient memory usage\n   - Suboptimal gradient computation\n\n### Innovation Opportunities\n\n1. **Adaptive Processing:**\n   - Input-dependent parameter generation\n   - Dynamic compression based on content importance\n   - Flexible allocation of computational resources\n\n2. **Hierarchical Structure:**\n   - Multi-scale feature representation\n   - Efficient combination of local and global processing\n   - Structured state space for improved gradient flow\n\n3. **Memory Optimization:**\n   - Selective state updates\n   - Compressed memory representations\n   - Efficient gradient computation\n\n## Core Idea and Philosophy\n\nAdaptiveHybridSSM introduces a novel approach to language modeling by combining:\n\n1. **Adaptive State Space Models:**\n   - Input-dependent parameter generation for enhanced expressiveness\n   - Dynamic state compression based on content importance\n   - Efficient gradient computation through structured representations\n\n2. **Hierarchical Processing:**\n   - Multi-scale feature extraction\n   - Local-global context integration\n   - Selective information routing\n\n3. **Memory-Efficient Design:**\n   - Compressed state representations\n   - Selective update mechanisms\n   - Optimized gradient computation paths\n\n## Design Plan\n\n### 1. Adaptive State Space Layer\n\n```python\nclass AdaptiveStateSpace(nn.Module):\n    def __init__(self, dim, n_heads):\n        super().__init__()\n        self.dim = dim\n        self.n_heads = n_heads\n        self.param_generator = ParameterGenerator(dim)\n        self.state_compressor = StateCompressor(dim)\n        \n    def forward(self, x, state):\n        # Generate input-dependent parameters\n        A, B = self.param_generator(x)\n        \n        # Update state with adaptive parameters\n        new_state = self.update_state(x, state, A, B)\n        \n        # Compress state based on importance\n        compressed_state = self.state_compressor(new_state)\n        \n        return compressed_state\n```\n\n### 2. Hierarchical Processing Module\n\n```python\nclass HierarchicalProcessor(nn.Module):\n    def __init__(self, dim, levels):\n        super().__init__()\n        self.levels = levels\n        self.local_processors = nn.ModuleList([\n            LocalProcessor(dim) for _ in range(levels)\n        ])\n        self.global_processor = GlobalProcessor(dim)\n        \n    def forward(self, x):\n        # Multi-scale feature extraction\n        features = []\n        for level in range(self.levels):\n            local_features = self.local_processors[level](x)\n            features.append(local_features)\n            \n        # Global context integration\n        global_features = self.global_processor(features)\n        \n        return global_features\n```\n\n### 3. Memory-Efficient Implementation\n\n```python\nclass MemoryEfficientBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.state_space = AdaptiveStateSpace(config.dim)\n        self.hierarchical = HierarchicalProcessor(config.dim)\n        self.memory_manager = MemoryManager(config.dim)\n        \n    def forward(self, x, state):\n        # Selective state update\n        new_state = self.state_space(x, state)\n        \n        # Hierarchical processing\n        features = self.hierarchical(x)\n        \n        # Memory-efficient integration\n        output = self.memory_manager(features, new_state)\n        \n        return output, new_state\n```\n\n## Implementation Guidelines\n\n### 1. Parameter Generation\n\nThe adaptive parameter generation follows:\n\n```python\ndef generate_parameters(x):\n    # Input-dependent parameter computation\n    A = f_A(x)  # State transition matrix\n    B = f_B(x)  # Input projection matrix\n    \n    return A, B\n```\n\n### 2. State Update Mechanism\n\n```python\ndef update_state(x, state, A, B):\n    # Efficient state update with structured matrices\n    new_state = torch.bmm(A, state) + torch.bmm(B, x)\n    return new_state\n```\n\n### 3. Memory Management\n\n```python\ndef manage_memory(features, state):\n    # Compress state based on importance\n    importance = compute_importance(state)\n    compressed_state = compress_state(state, importance)\n    \n    # Efficient feature integration\n    output = integrate_features(features, compressed_state)\n    \n    return output\n```\n\n## Theoretical Analysis\n\n### 1. Computational Complexity\n\n- Time complexity: O(n log n) for sequence length n\n- Memory complexity: O(n) through selective compression\n- Gradient computation: O(n) through structured matrices\n\n### 2. Memory Efficiency\n\n- State compression ratio: r = k/n where k is compressed dimension\n- Memory reduction: (1 - r) * 100% compared to full state\n- Gradient memory: O(k) for backward pass\n\n### 3. Stability Analysis\n\n- Bounded state updates through normalized parameters\n- Stable gradient flow through hierarchical structure\n- Controlled information compression through importance scoring\n\n## Expected Benefits\n\n1. **Improved Efficiency:**\n   - Linear memory scaling with sequence length\n   - Efficient gradient computation\n   - Reduced computational overhead\n\n2. **Enhanced Performance:**\n   - Better handling of long-range dependencies\n   - Improved feature representation\n   - Adaptive processing capabilities\n\n3. **Practical Advantages:**\n   - Scalable to longer sequences\n   - Hardware-friendly implementation\n   - Stable training dynamics\n\n## References\n\n[1] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\"\n\n[2] Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\"\n\n[3] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\"\n\n[4] Ren, L., et al. (2024). \"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context.\"\n\n[5] Wang, J., et al. (2024). \"MambaByte: Token-free Selective State Space Model.\"\n\n[6] Hong, D., et al. (2024). \"PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model.\"\n\n[7] Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\"",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel language model block design that combines adaptive state space models with hierarchical local-global processing and efficient memory management through selective compression, enabling scalable processing of long sequences while maintaining computational efficiency.",
    "ideation": null,
    "modelname": "adaptivehybridssm",
    "suggestions": "1. Implementation Clarity:\n- Provide detailed initialization strategies for adaptive components\n- Include specific gradient flow management techniques\n- Add hardware optimization guidelines\n- Elaborate on parallel processing implementation\n\n2. Training Stability:\n- Add specific techniques for handling varying sequence lengths\n- Include detailed optimization strategies\n- Provide comprehensive initialization guidelines\n- Detail gradient clipping and normalization approaches\n\n3. Memory Management:\n- Specify compression threshold selection criteria\n- Detail state management strategies\n- Include memory fragmentation mitigation techniques\n- Add IO optimization guidelines\n\n4. Validation Framework:\n- Define specific performance benchmarks\n- Include detailed ablation study methodology\n- Provide comprehensive comparison metrics\n- Specify evaluation criteria and methodology\n\n5. Hardware Optimization:\n- Detail specific hardware optimization techniques\n- Include parallel processing implementation guidelines\n- Specify IO optimization strategies\n- Add memory hierarchy optimization details\n\n6. Theoretical Analysis:\n- Expand on stability analysis for adaptive components\n- Include convergence analysis\n- Add theoretical bounds on memory efficiency\n- Provide more detailed complexity analysis",
    "user_input": ""
}