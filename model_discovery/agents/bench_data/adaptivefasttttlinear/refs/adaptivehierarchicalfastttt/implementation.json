{
    "implementation": {
        "review": null,
        "root": "TTT",
        "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
        "units": {
            "TTT": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "FastTTTLinear",
                    "SwiGluMLP",
                    "RMSNorm",
                    "Conv"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "RMSNorm": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "eps": 1e-05
                },
                "design_traces": null
            },
            "FastTTTLinear": {
                "review": "## Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.5```\n\n### 1. Overall Assessment\n\nThe implementation of the `FastTTTLinear` GAU is robust and aligns well with the proposal's objectives. It successfully incorporates advanced features like Flash Attention, adaptive chunk sizing, and performance monitoring, which enhance both efficiency and scalability. The functionality check has passed, indicating that the GAU integrates well within the larger model and performs as expected.\n\n### 2. Strengths of the Implementation\n\n- **Advanced Attention Mechanism**: The use of chunked causal attention with optional Flash Attention support is a significant strength, offering both efficiency and flexibility in handling long sequences.\n- **Memory and Performance Optimization**: The implementation includes memory profiling and performance logging, which are valuable for understanding and optimizing resource usage.\n- **Adaptive Chunk Sizing**: The dynamic adjustment of chunk size based on available memory is a thoughtful feature that enhances the model's scalability and efficiency.\n- **Comprehensive Documentation**: The docstring provides detailed information about the GAU's features, performance guidelines, and hardware requirements, aiding in understanding and utilization.\n\n### 3. Areas for Improvement and Suggestions\n\n- **Unit Tests**: While the functionality check passed, it's crucial to ensure that unit tests are comprehensive and cover various scenarios, including edge cases. This will help maintain robustness as the model evolves.\n- **Performance Metrics Utilization**: Consider implementing mechanisms to act on the performance metrics collected, such as adjusting operations dynamically based on profiling data. This could further optimize performance.\n- **Code Readability**: Although the code is well-structured, adding inline comments explaining complex operations or decisions could enhance readability and maintainability.\n\n### 4. Comments on Innovation and Potential Impact\n\n- **Innovation**: The integration of Flash Attention and adaptive chunk sizing are innovative features that align well with the proposal's goals. These enhancements have the potential to significantly improve the model's efficiency and scalability.\n- **Potential Impact**: The implementation is well-positioned to handle long sequences efficiently, making it a valuable component of the language model. The focus on memory and performance optimization ensures that it can scale effectively with larger datasets and models.\n\n### 5. Recommendations for the Coder\n\n- **Enhance Unit Testing**: Ensure that unit tests are comprehensive and cover both typical and edge cases. Use assertions to validate expected outcomes and maintain robustness.\n- **Leverage Performance Metrics**: Use the performance metrics collected to dynamically adjust operations or provide insights into potential optimizations.\n- **Maintain Documentation**: Keep the documentation updated with any changes or enhancements to the GAU, ensuring that it remains a valuable resource for understanding and utilizing the model.\n\nBy addressing these areas, the coder can further enhance the robustness and effectiveness of the `FastTTTLinear` GAU, ensuring it remains a strong and scalable component of the language model.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "test_fasttttlinear_causality": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_causality(device=None, dtype=None\n    ) ->None:\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    x = torch.randn(2, 1024, 512, device=device, dtype=dtype)\n    y1, _ = model(x[:, :512])\n    y2, _ = model(x)\n    assert torch.allclose(y1, y2[:, :512], atol=1e-05), 'Causality test failed'\n",
                    "test_fasttttlinear_gradient_flow": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_gradient_flow(device=None, dtype=None\n    ) ->None:\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    model.to(device=device, dtype=dtype)\n    x = torch.randn(2, 16, 512, device=device, dtype=dtype, requires_grad=True)\n    y, _ = model(x)\n    loss = y.mean()\n    loss.backward()\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            assert param.grad is not None, f'No gradient for parameter {name}'\n",
                    "test_memory_efficiency": "@gau_test\ndef test_FastTTTLinear_test_memory_efficiency(device=None, dtype=None) ->None:\n    if not torch.cuda.is_available():\n        return\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    x = torch.randn(2, 4096, 512, device=device, dtype=dtype)\n    torch.cuda.reset_peak_memory_stats()\n    y, _ = model(x)\n    mem_used = torch.cuda.max_memory_allocated()\n    assert mem_used < 1000000000.0, f'Memory usage is too high: {mem_used} bytes'\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport time\ntry:\n    from flash_attn import flash_attention_impl\n    HAS_FLASH_ATTENTION = True\nexcept ImportError:\n    HAS_FLASH_ATTENTION = False\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\n\n    Key Features:\n    - Causal attention with efficient chunked computation\n    - Memory-efficient implementation with gradient checkpointing\n    - Optional Flash Attention support for faster computation\n    - Adaptive chunk sizing based on sequence length\n    - Enhanced numerical stability through proper scaling and normalization\n    - Performance monitoring and memory profiling capabilities\n\n    Performance Guidelines:\n    - Recommended maximum sequence length: 32K\n    - Optimal chunk size: 1024 for 16GB GPU\n    - Memory usage: O(N) where N is sequence length\n\n    Hardware Requirements:\n    - Minimum GPU memory: 8GB\n    - Recommended GPU memory: 16GB\n    - Optional: Flash Attention support\n\n    Performance Characteristics:\n    - Time complexity: O(N) where N is sequence length\n    - Memory complexity: O(N) with constant factor optimization\n    - Optimal batch size: Depends on GPU memory (see formula in code)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, dropout=0.0,\n        attention_dropout=0.0, chunk_size=1024, max_position_embeddings=\n        32768, layer_norm_eps=1e-05, use_flash_attention=True, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        assert embed_dim % num_attention_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.embed_dim = embed_dim\n        self.base_chunk_size = chunk_size\n        self.max_position_embeddings = max_position_embeddings\n        self.use_flash_attention = use_flash_attention and HAS_FLASH_ATTENTION\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dropout = nn.Dropout(p=dropout)\n        self.attention_dropout = nn.Dropout(p=attention_dropout)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=0, groups=embed_dim, bias=True, **self.factory_kwargs)\n        self._init_weights()\n        self.gradient_checkpointing = False\n        self.memory_profiling = False\n        self.peak_memory = 0\n        self.perf_stats = {'forward_time': [], 'attention_time': []}\n\n    def _init_weights(self):\n        \"\"\"Initialize weights with proper scaling for stability.\"\"\"\n        gain = 1.0 / math.sqrt(2.0)\n        nn.init.xavier_uniform_(self.W_Q.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_K.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_V.weight, gain=gain)\n        nn.init.xavier_uniform_(self.gate_Q.weight, gain=gain)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight, gain=gain)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _track_memory(self):\n        if self.memory_profiling and torch.cuda.is_available():\n            current = torch.cuda.memory_allocated()\n            self.peak_memory = max(self.peak_memory, current)\n\n    def _log_performance(self, operation: str, time_taken: float):\n        if hasattr(self, 'perf_stats'):\n            self.perf_stats[f'{operation}_time'].append(time_taken)\n\n    def _efficient_attention(self, Q, K, V, mask):\n        \"\"\"Efficient attention computation.\"\"\"\n        self._track_memory()\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask, float('-inf'))\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        output = torch.matmul(attn_weights, V)\n        self._track_memory()\n        return output\n\n    def _causal_attention(self, Q, K, V, chunk_size):\n        \"\"\"Compute chunked causal attention with optional Flash Attention.\"\"\"\n        B, H, L, D = Q.shape\n        start_time = time.time()\n        if self.use_flash_attention and not self.training:\n            attn_output = flash_attention_impl(Q, K, V, causal=True)\n            self._log_performance('attention', time.time() - start_time)\n            return attn_output\n        outputs = []\n        for chunk_start in range(0, L, chunk_size):\n            chunk_end = min(chunk_start + chunk_size, L)\n            Q_chunk = Q[:, :, chunk_start:chunk_end]\n            K_chunk = K[:, :, :chunk_end]\n            V_chunk = V[:, :, :chunk_end]\n            causal_mask = torch.triu(torch.ones((chunk_end - chunk_start,\n                chunk_end), device=Q.device, dtype=torch.bool), diagonal=1)\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            chunk_output = self._efficient_attention(Q_chunk * self.scale,\n                K_chunk, V_chunk, causal_mask)\n            outputs.append(chunk_output)\n        attn_output = torch.cat(outputs, dim=2)\n        self._log_performance('attention', time.time() - start_time)\n        return attn_output\n\n    def _optimize_chunk_size(self, seq_len: int) ->int:\n        \"\"\"Dynamic chunk size optimization.\"\"\"\n        if torch.cuda.is_available():\n            total_memory = torch.cuda.get_device_properties(0).total_memory\n            free_memory = total_memory - torch.cuda.memory_allocated()\n            optimal_size = min(self.base_chunk_size, max(128, min(seq_len, \n                free_memory // (self.embed_dim * 4))))\n            return optimal_size // 8 * 8\n        return self.base_chunk_size\n\n    def _forward_impl(self, X, **Z):\n        \"\"\"Main implementation of forward pass with all optimizations.\"\"\"\n        start_time = time.time()\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        self.chunk_size = self._optimize_chunk_size(L)\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='replicate')\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)\n        X = X + self.dropout(X_conv)\n        Q = self.W_Q(X) * torch.sigmoid(self.gate_Q(X))\n        K = self.W_K(X) * torch.sigmoid(self.gate_K(X))\n        V = self.W_V(X)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        attn_output = self._causal_attention(Q, K, V, self.chunk_size)\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + 0.1 * self.dropout(output)\n        output, Z = self.norm(output, **Z)\n        self._log_performance('forward', time.time() - start_time)\n        return output, Z\n\n    def _forward(self, X, **Z):\n        \"\"\"Forward pass with optional gradient checkpointing.\"\"\"\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n",
                "rating": 4.5,
                "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\\n\\nKey Features:\\n- Causal attention with efficient chunked computation\\n- Memory-efficient implementation with gradient checkpointing\\n- Optional Flash Attention support for faster computation\\n- Adaptive chunk sizing based on sequence length\\n- Enhanced numerical stability through proper scaling and normalization\\n- Performance monitoring and memory profiling capabilities\\n\\nPerformance Guidelines:\\n- Recommended maximum sequence length: 32K\\n- Optimal chunk size: 1024 for 16GB GPU\\n- Memory usage: O(N) where N is sequence length\\n\\nHardware Requirements:\\n- Minimum GPU memory: 8GB\\n- Recommended GPU memory: 16GB\\n- Optional: Flash Attention support\\n\\nPerformance Characteristics:\\n- Time complexity: O(N) where N is sequence length\\n- Memory complexity: O(N) with constant factor optimization\\n- Optimal batch size: Depends on GPU memory (see formula in code)\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "attention_dropout": 0.0,
                    "num_attention_heads": 4,
                    "dropout": 0.0,
                    "layer_norm_eps": 1e-05,
                    "use_flash_attention": true,
                    "max_position_embeddings": 32768,
                    "chunk_size": 1024
                },
                "design_traces": null
            },
            "Conv": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "conv_kernel": 4,
                    "rms_norm_eps": 1e-06
                },
                "design_traces": null
            },
            "SwiGluMLP": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "intermediate_size": null
                },
                "design_traces": null
            }
        },
        "rating": null,
        "declares": {
            "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
            "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
        },
        "proposal_traces": [],
        "suggestions": null,
        "name": "adaptivehierarchicalfastttt"
    },
    "status": "implemented",
    "history": [
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "## Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.5```\n\n### 1. Overall Assessment\n\nThe implementation of the `FastTTTLinear` GAU is robust and aligns well with the proposal's objectives. It successfully incorporates advanced features like Flash Attention, adaptive chunk sizing, and performance monitoring, which enhance both efficiency and scalability. The functionality check has passed, indicating that the GAU integrates well within the larger model and performs as expected.\n\n### 2. Strengths of the Implementation\n\n- **Advanced Attention Mechanism**: The use of chunked causal attention with optional Flash Attention support is a significant strength, offering both efficiency and flexibility in handling long sequences.\n- **Memory and Performance Optimization**: The implementation includes memory profiling and performance logging, which are valuable for understanding and optimizing resource usage.\n- **Adaptive Chunk Sizing**: The dynamic adjustment of chunk size based on available memory is a thoughtful feature that enhances the model's scalability and efficiency.\n- **Comprehensive Documentation**: The docstring provides detailed information about the GAU's features, performance guidelines, and hardware requirements, aiding in understanding and utilization.\n\n### 3. Areas for Improvement and Suggestions\n\n- **Unit Tests**: While the functionality check passed, it's crucial to ensure that unit tests are comprehensive and cover various scenarios, including edge cases. This will help maintain robustness as the model evolves.\n- **Performance Metrics Utilization**: Consider implementing mechanisms to act on the performance metrics collected, such as adjusting operations dynamically based on profiling data. This could further optimize performance.\n- **Code Readability**: Although the code is well-structured, adding inline comments explaining complex operations or decisions could enhance readability and maintainability.\n\n### 4. Comments on Innovation and Potential Impact\n\n- **Innovation**: The integration of Flash Attention and adaptive chunk sizing are innovative features that align well with the proposal's goals. These enhancements have the potential to significantly improve the model's efficiency and scalability.\n- **Potential Impact**: The implementation is well-positioned to handle long sequences efficiently, making it a valuable component of the language model. The focus on memory and performance optimization ensures that it can scale effectively with larger datasets and models.\n\n### 5. Recommendations for the Coder\n\n- **Enhance Unit Testing**: Ensure that unit tests are comprehensive and cover both typical and edge cases. Use assertions to validate expected outcomes and maintain robustness.\n- **Leverage Performance Metrics**: Use the performance metrics collected to dynamically adjust operations or provide insights into potential optimizations.\n- **Maintain Documentation**: Keep the documentation updated with any changes or enhancements to the GAU, ensuring that it remains a valuable resource for understanding and utilizing the model.\n\nBy addressing these areas, the coder can further enhance the robustness and effectiveness of the `FastTTTLinear` GAU, ensuring it remains a strong and scalable component of the language model.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_fasttttlinear_causality": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_causality(device=None, dtype=None\n    ) ->None:\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    x = torch.randn(2, 1024, 512, device=device, dtype=dtype)\n    y1, _ = model(x[:, :512])\n    y2, _ = model(x)\n    assert torch.allclose(y1, y2[:, :512], atol=1e-05), 'Causality test failed'\n",
                            "test_fasttttlinear_gradient_flow": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_gradient_flow(device=None, dtype=None\n    ) ->None:\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    model.to(device=device, dtype=dtype)\n    x = torch.randn(2, 16, 512, device=device, dtype=dtype, requires_grad=True)\n    y, _ = model(x)\n    loss = y.mean()\n    loss.backward()\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            assert param.grad is not None, f'No gradient for parameter {name}'\n",
                            "test_memory_efficiency": "@gau_test\ndef test_FastTTTLinear_test_memory_efficiency(device=None, dtype=None) ->None:\n    if not torch.cuda.is_available():\n        return\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    x = torch.randn(2, 4096, 512, device=device, dtype=dtype)\n    torch.cuda.reset_peak_memory_stats()\n    y, _ = model(x)\n    mem_used = torch.cuda.max_memory_allocated()\n    assert mem_used < 1000000000.0, f'Memory usage is too high: {mem_used} bytes'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport time\ntry:\n    from flash_attn import flash_attention_impl\n    HAS_FLASH_ATTENTION = True\nexcept ImportError:\n    HAS_FLASH_ATTENTION = False\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\n\n    Key Features:\n    - Causal attention with efficient chunked computation\n    - Memory-efficient implementation with gradient checkpointing\n    - Optional Flash Attention support for faster computation\n    - Adaptive chunk sizing based on sequence length\n    - Enhanced numerical stability through proper scaling and normalization\n    - Performance monitoring and memory profiling capabilities\n\n    Performance Guidelines:\n    - Recommended maximum sequence length: 32K\n    - Optimal chunk size: 1024 for 16GB GPU\n    - Memory usage: O(N) where N is sequence length\n\n    Hardware Requirements:\n    - Minimum GPU memory: 8GB\n    - Recommended GPU memory: 16GB\n    - Optional: Flash Attention support\n\n    Performance Characteristics:\n    - Time complexity: O(N) where N is sequence length\n    - Memory complexity: O(N) with constant factor optimization\n    - Optimal batch size: Depends on GPU memory (see formula in code)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, dropout=0.0,\n        attention_dropout=0.0, chunk_size=1024, max_position_embeddings=\n        32768, layer_norm_eps=1e-05, use_flash_attention=True, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        assert embed_dim % num_attention_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.embed_dim = embed_dim\n        self.base_chunk_size = chunk_size\n        self.max_position_embeddings = max_position_embeddings\n        self.use_flash_attention = use_flash_attention and HAS_FLASH_ATTENTION\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dropout = nn.Dropout(p=dropout)\n        self.attention_dropout = nn.Dropout(p=attention_dropout)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=0, groups=embed_dim, bias=True, **self.factory_kwargs)\n        self._init_weights()\n        self.gradient_checkpointing = False\n        self.memory_profiling = False\n        self.peak_memory = 0\n        self.perf_stats = {'forward_time': [], 'attention_time': []}\n\n    def _init_weights(self):\n        \"\"\"Initialize weights with proper scaling for stability.\"\"\"\n        gain = 1.0 / math.sqrt(2.0)\n        nn.init.xavier_uniform_(self.W_Q.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_K.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_V.weight, gain=gain)\n        nn.init.xavier_uniform_(self.gate_Q.weight, gain=gain)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight, gain=gain)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _track_memory(self):\n        if self.memory_profiling and torch.cuda.is_available():\n            current = torch.cuda.memory_allocated()\n            self.peak_memory = max(self.peak_memory, current)\n\n    def _log_performance(self, operation: str, time_taken: float):\n        if hasattr(self, 'perf_stats'):\n            self.perf_stats[f'{operation}_time'].append(time_taken)\n\n    def _efficient_attention(self, Q, K, V, mask):\n        \"\"\"Efficient attention computation.\"\"\"\n        self._track_memory()\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask, float('-inf'))\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        output = torch.matmul(attn_weights, V)\n        self._track_memory()\n        return output\n\n    def _causal_attention(self, Q, K, V, chunk_size):\n        \"\"\"Compute chunked causal attention with optional Flash Attention.\"\"\"\n        B, H, L, D = Q.shape\n        start_time = time.time()\n        if self.use_flash_attention and not self.training:\n            attn_output = flash_attention_impl(Q, K, V, causal=True)\n            self._log_performance('attention', time.time() - start_time)\n            return attn_output\n        outputs = []\n        for chunk_start in range(0, L, chunk_size):\n            chunk_end = min(chunk_start + chunk_size, L)\n            Q_chunk = Q[:, :, chunk_start:chunk_end]\n            K_chunk = K[:, :, :chunk_end]\n            V_chunk = V[:, :, :chunk_end]\n            causal_mask = torch.triu(torch.ones((chunk_end - chunk_start,\n                chunk_end), device=Q.device, dtype=torch.bool), diagonal=1)\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            chunk_output = self._efficient_attention(Q_chunk * self.scale,\n                K_chunk, V_chunk, causal_mask)\n            outputs.append(chunk_output)\n        attn_output = torch.cat(outputs, dim=2)\n        self._log_performance('attention', time.time() - start_time)\n        return attn_output\n\n    def _optimize_chunk_size(self, seq_len: int) ->int:\n        \"\"\"Dynamic chunk size optimization.\"\"\"\n        if torch.cuda.is_available():\n            total_memory = torch.cuda.get_device_properties(0).total_memory\n            free_memory = total_memory - torch.cuda.memory_allocated()\n            optimal_size = min(self.base_chunk_size, max(128, min(seq_len, \n                free_memory // (self.embed_dim * 4))))\n            return optimal_size // 8 * 8\n        return self.base_chunk_size\n\n    def _forward_impl(self, X, **Z):\n        \"\"\"Main implementation of forward pass with all optimizations.\"\"\"\n        start_time = time.time()\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        self.chunk_size = self._optimize_chunk_size(L)\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='replicate')\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)\n        X = X + self.dropout(X_conv)\n        Q = self.W_Q(X) * torch.sigmoid(self.gate_Q(X))\n        K = self.W_K(X) * torch.sigmoid(self.gate_K(X))\n        V = self.W_V(X)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        attn_output = self._causal_attention(Q, K, V, self.chunk_size)\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + 0.1 * self.dropout(output)\n        output, Z = self.norm(output, **Z)\n        self._log_performance('forward', time.time() - start_time)\n        return output, Z\n\n    def _forward(self, X, **Z):\n        \"\"\"Forward pass with optional gradient checkpointing.\"\"\"\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\\n\\nKey Features:\\n- Causal attention with efficient chunked computation\\n- Memory-efficient implementation with gradient checkpointing\\n- Optional Flash Attention support for faster computation\\n- Adaptive chunk sizing based on sequence length\\n- Enhanced numerical stability through proper scaling and normalization\\n- Performance monitoring and memory profiling capabilities\\n\\nPerformance Guidelines:\\n- Recommended maximum sequence length: 32K\\n- Optimal chunk size: 1024 for 16GB GPU\\n- Memory usage: O(N) where N is sequence length\\n\\nHardware Requirements:\\n- Minimum GPU memory: 8GB\\n- Recommended GPU memory: 16GB\\n- Optional: Flash Attention support\\n\\nPerformance Characteristics:\\n- Time complexity: O(N) where N is sequence length\\n- Memory complexity: O(N) with constant factor optimization\\n- Optimal batch size: Depends on GPU memory (see formula in code)\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "attention_dropout": 0.0,
                            "num_attention_heads": 4,
                            "dropout": 0.0,
                            "layer_norm_eps": 1e-05,
                            "use_flash_attention": true,
                            "max_position_embeddings": 32768,
                            "chunk_size": 1024
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "adaptivehierarchicalfastttt"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_mini",
                    "IMPLEMENTATION_CODER": "o1_preview",
                    "PROPOSAL_REVIEWER": "claude3.5_sonnet",
                    "IMPLEMENTATION_OBSERVER": "gpt4o_0806",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.09328800000000001,
                "IMPLEMENTATION_CODER": 1.90485,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 0.20240750000000002,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "## Comprehensive Feedback Report for FastTTTLinear Implementation\n\n```rating 4.5```\n\n### 1. Overall Assessment\n\nThe implementation of the `FastTTTLinear` GAU is robust and aligns well with the proposal's objectives. It successfully incorporates advanced features like Flash Attention, adaptive chunk sizing, and performance monitoring, which enhance both efficiency and scalability. The functionality check has passed, indicating that the GAU integrates well within the larger model and performs as expected.\n\n### 2. Strengths of the Implementation\n\n- **Advanced Attention Mechanism**: The use of chunked causal attention with optional Flash Attention support is a significant strength, offering both efficiency and flexibility in handling long sequences.\n- **Memory and Performance Optimization**: The implementation includes memory profiling and performance logging, which are valuable for understanding and optimizing resource usage.\n- **Adaptive Chunk Sizing**: The dynamic adjustment of chunk size based on available memory is a thoughtful feature that enhances the model's scalability and efficiency.\n- **Comprehensive Documentation**: The docstring provides detailed information about the GAU's features, performance guidelines, and hardware requirements, aiding in understanding and utilization.\n\n### 3. Areas for Improvement and Suggestions\n\n- **Unit Tests**: While the functionality check passed, it's crucial to ensure that unit tests are comprehensive and cover various scenarios, including edge cases. This will help maintain robustness as the model evolves.\n- **Performance Metrics Utilization**: Consider implementing mechanisms to act on the performance metrics collected, such as adjusting operations dynamically based on profiling data. This could further optimize performance.\n- **Code Readability**: Although the code is well-structured, adding inline comments explaining complex operations or decisions could enhance readability and maintainability.\n\n### 4. Comments on Innovation and Potential Impact\n\n- **Innovation**: The integration of Flash Attention and adaptive chunk sizing are innovative features that align well with the proposal's goals. These enhancements have the potential to significantly improve the model's efficiency and scalability.\n- **Potential Impact**: The implementation is well-positioned to handle long sequences efficiently, making it a valuable component of the language model. The focus on memory and performance optimization ensures that it can scale effectively with larger datasets and models.\n\n### 5. Recommendations for the Coder\n\n- **Enhance Unit Testing**: Ensure that unit tests are comprehensive and cover both typical and edge cases. Use assertions to validate expected outcomes and maintain robustness.\n- **Leverage Performance Metrics**: Use the performance metrics collected to dynamically adjust operations or provide insights into potential optimizations.\n- **Maintain Documentation**: Keep the documentation updated with any changes or enhancements to the GAU, ensuring that it remains a valuable resource for understanding and utilizing the model.\n\nBy addressing these areas, the coder can further enhance the robustness and effectiveness of the `FastTTTLinear` GAU, ensuring it remains a strong and scalable component of the language model.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_fasttttlinear_causality": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_causality(device=None, dtype=None\n    ) ->None:\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    x = torch.randn(2, 1024, 512, device=device, dtype=dtype)\n    y1, _ = model(x[:, :512])\n    y2, _ = model(x)\n    assert torch.allclose(y1, y2[:, :512], atol=1e-05), 'Causality test failed'\n",
                            "test_fasttttlinear_gradient_flow": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_gradient_flow(device=None, dtype=None\n    ) ->None:\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    model.to(device=device, dtype=dtype)\n    x = torch.randn(2, 16, 512, device=device, dtype=dtype, requires_grad=True)\n    y, _ = model(x)\n    loss = y.mean()\n    loss.backward()\n    for name, param in model.named_parameters():\n        if param.requires_grad:\n            assert param.grad is not None, f'No gradient for parameter {name}'\n",
                            "test_memory_efficiency": "@gau_test\ndef test_FastTTTLinear_test_memory_efficiency(device=None, dtype=None) ->None:\n    if not torch.cuda.is_available():\n        return\n    model = FastTTTLinear(512, (0, 0), {}, device=device, dtype=dtype)\n    x = torch.randn(2, 4096, 512, device=device, dtype=dtype)\n    torch.cuda.reset_peak_memory_stats()\n    y, _ = model(x)\n    mem_used = torch.cuda.max_memory_allocated()\n    assert mem_used < 1000000000.0, f'Memory usage is too high: {mem_used} bytes'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport time\ntry:\n    from flash_attn import flash_attention_impl\n    HAS_FLASH_ATTENTION = True\nexcept ImportError:\n    HAS_FLASH_ATTENTION = False\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\n\n    Key Features:\n    - Causal attention with efficient chunked computation\n    - Memory-efficient implementation with gradient checkpointing\n    - Optional Flash Attention support for faster computation\n    - Adaptive chunk sizing based on sequence length\n    - Enhanced numerical stability through proper scaling and normalization\n    - Performance monitoring and memory profiling capabilities\n\n    Performance Guidelines:\n    - Recommended maximum sequence length: 32K\n    - Optimal chunk size: 1024 for 16GB GPU\n    - Memory usage: O(N) where N is sequence length\n\n    Hardware Requirements:\n    - Minimum GPU memory: 8GB\n    - Recommended GPU memory: 16GB\n    - Optional: Flash Attention support\n\n    Performance Characteristics:\n    - Time complexity: O(N) where N is sequence length\n    - Memory complexity: O(N) with constant factor optimization\n    - Optimal batch size: Depends on GPU memory (see formula in code)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, dropout=0.0,\n        attention_dropout=0.0, chunk_size=1024, max_position_embeddings=\n        32768, layer_norm_eps=1e-05, use_flash_attention=True, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        assert embed_dim % num_attention_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.embed_dim = embed_dim\n        self.base_chunk_size = chunk_size\n        self.max_position_embeddings = max_position_embeddings\n        self.use_flash_attention = use_flash_attention and HAS_FLASH_ATTENTION\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dropout = nn.Dropout(p=dropout)\n        self.attention_dropout = nn.Dropout(p=attention_dropout)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=0, groups=embed_dim, bias=True, **self.factory_kwargs)\n        self._init_weights()\n        self.gradient_checkpointing = False\n        self.memory_profiling = False\n        self.peak_memory = 0\n        self.perf_stats = {'forward_time': [], 'attention_time': []}\n\n    def _init_weights(self):\n        \"\"\"Initialize weights with proper scaling for stability.\"\"\"\n        gain = 1.0 / math.sqrt(2.0)\n        nn.init.xavier_uniform_(self.W_Q.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_K.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_V.weight, gain=gain)\n        nn.init.xavier_uniform_(self.gate_Q.weight, gain=gain)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight, gain=gain)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _track_memory(self):\n        if self.memory_profiling and torch.cuda.is_available():\n            current = torch.cuda.memory_allocated()\n            self.peak_memory = max(self.peak_memory, current)\n\n    def _log_performance(self, operation: str, time_taken: float):\n        if hasattr(self, 'perf_stats'):\n            self.perf_stats[f'{operation}_time'].append(time_taken)\n\n    def _efficient_attention(self, Q, K, V, mask):\n        \"\"\"Efficient attention computation.\"\"\"\n        self._track_memory()\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask, float('-inf'))\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        output = torch.matmul(attn_weights, V)\n        self._track_memory()\n        return output\n\n    def _causal_attention(self, Q, K, V, chunk_size):\n        \"\"\"Compute chunked causal attention with optional Flash Attention.\"\"\"\n        B, H, L, D = Q.shape\n        start_time = time.time()\n        if self.use_flash_attention and not self.training:\n            attn_output = flash_attention_impl(Q, K, V, causal=True)\n            self._log_performance('attention', time.time() - start_time)\n            return attn_output\n        outputs = []\n        for chunk_start in range(0, L, chunk_size):\n            chunk_end = min(chunk_start + chunk_size, L)\n            Q_chunk = Q[:, :, chunk_start:chunk_end]\n            K_chunk = K[:, :, :chunk_end]\n            V_chunk = V[:, :, :chunk_end]\n            causal_mask = torch.triu(torch.ones((chunk_end - chunk_start,\n                chunk_end), device=Q.device, dtype=torch.bool), diagonal=1)\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            chunk_output = self._efficient_attention(Q_chunk * self.scale,\n                K_chunk, V_chunk, causal_mask)\n            outputs.append(chunk_output)\n        attn_output = torch.cat(outputs, dim=2)\n        self._log_performance('attention', time.time() - start_time)\n        return attn_output\n\n    def _optimize_chunk_size(self, seq_len: int) ->int:\n        \"\"\"Dynamic chunk size optimization.\"\"\"\n        if torch.cuda.is_available():\n            total_memory = torch.cuda.get_device_properties(0).total_memory\n            free_memory = total_memory - torch.cuda.memory_allocated()\n            optimal_size = min(self.base_chunk_size, max(128, min(seq_len, \n                free_memory // (self.embed_dim * 4))))\n            return optimal_size // 8 * 8\n        return self.base_chunk_size\n\n    def _forward_impl(self, X, **Z):\n        \"\"\"Main implementation of forward pass with all optimizations.\"\"\"\n        start_time = time.time()\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        self.chunk_size = self._optimize_chunk_size(L)\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='replicate')\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)\n        X = X + self.dropout(X_conv)\n        Q = self.W_Q(X) * torch.sigmoid(self.gate_Q(X))\n        K = self.W_K(X) * torch.sigmoid(self.gate_K(X))\n        V = self.W_V(X)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        attn_output = self._causal_attention(Q, K, V, self.chunk_size)\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + 0.1 * self.dropout(output)\n        output, Z = self.norm(output, **Z)\n        self._log_performance('forward', time.time() - start_time)\n        return output, Z\n\n    def _forward(self, X, **Z):\n        \"\"\"Forward pass with optional gradient checkpointing.\"\"\"\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\\n\\nKey Features:\\n- Causal attention with efficient chunked computation\\n- Memory-efficient implementation with gradient checkpointing\\n- Optional Flash Attention support for faster computation\\n- Adaptive chunk sizing based on sequence length\\n- Enhanced numerical stability through proper scaling and normalization\\n- Performance monitoring and memory profiling capabilities\\n\\nPerformance Guidelines:\\n- Recommended maximum sequence length: 32K\\n- Optimal chunk size: 1024 for 16GB GPU\\n- Memory usage: O(N) where N is sequence length\\n\\nHardware Requirements:\\n- Minimum GPU memory: 8GB\\n- Recommended GPU memory: 16GB\\n- Optional: Flash Attention support\\n\\nPerformance Characteristics:\\n- Time complexity: O(N) where N is sequence length\\n- Memory complexity: O(N) with constant factor optimization\\n- Optimal batch size: Depends on GPU memory (see formula in code)\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "attention_dropout": 0.0,
                            "num_attention_heads": 4,
                            "dropout": 0.0,
                            "layer_norm_eps": 1e-05,
                            "use_flash_attention": true,
                            "max_position_embeddings": 32768,
                            "chunk_size": 1024
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "adaptivehierarchicalfastttt"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_mini",
                    "IMPLEMENTATION_CODER": "o1_preview",
                    "PROPOSAL_REVIEWER": "claude3.5_sonnet",
                    "IMPLEMENTATION_OBSERVER": "gpt4o_0806",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.09328800000000001,
                "IMPLEMENTATION_CODER": 1.90485,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 0.20240750000000002,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}