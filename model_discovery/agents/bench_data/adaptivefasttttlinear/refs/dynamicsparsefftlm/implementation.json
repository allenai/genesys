{
    "implementation": {
        "review": null,
        "root": "TTT",
        "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
        "units": {
            "TTT": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "FastTTTLinear",
                    "SwiGluMLP",
                    "RMSNorm",
                    "Conv"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "RMSNorm": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "eps": 1e-05
                },
                "design_traces": null
            },
            "FastTTTLinear": {
                "review": "```rating 4.6```\n\n---\n\n### **1. Overall Assessment**\n\nThe **FastTTTLinear** implementation exhibits significant advancements towards enhancing the GAU by integrating position-aware processing and maintaining efficient computation strategies. The code is well-organized, thoroughly documented, and incorporates critical optimizations such as gradient checkpointing and sinusoidal positional embeddings. However, further integration of dynamic sparsity and FFT-based computations, as outlined in the proposal \"DynamicSparseFFTLinear,\" remains to be addressed to fully realize the proposed enhancements.\n\n### **2. Strengths of the Implementation**\n\n1. **Comprehensive Documentation and Clarity:**\n   - Detailed docstrings provide clear explanations of the module's purpose, functionality, and usage.\n   - Key features are explicitly listed, facilitating easier understanding and onboarding for future developers.\n\n2. **Effective Code Organization:**\n   - The separation of the forward pass into `_forward_impl` enhances readability and maintainability.\n   - Logical structuring of layer initialization ensures clarity in the role of each component within the GAU.\n\n3. **Position-Aware Processing:**\n   - Implementation of sinusoidal positional embeddings within the `get_position_embeddings` method effectively integrates positional information.\n   - Position embeddings are seamlessly added to the input tensor, enhancing the model's ability to capture token positions.\n\n4. **Gradient Checkpointing Support:**\n   - The inclusion of gradient checkpointing optimizes memory usage during training, allowing for the handling of larger models or longer sequences without significant memory overhead.\n\n5. **Normalization and Stability Enhancements:**\n   - Application of LayerNorm to queries and keys (`q_norm` and `k_norm`) stabilizes attention computations.\n   - Use of ELU activation followed by an additive constant (`ELU + 1`) improves numerical stability and gradient flow.\n\n6. **Efficient Memory Utilization:**\n   - Vectorized operations eliminate Python-level loops, leveraging PyTorch's optimized tensor computations for better performance.\n   - Contiguous memory layouts in tensor reshaping and views enhance computational efficiency.\n\n7. **Integration with Existing GAUs:**\n   - Maintains a clean interface with the parent GAU, ensuring smooth integration within the larger model architecture.\n   - Proper handling of intermediate variables (`Z` dictionary) facilitates effective state management across layers.\n\n### **3. Areas for Improvement and Specific Suggestions**\n\n1. **Integration of Dynamic Sparsity:**\n   - **Implementation Need:** The current implementation lacks mechanisms for dynamic sparsity, which adaptively prunes less important tokens to optimize computation.\n   - **Suggestion:** Introduce an importance scoring mechanism to generate dynamic sparsity masks, selectively processing significant tokens.\n   - **Example:**\n     ```python\n     # Importance scoring\n     S = torch.sigmoid(self.score_net(X))  # S \u2208 \u211d^(B\u00d7L\u00d71)\n     \n     # Create dynamic sparsity mask\n     k = int(X.shape[1] * self.sparsity_ratio)\n     mask = torch.zeros_like(S)\n     topk_indices = S.topk(k, dim=1).indices\n     mask.scatter_(1, topk_indices, 1.0)\n     \n     # Apply mask\n     X_masked = X * mask\n     ```\n\n2. **Incorporation of FFT-Based Computation:**\n   - **Implementation Need:** The proposal emphasizes the use of FFT-based computations to handle long-range dependencies efficiently.\n   - **Suggestion:** Integrate FFT operations after applying dynamic sparsity masks to transform the input into the frequency domain and perform convolution.\n   - **Example:**\n     ```python\n     # Apply FFT\n     X_f = torch.fft.rfft(X_masked, dim=1)\n     \n     # FFT Convolution\n     Y_f = self.fft_conv(X_f)\n     Y = torch.fft.irfft(Y_f, dim=1)\n     \n     # Gated Output\n     G = torch.sigmoid(self.gate_net(Y))\n     O = G * Y\n     ```\n\n3. **Memory Optimization for Long Sequences:**\n   - **Implementation Need:** While gradient checkpointing aids memory efficiency, further optimizations can be introduced for handling very long sequences.\n   - **Suggestion:** Implement chunked processing where sequences are processed in manageable blocks, reducing peak memory usage.\n   - **Example:**\n     ```python\n     def _forward_impl(self, X, **Z):\n         B, L, D = X.size()\n         chunk_size = 512\n         outputs = []\n         for i in range(0, L, chunk_size):\n             chunk_end = min(i + chunk_size, L)\n             chunk_output = self._process_chunk(X[:, i:chunk_end], **Z)\n             outputs.append(chunk_output)\n         output = torch.cat(outputs, dim=1)\n         return output, Z\n     ```\n\n4. **Adaptive Computation Span Control:**\n   - **Implementation Need:** Enhance efficiency by making the attention span adaptive based on input importance.\n   - **Suggestion:** Introduce parameters that allow the attention span to adjust dynamically, possibly using learnable parameters to determine the span per attention head.\n   - **Example:**\n     ```python\n     def __init__(self, ...):\n         self.max_position = 2048\n         self.adaptive_span = nn.Parameter(torch.ones(num_attention_heads))\n     \n     def _compute_attention_mask(self, L):\n         span = torch.sigmoid(self.adaptive_span) * self.max_position\n         position_ids = torch.arange(L, device=span.device)\n         mask = position_ids[None, :] <= span[:, None]\n         return mask\n     ```\n\n5. **Enhanced Numerical Stability:**\n   - **Implementation Need:** While initial stability measures are present, additional safeguards can further ensure numerical robustness.\n   - **Suggestion:** Implement gradient clipping and monitoring of attention statistics to prevent instability during training.\n   - **Example:**\n     ```python\n     def _forward_impl(self, X, **Z):\n         ...\n         if self.training:\n             torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n             # Log attention statistics\n             self._log_attention_stats(Q, K, V)\n         ...\n     ```\n\n6. **Performance Monitoring and Debugging:**\n   - **Implementation Need:** Incorporate mechanisms to monitor performance metrics and debug potential issues.\n   - **Suggestion:** Add diagnostic features that track attention distributions, convergence metrics, and memory usage.\n   - **Example:**\n     ```python\n     def __init__(self, ...):\n         self.register_buffer('attention_stats', torch.zeros(4))  # Example buffer\n    \n     def _forward_impl(self, X, **Z):\n         ...\n         if self.training:\n             self.attention_stats += compute_attention_statistics(Q, K, V)\n         ...\n     ```\n\n7. **Completing and Optimizing Position Embeddings:**\n   - **Implementation Need:** Although position embeddings are implemented, ensure they are efficiently cached and do not add significant computational overhead.\n   - **Suggestion:** Implement caching mechanisms for position embeddings to prevent redundant computations during inference.\n   - **Example:**\n     ```python\n     def get_position_embeddings(self, position_ids):\n         # Cache position embeddings for fixed max sequence lengths\n         if not hasattr(self, 'cached_pos_emb'):\n             self.cached_pos_emb = self._generate_position_embeddings(max_len=self.max_position)\n         return self.cached_pos_emb[:, position_ids]\n     \n     def _generate_position_embeddings(self, max_len):\n         inv_freq = 1.0 / (10000 ** (torch.arange(0.0, self.head_dim, 2.0) / self.head_dim))\n         sinusoid_inp = torch.einsum('i,j->ij', torch.arange(max_len).float(), inv_freq)\n         pos_emb = torch.zeros(max_len, self.embed_dim)\n         pos_emb[:, 0::2] = torch.sin(sinusoid_inp)\n         pos_emb[:, 1::2] = torch.cos(sinusoid_inp)\n         return pos_emb.unsqueeze(0).to(inv_freq.device)\n     ```\n\n### **4. Comments on Innovation and Potential Impact**\n\n**Innovative Aspects:**\n- **Position-Aware Processing:** The integration of sinusoidal positional embeddings within FastTTTLinear is a robust approach to capturing token positions, enhancing the model's ability to understand sequence structure without relying on transformer-based positional encodings.\n- **Gradient Checkpointing:** Incorporating gradient checkpointing demonstrates an advanced optimization technique to manage memory efficiently during training, allowing for the scalability of larger models or longer sequences.\n- **Gated Linear Attention:** Utilizing gated mechanisms for queries and keys enhances the expressiveness and stability of the attention computations, building upon recent advancements in attention mechanisms.\n\n**Potential Impact:**\n- **Efficiency in Long Sequences:** By optimizing memory usage and introducing position-aware processing, FastTTTLinear can handle longer sequences more effectively, a critical capability for state-of-the-art language models.\n- **Scalability:** The thoughtful integration of memory optimization and gradient checkpointing allows the model to scale efficiently, accommodating both increased model sizes and expanded sequence lengths.\n- **Model Performance:** Enhancements in attention computation and position awareness are likely to result in improved performance on downstream tasks, particularly those requiring understanding of long-range dependencies.\n\n**Concerns about Integration and Scalability:**\n- **Dynamic Sparsity and FFT Integration:** The current implementation does not yet incorporate dynamic sparsity or FFT-based computations, which are pivotal for achieving the proposed efficiency and scalability improvements.\n- **Hardware Compatibility:** Integrating FFT-based operations and dynamic sparsity may necessitate careful consideration of hardware capabilities and optimizations to ensure these additions provide the intended performance benefits without introducing bottlenecks.\n\n### **5. Recommendations for the Coder**\n\n1. **Implement Dynamic Sparsity Mechanisms:**\n   - **Develop Importance Scoring:** Introduce a scoring mechanism to evaluate the importance of each token dynamically.\n   - **Generate Sparsity Masks:** Use the importance scores to create masks that selectively allow crucial tokens to be processed.\n   - **Integrate with Attention Computation:** Modify the attention mechanism to utilize these masks, thereby reducing computation on less important tokens.\n\n2. **Integrate FFT-Based Computations:**\n   - **Incorporate FFT Operations:** After applying dynamic sparsity masks, transform the input into the frequency domain using FFT to handle long-range dependencies efficiently.\n   - **Optimize FFT Convolutions:** Ensure that FFT-based convolutions are optimized for parallel computation and do not introduce significant computational overhead.\n\n3. **Enhance Memory Management:**\n   - **Chunk-Based Processing:** Implement chunked processing for very long sequences to prevent memory bottlenecks, processing sequences in manageable blocks.\n   - **Profile and Optimize Memory Usage:** Continuously monitor memory usage across different sequence lengths and optimize accordingly to maintain efficiency.\n\n4. **Reinforce Numerical Stability:**\n   - **Implement Gradient Clipping:** Introduce gradient clipping to prevent exploding gradients, especially when dealing with long sequences and complex attention mechanisms.\n   - **Monitor Attention Statistics:** Regularly log and monitor attention statistics to ensure consistent and stable training dynamics.\n\n5. **Expand Testing and Verification:**\n   - **Develop Specific Unit Tests:** Create unit tests targeting the newly integrated dynamic sparsity and FFT-based components to ensure their correct functionality.\n   - **Benchmark Performance:** Conduct extensive benchmarking with varying sequence lengths and model sizes to validate the efficiency and scalability improvements.\n\n6. **Improve Performance Monitoring and Debugging:**\n   - **Incorporate Logging Mechanisms:** Add logging for key performance indicators such as memory usage, computation time per layer, and attention distribution patterns.\n   - **Implement Diagnostic Features:** Develop features that can diagnose and report on the performance and stability of the attention mechanisms in real-time.\n\n7. **Maintain Code Readability and Documentation:**\n   - **Continue High Documentation Standards:** Ensure that all new components, especially those related to dynamic sparsity and FFT computations, are thoroughly documented.\n   - **Use Descriptive Variable Names:** Maintain descriptive naming conventions to enhance code readability and facilitate easier maintenance.\n   - **Modularize Complex Segments:** Break down complex operations into smaller, modular functions to simplify understanding and future enhancements.\n\nBy addressing these recommendations, you can align the **FastTTTLinear** implementation more closely with the proposal's objectives, fully leveraging dynamic sparsity and FFT-based computations to achieve enhanced efficiency, scalability, and performance in the language model.\n\n---",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "test_fast_ttt_linear": "@gau_test\ndef test_FastTTTLinear_test_fast_ttt_linear(device=None, dtype=None) ->None:\n    embed_dim = 64\n    batch_size = 2\n    seq_len = 10\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(\n        batch_size, 1)\n    Z = {'position_ids': position_ids}\n    kwarg_all = {}\n    block_loc = 0, 1\n    fast_ttt_linear = FastTTTLinear(embed_dim=embed_dim, block_loc=\n        block_loc, kwarg_all=kwarg_all, device=device, dtype=dtype)\n    Y, Z_out = fast_ttt_linear(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n\n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n\n    **Key Features:**\n\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python-level loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n    - **Position-Aware Processing**: Incorporates position embeddings implemented via sinusoidal positional encoding.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary containing intermediate variables, may include 'position_ids'.\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Updated dictionary of intermediate variables.\n\n    **Example:**\n\n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> position_ids = torch.arange(0, 1024).unsqueeze(0).repeat(2, 1)\n        >>> Y, Z = fast_ttt_linear(X, position_ids=position_ids)\n\n    **References:**\n\n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4,\n        gradient_checkpointing=False, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n        self.gradient_checkpointing = gradient_checkpointing\n\n    def _forward(self, X, **Z):\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n\n    def _forward_impl(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        epsilon = 1e-06\n        if 'position_ids' in Z:\n            position_ids = Z['position_ids']\n            position_embeddings = self.get_position_embeddings(position_ids)\n            X = X + position_embeddings\n        X_conv = self.local_conv(X.transpose(1, 2)).transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        KV_cumsum = torch.cumsum(K_prime * V, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = (Q_prime * K_cumsum).sum(-1, keepdim=True) + epsilon\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.norm(output, **Z)\n        return output, Z\n\n    def get_position_embeddings(self, position_ids):\n        batch_size, seq_len = position_ids.shape\n        dim = self.embed_dim\n        inv_freq = 1.0 / 10000 ** (torch.arange(0.0, dim, 2.0, device=\n            position_ids.device).float() / dim)\n        sinusoid_inp = torch.einsum('b i, j -> b i j', position_ids.float(),\n            inv_freq)\n        pos_emb = torch.zeros(batch_size, seq_len, dim, device=position_ids\n            .device)\n        pos_emb[..., 0::2] = torch.sin(sinusoid_inp)\n        pos_emb[..., 1::2] = torch.cos(sinusoid_inp)\n        return pos_emb\n",
                "rating": 4.6,
                "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python-level loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n- **Position-Aware Processing**: Incorporates position embeddings implemented via sinusoidal positional encoding.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z**: Dictionary containing intermediate variables, may include 'position_ids'.\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z**: Updated dictionary of intermediate variables.\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> position_ids = torch.arange(0, 1024).unsqueeze(0).repeat(2, 1)\\n    >>> Y, Z = fast_ttt_linear(X, position_ids=position_ids)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "num_attention_heads": 4,
                    "gradient_checkpointing": false
                },
                "design_traces": null
            },
            "Conv": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "conv_kernel": 4,
                    "rms_norm_eps": 1e-06
                },
                "design_traces": null
            },
            "SwiGluMLP": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "intermediate_size": null
                },
                "design_traces": null
            }
        },
        "rating": null,
        "declares": {
            "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
            "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
        },
        "proposal_traces": [],
        "suggestions": null,
        "name": "dynamicsparsefftlm"
    },
    "status": "implemented",
    "history": [
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "```rating 4.6```\n\n---\n\n### **1. Overall Assessment**\n\nThe **FastTTTLinear** implementation exhibits significant advancements towards enhancing the GAU by integrating position-aware processing and maintaining efficient computation strategies. The code is well-organized, thoroughly documented, and incorporates critical optimizations such as gradient checkpointing and sinusoidal positional embeddings. However, further integration of dynamic sparsity and FFT-based computations, as outlined in the proposal \"DynamicSparseFFTLinear,\" remains to be addressed to fully realize the proposed enhancements.\n\n### **2. Strengths of the Implementation**\n\n1. **Comprehensive Documentation and Clarity:**\n   - Detailed docstrings provide clear explanations of the module's purpose, functionality, and usage.\n   - Key features are explicitly listed, facilitating easier understanding and onboarding for future developers.\n\n2. **Effective Code Organization:**\n   - The separation of the forward pass into `_forward_impl` enhances readability and maintainability.\n   - Logical structuring of layer initialization ensures clarity in the role of each component within the GAU.\n\n3. **Position-Aware Processing:**\n   - Implementation of sinusoidal positional embeddings within the `get_position_embeddings` method effectively integrates positional information.\n   - Position embeddings are seamlessly added to the input tensor, enhancing the model's ability to capture token positions.\n\n4. **Gradient Checkpointing Support:**\n   - The inclusion of gradient checkpointing optimizes memory usage during training, allowing for the handling of larger models or longer sequences without significant memory overhead.\n\n5. **Normalization and Stability Enhancements:**\n   - Application of LayerNorm to queries and keys (`q_norm` and `k_norm`) stabilizes attention computations.\n   - Use of ELU activation followed by an additive constant (`ELU + 1`) improves numerical stability and gradient flow.\n\n6. **Efficient Memory Utilization:**\n   - Vectorized operations eliminate Python-level loops, leveraging PyTorch's optimized tensor computations for better performance.\n   - Contiguous memory layouts in tensor reshaping and views enhance computational efficiency.\n\n7. **Integration with Existing GAUs:**\n   - Maintains a clean interface with the parent GAU, ensuring smooth integration within the larger model architecture.\n   - Proper handling of intermediate variables (`Z` dictionary) facilitates effective state management across layers.\n\n### **3. Areas for Improvement and Specific Suggestions**\n\n1. **Integration of Dynamic Sparsity:**\n   - **Implementation Need:** The current implementation lacks mechanisms for dynamic sparsity, which adaptively prunes less important tokens to optimize computation.\n   - **Suggestion:** Introduce an importance scoring mechanism to generate dynamic sparsity masks, selectively processing significant tokens.\n   - **Example:**\n     ```python\n     # Importance scoring\n     S = torch.sigmoid(self.score_net(X))  # S \u2208 \u211d^(B\u00d7L\u00d71)\n     \n     # Create dynamic sparsity mask\n     k = int(X.shape[1] * self.sparsity_ratio)\n     mask = torch.zeros_like(S)\n     topk_indices = S.topk(k, dim=1).indices\n     mask.scatter_(1, topk_indices, 1.0)\n     \n     # Apply mask\n     X_masked = X * mask\n     ```\n\n2. **Incorporation of FFT-Based Computation:**\n   - **Implementation Need:** The proposal emphasizes the use of FFT-based computations to handle long-range dependencies efficiently.\n   - **Suggestion:** Integrate FFT operations after applying dynamic sparsity masks to transform the input into the frequency domain and perform convolution.\n   - **Example:**\n     ```python\n     # Apply FFT\n     X_f = torch.fft.rfft(X_masked, dim=1)\n     \n     # FFT Convolution\n     Y_f = self.fft_conv(X_f)\n     Y = torch.fft.irfft(Y_f, dim=1)\n     \n     # Gated Output\n     G = torch.sigmoid(self.gate_net(Y))\n     O = G * Y\n     ```\n\n3. **Memory Optimization for Long Sequences:**\n   - **Implementation Need:** While gradient checkpointing aids memory efficiency, further optimizations can be introduced for handling very long sequences.\n   - **Suggestion:** Implement chunked processing where sequences are processed in manageable blocks, reducing peak memory usage.\n   - **Example:**\n     ```python\n     def _forward_impl(self, X, **Z):\n         B, L, D = X.size()\n         chunk_size = 512\n         outputs = []\n         for i in range(0, L, chunk_size):\n             chunk_end = min(i + chunk_size, L)\n             chunk_output = self._process_chunk(X[:, i:chunk_end], **Z)\n             outputs.append(chunk_output)\n         output = torch.cat(outputs, dim=1)\n         return output, Z\n     ```\n\n4. **Adaptive Computation Span Control:**\n   - **Implementation Need:** Enhance efficiency by making the attention span adaptive based on input importance.\n   - **Suggestion:** Introduce parameters that allow the attention span to adjust dynamically, possibly using learnable parameters to determine the span per attention head.\n   - **Example:**\n     ```python\n     def __init__(self, ...):\n         self.max_position = 2048\n         self.adaptive_span = nn.Parameter(torch.ones(num_attention_heads))\n     \n     def _compute_attention_mask(self, L):\n         span = torch.sigmoid(self.adaptive_span) * self.max_position\n         position_ids = torch.arange(L, device=span.device)\n         mask = position_ids[None, :] <= span[:, None]\n         return mask\n     ```\n\n5. **Enhanced Numerical Stability:**\n   - **Implementation Need:** While initial stability measures are present, additional safeguards can further ensure numerical robustness.\n   - **Suggestion:** Implement gradient clipping and monitoring of attention statistics to prevent instability during training.\n   - **Example:**\n     ```python\n     def _forward_impl(self, X, **Z):\n         ...\n         if self.training:\n             torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n             # Log attention statistics\n             self._log_attention_stats(Q, K, V)\n         ...\n     ```\n\n6. **Performance Monitoring and Debugging:**\n   - **Implementation Need:** Incorporate mechanisms to monitor performance metrics and debug potential issues.\n   - **Suggestion:** Add diagnostic features that track attention distributions, convergence metrics, and memory usage.\n   - **Example:**\n     ```python\n     def __init__(self, ...):\n         self.register_buffer('attention_stats', torch.zeros(4))  # Example buffer\n    \n     def _forward_impl(self, X, **Z):\n         ...\n         if self.training:\n             self.attention_stats += compute_attention_statistics(Q, K, V)\n         ...\n     ```\n\n7. **Completing and Optimizing Position Embeddings:**\n   - **Implementation Need:** Although position embeddings are implemented, ensure they are efficiently cached and do not add significant computational overhead.\n   - **Suggestion:** Implement caching mechanisms for position embeddings to prevent redundant computations during inference.\n   - **Example:**\n     ```python\n     def get_position_embeddings(self, position_ids):\n         # Cache position embeddings for fixed max sequence lengths\n         if not hasattr(self, 'cached_pos_emb'):\n             self.cached_pos_emb = self._generate_position_embeddings(max_len=self.max_position)\n         return self.cached_pos_emb[:, position_ids]\n     \n     def _generate_position_embeddings(self, max_len):\n         inv_freq = 1.0 / (10000 ** (torch.arange(0.0, self.head_dim, 2.0) / self.head_dim))\n         sinusoid_inp = torch.einsum('i,j->ij', torch.arange(max_len).float(), inv_freq)\n         pos_emb = torch.zeros(max_len, self.embed_dim)\n         pos_emb[:, 0::2] = torch.sin(sinusoid_inp)\n         pos_emb[:, 1::2] = torch.cos(sinusoid_inp)\n         return pos_emb.unsqueeze(0).to(inv_freq.device)\n     ```\n\n### **4. Comments on Innovation and Potential Impact**\n\n**Innovative Aspects:**\n- **Position-Aware Processing:** The integration of sinusoidal positional embeddings within FastTTTLinear is a robust approach to capturing token positions, enhancing the model's ability to understand sequence structure without relying on transformer-based positional encodings.\n- **Gradient Checkpointing:** Incorporating gradient checkpointing demonstrates an advanced optimization technique to manage memory efficiently during training, allowing for the scalability of larger models or longer sequences.\n- **Gated Linear Attention:** Utilizing gated mechanisms for queries and keys enhances the expressiveness and stability of the attention computations, building upon recent advancements in attention mechanisms.\n\n**Potential Impact:**\n- **Efficiency in Long Sequences:** By optimizing memory usage and introducing position-aware processing, FastTTTLinear can handle longer sequences more effectively, a critical capability for state-of-the-art language models.\n- **Scalability:** The thoughtful integration of memory optimization and gradient checkpointing allows the model to scale efficiently, accommodating both increased model sizes and expanded sequence lengths.\n- **Model Performance:** Enhancements in attention computation and position awareness are likely to result in improved performance on downstream tasks, particularly those requiring understanding of long-range dependencies.\n\n**Concerns about Integration and Scalability:**\n- **Dynamic Sparsity and FFT Integration:** The current implementation does not yet incorporate dynamic sparsity or FFT-based computations, which are pivotal for achieving the proposed efficiency and scalability improvements.\n- **Hardware Compatibility:** Integrating FFT-based operations and dynamic sparsity may necessitate careful consideration of hardware capabilities and optimizations to ensure these additions provide the intended performance benefits without introducing bottlenecks.\n\n### **5. Recommendations for the Coder**\n\n1. **Implement Dynamic Sparsity Mechanisms:**\n   - **Develop Importance Scoring:** Introduce a scoring mechanism to evaluate the importance of each token dynamically.\n   - **Generate Sparsity Masks:** Use the importance scores to create masks that selectively allow crucial tokens to be processed.\n   - **Integrate with Attention Computation:** Modify the attention mechanism to utilize these masks, thereby reducing computation on less important tokens.\n\n2. **Integrate FFT-Based Computations:**\n   - **Incorporate FFT Operations:** After applying dynamic sparsity masks, transform the input into the frequency domain using FFT to handle long-range dependencies efficiently.\n   - **Optimize FFT Convolutions:** Ensure that FFT-based convolutions are optimized for parallel computation and do not introduce significant computational overhead.\n\n3. **Enhance Memory Management:**\n   - **Chunk-Based Processing:** Implement chunked processing for very long sequences to prevent memory bottlenecks, processing sequences in manageable blocks.\n   - **Profile and Optimize Memory Usage:** Continuously monitor memory usage across different sequence lengths and optimize accordingly to maintain efficiency.\n\n4. **Reinforce Numerical Stability:**\n   - **Implement Gradient Clipping:** Introduce gradient clipping to prevent exploding gradients, especially when dealing with long sequences and complex attention mechanisms.\n   - **Monitor Attention Statistics:** Regularly log and monitor attention statistics to ensure consistent and stable training dynamics.\n\n5. **Expand Testing and Verification:**\n   - **Develop Specific Unit Tests:** Create unit tests targeting the newly integrated dynamic sparsity and FFT-based components to ensure their correct functionality.\n   - **Benchmark Performance:** Conduct extensive benchmarking with varying sequence lengths and model sizes to validate the efficiency and scalability improvements.\n\n6. **Improve Performance Monitoring and Debugging:**\n   - **Incorporate Logging Mechanisms:** Add logging for key performance indicators such as memory usage, computation time per layer, and attention distribution patterns.\n   - **Implement Diagnostic Features:** Develop features that can diagnose and report on the performance and stability of the attention mechanisms in real-time.\n\n7. **Maintain Code Readability and Documentation:**\n   - **Continue High Documentation Standards:** Ensure that all new components, especially those related to dynamic sparsity and FFT computations, are thoroughly documented.\n   - **Use Descriptive Variable Names:** Maintain descriptive naming conventions to enhance code readability and facilitate easier maintenance.\n   - **Modularize Complex Segments:** Break down complex operations into smaller, modular functions to simplify understanding and future enhancements.\n\nBy addressing these recommendations, you can align the **FastTTTLinear** implementation more closely with the proposal's objectives, fully leveraging dynamic sparsity and FFT-based computations to achieve enhanced efficiency, scalability, and performance in the language model.\n\n---",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_fast_ttt_linear": "@gau_test\ndef test_FastTTTLinear_test_fast_ttt_linear(device=None, dtype=None) ->None:\n    embed_dim = 64\n    batch_size = 2\n    seq_len = 10\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(\n        batch_size, 1)\n    Z = {'position_ids': position_ids}\n    kwarg_all = {}\n    block_loc = 0, 1\n    fast_ttt_linear = FastTTTLinear(embed_dim=embed_dim, block_loc=\n        block_loc, kwarg_all=kwarg_all, device=device, dtype=dtype)\n    Y, Z_out = fast_ttt_linear(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n\n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n\n    **Key Features:**\n\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python-level loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n    - **Position-Aware Processing**: Incorporates position embeddings implemented via sinusoidal positional encoding.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary containing intermediate variables, may include 'position_ids'.\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Updated dictionary of intermediate variables.\n\n    **Example:**\n\n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> position_ids = torch.arange(0, 1024).unsqueeze(0).repeat(2, 1)\n        >>> Y, Z = fast_ttt_linear(X, position_ids=position_ids)\n\n    **References:**\n\n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4,\n        gradient_checkpointing=False, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n        self.gradient_checkpointing = gradient_checkpointing\n\n    def _forward(self, X, **Z):\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n\n    def _forward_impl(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        epsilon = 1e-06\n        if 'position_ids' in Z:\n            position_ids = Z['position_ids']\n            position_embeddings = self.get_position_embeddings(position_ids)\n            X = X + position_embeddings\n        X_conv = self.local_conv(X.transpose(1, 2)).transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        KV_cumsum = torch.cumsum(K_prime * V, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = (Q_prime * K_cumsum).sum(-1, keepdim=True) + epsilon\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.norm(output, **Z)\n        return output, Z\n\n    def get_position_embeddings(self, position_ids):\n        batch_size, seq_len = position_ids.shape\n        dim = self.embed_dim\n        inv_freq = 1.0 / 10000 ** (torch.arange(0.0, dim, 2.0, device=\n            position_ids.device).float() / dim)\n        sinusoid_inp = torch.einsum('b i, j -> b i j', position_ids.float(),\n            inv_freq)\n        pos_emb = torch.zeros(batch_size, seq_len, dim, device=position_ids\n            .device)\n        pos_emb[..., 0::2] = torch.sin(sinusoid_inp)\n        pos_emb[..., 1::2] = torch.cos(sinusoid_inp)\n        return pos_emb\n",
                        "rating": 4.6,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python-level loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n- **Position-Aware Processing**: Incorporates position embeddings implemented via sinusoidal positional encoding.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z**: Dictionary containing intermediate variables, may include 'position_ids'.\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z**: Updated dictionary of intermediate variables.\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> position_ids = torch.arange(0, 1024).unsqueeze(0).repeat(2, 1)\\n    >>> Y, Z = fast_ttt_linear(X, position_ids=position_ids)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "num_attention_heads": 4,
                            "gradient_checkpointing": false
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "dynamicsparsefftlm"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
                    "IMPLEMENTATION_CODER": "o1_preview",
                    "PROPOSAL_REVIEWER": "gpt4o_0806",
                    "IMPLEMENTATION_OBSERVER": "o1_mini",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.098028,
                "IMPLEMENTATION_CODER": 0.95811,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 0.156171,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "```rating 4.6```\n\n---\n\n### **1. Overall Assessment**\n\nThe **FastTTTLinear** implementation exhibits significant advancements towards enhancing the GAU by integrating position-aware processing and maintaining efficient computation strategies. The code is well-organized, thoroughly documented, and incorporates critical optimizations such as gradient checkpointing and sinusoidal positional embeddings. However, further integration of dynamic sparsity and FFT-based computations, as outlined in the proposal \"DynamicSparseFFTLinear,\" remains to be addressed to fully realize the proposed enhancements.\n\n### **2. Strengths of the Implementation**\n\n1. **Comprehensive Documentation and Clarity:**\n   - Detailed docstrings provide clear explanations of the module's purpose, functionality, and usage.\n   - Key features are explicitly listed, facilitating easier understanding and onboarding for future developers.\n\n2. **Effective Code Organization:**\n   - The separation of the forward pass into `_forward_impl` enhances readability and maintainability.\n   - Logical structuring of layer initialization ensures clarity in the role of each component within the GAU.\n\n3. **Position-Aware Processing:**\n   - Implementation of sinusoidal positional embeddings within the `get_position_embeddings` method effectively integrates positional information.\n   - Position embeddings are seamlessly added to the input tensor, enhancing the model's ability to capture token positions.\n\n4. **Gradient Checkpointing Support:**\n   - The inclusion of gradient checkpointing optimizes memory usage during training, allowing for the handling of larger models or longer sequences without significant memory overhead.\n\n5. **Normalization and Stability Enhancements:**\n   - Application of LayerNorm to queries and keys (`q_norm` and `k_norm`) stabilizes attention computations.\n   - Use of ELU activation followed by an additive constant (`ELU + 1`) improves numerical stability and gradient flow.\n\n6. **Efficient Memory Utilization:**\n   - Vectorized operations eliminate Python-level loops, leveraging PyTorch's optimized tensor computations for better performance.\n   - Contiguous memory layouts in tensor reshaping and views enhance computational efficiency.\n\n7. **Integration with Existing GAUs:**\n   - Maintains a clean interface with the parent GAU, ensuring smooth integration within the larger model architecture.\n   - Proper handling of intermediate variables (`Z` dictionary) facilitates effective state management across layers.\n\n### **3. Areas for Improvement and Specific Suggestions**\n\n1. **Integration of Dynamic Sparsity:**\n   - **Implementation Need:** The current implementation lacks mechanisms for dynamic sparsity, which adaptively prunes less important tokens to optimize computation.\n   - **Suggestion:** Introduce an importance scoring mechanism to generate dynamic sparsity masks, selectively processing significant tokens.\n   - **Example:**\n     ```python\n     # Importance scoring\n     S = torch.sigmoid(self.score_net(X))  # S \u2208 \u211d^(B\u00d7L\u00d71)\n     \n     # Create dynamic sparsity mask\n     k = int(X.shape[1] * self.sparsity_ratio)\n     mask = torch.zeros_like(S)\n     topk_indices = S.topk(k, dim=1).indices\n     mask.scatter_(1, topk_indices, 1.0)\n     \n     # Apply mask\n     X_masked = X * mask\n     ```\n\n2. **Incorporation of FFT-Based Computation:**\n   - **Implementation Need:** The proposal emphasizes the use of FFT-based computations to handle long-range dependencies efficiently.\n   - **Suggestion:** Integrate FFT operations after applying dynamic sparsity masks to transform the input into the frequency domain and perform convolution.\n   - **Example:**\n     ```python\n     # Apply FFT\n     X_f = torch.fft.rfft(X_masked, dim=1)\n     \n     # FFT Convolution\n     Y_f = self.fft_conv(X_f)\n     Y = torch.fft.irfft(Y_f, dim=1)\n     \n     # Gated Output\n     G = torch.sigmoid(self.gate_net(Y))\n     O = G * Y\n     ```\n\n3. **Memory Optimization for Long Sequences:**\n   - **Implementation Need:** While gradient checkpointing aids memory efficiency, further optimizations can be introduced for handling very long sequences.\n   - **Suggestion:** Implement chunked processing where sequences are processed in manageable blocks, reducing peak memory usage.\n   - **Example:**\n     ```python\n     def _forward_impl(self, X, **Z):\n         B, L, D = X.size()\n         chunk_size = 512\n         outputs = []\n         for i in range(0, L, chunk_size):\n             chunk_end = min(i + chunk_size, L)\n             chunk_output = self._process_chunk(X[:, i:chunk_end], **Z)\n             outputs.append(chunk_output)\n         output = torch.cat(outputs, dim=1)\n         return output, Z\n     ```\n\n4. **Adaptive Computation Span Control:**\n   - **Implementation Need:** Enhance efficiency by making the attention span adaptive based on input importance.\n   - **Suggestion:** Introduce parameters that allow the attention span to adjust dynamically, possibly using learnable parameters to determine the span per attention head.\n   - **Example:**\n     ```python\n     def __init__(self, ...):\n         self.max_position = 2048\n         self.adaptive_span = nn.Parameter(torch.ones(num_attention_heads))\n     \n     def _compute_attention_mask(self, L):\n         span = torch.sigmoid(self.adaptive_span) * self.max_position\n         position_ids = torch.arange(L, device=span.device)\n         mask = position_ids[None, :] <= span[:, None]\n         return mask\n     ```\n\n5. **Enhanced Numerical Stability:**\n   - **Implementation Need:** While initial stability measures are present, additional safeguards can further ensure numerical robustness.\n   - **Suggestion:** Implement gradient clipping and monitoring of attention statistics to prevent instability during training.\n   - **Example:**\n     ```python\n     def _forward_impl(self, X, **Z):\n         ...\n         if self.training:\n             torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n             # Log attention statistics\n             self._log_attention_stats(Q, K, V)\n         ...\n     ```\n\n6. **Performance Monitoring and Debugging:**\n   - **Implementation Need:** Incorporate mechanisms to monitor performance metrics and debug potential issues.\n   - **Suggestion:** Add diagnostic features that track attention distributions, convergence metrics, and memory usage.\n   - **Example:**\n     ```python\n     def __init__(self, ...):\n         self.register_buffer('attention_stats', torch.zeros(4))  # Example buffer\n    \n     def _forward_impl(self, X, **Z):\n         ...\n         if self.training:\n             self.attention_stats += compute_attention_statistics(Q, K, V)\n         ...\n     ```\n\n7. **Completing and Optimizing Position Embeddings:**\n   - **Implementation Need:** Although position embeddings are implemented, ensure they are efficiently cached and do not add significant computational overhead.\n   - **Suggestion:** Implement caching mechanisms for position embeddings to prevent redundant computations during inference.\n   - **Example:**\n     ```python\n     def get_position_embeddings(self, position_ids):\n         # Cache position embeddings for fixed max sequence lengths\n         if not hasattr(self, 'cached_pos_emb'):\n             self.cached_pos_emb = self._generate_position_embeddings(max_len=self.max_position)\n         return self.cached_pos_emb[:, position_ids]\n     \n     def _generate_position_embeddings(self, max_len):\n         inv_freq = 1.0 / (10000 ** (torch.arange(0.0, self.head_dim, 2.0) / self.head_dim))\n         sinusoid_inp = torch.einsum('i,j->ij', torch.arange(max_len).float(), inv_freq)\n         pos_emb = torch.zeros(max_len, self.embed_dim)\n         pos_emb[:, 0::2] = torch.sin(sinusoid_inp)\n         pos_emb[:, 1::2] = torch.cos(sinusoid_inp)\n         return pos_emb.unsqueeze(0).to(inv_freq.device)\n     ```\n\n### **4. Comments on Innovation and Potential Impact**\n\n**Innovative Aspects:**\n- **Position-Aware Processing:** The integration of sinusoidal positional embeddings within FastTTTLinear is a robust approach to capturing token positions, enhancing the model's ability to understand sequence structure without relying on transformer-based positional encodings.\n- **Gradient Checkpointing:** Incorporating gradient checkpointing demonstrates an advanced optimization technique to manage memory efficiently during training, allowing for the scalability of larger models or longer sequences.\n- **Gated Linear Attention:** Utilizing gated mechanisms for queries and keys enhances the expressiveness and stability of the attention computations, building upon recent advancements in attention mechanisms.\n\n**Potential Impact:**\n- **Efficiency in Long Sequences:** By optimizing memory usage and introducing position-aware processing, FastTTTLinear can handle longer sequences more effectively, a critical capability for state-of-the-art language models.\n- **Scalability:** The thoughtful integration of memory optimization and gradient checkpointing allows the model to scale efficiently, accommodating both increased model sizes and expanded sequence lengths.\n- **Model Performance:** Enhancements in attention computation and position awareness are likely to result in improved performance on downstream tasks, particularly those requiring understanding of long-range dependencies.\n\n**Concerns about Integration and Scalability:**\n- **Dynamic Sparsity and FFT Integration:** The current implementation does not yet incorporate dynamic sparsity or FFT-based computations, which are pivotal for achieving the proposed efficiency and scalability improvements.\n- **Hardware Compatibility:** Integrating FFT-based operations and dynamic sparsity may necessitate careful consideration of hardware capabilities and optimizations to ensure these additions provide the intended performance benefits without introducing bottlenecks.\n\n### **5. Recommendations for the Coder**\n\n1. **Implement Dynamic Sparsity Mechanisms:**\n   - **Develop Importance Scoring:** Introduce a scoring mechanism to evaluate the importance of each token dynamically.\n   - **Generate Sparsity Masks:** Use the importance scores to create masks that selectively allow crucial tokens to be processed.\n   - **Integrate with Attention Computation:** Modify the attention mechanism to utilize these masks, thereby reducing computation on less important tokens.\n\n2. **Integrate FFT-Based Computations:**\n   - **Incorporate FFT Operations:** After applying dynamic sparsity masks, transform the input into the frequency domain using FFT to handle long-range dependencies efficiently.\n   - **Optimize FFT Convolutions:** Ensure that FFT-based convolutions are optimized for parallel computation and do not introduce significant computational overhead.\n\n3. **Enhance Memory Management:**\n   - **Chunk-Based Processing:** Implement chunked processing for very long sequences to prevent memory bottlenecks, processing sequences in manageable blocks.\n   - **Profile and Optimize Memory Usage:** Continuously monitor memory usage across different sequence lengths and optimize accordingly to maintain efficiency.\n\n4. **Reinforce Numerical Stability:**\n   - **Implement Gradient Clipping:** Introduce gradient clipping to prevent exploding gradients, especially when dealing with long sequences and complex attention mechanisms.\n   - **Monitor Attention Statistics:** Regularly log and monitor attention statistics to ensure consistent and stable training dynamics.\n\n5. **Expand Testing and Verification:**\n   - **Develop Specific Unit Tests:** Create unit tests targeting the newly integrated dynamic sparsity and FFT-based components to ensure their correct functionality.\n   - **Benchmark Performance:** Conduct extensive benchmarking with varying sequence lengths and model sizes to validate the efficiency and scalability improvements.\n\n6. **Improve Performance Monitoring and Debugging:**\n   - **Incorporate Logging Mechanisms:** Add logging for key performance indicators such as memory usage, computation time per layer, and attention distribution patterns.\n   - **Implement Diagnostic Features:** Develop features that can diagnose and report on the performance and stability of the attention mechanisms in real-time.\n\n7. **Maintain Code Readability and Documentation:**\n   - **Continue High Documentation Standards:** Ensure that all new components, especially those related to dynamic sparsity and FFT computations, are thoroughly documented.\n   - **Use Descriptive Variable Names:** Maintain descriptive naming conventions to enhance code readability and facilitate easier maintenance.\n   - **Modularize Complex Segments:** Break down complex operations into smaller, modular functions to simplify understanding and future enhancements.\n\nBy addressing these recommendations, you can align the **FastTTTLinear** implementation more closely with the proposal's objectives, fully leveraging dynamic sparsity and FFT-based computations to achieve enhanced efficiency, scalability, and performance in the language model.\n\n---",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_fast_ttt_linear": "@gau_test\ndef test_FastTTTLinear_test_fast_ttt_linear(device=None, dtype=None) ->None:\n    embed_dim = 64\n    batch_size = 2\n    seq_len = 10\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    position_ids = torch.arange(seq_len, device=device).unsqueeze(0).repeat(\n        batch_size, 1)\n    Z = {'position_ids': position_ids}\n    kwarg_all = {}\n    block_loc = 0, 1\n    fast_ttt_linear = FastTTTLinear(embed_dim=embed_dim, block_loc=\n        block_loc, kwarg_all=kwarg_all, device=device, dtype=dtype)\n    Y, Z_out = fast_ttt_linear(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    **FastTTTLinear**\n\n    FastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\n    and concepts from the RWKV architecture to enhance computational efficiency for long sequences.\n    This implementation addresses inefficiency concerns by vectorizing operations, eliminating\n    Python-level for-loops, and optimizing tensor computations.\n\n    **Key Features:**\n\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\n    - **Vectorized Computations**: Eliminates Python-level loops by using efficient tensor operations.\n    - **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\n    - **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\n    - **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\n    - **Position-Aware Processing**: Incorporates position embeddings implemented via sinusoidal positional encoding.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device on which to allocate tensors.\n        dtype (torch.dtype, optional): Data type of the tensors.\n        num_attention_heads (int, optional): Number of attention heads. Default: 4.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary containing intermediate variables, may include 'position_ids'.\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Updated dictionary of intermediate variables.\n\n    **Example:**\n\n        >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> position_ids = torch.arange(0, 1024).unsqueeze(0).repeat(2, 1)\n        >>> Y, Z = fast_ttt_linear(X, position_ids=position_ids)\n\n    **References:**\n\n    - Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\n    - Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4,\n        gradient_checkpointing=False, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.local_conv = nn.Conv1d(in_channels=embed_dim, out_channels=\n            embed_dim, kernel_size=3, padding=2, bias=True, **self.\n            factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        nn.init.xavier_uniform_(self.W_Q.weight)\n        nn.init.xavier_uniform_(self.W_K.weight)\n        nn.init.xavier_uniform_(self.W_V.weight)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.gate_Q.weight)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n        self.gradient_checkpointing = gradient_checkpointing\n\n    def _forward(self, X, **Z):\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n\n    def _forward_impl(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        epsilon = 1e-06\n        if 'position_ids' in Z:\n            position_ids = Z['position_ids']\n            position_embeddings = self.get_position_embeddings(position_ids)\n            X = X + position_embeddings\n        X_conv = self.local_conv(X.transpose(1, 2)).transpose(1, 2)[:, :L, :]\n        X = X + X_conv\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        V = self.W_V(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        KV_cumsum = torch.cumsum(K_prime * V, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = (Q_prime * K_cumsum).sum(-1, keepdim=True) + epsilon\n        output = numerator / denominator\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output, Z = self.norm(output, **Z)\n        return output, Z\n\n    def get_position_embeddings(self, position_ids):\n        batch_size, seq_len = position_ids.shape\n        dim = self.embed_dim\n        inv_freq = 1.0 / 10000 ** (torch.arange(0.0, dim, 2.0, device=\n            position_ids.device).float() / dim)\n        sinusoid_inp = torch.einsum('b i, j -> b i j', position_ids.float(),\n            inv_freq)\n        pos_emb = torch.zeros(batch_size, seq_len, dim, device=position_ids\n            .device)\n        pos_emb[..., 0::2] = torch.sin(sinusoid_inp)\n        pos_emb[..., 1::2] = torch.cos(sinusoid_inp)\n        return pos_emb\n",
                        "rating": 4.6,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"**FastTTTLinear**\\n\\nFastTTTLinear is a modified version of TTTLinear that integrates Gated Linear Attention (GLA)\\nand concepts from the RWKV architecture to enhance computational efficiency for long sequences.\\nThis implementation addresses inefficiency concerns by vectorizing operations, eliminating\\nPython-level for-loops, and optimizing tensor computations.\\n\\n**Key Features:**\\n\\n- **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enabling linear attention computation.\\n- **Vectorized Computations**: Eliminates Python-level loops by using efficient tensor operations.\\n- **Normalization**: Applies LayerNorm to queries and keys to stabilize computations.\\n- **Adjustments for Numerical Stability**: Uses appropriate scaling, activation functions, and safeguards.\\n- **Local Convolutional Augmentation**: Applies causal convolution to prevent information leakage and enhance local context.\\n- **Position-Aware Processing**: Incorporates position embeddings implemented via sinusoidal positional encoding.\\n\\n**Args:**\\n    embed_dim (int): Embedding dimension.\\n    block_loc (tuple): Location of this block in the model architecture.\\n    kwarg_all (dict): Additional keyword arguments.\\n    device (torch.device, optional): Device on which to allocate tensors.\\n    dtype (torch.dtype, optional): Data type of the tensors.\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4.\\n\\n**Inputs:**\\n    - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z**: Dictionary containing intermediate variables, may include 'position_ids'.\\n\\n**Outputs:**\\n    - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\\n    - **Z**: Updated dictionary of intermediate variables.\\n\\n**Example:**\\n\\n    >>> fast_ttt_linear = FastTTTLinear(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> X = torch.randn(2, 1024, 512)\\n    >>> position_ids = torch.arange(0, 1024).unsqueeze(0).repeat(2, 1)\\n    >>> Y, Z = fast_ttt_linear(X, position_ids=position_ids)\\n\\n**References:**\\n\\n- Yang, S., et al. (2023). *Gated Linear Attention Transformers with Hardware-Efficient Training*.\\n- Peng, B., et al. (2023). *RWKV: Reinventing RNNs for the Transformer Era*.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "num_attention_heads": 4,
                            "gradient_checkpointing": false
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "dynamicsparsefftlm"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
                    "IMPLEMENTATION_CODER": "o1_preview",
                    "PROPOSAL_REVIEWER": "gpt4o_0806",
                    "IMPLEMENTATION_OBSERVER": "o1_mini",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.098028,
                "IMPLEMENTATION_CODER": 0.95811,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 0.156171,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}