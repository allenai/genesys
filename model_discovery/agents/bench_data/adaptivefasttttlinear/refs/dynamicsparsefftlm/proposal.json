{
    "variantname": "DynamicSparseFFTLinear",
    "review": "# Review of \"DynamicSparseFFTLinear: Enhancing FastTTTLinear with Dynamic Sparsity and Efficient FFT Computation\"\n\n## 1. Summary and Implications of Search Results\n\nThe proposal introduces **DynamicSparseFFTLinear**, aiming to enhance the existing **FastTTTLinear** GAU by integrating **dynamic sparsity** and **efficient FFT-based computation**. The primary objectives are to:\n\n- **Improve performance on long sequences** by reducing computational and memory requirements.\n- **Combine selective computation** through learned sparsity patterns with hardware-efficient FFT implementations.\n- **Maintain or improve model accuracy and robustness** while enhancing efficiency and scalability.\n\n**Novelty and Meaningfulness:**\n\nBased on an extensive review of the literature and existing research:\n\n- **Dynamic sparsity** and **FFT-based computations** have individually been explored to improve efficiency in neural networks and Transformer architectures.\n- **Combining dynamic sparsity with FFT-based computation** within Transformer models or GAUs appears to be **novel**, with limited evidence of prior work directly addressing this integration.\n- The proposal aligns with current research trends aiming to make Transformers more efficient and scalable, particularly for long-sequence processing.\n\n**Implications:**\n\n- The integration of these two techniques could lead to substantial improvements in efficiency and scalability, enabling models to handle longer sequences with reduced computational resources.\n- However, combining dynamic sparsity with FFT introduces challenges in implementation, hardware compatibility, and training stability.\n\n---\n\n## 2. Comprehensive Analysis\n\n### **Clarity (4.0/5):**\n\n**Strengths:**\n\n- The proposal is **clearly articulated**, with well-defined objectives to enhance FastTTTLinear by integrating dynamic sparsity and FFT-based computation.\n- It provides detailed mathematical formulations and implementation guidelines, including pseudo-code examples.\n- The design outlines specific steps for integrating dynamic sparsity and FFT, explaining how each component contributes to improved efficiency.\n\n**Concerns:**\n\n- The explanation of how dynamic sparsity patterns align with FFT computations could be elaborated further to enhance understanding.\n- Details on how training stability will be maintained amidst the complexity introduced by combining these techniques are limited.\n\n### **Innovation (4.5/5):**\n\n**Strengths:**\n\n- The proposal appears to be **novel**, as existing literature does not report combining dynamic sparsity with FFT-based computation in Transformer models or GAUs.\n- It introduces an innovative approach to efficient sequence modeling, potentially contributing significant advancements to the field.\n- By addressing both computational efficiency and scalability, the proposal tackles key challenges in modern language modeling.\n\n**Comparison with Existing Research:**\n\n- While dynamic sparsity and FFT-based methods have individually been explored (e.g., Roy et al., 2020; He et al., 2023), their integration within a GAU like FastTTTLinear is a unique contribution.\n- The proposal builds upon current research trends but extends them in a novel direction.\n\n### **Feasibility (3.5/5):**\n\n**Strengths:**\n\n- The proposal includes detailed mathematical formulations and pseudo-code, demonstrating a clear implementation pathway.\n- The integration maintains the structure of FastTTTLinear while introducing new components, suggesting compatibility within the existing framework.\n\n**Concerns:**\n\n- **Implementation Challenges:**\n\n  - Aligning dynamically changing sparsity patterns with FFT computations is non-trivial due to FFT's requirement for regular, structured data.\n  - Sparse FFT algorithms are less commonly used and may present challenges in efficient implementation on current hardware.\n\n- **Hardware Compatibility:**\n\n  - GPUs are typically optimized for dense operations; implementing sparse FFT may require specialized hardware or lead to suboptimal performance.\n  - Memory access patterns and efficient use of tensor cores need careful consideration.\n\n- **Training Stability:**\n\n  - Combining these techniques may introduce complexities that affect training convergence and stability.\n  - Handling potential numerical issues in FFT operations with sparse inputs requires careful design.\n\n### **Scalability (4.0/5):**\n\n**Strengths:**\n\n- The design aims to improve scalability by reducing computational complexity and memory usage, particularly beneficial for long-sequence processing.\n- Dynamic sparsity allows the model to focus computational resources on important inputs, enhancing efficiency as model and data sizes grow.\n\n**Concerns:**\n\n- The scalability benefits depend on successful implementation of sparse FFT on available hardware.\n- Potential overheads in managing dynamic sparsity and FFT operations may offset some efficiency gains if not carefully optimized.\n\n### **Accuracy and Robustness (3.8/5):**\n\n**Strengths:**\n\n- The proposal aims to maintain or improve model accuracy by using learned importance scores to focus on relevant inputs.\n- By preserving gradient flow through sparsity masks and gating mechanisms, the design seeks to ensure robust learning.\n\n**Concerns:**\n\n- Introducing sparsity may lead to information loss if important inputs are incorrectly pruned.\n- FFT-based computations may introduce numerical stability issues, potentially affecting model robustness.\n- Ensuring that dynamic sparsity does not negatively impact the model's ability to generalize requires careful validation.\n\n### **Efficiency (4.2/5):**\n\n**Strengths:**\n\n- Combining dynamic sparsity with FFT-based computation has the potential to significantly reduce computational load and memory usage.\n- The proposal directly addresses efficiency challenges in processing long sequences, a critical concern in large-scale language models.\n- Hardware-aware optimizations are suggested to improve practical efficiency.\n\n**Concerns:**\n\n- The actual efficiency gains may depend on the effectiveness of sparse FFT implementations and hardware support.\n- Overheads in managing dynamic sparsity patterns and integrating FFT operations may diminish expected improvements if not optimized.\n\n---\n\n## 3. Suggestions for Improvement\n\n1. **Elaborate on Sparse FFT Implementation:**\n\n   - Provide more detailed strategies for implementing sparse FFT computations efficiently on existing hardware.\n   - Explore whether specialized algorithms or approximations can mitigate potential overheads.\n\n2. **Address Training Stability:**\n\n   - Discuss methods to ensure training stability when combining dynamic sparsity and FFT, such as initialization schemes, regularization techniques, or gradient clipping.\n   - Include plans for handling numerical issues that may arise from FFT computations with sparse inputs.\n\n3. **Hardware Considerations:**\n\n   - Assess the compatibility of the proposed design with common GPU architectures.\n   - Explore potential hardware optimizations or the need for specialized hardware to support the integration effectively.\n\n4. **Empirical Validation Plan:**\n\n   - Outline a plan for developing prototypes and conducting experiments to empirically validate the proposed benefits.\n   - Include benchmarks for measuring efficiency gains, accuracy, and robustness compared to existing models.\n\n5. **Robustness to Sparsity Decisions:**\n\n   - Describe methods to prevent important information from being pruned due to incorrect importance scoring.\n   - Consider incorporating mechanisms to recover or adjust sparsity patterns during training.\n\n6. **Clarify Integration Details:**\n\n   - Provide more in-depth explanations of how dynamic sparsity masks will be synchronized with FFT operations.\n   - Discuss whether sparsity patterns will be structured in a way that aligns with FFT requirements or how FFT algorithms will be adapted.\n\n7. **Related Work Comparison:**\n\n   - Include a more comprehensive comparison with existing sparse attention methods and FFT-based models, highlighting key differences and advantages.\n\n---\n\n## 4. Final Rating\n\n**Overall Rating:**\n\n```rating 4.0```\n\n---\n\n## **Justification:**\n\nThe proposal demonstrates **strong innovation** by integrating dynamic sparsity with FFT-based computation in a GAU, addressing important challenges in efficiency and scalability for long-sequence processing. It is **clearly articulated** and has the potential to make a significant impact on the field.\n\nHowever, **feasibility concerns** regarding implementation challenges, training stability, and hardware compatibility need to be addressed. The proposal would benefit from a more detailed plan for overcoming these challenges and from empirical validation to demonstrate its practical viability.\n\nWith these improvements, the proposal could offer a valuable contribution to efficient language modeling and inspire further research in combining sparsity and FFT techniques.",
    "search_stack": [
        {
            "ready": false,
            "query": "state space models attention efficiency",
            "detail": "Analyze implementations and performance characteristics of state space models and attention mechanisms in language models, focusing on efficiency and scalability",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations and performance characteristics of state space models and attention mechanisms in language models, focusing on efficiency and scalability\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 1.00)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 2/35 (Score: 1.00)*\n\n```\nHowever, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State space $\\underline{A} u g m e n t e \\underline{D}$ TransformEr. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks. ## 1 Introduction\n\nTransformer models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019; He et al., 2021). These models leverage the attention mechanism (Vaswani et al., 2017), which computes a dependency score for every pair\n\n[^0]of tokens in an input sequence. Therefore, full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019). For example, empirically we find that a Transformer model ( 250 M parameters) consumes over 80 G of GPU memory when the sequence length is 8 k . Additionally, Transformer models equipped with the full attention are easy to overfit because of the lack of structural biases (Lin et al., 2022). That is, the attention mechanism does not assume any structural prior over the inputs. For example, we even need order information (e.g., through sinusoidal encoding) to train a Transformer model. Therefore, the full attention is too flexible such that Transformer models may easily overfit to the noise. This significantly limits the models' practicality in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is often low. Empirically, we find that on a two-way classification task, Transformer with the full attention has a $57.5 \\%$ accuracy, nearly $30 \\%$ less than stateof-the-art methods with powerful structural biases (see Section 4.1 for details). Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases. In approximation methods, we approximate the full attention using fast algorithms with linear complexity. For example, we can approximate and speedup the computation of the attention score matrix (i.e., $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right)$ in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021). However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue. To incorporate structural biases to the Transformer model, partial attention methods are pro-\nposed. Such methods can be further categorized into sparse attention and clustering methods. In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns. In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed. However, the introduced structural biases restrict the models' ability to capture global information. For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information. Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation. These models can be seen as linear recurrent neural networks with specifically designed fixed weights. Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information. This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens. We propose SPADE, short for State space $\\underline{\\text { Augmente }} \\underline{\\mathbf{D}}$ TransformEr. The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information. In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods. We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and finetuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Softmax Attention with Constant Cost per Token (Avg. Score: 0.98)\n\n*Franz A. Heinsen*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work proposes a simple modification to the conventional attention mechanism applied by Transformers, which quantifies pairwise query-key similarity with scaled dot-products with the logarithms of scaled dot-products of exponentials, and linearizes attention with exponential kernel feature maps.\n\n**Abstract:** We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Our modification linearizes attention with exponential kernel feature maps, whose corresponding feature function is infinite dimensional. We show that our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size, enabling application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.\n\n##### *Relevant Chunk: No. 3/8 (Score: 0.98)*\n\n```\n(2021), and Poli et al. (2023). More recently, generalized state space models that build on previous research (Martin and Cundy, 2017) (Gu et al., 2021) have shown promise by incorporating data-driven mechanisms to control the evolution of a fixed-size latent state (Peng et al., 2023) (Gu and Dao, 2023) (Katsch, 2023), but their performance is inferior on certain tasks (e.g., recalling arbitrary parts of the input context), motivating the hypothesis that methods with a fixed-size latent space cannot outperform conventional attention (Jelassi et al., 2024). ### 1.1 Modifying Attention\n\nWe find that a simple modification to conventional attention linearizes it (Katharopoulos et al., 2020) with exponential kernel feature maps, and we show that this modification renders attention expressible as a composition of log-sums of exponentials, with a fixed-size latent space, for sequential application with constant cost per token. We implement our modification, verify that it works, and conclude that it is a promising alternative. The modification we propose is:\n\n$$\n\\begin{gathered}\n\\text { modified }(Q, K, V):= \\\\\n\\text { Attention }(Q) \\\\\n\\text { Softmax }\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V\n\\end{gathered}\n$$\n\nwhere queries $Q$, keys $K$ and values $V$ have $n_{Q} \\times d_{K}, n_{K} \\times d_{K}$, and $n_{K} \\times d_{V}$ elements, respectively, and $c$ is a scalar constant, all in $\\mathbb{R}$. We compute all exponentials elementwise. ### 1.2 As Log-Sums of Exponentials\n\nIn Section 2, we prove that\n\n$$\n\\begin{aligned}\n& \\text { modified } \\\\\n& \\operatorname{Attention}(Q, K, V)=\\exp (\\log S-\\log Z) \\text {, }\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nThe elementwise sums are over compatible dimensions, broadcasting over all other dimensions, from left to right-e.g., before reduction, the broadcasted elementwise sum $K^{T}+\\log V$ has $d_{K} \\times n_{K} \\times d_{V}$ elements. The functions $\\operatorname{LSE}_{\\left[d_{K}\\right]}(\\cdot)$ and $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$ compute log-sums of exponentials over the dimension indexed by $\\left(1,2, \\ldots, d_{K}\\right)$ and $\\left(1,2, \\ldots, n_{K}\\right)$, respectively. If any of $V$ 's elements are negative, $\\log V$ is complex, and therefore so is $\\log S$, but all Softmax mixtures of $V$ remain over $\\mathbb{R}$ because they are a composition of operations under which $\\mathbb{R}$ is closed (1). ### 1.3 Autoregressive Case\n\nFor autoregressive attention, in which $n_{Q}=n_{K}$ and for each query at step $t$ we compute attention only over $t$ trailing tokens, we note that in (3), all sequential dependencies are modeled by the logsums computed with $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$, so we can compute autoregressive $\\log S$ and $\\log Z$ with:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times n_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\times n_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nwhere the function $\\operatorname{LCSE}_{\\left[n_{K}\\right]}(\\cdot)$ computes a logcumulative-sum of exponentials over the dimension indexed by and $\\left(1,2, \\ldots, n_{K}\\right)$. For sequential application, given a new query $Q_{t}$ at step $t$, we need only the end-states of the two log-cumulative-sums of exponentials:\n\n$$\n\\begin{aligned}\n& \\log S_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(S)}}_{d_{K} \\times d_{V}}) \\\\\n& \\log Z_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(Z)}}_{d_{K}})\n\\end{aligned}\n$$\n\nwhere hidden states $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the states of the two log-cumulative-sums at step $t$ :\n\n$$\n\\begin{aligned}\n& H_{t}^{(S)}=\\log \\left(\\exp \\left(H_{t-1}^{(S)}\\right)+\\exp \\left(K_{t}+\\log V_{t}\\right)\\right) \\\\\n& H_{t}^{(Z)}=\\log \\left(\\exp \\left(H_{t-1}^{(Z)}\\right)+\\exp \\left(K_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nwith zeros as their initial condition:\n\n$$\n\\begin{aligned}\n& H_{0}^{(S)}=\\{0\\}^{d_{K} \\times d_{V}} \\\\\n& H_{0}^{(Z)}=\\{0\\}^{d_{K}}\n\\end{aligned}\n$$\n\nTogether, $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ hold the latent, or hidden, state of autoregressive attention's computation at step $t$. They enable us to compute autoregressive attention sequentially with constant time and space complexity per token, $\\mathcal{O}(1)$. ### 1.4 Non-Autoregressive Case\n\nFor non-autoregressive attention, in which $n_{Q}$ may differ from $n_{K}$ and for each query we compute attention over all tokens in the sequence, we compute $\\log S$ and $\\log Z$ with (3). For sequential application, in which we add a new token to the input context at step $t$, with key $K_{t}$ and value $V_{t}$, we compute $\\log S$ and $\\log Z$ for all queries from the updated hidden states:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(S)}\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(Z)}\\right)\n\\end{aligned}\n$$\n\nwhere $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the hidden states at step $t$ (6), with zeros as their initial condition (7). ## 2 Proof\n\nGiven a query $q$ and a key $k$ in $\\mathbb{R}^{d_{K}}$, the logarithm of the dot-product of their exponentials is $\\log \\left(\\sum(\\exp (q) \\odot \\exp (k))\\right)=\\operatorname{LSE}(q+k)$, where $\\odot$ denotes an elementwise product. Log-sums of exponentials are associative and commutative, making the proof fairly straightforward. For clarity's sake, we walk step-by-step through a sequence of algebraic manipulations. We start by expanding the Softmax function in (1) and simplifying the resulting expression. We obtain a form of linear attention (Katharopoulos et al., 2020) with exponential kernel feature maps:\n\n$$\n\\begin{gathered}\n\\operatorname{Softmax}\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V= \\\\\n{\\left[\\frac{\\exp (Q) \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\exp (Q) \\exp (K)^{T}}\\right] V}\n\\end{gathered}\n$$\n\nwhere $\\sum_{\\left[n_{K}\\right]}$ normalizes each row to a probability distribution. The scaling constant $\\exp (c)$ disappears because it becomes a common divisor of numerator and denominator expressions. Note that the feature function corresponding to the exponential kernel is infinite dimensional. Substitute the dot-products of exponentiated queries and exponentiated keys with equivalent explicit summations over elementwise products:\n\n$$\n\\left[\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\\right] V\n$$\n\nwhere the elementwise product $\\odot$ is over compatible dimensions, broadcasting over any other dimensions, from left to right, such that the broadcasted elementwise product $\\exp (Q) \\odot \\exp (K)^{T}$ has $n_{Q} \\times d_{K} \\times n_{K}$ elements. ${ }^{2}$\nExpress matrix multiplication with $V$ as a summation over broadcasted elementwise products:\n\n$$\n\\frac{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T} \\odot V}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\n$$\n\nBoth $\\exp (K)^{T}$ and $V$ have a dimension indexed by $\\left(1,2, \\ldots, n_{K}\\right)$, but $\\exp (Q)$ does not, so we can sum over that dimension before broadcastmultiplying elementwise with $\\exp (Q)$ :\n\n$$\n\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V}{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}}\n$$\n\nDefine $S$ and $Z$ as the expressions that compute numerators and denominators, respectively,\n\n$$\n\\begin{aligned}\n& S:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V \\\\\n& Z:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}\n\\end{aligned}\n$$\n\nand take their logarithms. We obtain:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)\\right),\n\\end{aligned}\n$$\n\nwhich is the same as (3). [^1]\n## 3 Implementation\n\nAs proof of concept, we implement our attention mechanism for both autoregressive applications (e.g., generative language modeling) and nonautoregressive applications (e.g., masked language modeling). For simplicity and expediency, we limit our implementation in two significant ways: First, we restrict $V$ to elements $\\geq 0$ to avoid dealing with complex floating-point numbers, which incur greater overhead and are more cumbersome to manipulate than real floating-point numbers with existing software infrastructure. Second, when computing autoregressive attention over $n_{K}$ tokens, we first compute all $n_{K}$ hidden states with a parallel scan, and then reduce them, which is space-inefficient but easier to implement with existing software infrastructure. ${ }^{3}$\n\nWe apply our implementation in a small generative language model ( 125 M parameters, 50257 token ids, 768 embedding features). For numerical stability, in each layer we compute $\\log V$ over $\\mathbb{R}$ directly, with a dense feed-forward transformation of token states, implicitly defining $V$ as $\\log V$ 's exponential but never actually computing it. To remain in $\\mathbb{R}$, we use the logarithm of attention as input to subsequent transformations in the layer, i.e., the input to subsequent transformations is $\\log S-\\log Z$ instead of $\\exp (\\log S-\\log Z)$. Please see our published code for all model details. We train the model on 300B tokens from The Pile (Gao et al., 2020) with a conventional sequence length of 1024 tokens, and obtain a crossentropy loss of 2.47 , competitive with state-of-theart generative language models of similar size. ## 4 Conclusions\n\nBy all indications, our attention mechanism is a promising alternative to the conventional one, but the evidence we have so far is too scant to be conclusive. An adequate comparison requires addressing our implementation's temporary limitations and evaluating models with one to several orders of magnitude more parameters on a diverse set of benchmarks and downstream tasks. [^2]\n## References\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR abs/1904.10509. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.98)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.98)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 5. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.97)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.97)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space models attention efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: state space models attention efficiency*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 361  (*Influential: 58*)\n\n#### 2. Is Space-Time Attention All You Need for Video Understanding?\n\n*From Search Query: state space models attention efficiency*\n\n*Gedas Bertasius, Heng Wang, L. Torresani*\n\n**TL;DR:** This work presents a convolution-free approach to video classification built exclusively on self-attention over space and time, which adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches.\n\n**Abstract:** We present a convolution-free approach to video classification built exclusively on self-attention over space and time. Our method, named\"TimeSformer,\"adapts the standard Transformer architecture to video by enabling spatiotemporal feature learning directly from a sequence of frame-level patches. Our experimental study compares different self-attention schemes and suggests that\"divided attention,\"where temporal attention and spatial attention are separately applied within each block, leads to the best video classification accuracy among the design choices considered. Despite the radically new design, TimeSformer achieves state-of-the-art results on several action recognition benchmarks, including the best reported accuracy on Kinetics-400 and Kinetics-600. Finally, compared to 3D convolutional networks, our model is faster to train, it can achieve dramatically higher test efficiency (at a small drop in accuracy), and it can also be applied to much longer video clips (over one minute long). Code and models are available at: https://github.com/facebookresearch/TimeSformer.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 1657  (*Influential: 266*)\n\n#### 3. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: state space models attention efficiency*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. AudioLDM: Text-to-Audio Generation with Latent Diffusion Models\n\n*From Search Query: state space models attention efficiency*\n\n*Haohe Liu, Zehua Chen, Yiitan Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, M. Plumbley*\n\n**TL;DR:** AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents, is proposed and achieves state-of-the-art TTA performance measured by both objective and subjective metrics.\n\n**Abstract:** Text-to-audio (TTA) system has recently gained attention for its ability to synthesize general audio based on text descriptions. However, previous studies in TTA have limited generation quality with high computational costs. In this study, we propose AudioLDM, a TTA system that is built on a latent space to learn the continuous audio representations from contrastive language-audio pretraining (CLAP) latents. The pretrained CLAP models enable us to train LDMs with audio embedding while providing text embedding as a condition during sampling. By learning the latent representations of audio signals and their compositions without modeling the cross-modal relationship, AudioLDM is advantageous in both generation quality and computational efficiency. Trained on AudioCaps with a single GPU, AudioLDM achieves state-of-the-art TTA performance measured by both objective and subjective metrics (e.g., frechet distance). Moreover, AudioLDM is the first TTA system that enables various text-guided audio manipulations (e.g., style transfer) in a zero-shot fashion. Our implementation and demos are available at https://audioldm.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 382  (*Influential: 59*)\n\n#### 5. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: state space models attention efficiency*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Mega: Moving Average Equipped Gated Attention\n\n*From Search Query: state space models attention efficiency*\n\n*Luke Zettlemoyer, Jonathan May, Graham Neubig, Liangke Gui, Junxian He, Xiang Kong, Chunting Zhou, Xuezhe Ma*\n\n**Abstract:** The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.\n\n**Published:** 2022-09-21\n\n\n\n#### 2. Linformer: Self-Attention with Linear Complexity\n\n*From Search Query: state space models attention efficiency*\n\n*Madian Khabsa, Hao Ma, Sinong Wang, Belinda Z. Li, Han Fang*\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n**Published:** 2020-06-08\n\n\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space models attention efficiency*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using state space models (SSMs) and attention mechanisms, several key points and recent advancements are noteworthy:\n\n## Hybrid Architectures Combining SSMs and Attention\n\n- The paper on \"Taipan: Efficient and Expressive State Space Language Models with Selective Attention\" introduces a hybrid architecture that combines the efficiency of state space models (Mamba-2) with the expressive power of selective attention layers (SALs). This approach allows for accurate predictions over long context lengths (up to 1 million tokens) while maintaining computational efficiency. The SALs identify tokens requiring long-range interactions, augment their representations using attention, and remove less important features, thus balancing efficiency with performance.\n\n## Efficiency and Scalability of SSMs\n\n- State Space Models (SSMs) are highlighted for their ability to maintain linear complexity, which is crucial for handling long sequences efficiently. Unlike traditional transformer models that suffer from quadratic computational complexity, SSMs offer constant memory usage, making them scalable for long-context tasks.\n\n## Integration with Attention Mechanisms\n\n- Recent studies have shown that integrating SSMs with local self-attention can enable gradient-based learning and in-context learning similar to transformers. This integration allows SSMs to reproduce the outputs of implicit linear models after a single step of gradient descent, demonstrating their potential for scalable training and effectiveness in general tasks.\n\n## Selective and Sparse Computation\n\n- The concept of selective attention, as seen in Taipan, involves identifying and focusing on tokens that require long-range interactions, which can significantly reduce computational costs. This selective computation approach aligns with the need for efficient attention mechanisms and sparse computation to manage memory and computational resources effectively.\n\n## Position-Aware Mechanisms and State Management\n\n- Enhancing position-aware mechanisms, such as using rotary embeddings or more sophisticated position encoding techniques, can be integrated with state representations in SSMs. This can improve the model's ability to handle sequential data efficiently. The hierarchical state compression used in the seed design can be further optimized by incorporating these advanced position-aware mechanisms.\n\n## Feedback Mechanisms and State Management\n\n- Feedback attention mechanisms, such as those proposed in TransformerFAM, can enhance state management by allowing the network to attend to its own latent representations. This feedback loop can improve the model's memory capabilities and stability during training.\n\n## Technical Considerations and Implementation Challenges\n\n- To balance complexity with computational efficiency, it is crucial to maintain linear complexity while improving expressiveness. Hybrid approaches that combine SSMs with minimal attention modules, as seen in Zamba, can achieve this balance effectively. Ensuring stable training and managing memory requirements are key implementation challenges that need to be addressed through careful architectural design and optimization techniques.\n\nBy focusing on these aspects, researchers can develop more efficient and scalable autoregressive language models that leverage the strengths of state space models and attention mechanisms. This includes exploring hybrid architectures, optimizing state management and position-aware mechanisms, and implementing selective and sparse computation strategies.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe seed design HierarchicalPositionAwareTTTLinear presents a sophisticated approach combining hierarchical state compression with position-aware processing through rotary embeddings. From analyzing this and the provided references, several key themes and opportunities emerge:\n\na) State Management Approaches:\n- The seed design uses hierarchical states with compression\n- Recent works like Mamba and Zamba show success with state space models (SSMs)\n- There's potential to combine SSMs with selective attention mechanisms\n\nb) Position Encoding:\n- Current design uses rotary embeddings\n- Could be enhanced with more sophisticated position-aware mechanisms\n- Opportunity to integrate with state representations\n\nc) Efficiency Considerations:\n- Current approaches still face memory and computation challenges\n- Need for more efficient attention mechanisms\n- Potential for selective or sparse computation\n\n2. Useful References with Excerpts\n\nFrom Sparse Sinkhorn Attention:\n\"We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations.\"\nKey insight: Using sorting networks for efficient attention computation\n\nFrom Zamba paper:\n\"Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost.\"\nKey insight: Hybrid architectures combining SSMs with minimal attention can be highly effective\n\nFrom TransformerFAM:\n\"Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations.\"\nKey insight: Feedback mechanisms can enhance memory capabilities\n\n3. Key Insights and Analysis\n\na) Architectural Innovation Opportunities:\n- Combining SSMs with sparse attention mechanisms could provide efficiency while maintaining expressiveness\n- Feedback loops could enhance state management\n- Selective computation based on input characteristics could improve efficiency\n\nb) Technical Considerations:\n- Need to maintain linear complexity while improving expressiveness\n- State space models show promise for efficient sequence processing\n- Hybrid approaches combining multiple mechanisms appear most promising\n\nc) Implementation Challenges:\n- Balancing complexity with computational efficiency\n- Ensuring stable training with sophisticated architectures\n- Managing memory requirements effectively\n\n4. Future Search Plan\n\nNext search queries should focus on:\n1. Specific implementations of hybrid SSM-attention architectures\n2. Memory-efficient state management techniques\n3. Dynamic or adaptive computation mechanisms\n\n5. References\n\n1. Tay, Y., Bahri, D., Yang, L., Metzler, D., & Juan, D. (2020). Sparse Sinkhorn Attention. In International Conference on Machine Learning.\n\n2. Glorioso, P., Anthony, Q., Tokpanov, Y., Whittington, J., Pilault, J., Ibrahim, A., & Millidge, B. (2024). Zamba: A Compact 7B SSM Hybrid Model. arXiv preprint.\n\n3. Hwang, D., Wang, W., Huo, Z., Sim, K., & Mengibar, P. M. (2024). TransformerFAM: Feedback attention is working memory. arXiv preprint.\n\n4. Su, J., Lu, Y., Pan, S., Wen, B., & Liu, Y. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding. Neurocomputing."
        },
        {
            "ready": false,
            "query": "selective attention mechanisms implementation efficiency",
            "detail": "Analyze specific implementations and performance characteristics of selective attention mechanisms, focusing on efficiency and integration with state space models",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze specific implementations and performance characteristics of selective attention mechanisms, focusing on efficiency and integration with state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.99)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.99)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n#### 2. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.97)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.97)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.88)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.88)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective attention mechanisms implementation efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Invariant Slot Attention: Object Discovery with Slot-Centric Reference Frames\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*Ondrej Biza, Sjoerd van Steenkiste, Mehdi S. M. Sajjadi, Gamaleldin F. Elsayed, Aravindh Mahendran, Thomas Kipf*\n\n**TL;DR:** This paper incorporates equivariance to per-object pose transformations into the attention and generation mechanism of Slot Attention by translating, scaling, and rotating position encodings, and can result in large gains in terms of data efficiency and overall improvements to object discovery.\n\n**Abstract:** Automatically discovering composable abstractions from raw perceptual data is a long-standing challenge in machine learning. Recent slot-based neural networks that learn about objects in a self-supervised manner have made exciting progress in this direction. However, they typically fall short at adequately capturing spatial symmetries present in the visual world, which leads to sample inefficiency, such as when entangling object appearance and pose. In this paper, we present a simple yet highly effective method for incorporating spatial symmetries via slot-centric reference frames. We incorporate equivariance to per-object pose transformations into the attention and generation mechanism of Slot Attention by translating, scaling, and rotating position encodings. These changes result in little computational overhead, are easy to implement, and can result in large gains in terms of data efficiency and overall improvements to object discovery. We evaluate our method on a wide range of synthetic object discovery benchmarks namely CLEVR, Tetrominoes, CLEVRTex, Objects Room and MultiShapeNet, and show promising improvements on the challenging real-world Waymo Open dataset.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 4*)\n\n#### 3. How Does Selective Mechanism Improve Self-Attention Networks?\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, Zhaopeng Tu*\n\n**TL;DR:** Assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax, shows that SSANs consistently outperform the standard SANs.\n\n**Abstract:** Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 4. Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*Yingyi Chen, Qinghua Tao, F. Tonin, J. Suykens*\n\n**TL;DR:** This work provides a primal-dual representation for the asymmetric kernel in self-attention and successfully applies it to modeling and optimization, and demonstrates that the deployed KSVD optimization regularizes Primal-Attention with a sharper singular value decay than that of the canonical self-ATTention.\n\n**Abstract:** Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stationary solution to the KSVD optimization in Primal-Attention yields a zero-value objective. In this manner, KSVD optimization can be implemented by simply minimizing a regularization loss, so that low-rank property is promoted without extra decomposition. Numerical experiments show state-of-the-art performance of our Primal-Attention with improved efficiency. Moreover, we demonstrate that the deployed KSVD optimization regularizes Primal-Attention with a sharper singular value decay than that of the canonical self-attention, further verifying the great potential of our method. To the best of our knowledge, this is the first work that provides a primal-dual representation for the asymmetric kernel in self-attention and successfully applies it to modeling and optimization.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 5. Mixture of Attention Heads: Selecting Attention Heads Per Token\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*Xiaofeng Zhang, Yikang Shen, Zeyu Huang, Jie Zhou, Wenge Rong, Zhang Xiong*\n\n**TL;DR:** The Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism, is proposed, which can easily scale up the number of attention heads and theNumber of parameters while preserving computational efficiency.\n\n**Abstract:** Mixture-of-Experts (MoE) networks have been proposed as an efficient way to scale up model capacity and implement conditional computing. However, the study of MoE components mostly focused on the feedforward layer in Transformer architecture. This paper proposes the Mixture of Attention Heads (MoA), a new architecture that combines multi-head attention with the MoE mechanism. MoA includes a set of attention heads that each has its own set of parameters. Given an input, a router dynamically selects a subset of k attention heads per token. This conditional computation schema allows MoA to achieve stronger performance than the standard multi-head attention layer. Furthermore, the sparsely gated MoA can easily scale up the number of attention heads and the number of parameters while preserving computational efficiency. Despite performance improvements, MoA also automatically differentiates heads\u2019 utilities, providing a new perspective to discuss the model\u2019s interpretability. We conducted experiments on several important tasks, including Machine Translation and Masked Language Modeling. Experiments have shown promising results on several tasks against strong baselines that involve large and very deep models.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 27  (*Influential: 5*)\n\n### 3 related papers from Papers with Code\n\n#### 1. ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*QinGhua Hu, WangMeng Zuo, Peihua Li, Qilong Wang, Pengfei Zhu, Banggu Wu*\n\n**Abstract:** Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via $1D$ convolution. Furthermore, we develop a method to adaptively select kernel size of $1D$ convolution, determining coverage of local cross-channel interaction. The proposed ECA module is efficient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.\n\n**Conference:** eca-net-efficient-channel-attention-for-deep-1\n\n**Published:** 2019-10-08\n\n\n\n#### 2. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 3. Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis\n\n*From Search Query: selective attention mechanisms implementation efficiency*\n\n*Ilker Hacihaliloglu, Dorit Merhof, Ulas Bagci, Ali Diba, Reza Azad, Soheila Hatami, Afshin Bozorgpour, Bobby Azad, Sanaz Karimijafarbigloo, Sina Ghorbani Kolahi, Moein Heidari*\n\n**Abstract:** Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks. However, the emergence of transformers has altered this paradigm due to their superior performance. Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations. However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation. State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence. Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models. Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging. Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context. Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs. Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field. In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository.\n\n**Published:** 2024-06-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by incorporating selective attention mechanisms efficiently, several key points and strategies can be considered based on the provided sources and analysis notes:\n\n## Combining Selective Attention with State Space Models (SSMs)\n\n- The integration of selective attention mechanisms with SSMs is a promising approach. The paper on \"Taipan\" highlights how combining selective attention with efficient architectures like Mamba can enhance performance while maintaining computational efficiency. Taipan uses Selective Attention Layers (SALs) along with a Markovian structure to handle long sequences efficiently, which is crucial for balancing expressiveness and efficiency.\n\n## Efficient Attention Mechanisms\n\n- **Selective Attention**: Taipan's architecture applies selective attention only to tokens that benefit from it, optimizing resource allocation and reducing computational costs. This approach allows for a sparser attention weight map, enabling the model to handle longer context lengths efficiently.\n- **Sparse Attention Mechanisms**: Implementing sparse attention can significantly reduce memory and computational requirements. Techniques like those used in Taipan or other models such as Linformer or Performer can help mitigate the quadratic complexity of traditional self-attention mechanisms.\n\n## State Management and Hierarchical Processing\n\n- SSMs offer efficient state representation and processing. Hierarchical approaches can improve information retention, and feedback mechanisms can enhance memory capabilities. For example, the \"Vision Mamba\" model uses a bidirectional state space model to achieve efficient visual representation learning, which can be adapted for language models[Analysis Note].\n\n## Computational and Memory Efficiency\n\n- **Linear Complexity**: Achieving linear complexity is crucial for scalability. Models like LOCOST and Mamba demonstrate how SSMs can handle longer sequences with a computational complexity of \\(O(L \\log L)\\) or even \\(O(L)\\), which is more efficient than the quadratic complexity of traditional self-attention[Analysis Note].\n- **Memory Optimization**: Techniques such as quantization, pruning, and memory compression can help reduce the memory footprint of the model. FlashAttention, for instance, uses tiling and recomputation to minimize memory usage and speed up attention calculations.\n\n## Attention Budget and Resource Allocation\n\n- Implementing an attention budget constraint, as seen in Taipan, can help balance efficiency and expressiveness. This involves setting a predefined budget for the fraction of tokens to receive attention, ensuring that computational resources are allocated efficiently.\n\n## Integration and Optimization\n\n- The integration of different mechanisms requires careful optimization. For example, Taipan's use of a gating network to compute a score vector for each token representation helps in determining which tokens should receive attention, ensuring efficient resource allocation.\n- The Multi-Head Density Adaptive Attention Mechanism (DAAM) proposed in another study integrates learnable mean and variance into the attention mechanism, which can be used for parameter-efficient fine-tuning and enhancing information aggregation across multiple modalities.\n\nBy combining these strategies, researchers can design an autoregressive language model that leverages the efficiency of SSMs, the selectivity of attention mechanisms, and optimized resource allocation, ultimately leading to improved performance, scalability, and computational efficiency.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing the seed design:\n\na) Hybrid Architectures:\n- Combining SSMs with selective attention shows strong potential\n- Need to balance efficiency with expressiveness\n- Opportunity to leverage linear complexity of SSMs while maintaining attention capabilities\n\nb) State Management:\n- SSMs offer efficient state representation and processing\n- Hierarchical approaches can improve information retention\n- Feedback mechanisms can enhance memory capabilities\n\nc) Computational Efficiency:\n- Linear complexity achievable through careful design\n- Selective computation can reduce resource requirements\n- Memory optimization crucial for scalability\n\n2. Useful References with Excerpts\n\nFrom \"Understanding the differences in Foundation Models\":\n\"Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent.\"\nKey insight: Framework for comparing different architectures systematically\n\nFrom \"LOCOST\":\n\"With a computational complexity of O(L log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns.\"\nKey insight: SSM-based architectures can achieve superior efficiency\n\nFrom \"Vision Mamba\":\n\"Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency.\"\nKey insight: SSMs can outperform attention while being more efficient\n\n3. Key Insights and Analysis\n\na) Architectural Innovations:\n- SSMs can be effectively combined with selective attention mechanisms\n- Hierarchical processing improves information flow\n- Position-aware mechanisms remain important\n\nb) Efficiency Considerations:\n- Linear complexity achievable through careful design\n- Memory optimization crucial for scalability\n- Selective computation can reduce resource requirements\n\nc) Implementation Challenges:\n- Need to balance complexity with performance\n- State management requires careful design\n- Integration of different mechanisms needs optimization\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of selective attention mechanisms\n2. Techniques for efficient state compression and management\n3. Methods for combining SSMs with attention mechanisms effectively\n\n5. References\n\n1. Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint.\n\n2. Le Bronnec, F., et al. (2024). LOCOST: State-Space Models for Long Document Abstractive Summarization. Conference of the European Chapter of the Association for Computational Linguistics.\n\n3. Zhu, L., et al. (2024). Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model. International Conference on Machine Learning.\n\n4. Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n5. Dao, T., & Gu, A. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint."
        },
        {
            "ready": false,
            "query": "efficient selective computation hardware implementation",
            "detail": "Analyze specific techniques and implementations for efficient selective computation in neural networks, focusing on hardware optimization and memory management",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze specific techniques and implementations for efficient selective computation in neural networks, focusing on hardware optimization and memory management\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.63)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.63)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.53)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.53)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 3. Large Memory Layers with Product Keys (Avg. Score: 0.34)\n\n*Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, H. J\u00e9gou*\n\n**Published in:** Neural Information Processing Systems (2019)\t**Cited by** 113  (*Influential: 17*)\n\n**TL;DR:** A structured memory which can be easily integrated into a neural network and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead is introduced.\n\n**Abstract:** This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.\n\n##### *Relevant Chunk: No. 4/21 (Score: 0.34)*\n\n```\nThe output is the sparse weighted sum over the memories associated with the selected keys. For a large number of keys $|\\mathcal{K}|$, the key selection procedure becomes too expensive in practice. Our product key method is exact and makes this search process very fast. Figure 11, we define keys as the concatenation of two sub-keys, in the spirit of product quantization [21]. As shown in more details in Figure 2, this structure implicitly defines a very large set of keys, each being associated with a value memory slot. The set of value vectors introduces the bulk of the parameters, as it scales quadratically with the number of sub-keys. Despite the large number of memory slots, finding the exact closest keys to the input is very efficient, typically requiring $\\mathcal{O}(\\sqrt{|\\mathcal{K}|})$ vector comparisons, where $|\\mathcal{K}|$ is the total number of memory slots. All the memory parameters are trainable, yet only a handful of memory slots are updated for each input at training time. Sparsity of key selection and parameter updates make both training and inference very efficient. Our layer allows us to tackle problems where current architectures underfit given the vast amount of available data, or when they are too slow to work in practice. We thus focus on the language modeling task, integrating our memory within the popular transformer architecture [44]. This choice is motivated by the success of BERT [11] and GPT-2 [36], which demonstrated that increasing the capacity of large models directly translates to large improvements in language modeling, which in turn translates to better performance in both language understanding tasks [11, 46] and text generation [36]. Overall, our paper makes the following contributions:\n\n- We introduce a new layer that provides a large capacity to a neural network for only a slight computational overhead both at train and test time. - Our fast indexing strategy offers exact nearest neighbor search by construction, and avoids the pitfall of relying on an indexing structure that needs to be re-learned during training. - We demonstrate our method within a large state-of-the-art transformer, composed of 24 layers of dimension 1600. Our method with 1 memory and 12 layers outperforms a 24 layer transformer while being twice faster at inference time. We show that adding more memory layers to transformers of various complexities provides systematic and significant improvements on our target task. ## 2 Related work\n\nDifferent approaches have been proposed to increase the capacity of neural networks without increasing too much the computational complexity. For instance, conditional computation models aim at routing inputs into very large neural networks such that only a subset of connections and/or layers are used to process each input. Different methods have been developed like large mixture of experts [40], gating techniques [3, 12, 6] or even reinforcement learning-based approaches [10]. Another line of research is the development of memory augmented neural networks. For instance, memory-based neural layers [47, 42] are an efficient way to represent variable length inputs for complex problems such as question answering [48]. Such memories can also operate in feature space and have various reading and writing mechanisms [23, 17]. Unfortunately, these approaches scale linearly with the size of the memory which is prohibitive for very large memories. Neural cache models [15] suffer from the same scaling issues, which are circumvented by adopting approximate lookup techniques at test time [14]. Discretization techniques have been intensively studied for compressing network weights [8, 38] and/or activations [7, 38] or to accelerate inference. For instance, Gerald et al. [13] propose to map an input to a low-dimensional binary code, each code being associated with one category, thus reducing the complexity of inference by avoiding the use of a final large linear layer. Another model is proposed in [45], where the authors develop a fast locality-sensitive hashing technique to approximate the dot product between large matrices and vectors in neural networks. However, exploiting binary codes or approximate techniques at training time raises several challenges in terms of optimization, because approximate indexes are not accurate in high-dimensional spaces. In our paper, we borrow some ideas from product quantization (PQ) [21]. This is an approximate search technique that maps database vectors into compact codes. However, our goal is different: we do not build an approximate index, but rather we exploit the idea to represent a large set of key vectors by a drastically smaller number of vectors, that we update by regular back-propagation. As discussed later, the selection of the closest keys is exact and inherits from the fast neighbor search of PQ. Our model is also related to sparsity models which have been mainly studied in the unsupervised learning setting [34, 24]. For instance, the k-sparse autoencoder [30] only keeps the k largest values in the latent representation of an auto-encoder, similar to our memory layer but without the product keys component. In winner take all autoencoders [31], sparsity is induced by using mini-batch statistics, while in the sparse access memory [37] reports some speed-up by both thresholding the memory to a sparse subset, and by using efficient data structures for content-based read operations.\n```\n\n#### 4. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.20)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.20)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n#### 5. Attention as an RNN (Avg. Score: 0.13)\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed Osama Ahmed, Y. Bengio, Greg Mori*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Aaren is introduced, an attention-based module that can not only be trained in parallel but also be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs).\n\n**Abstract:** The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \\textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \\textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \\textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.\n\n##### *Relevant Chunk: No. 21/34 (Score: 0.13)*\n\n```\nO. (2023). Memory efficient neural processes via constant memory attention block.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient selective computation hardware implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Towards Efficient and Accurate Winograd Convolution via Full Quantization\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Tianqi Chen, Weixiang Xu, Weihan Chen, Peisong Wang, Jian Cheng*\n\n**TL;DR:** Observing the inconsistency among different transformation procedures, PTQ-Aware Winograd (PAW) is presented to optimize them collaboratively under a unified objective function and the full quantization of faster Winograd (tile size \u2265 4 ) for the first time.\n\n**Abstract:** The Winograd algorithm is an efficient convolution implementation, which performs calculations in the transformed domain. To further improve the computation efficiency, recent works propose to combine it with model quantization. Although Post-Training Quantization has the advantage of low computational cost and has been successfully applied in many other scenarios, a severe accuracy drop exists when utilizing it in Winograd convolution. Besides, despite the Winograd algorithm consisting of four stages, most existing methods only quantize the element-wise multiplication stage, leaving a considerable portion of calculations in full precision. In this paper, observing the inconsistency among different transformation procedures, we present PTQ-Aware Winograd (PAW) to optimize them collaboratively under a unified objective function. Moreover, we explore the full quantization of faster Winograd (tile size \u2265 4 ) for the first time. We further propose a hardware-friendly method called Factorized Scale Quantization (FSQ), which can effectively balance the significant range differences in the Winograd domain. Experiments demonstrate the effectiveness of our method, e.g., with 8-bit quantization and a tile size of 6, our method outperforms the previous Winograd PTQ method by 8.27% and 5.38% in terms of the top-1 accuracy on ResNet-18 and ResNet-34, respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 3. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher R\u00e9, Beidi Chen*\n\n**TL;DR:** DejaVu is proposed, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference.\n\n**Abstract:** Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 129  (*Influential: 16*)\n\n#### 5. Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, S. Naor, Daniel Soudry*\n\n**TL;DR:** A new measure called mask-diversity is suggested which correlates with the expected accuracy of the different types of structural pruning, and a novel transposable fine-grained sparsity mask is suggested, which guarantees that both the weight matrix and its transpose follow the same sparsity pattern.\n\n**Abstract:** Unstructured pruning reduces the memory footprint in deep neural networks (DNNs). Recently, researchers proposed different types of structural pruning intending to reduce also the computation complexity. In this work, we first suggest a new measure called mask-diversity which correlates with the expected accuracy of the different types of structural pruning. We focus on the recently suggested N:M fine-grained block sparsity mask, in which for each block of M weights, we have at least N zeros. While N:M fine-grained block sparsity allows acceleration in actual modern hardware, it can be used only to accelerate the inference phase. In order to allow for similar accelerations in the training phase, we suggest a novel transposable fine-grained sparsity mask, where the same mask can be used for both forward and backward passes. Our transposable mask guarantees that both the weight matrix and its transpose follow the same sparsity pattern; thus, the matrix multiplication required for passing the error backward can also be accelerated. We formulate the problem of finding the optimal transposable-mask as a minimum-cost flow problem. Additionally, to speed up the minimum-cost flow computation, we also introduce a fast linear-time approximation that can be used when the masks dynamically change during training. Our experiments suggest a 2x speed-up in the matrix multiplications with no accuracy degradation over vision and language models. Finally, to solve the problem of switching between different structure constraints, we suggest a method to convert a pre-trained model with unstructured sparsity to an N:M fine-grained block sparsity model with little to no training. A reference implementation can be found at https://github.com/papers-submission/structured_transposable_masks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 95  (*Influential: 17*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Tune: A Research Platform for Distributed Model Selection and Training\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Ion Stoica, Philipp Moritz, Robert Nishihara, Joseph E. Gonzalez, Eric Liang, Richard Liaw*\n\n**Abstract:** Modern machine learning algorithms are increasingly computationally\ndemanding, requiring specialized hardware and distributed computation to\nachieve high performance in a reasonable time frame. Many hyperparameter search\nalgorithms have been proposed for improving the efficiency of model selection,\nhowever their adaptation to the distributed compute environment is often\nad-hoc. We propose Tune, a unified framework for model selection and training\nthat provides a narrow-waist interface between training scripts and search\nalgorithms. We show that this interface meets the requirements for a broad\nrange of hyperparameter search algorithms, allows straightforward scaling of\nsearch to large clusters, and simplifies algorithm implementation. We\ndemonstrate the implementation of several state-of-the-art hyperparameter\nsearch algorithms in Tune. Tune is available at\nhttp://ray.readthedocs.io/en/latest/tune.html.\n\n**Published:** 2018-07-13\n\n\n\n#### 2. Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Davide Scaramuzza, Balazs Nagy, Philipp Foehn*\n\n**Abstract:** The recent introduction of powerful embedded graphics processing units (GPUs) has allowed for unforeseen improvements in real-time computer vision applications. It has enabled algorithms to run onboard, well above the standard video rates, yielding not only higher information processing capability, but also reduced latency. This work focuses on the applicability of efficient low-level, GPU hardware-specific instructions to improve on existing computer vision algorithms in the field of visual-inertial odometry (VIO). While most steps of a VIO pipeline work on visual features, they rely on image data for detection and tracking, of which both steps are well suited for parallelization. Especially non-maxima suppression and the subsequent feature selection are prominent contributors to the overall image processing latency. Our work first revisits the problem of non-maxima suppression for feature detection specifically on GPUs, and proposes a solution that selects local response maxima, imposes spatial feature distribution, and extracts features simultaneously. Our second contribution introduces an enhanced FAST feature detector that applies the aforementioned non-maxima suppression method. Finally, we compare our method to other state-of-the-art CPU and GPU implementations, where we always outperform all of them in feature tracking and detection, resulting in over 1000fps throughput on an embedded Jetson TX2 platform. Additionally, we demonstrate our work integrated in a VIO pipeline achieving a metric state estimation at ~200fps.\n\n**Published:** 2020-03-30\n\n\n\n#### 3. Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis\n\n*From Search Query: efficient selective computation hardware implementation*\n\n*Ilker Hacihaliloglu, Dorit Merhof, Ulas Bagci, Ali Diba, Reza Azad, Soheila Hatami, Afshin Bozorgpour, Bobby Azad, Sanaz Karimijafarbigloo, Sina Ghorbani Kolahi, Moein Heidari*\n\n**Abstract:** Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks. However, the emergence of transformers has altered this paradigm due to their superior performance. Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations. However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation. State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence. Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models. Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging. Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context. Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs. Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field. In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository.\n\n**Published:** 2024-06-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on efficient selective computation, hardware optimization, and memory management, here are some key techniques and implementations from the provided sources and additional insights:\n\n## Efficient Selective Computation\n\n### Pruning and Quantization\nTechniques like pruning and quantization can significantly reduce the computational cost and memory usage of neural networks. For example, the \"Deep Compression\" technique proposed by Song Han involves pruning and quantization, which can shrink neural networks by more than 10 times without hurting accuracy. This method is particularly useful for efficient selective computation as it reduces the number of parameters and operations required.\n\n### SynFlow and SNIP\nMethods like SynFlow and SNIP are designed to prune neural networks efficiently. SynFlow considers the \"network flow\" perspective, which can lead to more effective pruning without significant accuracy loss. SNIP prunes weights before training based on their sensitivity to the loss function, which can also help in reducing the computational load.\n\n### Tiny-Transfer-Learning (TinyTL)\nTinyTL is another approach that freezes the weights of a network and only learns the bias modules, significantly reducing memory usage. This method introduces a lite residual module to refine the feature extractor, adding only a small memory overhead. It is particularly useful for on-device learning and can be adapted for selective computation in autoregressive models.\n\n## Hardware Optimization\n\n### Hardware-Aware Neural Architecture Search\nTechniques such as hardware-aware neural architecture search (e.g., 3D-NAS) can efficiently search for optimal network architectures under given resource constraints. This approach can be adapted for autoregressive language models to ensure that the model is optimized for the available hardware resources, leading to better performance and efficiency.\n\n### Efficient Inference Engines\nThe development of efficient inference engines, such as those proposed by Song Han, can bring significant improvements in hardware efficiency. For instance, the Efficient Inference Engine (EIE) exploits weight sparsity to accelerate inference, which can be crucial for autoregressive models that require efficient processing of sequential data.\n\n## Memory Management\n\n### Hierarchical State Representation\nHierarchical state representation, as discussed in the analysis note, is crucial for efficient memory management. Techniques like CHELA (short-long Convolutions with Hardware-Efficient Linear Attention) use hierarchical processing to optimize memory access patterns and reduce computational costs. This approach can be applied to autoregressive models to manage state efficiently[Analysis Note].\n\n### Memory Hierarchy Optimization\nOptimizing the memory hierarchy is essential for performance. Techniques like TinyEngine, which adapt memory scheduling according to the overall network topology, can reduce memory usage significantly. For autoregressive models, careful management of the memory hierarchy can help in efficient selective computation by ensuring that the most critical states are readily accessible.\n\n## Hybrid Architectures\n\n### Combining Selective State Space Models (SSMs) with Attention\nCombining SSMs with selective attention mechanisms can enhance the expressiveness of autoregressive models while maintaining efficiency. This hybrid approach allows for input-dependent parameter selection and efficient propagation or forgetting of information along the sequence length dimension, as seen in the Mamba paper[Analysis Note].\n\n## Additional Insights\n\n### Modularity\nModular neural networks can improve efficiency and performance. Studies have shown that modularity can enhance generalization, robustness, and efficiency, although the optimal amount of modularity is complex and dependent on various network and problem features. Implementing modularity in autoregressive models could help in selective computation by allowing different modules to handle different aspects of the input sequence.\n\nBy integrating these techniques\u2014pruning and quantization, hardware-aware architecture search, hierarchical state representation, memory hierarchy optimization, and hybrid architectures\u2014researchers can develop more efficient and scalable autoregressive language models that excel in selective computation, hardware optimization, and memory management.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing the seed design through selective mechanisms and efficient state management:\n\na) Selective State Space Models (SSMs):\n- Mamba introduces selective SSMs that allow input-dependent dynamics\n- Hardware-aware implementations achieve linear complexity\n- Efficient state management through parallel scan algorithms\n\nb) Hybrid Architectures:\n- Combining SSMs with selective attention shows strong potential\n- Need to balance between expressiveness and efficiency\n- Opportunity to leverage both local and global dependencies\n\nc) Implementation Efficiency:\n- Hardware-aware algorithms crucial for performance\n- Memory hierarchy optimization important\n- Careful state management needed for scalability\n\n2. Useful References with Excerpts\n\nFrom Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\nKey insight: Input-dependent parameter selection improves expressiveness\n\nFrom CHELA paper:\n\"To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.\"\nKey insight: Hierarchical processing with efficient implementation\n\nFrom DSF paper:\n\"Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent.\"\nKey insight: Understanding relationships between different architectures\n\n3. Key Insights and Analysis\n\na) Selective Mechanisms:\n- Input-dependent parameter selection crucial for expressiveness\n- Need efficient implementation of selective computation\n- Balance between flexibility and computational cost\n\nb) State Management:\n- Hierarchical state representation important\n- Memory hierarchy optimization crucial\n- Efficient parallel computation needed\n\nc) Implementation Considerations:\n- Hardware-aware algorithms essential\n- Memory access patterns critical for performance\n- Need for efficient parallel scan implementations\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient selective computation\n2. Methods for hierarchical state compression\n3. Hardware-efficient implementations of hybrid architectures\n\n5. References\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n2. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n3. Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint."
        },
        {
            "ready": false,
            "query": "dynamic sparsity neural networks implementation",
            "detail": "Analyze specific techniques and implementations for dynamic sparsity in neural networks, focusing on efficient state management and computation",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze specific techniques and implementations for dynamic sparsity in neural networks, focusing on efficient state management and computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.59)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 0.59)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.43)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 22/46 (Score: 0.43)*\n\n```\nIn International Conference on Machine Learning, pages 5547-5569. PMLR, 2022 . [34] Yuli Eidelman and Israel Gohberg. On a new class of structured matrices. Integral Equations and Operator Theory, 34(3):293-324, 1999. [35] Murali Emani, Venkatram Vishwanath, Corey Adams, Michael E Papka, Rick Stevens, Laura Florescu, Sumti Jairath, William Liu, Tejas Nama, and Arvind Sujeeth. Accelerating scientific applications with sambanova reconfigurable dataflow architecture. Computing in Science 8 Engineering, 23(2):114-119, 2021 . [36] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. Multi-head state space model for speech recognition. arXiv preprint arXiv:2305.12498, 2023. [37] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022 . [38] Quentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, 2021. [39] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [40] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.\n```\n\n#### 3. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.28)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 2/41 (Score: 0.28)*\n\n```\nIn this work, we introduce SparseK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SparseK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available. ## 1 Introduction\n\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges $[1,20,19]$, owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices. Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods $[39,61]$ allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually $\\operatorname{cost} O(n \\log n)$ to maintain clusters. Ainslie et al. [1]\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_81cbe08ae077927ce965g-02.jpg?height=549&width=1261&top_left_y=254&top_left_x=432)\n\nFigure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query ( $\\tau(\\mathbf{u})$ ) such that the sum of normalized scores is $k$, which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency. incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SofTTOPK for variable-length context associated with different queries requires quadratic time in total. To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows. ## Incremental KV Selection. The SPARSEK operator (\u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results. Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (\u00a73.2). Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency. We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments\non language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention. ## 2 Related Work\n\nLong-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence $[17,35,36,12]$ or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions $[15,27,42]$. Sparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns $[34,13]$, or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27].\n```\n\n#### 4. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.14)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 8/38 (Score: 0.14)*\n\n```\nSee App. B for details and to see how to adapt the backward pass in a similar fashion. Overhead. As for the previous method, sorting and re-ordering $\\boldsymbol{Q}, \\boldsymbol{K}$ and $\\boldsymbol{V}$ is inducing some overhead increasing linearly with the sequence length. As shown in our experiments in $\\S 4.2$, this overhead is by large compensated for as the sequence length increases. ![](https://cdn.mathpix.com/cropped/2024_09_17_7f5662ba13a7e3d1a640g-06.jpg?height=559&width=1050&top_left_y=474&top_left_x=532)\n\nFigure 3: Comparing several hash-based sparse attention implementations with FlashAttention. Similarly to QK-dropping-based sparsity in Fig. 7, due to the non-triangular causal mask resulting from re-ordering the tensors based on the hash buckets (see Fig. 1), a naive implementation would force the computation of the entire attention matrix before applying a custom mask. This results in very large runtimes independent of the number of buckets. On the other hand, our implementation modifies the basic FlashAttention method to compute only what is required. While there is a cost to reordering the tensors based on the hash buckets, this cost is largely compensated for as the number of buckets $n b$ increases, and as the sequence length increases. ## 4 Experiments \\& Results\n\nIn this section we present our experimental setup and results. We show that (i) unlike naive implementations using existing libraries, our dynamic sparsity attention schemes can significantly improve over the FlashAttention runtime, (ii) this still holds in real-world sequence modeling tasks after factoring in all the non-attention operations, and (iii) it is possible to match-and sometimes outperform-the baselines in terms of perplexity while significantly gaining in speed. ### 4.1 Experimental Setup\n\nDatasets. We test our hash-based sparsity scheme on MNIST (LeCun et al., 1998) for autoregressive image generation, enwik8 (Hutter, 2012), and OpenWebText2 (Gao et al., 2020). We experiment with QK-dropping based sparsity on OpenWebText2. Models \\& Baselines. For our language modeling experiments on OpenWebText2, we use a base autoregressive transformer architecture with 12 layers, a hidden size of 768,12 heads of 64 dimensions each. For experiments on sequence length $T=8192$, we use a batch size of $96=4 \\times 8 \\times 2$ (batch size 4 with 8 accumulation steps and data parallelism over 2 node). When $T=16384$ we use a batch size of $30=2 \\times 5 \\times 3$. The resulting models are of around 122 M parameters. The goal not being to outperform the state-of-the-art perplexity, we train for $15 k$ iterations. The attention modules used are either using FlashAttention for the baselines or one of our sparse kernels for our methods. To ensure a fair comparison, and similarly to Kitaev et al. (2020), we set the keys equal to normalized queries for all of our models.\n```\n\n#### 5. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.07)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 8/30 (Score: 0.07)*\n\n```\nAs context windows become larger by the day in state-of-the-art models, we expect these gains to become even more relevant. Interpretability. Fig. 8 provides insights into the interpretability aspect of the model's decisionmaking process. It is observed that token removal predominantly occurs when encountering stop words (punctuation), which aligns with the intuition that local information within a sentence becomes less relevant after its completion. Furthermore, it is worth noting that layers at varying depths exhibit distinct behaviors, reinforcing our rationale for dissecting token removal decisions across depth. The variance in sparsity distribution across different depths indicates the necessity of conducting additional interpretability research to obtain valuable insights in the interactions of the tokens within the model. We provide more insights towards this direction in the Appendix C. ## 5 Discussion\n\nWe proposed Adaptively Sparse Attention, a novel approach to dynamically prune the context in decoder-only Transformer architectures. Our results indicate that our technique performs favourably compared to competitive baselines in terms of the ratio between perplexity and sparsity of the attention weights. Remarkably our approach also significantly reduces the computational and memory requirements without affecting its final performance. We practically showcase these benefits achieving more than double the throughput at cases. Adaptively sparse attention comes with two additional practical advantages: first, it can be seamlessly integrated into existing pre-trained models via a cheap fine-tuning step; second, it represents an orthogonal contribution to the burgeoning research line aimed at increasing the level of efficiency of modern LLMs. As such, we envision its combination with existing techniques like weight pruning and quantization to be a promising avenue for future research. ## References\n\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence, (2):179-190, 1983. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: dynamic sparsity neural networks implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Dynamic Sparsity Is Channel-Level Sparsity Learner\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*Lu Yin, Gen Li, Meng Fang, Lijuan Shen, Tianjin Huang, Zhangyang Wang, V. Menkovski, Xiaolong Ma, Mykola Pechenizkiy, Shiwei Liu*\n\n**TL;DR:** Channel-aware dynamic sparse (Chase) is proposed, which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity during one end-to-end training process, without any ad-hoc operations.\n\n**Abstract:** Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using any particularly sparsity-aware hardware accelerators. This appealing outcome is partially motivated by a hidden phenomenon of dynamic sparsity: off-the-shelf unstructured DST implicitly involves biased parameter reallocation across channels, with a large fraction of channels (up to 60%) being sparser than others. By progressively identifying and removing these channels during training, our approach translates unstructured sparsity to channel-wise sparsity. Our experimental results demonstrate that Chase achieves 1.7 X inference throughput speedup on common GPU devices without compromising accuracy with ResNet-50 on ImageNet. We release our codes in https://github.com/luuyin/chase.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 2. SparseProp: Efficient Sparse Backpropagation for Faster Training of Neural Networks\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*Mahdi Nikdan, Tommaso Pegolotti, Eugenia Iofinova, Eldar Kurtic, Dan Alistarh*\n\n**TL;DR:** This work provides a new efficient version of the backpropagation algorithm, specialized to the case where the weights of the neural network being trained are sparse, and provides the first support for sparse training on commodity hardware.\n\n**Abstract:** We provide a new efficient version of the backpropagation algorithm, specialized to the case where the weights of the neural network being trained are sparse. Our algorithm is general, as it applies to arbitrary (unstructured) sparsity and common layer types (e.g., convolutional or linear). We provide a fast vectorized implementation on commodity CPUs, and show that it can yield speedups in end-to-end runtime experiments, both in transfer learning using already-sparsified networks, and in training sparse networks from scratch. Thus, our results provide the first support for sparse training on commodity hardware.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 3. TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*Jialin Chen, Rex Ying*\n\n**TL;DR:** A novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs, and extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation.\n\n**Abstract:** Temporal graphs are widely used to model dynamic systems with time-varying interactions. In real-world scenarios, the underlying mechanisms of generating future interactions in dynamic systems are typically governed by a set of recurring substructures within the graph, known as temporal motifs. Despite the success and prevalence of current temporal graph neural networks (TGNN), it remains uncertain which temporal motifs are recognized as the significant indications that trigger a certain prediction from the model, which is a critical challenge for advancing the explainability and trustworthiness of current TGNNs. To address this challenge, we propose a novel approach, called Temporal Motifs Explainer (TempME), which uncovers the most pivotal temporal motifs guiding the prediction of TGNNs. Derived from the information bottleneck principle, TempME extracts the most interaction-related motifs while minimizing the amount of contained information to preserve the sparsity and succinctness of the explanation. Events in the explanations generated by TempME are verified to be more spatiotemporally correlated than those of existing approaches, providing more understandable insights. Extensive experiments validate the superiority of TempME, with up to 8.21% increase in terms of explanation accuracy across six real-world datasets and up to 22.96% increase in boosting the prediction Average Precision of current TGNNs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 4. Implicit Bias of the Step Size in Linear Diagonal Neural Networks\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*M. S. Nacson, Kavya Ravichandran, N. Srebro, Daniel Soudry*\n\n**TL;DR:** This work shows how using large step size for non-centered data can change the implicit bias from a \u201dkernel\u201d type behavior to a \u201drich\u201d (sparsity-inducing) regime \u2014 even when gradient flow, studied in previous works, would not escape the \u201dkernel\u201d regime.\n\n**Abstract:** Focusing on diagonal linear networks as a model for understanding the implicit bias in underdeter-mined models, we show how the gradient descent step size can have a large qualitative effect on the implicit bias, and thus on generalization ability. In particular, we show how using large step size for non-centered data can change the implicit bias from a \u201dkernel\u201d type behavior to a \u201drich\u201d (sparsity-inducing) regime \u2014 even when gradient flow, studied in previous works, would not escape the \u201dkernel\u201d regime. We do so by using dynamic stability, proving that convergence to dynamically stable global minima entails a bound on some weighted \u2113 1 -norm of the linear predictor, i.e. a \u201drich\u201d regime. We prove this leads to good generalization in a sparse regression setting.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 42  (*Influential: 4*)\n\n#### 5. Sparsity in Continuous-Depth Neural Networks\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*H. Aliee, Till Richter, Mikhail Solonin, I. Ibarra, F. Theis, Niki Kilbertus*\n\n**TL;DR:** Weight sparsity can indeed help with recovering sparse ground-truth dynamics compared to unregularized NODEs, but it does not prevent learning spurious feature dependencies in the inferred dynamics, rendering them impractical for predictions under interventions, or for inferring the true underlying dynamics.\n\n**Abstract:** Neural Ordinary Differential Equations (NODEs) have proven successful in learning dynamical systems in terms of accurately recovering the observed trajectories. While different types of sparsity have been proposed to improve robustness, the generalization properties of NODEs for dynamical systems beyond the observed data are underexplored. We systematically study the influence of weight and feature sparsity on forecasting as well as on identifying the underlying dynamical laws. Besides assessing existing methods, we propose a regularization technique to sparsify\"input-output connections\"and extract relevant features during training. Moreover, we curate real-world datasets consisting of human motion capture and human hematopoiesis single-cell RNA-seq data to realistically analyze different levels of out-of-distribution (OOD) generalization in forecasting and dynamics identification respectively. Our extensive empirical evaluation on these challenging benchmarks suggests that weight sparsity improves generalization in the presence of noise or irregular sampling. However, it does not prevent learning spurious feature dependencies in the inferred dynamics, rendering them impractical for predictions under interventions, or for inferring the true underlying dynamics. Instead, feature sparsity can indeed help with recovering sparse ground-truth dynamics compared to unregularized NODEs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 11  (*Influential: 3*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Powerpropagation: A sparsity inducing weight reparameterisation\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*Yee Whye Teh, Peter E. Latham, Razvan Pascanu, Siddhant M. Jayakumar, Jonathan Schwarz*\n\n**Abstract:** The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a \"rich get richer\" dynamic, leaving low-magnitude parameters largely unaffected by learning. Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely. Powerpropagation is general, intuitive, cheap and straight-forward to implement and can readily be combined with various other techniques. To highlight its versatility, we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representations allow accommodating a large number of tasks at fixed model capacity. In all cases our reparameterisation considerably increases the efficacy of the off-the-shelf methods.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-10-01\n\n\n\n#### 2. Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*Tinne Tuytelaars, Thomas Verelst*\n\n**Abstract:** Modern convolutional neural networks apply the same operations on every pixel in an image. However, not all image regions are equally important. To address this inefficiency, we propose a method to dynamically apply convolutions conditioned on the input image. We introduce a residual block where a small gating branch learns which spatial positions should be evaluated. These discrete gating decisions are trained end-to-end using the Gumbel-Softmax trick, in combination with a sparsity criterion. Our experiments on CIFAR, ImageNet and MPII show that our method has better focus on the region of interest and better accuracy than existing methods, at a lower computational complexity. Moreover, we provide an efficient CUDA implementation of our dynamic convolutions using a gather-scatter approach, achieving a significant improvement in inference speed with MobileNetV2 residual blocks. On human pose estimation, a task that is inherently spatially sparse, the processing speed is increased by 60% with no loss in accuracy.\n\n**Conference:** dynamic-convolutions-exploiting-spatial-1\n\n**Published:** 2019-12-06\n\n\n\n#### 3. Sparse maximal update parameterization: A holistic approach to sparse training dynamics\n\n*From Search Query: dynamic sparsity neural networks implementation*\n\n*Joel Hestness, Shane Bergsma, Nolan Dey*\n\n**Abstract:** Several challenges make it difficult for sparse neural networks to compete with dense models. First, setting a large fraction of weights to zero impairs forward and gradient signal propagation. Second, sparse studies often need to test multiple sparsity levels, while also introducing new hyperparameters (HPs), leading to prohibitive tuning costs. Indeed, the standard practice is to re-use the learning HPs originally crafted for dense models. Unfortunately, we show sparse and dense networks do not share the same optimal HPs. Without stable dynamics and effective training recipes, it is costly to test sparsity at scale, which is key to surpassing dense networks and making the business case for sparsity acceleration in hardware. A holistic approach is needed to tackle these challenges and we propose S$\\mu$Par as one such approach. For random unstructured static sparsity, S$\\mu$Par ensures activations, gradients, and weight updates all scale independently of sparsity level. Further, by reparameterizing the HPs, S$\\mu$Par enables the same HP values to be optimal as we vary both sparsity level and model width. HPs can be tuned on small dense networks and transferred to large sparse models, greatly reducing tuning costs. On large-scale language modeling, S$\\mu$Par shows increasing improvements over standard parameterization as sparsity increases, leading up to 11.9% relative loss improvement at 99.2% sparsity. A minimal implementation of S$\\mu$Par is available at https://github.com/EleutherAI/nanoGPT-mup/tree/supar.\n\n**Published:** 2024-05-24\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on dynamic sparsity, efficient state management, and computation, here are some specific techniques and implementations that can be considered:\n\n## Dynamic Sparsity Techniques\n\n### Contextual Sparsity\n- Techniques like those described in DejaVu, which introduce contextual sparsity, can be highly effective. This involves selecting small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input. This approach can maintain model quality while reducing computational overhead[3,.\n\n### Dynamic Sparse Training (DST)\n- DST algorithms, as mentioned in the context of sparse neural networks, can be adapted for autoregressive models. These algorithms dynamically adjust the sparsity pattern during training, which can significantly reduce computational overhead while maintaining performance.\n\n## Efficient State Management\n\n### Tiling and Hierarchical Processing\n- Implementing tiling and hierarchical processing, as seen in FlashAttention, can optimize memory access patterns and improve hardware utilization. This involves dividing the computation into smaller blocks and managing the state efficiently across these blocks[5,.\n\n### IO-Aware Algorithms\n- IO-aware algorithms can reduce the number of memory reads/writes, which is crucial for efficient state management. This can be achieved by optimizing the memory access patterns between different levels of memory hierarchy (e.g., GPU HBM and SRAM)[5,.\n\n## Computation Optimization\n\n### Hybrid Computation Strategies\n- Hybrid approaches, such as those used in Lightning Attention, can optimize different parts of the model. For example, splitting the attention calculation into intra-blocks and inter-blocks and using different computation strategies for each can improve efficiency without compromising quality[5,.\n\n### Selective Computation\n- Implementing selective computation based on input characteristics can reduce unnecessary computations. This can involve input-dependent parameter selection, where only relevant parameters are computed for a given input, thus improving efficiency[3,.\n\n### Hardware-Specific Optimization\n- Optimizing the model for specific hardware targets is crucial. This includes considering the GPU memory hierarchy, using asynchronous execution to improve hardware utilization, and implementing specialized algorithms that leverage the strengths of the target hardware[3,.\n\n## Implementation Strategies\n\n### Block-Based Processing\n- Block-based processing can improve cache utilization and reduce memory bandwidth requirements. This involves processing the input in blocks rather than as a whole, which can be particularly beneficial for long sequences in autoregressive models[5,.\n\n### Automatic Differentiation and Gradient Management\n- Efficiently managing gradients during backpropagation is essential. Techniques like automatic differentiation can help in calculating the gradients accurately and efficiently, which is critical for training sparse and dynamic models[2,.\n\nBy integrating these techniques, researchers can develop autoregressive language models that are more efficient in terms of computation, memory usage, and overall performance, while maintaining or even improving the model's accuracy and robustness.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe search results reveal several promising techniques for enhancing the FastTTTLinear design with a focus on hardware efficiency and memory management:\n\na) Memory-Efficient Attention:\n- FlashAttention introduces IO-aware algorithms that optimize memory access patterns\n- Lightning Attention achieves constant training speed across sequence lengths\n- DejaVu demonstrates effective contextual sparsity for inference optimization\n\nb) Hardware Optimization:\n- Tiling and hierarchical processing improve hardware utilization\n- Hardware-aware implementations crucial for real performance gains\n- Memory hierarchy optimization key for efficiency\n\nc) Selective Computation:\n- Contextual sparsity can reduce computation without quality loss\n- Input-dependent parameter selection improves efficiency\n- Hybrid architectures combining multiple mechanisms show promise\n\n2. Useful References with Excerpts\n\nFrom FlashAttention:\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.\"\nKey insight: IO-aware algorithms crucial for real efficiency gains\n\nFrom DejaVu:\n\"We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues.\"\nKey insight: Input-dependent sparsity can maintain quality while improving efficiency\n\nFrom Lightning Attention:\n\"We split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks.\"\nKey insight: Hybrid computation strategies can optimize different parts of the model\n\n3. Key Insights and Analysis\n\na) Memory Management:\n- Tiling and blocking crucial for efficient memory access\n- IO-aware algorithms can significantly reduce memory bandwidth requirements\n- Hierarchical state management improves efficiency\n\nb) Hardware Optimization:\n- GPU memory hierarchy must be considered in algorithm design\n- Asynchronous execution can improve hardware utilization\n- Specialized implementations needed for different hardware targets\n\nc) Implementation Strategies:\n- Block-based processing improves cache utilization\n- Selective computation based on input characteristics\n- Hybrid approaches combining multiple techniques show promise\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient state compression\n2. Methods for dynamic sparsity in neural networks\n3. Hardware-specific optimization strategies\n\n5. References\n\n1. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n2. Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z., ... & Chen, B. (2023). Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time. International Conference on Machine Learning.\n\n3. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention. International Conference on Machine Learning.\n\n4. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n5. Hubara, I., Chmiel, B., Island, M., Banner, R., Naor, S., & Soudry, D. (2021). Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N:M Transposable Masks. Neural Information Processing Systems."
        },
        {
            "ready": true,
            "query": null,
            "detail": "**Analysis Note:**\n\n1. **Summary of the Analysis:**\n\n   The proposal aims to enhance the **FastTTTLinear** GAU by introducing **dynamic sparsity** and **efficient FFT-based computation**, resulting in the **DynamicSparseFFTLinear** unit. The objectives are to:\n\n   - Improve performance on long sequences.\n   - Reduce memory and computational requirements.\n   - Combine selective computation through learned sparsity patterns with hardware-efficient FFT implementations.\n\n   After thorough investigation across multiple rounds of searches, it appears that:\n\n   - **Dynamic sparsity** and **FFT-based computation** have individually been explored in Transformer models and neural networks to improve efficiency and scalability.\n   - There is **limited evidence** of prior work that combines both **dynamic sparsity** and **FFT-based computation** within Transformer architectures or similar models.\n   - The proposal is therefore likely **novel**, contributing a new approach to efficient sequence modeling.\n\n2. **All Useful References with Excerpts:**\n\n   - **Roy et al. (2020): Efficient Content-Based Sparse Attention with Routing Transformers**\n\n     *Excerpt:* \"Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest... It combines the modeling flexibility of prior work on content-based sparse attention with efficiency gains from approaches based on local, temporal sparse attention.\"\n\n   - **Anagnostidis et al. (2023): Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**\n\n     *Excerpt:* \"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process.\"\n\n   - **Fu et al. (2023): FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**\n\n     *Excerpt:* \"We study how to optimize the FFT convolution... Present two sparse convolution algorithms\u20141) partial convolutions and 2) frequency-sparse convolutions\u2014which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings.\"\n\n   - **He et al. (2023): Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**\n\n     *Excerpt:* \"We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequences using the ready-made Fast Fourier Transform (FFT) operator... Our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space.\"\n\n   - **Pagliardini et al. (2023): Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention**\n\n     *Excerpt:* \"We extend FlashAttention to accommodate a large class of attention sparsity patterns... This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\"\n\n   - **Lee et al. (2023): SEA: Sparse Linear Attention with Estimated Attention Mask**\n\n     *Excerpt:* \"SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then subsequently creates a sparse attention matrix with a top-k selection to perform a sparse attention operation... SEA achieves better perplexity than OPT-1.3B, using roughly half the memory.\"\n\n   - **Lou et al. (2024): Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**\n\n     *Excerpt:* \"We propose SPARSEK Attention, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance... Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query.\"\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Dynamic Sparsity in Transformers:**\n     - Dynamic sparsity reduces computational load by focusing attention on relevant input parts.\n     - Techniques like content-based sparse attention (Roy et al., 2020) and dynamic context pruning (Anagnostidis et al., 2023) have shown efficiency gains without significant performance loss.\n     - Channel-level sparsity (Yin et al., 2023) translates unstructured sparsity into hardware-friendly formats.\n\n   - **FFT-Based Computation in Transformers:**\n     - FFT accelerates computations for long sequences by transforming operations into the frequency domain.\n     - Fourier Transformer (He et al., 2023) uses FFT to remove sequence redundancy, improving efficiency.\n     - FlashFFTConv (Fu et al., 2023) optimizes FFT convolutions and introduces sparse convolution algorithms by skipping blocks in matrix decompositions.\n\n   - **Combining Dynamic Sparsity with FFT:**\n     - Sparse FFT algorithms are less common in neural network contexts and present implementation challenges.\n     - FFT assumes regular data structures, whereas dynamic sparsity introduces irregular patterns.\n     - Potential approaches might involve structuring sparsity patterns to align with FFT's requirements or modifying FFT algorithms to handle sparse inputs.\n\n   - **Implementation Challenges:**\n     - Aligning dynamically changing sparsity patterns with FFT's structured computations.\n     - Ensuring efficient hardware utilization, especially on GPUs optimized for dense operations.\n     - Maintaining training stability and model performance while integrating both techniques.\n\n   - **Novelty Assessment:**\n     - No direct evidence of prior work combining dynamic sparsity with FFT-based computation in Transformers or GAUs.\n     - The proposal is likely novel and could offer significant contributions to efficient long-sequence modeling.\n\n   - **Potential Impact:**\n     - If successfully implemented, could lead to substantial efficiency improvements, enabling models to handle longer sequences with reduced computational resources.\n     - Aligns with ongoing research trends focusing on making Transformers more efficient and scalable.\n\n4. **Future Search Plan or Next Steps:**\n\n   - **Investigate Sparse FFT Algorithms:**\n     - Explore literature on sparse FFT algorithms and their applicability to neural networks.\n     - Assess how sparse FFTs have been used in other domains and whether those methods can be adapted.\n\n   - **Examine Recent Publications:**\n     - Look for the latest research (late 2023 and early 2024) that might have explored similar integrations.\n     - Use platforms like arXiv and conference proceedings from NeurIPS, ICLR, ICML, etc.\n\n   - **Assess Hardware Feasibility:**\n     - Research hardware capabilities for supporting sparse FFT operations.\n     - Evaluate whether current GPU architectures can efficiently handle the proposed computations.\n\n   - **Prototype Development:**\n     - Consider developing a small-scale prototype to test the integration of dynamic sparsity with FFT-based computation.\n     - Use empirical results to inform feasibility and potential performance gains.\n\n   - **Community Engagement:**\n     - Engage with researchers working on efficient Transformers, dynamic sparsity, and FFT computations.\n     - Seek feedback and identify any ongoing unpublished work related to the proposal.\n\n5. **List of References:**\n\n   - Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). **Efficient Content-Based Sparse Attention with Routing Transformers**. *Transactions of the Association for Computational Linguistics*, 9, 53-68.\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). **Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**. *Advances in Neural Information Processing Systems*.\n\n   - Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). **FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**. *arXiv preprint arXiv:2306.09322*.\n\n   - He, Z., Yang, M., Feng, M., Yin, J., Wang, X., Leng, J., & Lin, Z. (2023). **Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**. *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics*.\n\n   - Pagliardini, M., Paliotta, D., Jaggi, M., & Fleuret, F. (2023). **Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention**. *Advances in Neural Information Processing Systems*.\n\n   - Lee, H., Kim, J., Willette, J., & Hwang, S. J. (2023). **SEA: Sparse Linear Attention with Estimated Attention Mask**. *International Conference on Learning Representations*.\n\n   - Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). **Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**. *arXiv preprint arXiv:2401.01234*.\n\n   - Yin, L., Li, G., Fang, M., Shen, L., Huang, T., Wang, Z., Menkovski, V., Ma, X., Pechenizkiy, M., & Liu, S. (2023). **Dynamic Sparsity Is Channel-Level Sparsity Learner**. *Advances in Neural Information Processing Systems*.\n\n   - Additional references as needed.\n\n---\n\n**I'm ready.**",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\n**Analysis Note:**\n\n1. **Summary of the Analysis:**\n\n   The proposal aims to enhance the **FastTTTLinear** GAU by introducing **dynamic sparsity** and **efficient FFT-based computation**, resulting in the **DynamicSparseFFTLinear** unit. The objectives are to:\n\n   - Improve performance on long sequences.\n   - Reduce memory and computational requirements.\n   - Combine selective computation through learned sparsity patterns with hardware-efficient FFT implementations.\n\n   After thorough investigation across multiple rounds of searches, it appears that:\n\n   - **Dynamic sparsity** and **FFT-based computation** have individually been explored in Transformer models and neural networks to improve efficiency and scalability.\n   - There is **limited evidence** of prior work that combines both **dynamic sparsity** and **FFT-based computation** within Transformer architectures or similar models.\n   - The proposal is therefore likely **novel**, contributing a new approach to efficient sequence modeling.\n\n2. **All Useful References with Excerpts:**\n\n   - **Roy et al. (2020): Efficient Content-Based Sparse Attention with Routing Transformers**\n\n     *Excerpt:* \"Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest... It combines the modeling flexibility of prior work on content-based sparse attention with efficiency gains from approaches based on local, temporal sparse attention.\"\n\n   - **Anagnostidis et al. (2023): Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**\n\n     *Excerpt:* \"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process.\"\n\n   - **Fu et al. (2023): FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**\n\n     *Excerpt:* \"We study how to optimize the FFT convolution... Present two sparse convolution algorithms\u20141) partial convolutions and 2) frequency-sparse convolutions\u2014which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings.\"\n\n   - **He et al. (2023): Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**\n\n     *Excerpt:* \"We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequences using the ready-made Fast Fourier Transform (FFT) operator... Our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space.\"\n\n   - **Pagliardini et al. (2023): Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention**\n\n     *Excerpt:* \"We extend FlashAttention to accommodate a large class of attention sparsity patterns... This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\"\n\n   - **Lee et al. (2023): SEA: Sparse Linear Attention with Estimated Attention Mask**\n\n     *Excerpt:* \"SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then subsequently creates a sparse attention matrix with a top-k selection to perform a sparse attention operation... SEA achieves better perplexity than OPT-1.3B, using roughly half the memory.\"\n\n   - **Lou et al. (2024): Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**\n\n     *Excerpt:* \"We propose SPARSEK Attention, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance... Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query.\"\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Dynamic Sparsity in Transformers:**\n     - Dynamic sparsity reduces computational load by focusing attention on relevant input parts.\n     - Techniques like content-based sparse attention (Roy et al., 2020) and dynamic context pruning (Anagnostidis et al., 2023) have shown efficiency gains without significant performance loss.\n     - Channel-level sparsity (Yin et al., 2023) translates unstructured sparsity into hardware-friendly formats.\n\n   - **FFT-Based Computation in Transformers:**\n     - FFT accelerates computations for long sequences by transforming operations into the frequency domain.\n     - Fourier Transformer (He et al., 2023) uses FFT to remove sequence redundancy, improving efficiency.\n     - FlashFFTConv (Fu et al., 2023) optimizes FFT convolutions and introduces sparse convolution algorithms by skipping blocks in matrix decompositions.\n\n   - **Combining Dynamic Sparsity with FFT:**\n     - Sparse FFT algorithms are less common in neural network contexts and present implementation challenges.\n     - FFT assumes regular data structures, whereas dynamic sparsity introduces irregular patterns.\n     - Potential approaches might involve structuring sparsity patterns to align with FFT's requirements or modifying FFT algorithms to handle sparse inputs.\n\n   - **Implementation Challenges:**\n     - Aligning dynamically changing sparsity patterns with FFT's structured computations.\n     - Ensuring efficient hardware utilization, especially on GPUs optimized for dense operations.\n     - Maintaining training stability and model performance while integrating both techniques.\n\n   - **Novelty Assessment:**\n     - No direct evidence of prior work combining dynamic sparsity with FFT-based computation in Transformers or GAUs.\n     - The proposal is likely novel and could offer significant contributions to efficient long-sequence modeling.\n\n   - **Potential Impact:**\n     - If successfully implemented, could lead to substantial efficiency improvements, enabling models to handle longer sequences with reduced computational resources.\n     - Aligns with ongoing research trends focusing on making Transformers more efficient and scalable.\n\n4. **Future Search Plan or Next Steps:**\n\n   - **Investigate Sparse FFT Algorithms:**\n     - Explore literature on sparse FFT algorithms and their applicability to neural networks.\n     - Assess how sparse FFTs have been used in other domains and whether those methods can be adapted.\n\n   - **Examine Recent Publications:**\n     - Look for the latest research (late 2023 and early 2024) that might have explored similar integrations.\n     - Use platforms like arXiv and conference proceedings from NeurIPS, ICLR, ICML, etc.\n\n   - **Assess Hardware Feasibility:**\n     - Research hardware capabilities for supporting sparse FFT operations.\n     - Evaluate whether current GPU architectures can efficiently handle the proposed computations.\n\n   - **Prototype Development:**\n     - Consider developing a small-scale prototype to test the integration of dynamic sparsity with FFT-based computation.\n     - Use empirical results to inform feasibility and potential performance gains.\n\n   - **Community Engagement:**\n     - Engage with researchers working on efficient Transformers, dynamic sparsity, and FFT computations.\n     - Seek feedback and identify any ongoing unpublished work related to the proposal.\n\n5. **List of References:**\n\n   - Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). **Efficient Content-Based Sparse Attention with Routing Transformers**. *Transactions of the Association for Computational Linguistics*, 9, 53-68.\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). **Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**. *Advances in Neural Information Processing Systems*.\n\n   - Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). **FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**. *arXiv preprint arXiv:2306.09322*.\n\n   - He, Z., Yang, M., Feng, M., Yin, J., Wang, X., Leng, J., & Lin, Z. (2023). **Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**. *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics*.\n\n   - Pagliardini, M., Paliotta, D., Jaggi, M., & Fleuret, F. (2023). **Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention**. *Advances in Neural Information Processing Systems*.\n\n   - Lee, H., Kim, J., Willette, J., & Hwang, S. J. (2023). **SEA: Sparse Linear Attention with Estimated Attention Mask**. *International Conference on Learning Representations*.\n\n   - Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). **Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**. *arXiv preprint arXiv:2401.01234*.\n\n   - Yin, L., Li, G., Fang, M., Shen, L., Huang, T., Wang, Z., Menkovski, V., Ma, X., Pechenizkiy, M., & Liu, S. (2023). **Dynamic Sparsity Is Channel-Level Sparsity Learner**. *Advances in Neural Information Processing Systems*.\n\n   - Additional references as needed.\n\n---\n\n**I'm ready.**\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.96)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 40/41 (Score: 0.96)*\n\n```\nHowever, our method is not dependent on the input modality. Future research involving vision or speech could further substantiate the robustness of our method. ## E Impact Statement\n\nThis paper presents SPARSEK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing. We believe our innovative attention mechanism can benefit both NLP and machine learning communities in constructing long-range foundation models. Specifically, we highlight the potential impacts of SPARSEK as follows:\n\n- Efficient Long-Range Modeling. First and foremost, the SPARSEK attention mechanism significantly reduces computational requirements compared to traditional self-attention mechanisms. By prioritizing a subset of key-value pairs, SPARSEK attention effectively reduces the memory footprint without sacrificing model performance.\n```\n\n#### 2. Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention (Avg. Score: 0.93)\n\n*Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Chuanfu Xiao, Xingcheng Zhang, Dahua Lin, Chao Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper proposes SampleAttention, an adaptive structured and near-lossless sparse attention, which can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times compared with FlashAttention.\n\n**Abstract:** Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.\n\n##### *Relevant Chunk: No. 8/25 (Score: 0.93)*\n\n```\narXiv preprint arXiv:2312.11805, 2023. [15] Anthropic. Claude. https://www.anthropic.com/claude 2023. [16] Moonshot. Kimi chat. https://kimi.moonshot.cn/, 2023. [17] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360, 2021. [18] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. ETC: Encoding long and structured inputs in transformers. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268-284, Online, November 2020. Association for Computational Linguistics. [19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [20] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020. [21] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. [22] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to $1,000,000,000$ tokens. arXiv preprint arXiv:2307.02486, 2023. [23] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [24] Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, and Fran\u00e7ois Fleuret. Faster causal attention over large sequences through sparse flash attention. arXiv preprint arXiv:2306.01160, 2023. [25] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. [26] Insu Han, Rajesh Jayaram, Amin Karbasi, Vahab Mirrokni, David Woodruff, and Amir Zandieh. Hyperattention: Long-context attention in near-linear time. In The Twelfth International Conference on Learning Representations, 2023. [27] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. [28] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020. [29] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.93)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 14/46 (Score: 0.93)*\n\n```\narXiv preprint arXiv:2204.02311, 2022. [18] Eleanor Chu and Alan George. Inside the FFT black box: serial and parallel fast Fourier transform algorithms. CRC press, 1999. [19] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965. [20] Fiona Cunningham, James E Allen, Jamie Allen, Jorge Alvarez-Jarreta, M Ridwan Amode, Irina M Armean, Olanrewaju Austine-Orimoloye, Andrey G Azov, If Barnes, Ruth Bennett, et al. Ensembl 2022. Nucleic acids research, 50(D1):D988-D995, 2022. [21] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, $33: 4271-4282,2020$\n[22] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [23] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training.\n```\n\n#### 4. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.90)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 37/41 (Score: 0.90)*\n\n```\narXiv preprint arXiv:2304.10592, 2023. ## A. Comprehensive Related Works\n\nAutoregressive LLMs. Transformers (Vaswani et al., 2017; Dosovitskiy et al., 2021) have significantly advanced the fields of language and vision, leading to the development of foundation LLMs such as ChatGPT (Brown et al., 2020; OpenAI, 2023b), LLaMA (Touvron et al., 2023a;b), Gemini (Anil et al., 2023), DALL-E (Ramesh et al., 2021), etc. To date, various Transformers have emerged to serve distinct needs, broadly categorized into three types: encoder-based, decoder-based, and encoder-decoder models. Encoder-based models like BERT (Devlin et al., 2018) focus on natural language understanding and are also commonly used in image processing (Dosovitskiy et al., 2021). Encoder-decoder models like the original Transformer (Vaswani et al., 2017), Bard (Waisberg et al., 2023), and T5 (Raffel et al., 2020; Roberts et al., 2022) are designed for sequence-to-sequence tasks (e.g., translation, speech recognition), where the encoder extracts features and the decoder produces outputs based on these features. Decoder-based models, including GPT (Radford et al., 2019; OpenAI, 2023b) and LLaMA (Touvron et al., 2023a), generate text sequentially by predicting the next token based on previous ones. All these models leverage Transformer architectures but differ in their specific purposes and structures. Both encoders and decoders are leveraged in multimodal models like MiniGPT (Zhu et al., 2023; Chen et al., 2023b) and DALL-E (Ramesh et al., 2021). Note that the model architectures used in all categories are based on Transformer. The primary difference lies in their purpose: the encoder is designed to extract features, while the decoder focuses on scoring and generating outputs. Our work presents a comprehensive study of applying linear attention techniques to the encoder/decoder-based LLMs. Efficient Linear Attention. Transformers' self-attention modules, known for their quadratic computational complexity (Zhu et al., 2021; Katharopoulos et al., 2020), have spurred the development of linear attention methods to improve efficiency, especially in encoder-based LLMs for better training and inference. Techniques such as local attentions (Liu et al., 2021; Arar et al., 2022; Wang et al., 2020; Tu et al., 2022; You et al., 2023a) limit self-attention to neighboring tokens or group attention queries to reduce the computational cost, while kernel-based linear attentions (Liu et al., 2021; Arar et al., 2022; Wang et al., 2020; Tu et al., 2022; You et al., 2024) decompose the softmax with kernel functions and exchange the computation order. However, only a few linear attention approaches focus on decoder-based autoregressive LLMs, aiming to reduce RNN-style sequential state updates over a large number of steps (Hua et al., 2022; Katharopoulos et al., 2020). Recent studies, like LongLoRA (Chen et al., 2023c), aim to adapt local attention techniques for efficient fine-tuning of pre-trained autoregressive LLMs, yet a thorough analysis comparing various linear attention methods for autoregressive LLMs remains lacking. This paper uniquely provides a systematic review of existing linear attentions for decoder-based autoregressive LLMs and investigates how to efficiently enhance less effective linear attention methods. Speculative Decoding. Linear attention techniques alleviate the training inefficiency in LLMs by mitigating the quadratic complexity with regard to the number of input tokens. However, during deployment, autoregressive decoding necessitates sequential token-by-token text generation, which curtails parallelism and restricts the number of input tokens. Speculative decoding (Chen et al., 2023a; Miao et al., 2023; Kim et al., 2023; Leviathan et al., 2023; Cai et al., 2023b) has proven to be an effective strategy for boosting parallelism in LLM serving, utilizing small speculative models for initial generation, with original LLMs serving as validators to assess if the output meets standards or needs resampling. Recent works like Medusa (Cai et al., 2023b) further argue that the small speculative models and LLMs can be the same model, and other studies (Schuster et al., 2022; Bae et al., 2023) suggest using shallow layers for generation and deeper layers for verification, based on early exit strategies. Such speculative decoding and linear attention jointly ensure efficient LLM training and generation, especially for long sequence inputs. In this paper, we take the initiative to investigate the synergy between linearized LLMs and speculative sampling, to improve the efficiency of training and serving LLMs. ## B. More Visualization of Training Trajectories. As detailed in Sec. 5.3, we present a quantitative analysis comparing local LAs, grouped LAs, and our augmented LAs that combine both local augmentation and grouped LAs. This appendix provides the training trajectories for GPT-2 using these LA methods. Fig. 10 demonstrates that our local augmentation, specifically masked DWConv, effectively enhances both local and grouped LAs. Moreover, our augmented LAs, which integrate local augmentation with grouped LAs, exhibit the most favorable convergence in terms of loss. ## C. More Profiling on the LLaMA-2-7B Model\n\nWe provide detailed profiling and comparisons below to illustrate the runtime distribution between attention and feed-forward networks (FFNs), highlighting that attention is a bottleneck even for LLMs with 7B parameters. To ensure a real-world\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1cc36bd06f7697d750b0g-15.jpg?height=446&width=859&top_left_y=227&top_left_x=596)\n\nFigure 10. Visualizing the training trajectories of baseline LAs and our augmented LAs. application scenario, we profiled the LLaMA-2-7B model across four settings of prefill and decode sizes, adhering to benchmarks commonly used in academia and industry, as summarized in Tab. 10. | (Prefill, Decode) | Task | Dataset | Referenced Paper |\n| :---: | :---: | :---: | :---: |\n| $(340,160)$ | Chat | ShareGPT | (Kwon et al., 2023) |\n| $(60,20)$ | Chat | Stanford Alpaca | (Taori et al., 2023) |\n| $(7000,200)$ | Summarization | ArXiv Summarization | (Agrawal et al., 2024) |\n| $(1700,400)$ | Chat | OpenChat ShareGPT 4 | (Agrawal et al., 2024) |\n\nAs shown in Tab. 11, profiling the LLaMA-2-7B models under the four prefill and decode size settings reveals that the average runtime latency attributed to attention and FFNs accounts for $55 \\%$ and $21 \\%$ of the total runtime across these settings, respectively. This indicates that although FFNs are a bottleneck in the model, attention is an even more significant bottleneck, especially for large-scale LLMs and extended dialogue sequences (e.g., $67.8 \\%$ runtime latency for the arxiv summarization task). Therefore, optimizing attention blocks can yield considerable speed improvements, particularly for tasks with large prefill or decode sequence lengths. This is corroborated by contemporary studies on linear attention-based LLMs (Lee et al., 2023; Yang et al., 2023) and efforts to optimize attention, such as FlashAttention (Dao et al., 2022) and FLAT (Kao et al., 2023). Table 11. Latency breakdown of LLaMA-2-7B models under different prefill and decode size settings. | (Prefill, Decode) | $\\mathbf{( 3 4 0 , 1 6 0 )}$ | $\\mathbf{( 6 0 , 2 0 )}$ | $\\mathbf{( 7 0 0 0 , 2 0 0 )}$ | $\\mathbf{( 1 7 0 0 , 4 0 0 )}$ |\n| :--- | :---: | :---: | :---: | :---: |\n| Attention (ms) | $158.97 \\mathbf{( 4 8 .\n```\n\n#### 5. FLuRKA: Fast and accurate unified Low-Rank&Kernel Attention (Avg. Score: 0.90)\n\n*Ahan Gupta, Hao Guo, Yueming Yuan, Yan-Quan Zhou, Charith Mendis*\n\n**Published in:**  (2023)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work theoretically and empirically evaluates the speed and quality of FLuRKA, a new class of transformers that achieve comparable accuracy with underlying low-rank and kernel approximations, occasionally surpassing both.\n\n**Abstract:** Many efficient $\\textit{approximate}$ self-attention techniques have become prevalent since the inception of the transformer architecture. Two popular classes of these techniques are low-rank and kernel methods. Each of these methods has its strengths. We observe these strengths synergistically complement each other and exploit them to fuse low-rank and kernel methods, producing a new class of transformers: FLuRKA ($\\textbf{F}$ast $\\textbf{L}$ow-$\\textbf{R}$ank&$\\textbf{K}$ernel$ \\textbf{A}$ttention). FLuRKA are highly $\\textit{training-efficient}$ with faster model speeds $\\textit{and}$ similar model qualities compared to constituent low-rank and kernel methods. We theoretically and empirically evaluate the speed and quality of FLuRKA. Our model speed analysis posits a variety of parameter configurations where FLuRKA exhibit speedups over low-rank and kernel approximations and our model quality analysis bounds the error of FLuRKA with respect to full-attention. Empirically, we instantiate three FLuRKA variants which experience speedups of up to 3.3x and 1.7x over low-rank and kernel methods respectively. This translates to speedups of up to 20x over models with flash-attention. Across a diverse set of tasks spanning language modeling, language understanding, long sequence modeling, machine translation, and image classification, FLuRKA achieve comparable accuracy with underlying low-rank and kernel approximations, occasionally surpassing both.\n\n##### *Relevant Chunk: No. 3/83 (Score: 0.90)*\n\n```\n## 1 Introduction\n\nTransformers have been widely adopted across various domains, powering popular applications like ChatGPT, Gemini Pro, and Claude, which handle millions of queries per day Meyer et al. [2023]. To effectively train and deploy high-quality models at this scale, transformers must be training efficient. This entails balancing two crucial factors. (1) Model speed, which corresponds to models with low step-times and fewer FLOPs per step. (2) Model quality, which corresponds to expressive models that yield low losses with few training tokens. Highly training-efficient transformers are fast and of high quality. However, achieving both simultaneously is challenging, as higher quality transformers often require larger parameter counts and data-set sizes Li et al. [2020], Hoffmann et al. [2022], leading to slower model speeds. This trade-off is further exacerbated by the quadratic dependence in run-time\non increasing input sequence lengths ( 32 k for GPT-4, 100k for Claude, and 1.5 M for Gemini-pro OpenAI 2023], Anthropic [2023], Team et al. [2024]). To design training-efficient transformers, researchers have developed linear time self-attention approximations, falling into three main categories: sparse Child et al.\n```\n\n\n\n---\n## Web search results\n\n To help the researcher achieve the goal of designing a novel autoregressive language model (LM) block that combines dynamic sparsity and FFT-based computation, here are the key points and useful information extracted from the analysis note and additional sources:\n\n## Key Objectives and Insights\n\n- **Improve Performance on Long Sequences**: The researcher aims to enhance the `FastTTTLinear` GAU by introducing dynamic sparsity and efficient FFT-based computation to handle long sequences more efficiently[Analysis Note].\n- **Reduce Computational and Memory Requirements**: Techniques such as dynamic sparsity and FFT-based computation are intended to reduce the computational load and memory usage[Analysis Note].\n\n## Dynamic Sparsity in Transformers\n\n- **Content-Based Sparse Attention**: Roy et al. (2020) proposed learning dynamic sparse attention patterns to avoid unnecessary computations, which aligns with the researcher's goal of dynamic sparsity[Analysis Note, Roy et al., 2020].\n- **Dynamic Context Pruning**: Anagnostidis et al. (2023) introduced dynamic context pruning to reduce memory and computational requirements, which is relevant for maintaining model expressiveness while reducing resources[Analysis Note, Anagnostidis et al., 2023].\n- **Channel-Level Sparsity**: Yin et al. (2023) discussed translating unstructured sparsity into hardware-friendly formats, which could be crucial for efficient implementation on GPUs[Analysis Note, Yin et al., 2023].\n\n## FFT-Based Computation in Transformers\n\n- **Fourier Transformer**: He et al. (2023) proposed using the FFT operator to remove sequence redundancy, improving efficiency in long-range modeling[Analysis Note, He et al., 2023].\n- **FlashFFTConv**: Fu et al. (2023) optimized FFT convolutions and introduced sparse convolution algorithms, which can be adapted for the proposed model[Analysis Note, Fu et al., 2023].\n\n## Combining Dynamic Sparsity with FFT\n\n- **Challenges and Opportunities**: The main challenge is aligning dynamically changing sparsity patterns with the structured computations of FFT. Potential approaches include structuring sparsity patterns to align with FFT requirements or modifying FFT algorithms to handle sparse inputs[Analysis Note].\n- **Novelty**: There is limited evidence of prior work combining both dynamic sparsity and FFT-based computation within Transformer architectures, making this proposal likely novel[Analysis Note].\n\n## Future Search Plan and Next Steps\n\n### Investigate Sparse FFT Algorithms\n- **Literature Review**: Explore literature on sparse FFT algorithms and their applicability to neural networks. This could involve looking into signal processing and computational mathematics research where sparse FFTs are commonly used.\n- **Adaptation to Neural Networks**: Assess how sparse FFTs have been used in other domains and whether those methods can be adapted for neural networks.\n\n### Examine Recent Publications\n- **Latest Research**: Look for the latest research (late 2023 and early 2024) on platforms like arXiv, NeurIPS, ICLR, and ICML proceedings. This will help identify any recent advancements or similar integrations that might have been explored.\n\n### Assess Hardware Feasibility\n- **GPU Capabilities**: Research current GPU architectures and their capabilities for supporting sparse FFT operations. Evaluate whether existing hardware can efficiently handle the proposed computations.\n- **Custom Hardware Solutions**: If necessary, explore custom hardware solutions or optimizations that could support the unique requirements of combining dynamic sparsity with FFT.\n\n### Prototype Development\n- **Small-Scale Prototype**: Develop a small-scale prototype to test the integration of dynamic sparsity with FFT-based computation. Use empirical results to inform feasibility and potential performance gains.\n- **Empirical Testing**: Conduct thorough empirical testing to validate the performance, efficiency, and stability of the proposed model.\n\n### Community Engagement\n- **Researcher Collaboration**: Engage with researchers working on efficient Transformers, dynamic sparsity, and FFT computations. This could involve attending conferences, participating in research forums, or collaborating directly with experts in these areas.\n- **Feedback and Ongoing Work**: Seek feedback on the proposal and identify any ongoing unpublished work related to the integration of dynamic sparsity and FFT-based computation.\n\n## Additional Resources\n\nWhile the provided sources do not directly address the specific combination of dynamic sparsity and FFT-based computation in Transformers, here are some general resources that could be useful:\n\n- **Signal Processing Literature**: Resources like the IEEE Signal Processing Society publications or journals focused on computational mathematics could provide insights into sparse FFT algorithms and their adaptations.\n- **Neural Network Optimization**: Research papers and articles on optimizing neural networks, especially those focusing on efficient attention mechanisms and FFT-based computations, would be highly relevant.\n- **Hardware Optimization**: Resources from GPU manufacturers (e.g., NVIDIA, AMD) and computational hardware research could help in assessing and optimizing hardware feasibility.\n\nBy following these steps and leveraging the insights from the analysis note, the researcher can make significant progress in developing a novel autoregressive LM block that combines dynamic sparsity and FFT-based computation.\n\n\nWarning: No keywords detected, external search skipped, please wrap your keywords in a quoted block like this: ```keywords {{Your keywods}} ``` in your response next time.\n\nWarning: No description detected, will use full response to search internal library, please wrap your description in a quoted block like this: ```description {{Your description}}``` in your response next time.",
            "analysis": "**Analysis Note:**\n\n1. **Summary of the Analysis:**\n\n   The proposal aims to enhance the **FastTTTLinear** GAU by introducing **dynamic sparsity** and **efficient FFT-based computation**, resulting in the **DynamicSparseFFTLinear** unit. The objectives are to:\n\n   - Improve performance on long sequences.\n   - Reduce memory and computational requirements.\n   - Combine selective computation through learned sparsity patterns with hardware-efficient FFT implementations.\n\n   After thorough investigation across multiple rounds of searches, it appears that:\n\n   - **Dynamic sparsity** and **FFT-based computation** have individually been explored in Transformer models and neural networks to improve efficiency and scalability.\n   - There is **limited evidence** of prior work that combines both **dynamic sparsity** and **FFT-based computation** within Transformer architectures or similar models.\n   - The proposal is therefore likely **novel**, contributing a new approach to efficient sequence modeling.\n\n2. **All Useful References with Excerpts:**\n\n   - **Roy et al. (2020): Efficient Content-Based Sparse Attention with Routing Transformers**\n\n     *Excerpt:* \"Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest... It combines the modeling flexibility of prior work on content-based sparse attention with efficiency gains from approaches based on local, temporal sparse attention.\"\n\n   - **Anagnostidis et al. (2023): Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**\n\n     *Excerpt:* \"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process.\"\n\n   - **Fu et al. (2023): FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**\n\n     *Excerpt:* \"We study how to optimize the FFT convolution... Present two sparse convolution algorithms\u20141) partial convolutions and 2) frequency-sparse convolutions\u2014which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings.\"\n\n   - **He et al. (2023): Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**\n\n     *Excerpt:* \"We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequences using the ready-made Fast Fourier Transform (FFT) operator... Our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space.\"\n\n   - **Pagliardini et al. (2023): Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention**\n\n     *Excerpt:* \"We extend FlashAttention to accommodate a large class of attention sparsity patterns... This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\"\n\n   - **Lee et al. (2023): SEA: Sparse Linear Attention with Estimated Attention Mask**\n\n     *Excerpt:* \"SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then subsequently creates a sparse attention matrix with a top-k selection to perform a sparse attention operation... SEA achieves better perplexity than OPT-1.3B, using roughly half the memory.\"\n\n   - **Lou et al. (2024): Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**\n\n     *Excerpt:* \"We propose SPARSEK Attention, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance... Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query.\"\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Dynamic Sparsity in Transformers:**\n     - Dynamic sparsity reduces computational load by focusing attention on relevant input parts.\n     - Techniques like content-based sparse attention (Roy et al., 2020) and dynamic context pruning (Anagnostidis et al., 2023) have shown efficiency gains without significant performance loss.\n     - Channel-level sparsity (Yin et al., 2023) translates unstructured sparsity into hardware-friendly formats.\n\n   - **FFT-Based Computation in Transformers:**\n     - FFT accelerates computations for long sequences by transforming operations into the frequency domain.\n     - Fourier Transformer (He et al., 2023) uses FFT to remove sequence redundancy, improving efficiency.\n     - FlashFFTConv (Fu et al., 2023) optimizes FFT convolutions and introduces sparse convolution algorithms by skipping blocks in matrix decompositions.\n\n   - **Combining Dynamic Sparsity with FFT:**\n     - Sparse FFT algorithms are less common in neural network contexts and present implementation challenges.\n     - FFT assumes regular data structures, whereas dynamic sparsity introduces irregular patterns.\n     - Potential approaches might involve structuring sparsity patterns to align with FFT's requirements or modifying FFT algorithms to handle sparse inputs.\n\n   - **Implementation Challenges:**\n     - Aligning dynamically changing sparsity patterns with FFT's structured computations.\n     - Ensuring efficient hardware utilization, especially on GPUs optimized for dense operations.\n     - Maintaining training stability and model performance while integrating both techniques.\n\n   - **Novelty Assessment:**\n     - No direct evidence of prior work combining dynamic sparsity with FFT-based computation in Transformers or GAUs.\n     - The proposal is likely novel and could offer significant contributions to efficient long-sequence modeling.\n\n   - **Potential Impact:**\n     - If successfully implemented, could lead to substantial efficiency improvements, enabling models to handle longer sequences with reduced computational resources.\n     - Aligns with ongoing research trends focusing on making Transformers more efficient and scalable.\n\n4. **Future Search Plan or Next Steps:**\n\n   - **Investigate Sparse FFT Algorithms:**\n     - Explore literature on sparse FFT algorithms and their applicability to neural networks.\n     - Assess how sparse FFTs have been used in other domains and whether those methods can be adapted.\n\n   - **Examine Recent Publications:**\n     - Look for the latest research (late 2023 and early 2024) that might have explored similar integrations.\n     - Use platforms like arXiv and conference proceedings from NeurIPS, ICLR, ICML, etc.\n\n   - **Assess Hardware Feasibility:**\n     - Research hardware capabilities for supporting sparse FFT operations.\n     - Evaluate whether current GPU architectures can efficiently handle the proposed computations.\n\n   - **Prototype Development:**\n     - Consider developing a small-scale prototype to test the integration of dynamic sparsity with FFT-based computation.\n     - Use empirical results to inform feasibility and potential performance gains.\n\n   - **Community Engagement:**\n     - Engage with researchers working on efficient Transformers, dynamic sparsity, and FFT computations.\n     - Seek feedback and identify any ongoing unpublished work related to the proposal.\n\n5. **List of References:**\n\n   - Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). **Efficient Content-Based Sparse Attention with Routing Transformers**. *Transactions of the Association for Computational Linguistics*, 9, 53-68.\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). **Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**. *Advances in Neural Information Processing Systems*.\n\n   - Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). **FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**. *arXiv preprint arXiv:2306.09322*.\n\n   - He, Z., Yang, M., Feng, M., Yin, J., Wang, X., Leng, J., & Lin, Z. (2023). **Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**. *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics*.\n\n   - Pagliardini, M., Paliotta, D., Jaggi, M., & Fleuret, F. (2023). **Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention**. *Advances in Neural Information Processing Systems*.\n\n   - Lee, H., Kim, J., Willette, J., & Hwang, S. J. (2023). **SEA: Sparse Linear Attention with Estimated Attention Mask**. *International Conference on Learning Representations*.\n\n   - Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). **Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**. *arXiv preprint arXiv:2401.01234*.\n\n   - Yin, L., Li, G., Fang, M., Shen, L., Huang, T., Wang, Z., Menkovski, V., Ma, X., Pechenizkiy, M., & Liu, S. (2023). **Dynamic Sparsity Is Channel-Level Sparsity Learner**. *Advances in Neural Information Processing Systems*.\n\n   - Additional references as needed.\n\n---\n\n**I'm ready.**"
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": [
                "dynamic sparsity",
                "FFT-based attention",
                "efficient transformers"
            ],
            "detail": "Seeking research papers, implementations, or design proposals that involve integrating dynamic sparsity and FFT-based computations into Transformer models or similar architectures. Specifically interested in works that enhance efficiency and scalability for long-sequence processing using these techniques. Also looking for any previous design proposals similar to 'DynamicSparseFFTLinear' or modifications to FastTTTLinear incorporating dynamic sparsity and FFT computations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSeeking research papers, implementations, or design proposals that involve integrating dynamic sparsity and FFT-based computations into Transformer models or similar architectures. Specifically interested in works that enhance efficiency and scalability for long-sequence processing using these techniques. Also looking for any previous design proposals similar to 'DynamicSparseFFTLinear' or modifications to FastTTTLinear incorporating dynamic sparsity and FFT computations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.98)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 0.98)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n#### 2. Linear Attention Sequence Parallelism (Avg. Score: 0.95)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 13/24 (Score: 0.95)*\n\n```\nA., Tanaka, M., Zhang, C., Zhang, M., Song, S. L., Rajbhandari, S., and He, Y. Deepspeed Ulysses: System optimizations for enabling training of extreme long sequence transformer models, 2023. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Korthikanti, V., Casper, J., Lym, S., McAfee, L., Andersch, M., Shoeybi, M., and Catanzaro, B. Reducing activation recomputation in large transformer models, 2022.\n```\n\n#### 3. Staircase Attention for Recurrent Processing of Sequences (Avg. Score: 0.95)\n\n*Da Ju, Stephen Roller, Sainbayar Sukhbaatar, J. Weston*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 10  (*Influential: 2*)\n\n**TL;DR:** Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, and to provide improved modeling power compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.\n\n**Abstract:** Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.\n\n##### *Relevant Chunk: No. 9/21 (Score: 0.95)*\n\n```\narXiv preprint arXiv:1804.09849, 2018. [14] Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for transformer. arXiv preprint arXiv:1904.03092, 2019. [15] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL (1), pages 2978-2988. Association for Computational Linguistics, 2019. [16] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [17] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019. [18] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [19] Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. [20] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021. [21] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models.\n```\n\n#### 4. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.95)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 8/30 (Score: 0.95)*\n\n```\nAs context windows become larger by the day in state-of-the-art models, we expect these gains to become even more relevant. Interpretability. Fig. 8 provides insights into the interpretability aspect of the model's decisionmaking process. It is observed that token removal predominantly occurs when encountering stop words (punctuation), which aligns with the intuition that local information within a sentence becomes less relevant after its completion. Furthermore, it is worth noting that layers at varying depths exhibit distinct behaviors, reinforcing our rationale for dissecting token removal decisions across depth. The variance in sparsity distribution across different depths indicates the necessity of conducting additional interpretability research to obtain valuable insights in the interactions of the tokens within the model. We provide more insights towards this direction in the Appendix C. ## 5 Discussion\n\nWe proposed Adaptively Sparse Attention, a novel approach to dynamically prune the context in decoder-only Transformer architectures. Our results indicate that our technique performs favourably compared to competitive baselines in terms of the ratio between perplexity and sparsity of the attention weights. Remarkably our approach also significantly reduces the computational and memory requirements without affecting its final performance. We practically showcase these benefits achieving more than double the throughput at cases. Adaptively sparse attention comes with two additional practical advantages: first, it can be seamlessly integrated into existing pre-trained models via a cheap fine-tuning step; second, it represents an orthogonal contribution to the burgeoning research line aimed at increasing the level of efficiency of modern LLMs. As such, we envision its combination with existing techniques like weight pruning and quantization to be a promising avenue for future research. ## References\n\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence, (2):179-190, 1983. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 5. MambaByte: Token-free Selective State Space Model (Avg. Score: 0.95)\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 18  (*Influential: 1*)\n\n**TL;DR:** This work proposes MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences, and develops an adaptation of speculative decoding with tokenized drafting and byte-level verification, establishing the viability of SSMs in enabling token-free language modeling.\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.\n\n##### *Relevant Chunk: No. 22/40 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2303.06349, 2023. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive Transformers for Long-Range Sequence Modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Sy lKikSYDH. Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient ContentBased Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. doi: 10.1162/tacl_a_00353. URL https: //aclant hology.org/2021.tacl-1.4. Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic sparsity, FFT-based attention, efficient transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Dynamic Sparsity Is Channel-Level Sparsity Learner\n\n*From Search Query: dynamic sparsity*\n\n*Lu Yin, Gen Li, Meng Fang, Lijuan Shen, Tianjin Huang, Zhangyang Wang, V. Menkovski, Xiaolong Ma, Mykola Pechenizkiy, Shiwei Liu*\n\n**TL;DR:** Channel-aware dynamic sparse (Chase) is proposed, which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity during one end-to-end training process, without any ad-hoc operations.\n\n**Abstract:** Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using any particularly sparsity-aware hardware accelerators. This appealing outcome is partially motivated by a hidden phenomenon of dynamic sparsity: off-the-shelf unstructured DST implicitly involves biased parameter reallocation across channels, with a large fraction of channels (up to 60%) being sparser than others. By progressively identifying and removing these channels during training, our approach translates unstructured sparsity to channel-wise sparsity. Our experimental results demonstrate that Chase achieves 1.7 X inference throughput speedup on common GPU devices without compromising accuracy with ResNet-50 on ImageNet. We release our codes in https://github.com/luuyin/chase.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 2. Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity\n\n*From Search Query: dynamic sparsity*\n\n*Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, Mykola Pechenizkiy, Zhangyang Wang, D. Mocanu*\n\n**TL;DR:** This work draws a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called FreeTickets, which surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference.\n\n**Abstract:** The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit the promising results, naively training multiple deep neural networks and combining their predictions at inference leads to prohibitive computational costs and memory requirements. Recently proposed efficient ensemble approaches reach the performance of the traditional deep ensembles with significantly lower costs. However, the training resources required by these approaches are still at least the same as training a single dense model. In this work, we draw a unique connection between sparse neural network training and deep ensembles, yielding a novel efficient ensemble learning framework called FreeTickets. Instead of training multiple dense networks and averaging them, we directly train sparse subnetworks from scratch and extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. Our framework, FreeTickets, is defined as the ensemble of these relatively cheap sparse subnetworks. Despite being an ensemble method, FreeTickets has even fewer parameters and training FLOPs than a single dense model. This seemingly counter-intuitive outcome is due to the ultra training/inference efficiency of dynamic sparse training. FreeTickets surpasses the dense baseline in all the following criteria: prediction accuracy, uncertainty estimation, out-of-distribution (OoD) robustness, as well as efficiency for both training and inference. Impressively, FreeTickets outperforms the naive deep ensemble with ResNet50 on ImageNet using around only 1/5 of the training FLOPs required by the latter. We have released our source code at https://github.com/VITA-Group/FreeTickets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 45  (*Influential: 5*)\n\n#### 3. Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator\n\n*From Search Query: FFT-based attention*\n\n*Ziwei He, Meng-Da Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, Zhouhan Lin*\n\n**TL;DR:** Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models, and achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA.\n\n**Abstract:** The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. \\footnote{Our code is publicly available at \\url{https://github.com/LUMIA-Group/FourierTransformer}}\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 4. Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding\n\n*From Search Query: FFT-based attention*\n\n*Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu*\n\n**TL;DR:** A novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention using Fast Fourier Transform (FFT), and demonstrates that properly using relative positional encoding can mitigate the training instability problem of vanilla Kernelized attention.\n\n**Abstract:** The attention module, which is a crucial component in Transformer, cannot scale efficiently to long sequences due to its quadratic complexity. Many works focus on approximating the dot-then-exponentiate softmax function in the original attention, leading to sub-quadratic or even linear-complexity Transformer architectures. However, we show that these methods cannot be applied to more powerful attention modules that go beyond the dot-then-exponentiate style, e.g., Transformers with relative positional encoding (RPE). Since in many state-of-the-art models, relative positional encoding is used as default, designing efficient Transformers that can incorporate RPE is appealing. In this paper, we propose a novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention. Based upon the observation that relative positional encoding forms a Toeplitz matrix, we mathematically show that kernelized attention with RPE can be calculated efficiently using Fast Fourier Transform (FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity. Interestingly, we further demonstrate that properly using relative positional encoding can mitigate the training instability problem of vanilla kernelized attention. On a wide range of tasks, we empirically show that our models can be trained from scratch without any optimization issues. The learned model performs better than many efficient Transformer variants and is faster than standard Transformer in the long-sequence regime.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 45  (*Influential: 4*)\n\n#### 5. Syntax-Based Attention Masking for Neural Machine Translation\n\n*From Search Query: FFT-based attention*\n\n*Colin McDonald, David Chiang*\n\n**TL;DR:** A simple method for extending transformers to source-side trees that defines a number of masks that limit self-attention based on relationships among tree nodes, and allows each attention head to learn which mask or masks to use.\n\n**Abstract:** We present a simple method for extending transformers to source-side trees. We define a number of masks that limit self-attention based on relationships among tree nodes, and we allow each attention head to learn which mask or masks to use. On translation from English to various low-resource languages, and translation in both directions between English and German, our method always improves over simple linearization of the source-side parse tree and almost always improves over a sequence-to-sequence baseline, by up to +2.1 BLEU.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 6. Training data-efficient image transformers & distillation through attention\n\n*From Search Query: efficient transformers*\n\n*Hugo Touvron, M. Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv'e J'egou*\n\n**TL;DR:** This work produces a competitive convolution-free transformer by training on Imagenet only and introduces a teacher-student strategy specific to transformers that relies on a distillation token ensuring that the student learns from the teacher through attention.\n\n**Abstract:** Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 5607  (*Influential: 912*)\n\n#### 7. Long Range Arena: A Benchmark for Efficient Transformers\n\n*From Search Query: efficient transformers*\n\n*Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, J. Rao, Liu Yang, Sebastian Ruder, Donald Metzler*\n\n**TL;DR:** A systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios is proposed, paving the way towards better understanding this class of efficient Transformer models.\n\n**Abstract:** Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at this https URL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 611  (*Influential: 139*)\n\n### 4 related papers from ArXiv\n\n#### 1. RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval\n\n*From Search Query: dynamic sparsity*\n\n*Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu*\n\n**Abstract:** Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.\n\n**Published:** 2024-09-16T17:59:52Z  (*Updated: 2024-09-18T13:11:13Z*)\n\n\n\n#### 2. Dynamic Sparsity Neural Networks for Automatic Speech Recognition\n\n*From Search Query: dynamic sparsity*\n\n*Zhaofeng Wu, Ding Zhao, Qiao Liang, Jiahui Yu, Anmol Gulati, Ruoming Pang*\n\n**Abstract:** In automatic speech recognition (ASR), model pruning is a widely adopted\ntechnique that reduces model size and latency to deploy neural network models\non edge devices with resource constraints. However, multiple models with\ndifferent sparsity levels usually need to be separately trained and deployed to\nheterogeneous target hardware with different resource specifications and for\napplications that have various latency requirements. In this paper, we present\nDynamic Sparsity Neural Networks (DSNN) that, once trained, can instantly\nswitch to any predefined sparsity configuration at run-time. We demonstrate the\neffectiveness and flexibility of DSNN using experiments on internal production\ndatasets with Google Voice Search data, and show that the performance of a DSNN\nmodel is on par with that of individually trained single sparsity networks. Our\ntrained DSNN model, therefore, can greatly ease the training process and\nsimplify deployment in diverse scenarios with resource constraints.\n\n**Published:** 2020-05-16T22:08:54Z  (*Updated: 2021-02-08T08:01:58Z*)\n\n\n\n#### 3. A Quantitative Review on Language Model Efficiency Research\n\n*From Search Query: efficient transformers*\n\n*Meng Jiang, Hy Dang, Lingbo Tong*\n\n**Abstract:** Language models (LMs) are being scaled and becoming powerful. Improving their\nefficiency is one of the core research topics in neural information processing\nsystems. Tay et al. (2022) provided a comprehensive overview of efficient\nTransformers that have become an indispensable staple in the field of NLP.\nHowever, in the section of \"On Evaluation\", they left an open question \"which\nfundamental efficient Transformer one should consider,\" answered by \"still a\nmystery\" because \"many research papers select their own benchmarks.\"\nUnfortunately, there was not quantitative analysis about the performances of\nTransformers on any benchmarks. Moreover, state space models (SSMs) have\ndemonstrated their abilities of modeling long-range sequences with\nnon-attention mechanisms, which were not discussed in the prior review. This\narticle makes a meta analysis on the results from a set of papers on efficient\nTransformers as well as those on SSMs. It provides a quantitative review on LM\nefficiency research and gives suggestions for future research.\n\n**Published:** 2023-05-28T20:25:20Z  (*Updated: 2023-05-28T20:25:20Z*)\n\n\n\n#### 4. Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for\n  Long Sequences\n\n*From Search Query: efficient transformers*\n\n*Aosong Feng, Irene Li, Yuang Jiang, Rex Ying*\n\n**Abstract:** Efficient Transformers have been developed for long sequence modeling, due to\ntheir subquadratic memory and time complexity. Sparse Transformer is a popular\napproach to improving the efficiency of Transformers by restricting\nself-attention to locations specified by the predefined sparse patterns.\nHowever, leveraging sparsity may sacrifice expressiveness compared to\nfull-attention, when important token correlations are multiple hops away. To\ncombine advantages of both the efficiency of sparse transformer and the\nexpressiveness of full-attention Transformer, we propose \\textit{Diffuser}, a\nnew state-of-the-art efficient Transformer. Diffuser incorporates all token\ninteractions within one attention layer while maintaining low computation and\nmemory costs. The key idea is to expand the receptive field of sparse attention\nusing Attention Diffusion, which computes multi-hop token correlations based on\nall paths between corresponding disconnected tokens, besides attention among\nneighboring tokens. Theoretically, we show the expressiveness of Diffuser as a\nuniversal sequence approximator for sequence-to-sequence modeling, and\ninvestigate its ability to approximate full-attention by analyzing the graph\nexpander property from the spectral perspective. Experimentally, we investigate\nthe effectiveness of Diffuser with extensive evaluations, including language\nmodeling, image modeling, and Long Range Arena (LRA). Evaluation results show\nthat Diffuser achieves improvements by an average of 0.94% on text\nclassification tasks and 2.30% on LRA, with 1.67$\\times$ memory savings\ncompared to state-of-the-art benchmarks, which demonstrates superior\nperformance of Diffuser in both expressiveness and efficiency aspects.\n\n**Published:** 2022-10-21T08:13:34Z  (*Updated: 2023-01-31T15:57:19Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Dynamic Convolutions: Exploiting Spatial Sparsity for Faster Inference\n\n*From Search Query: dynamic sparsity*\n\n*Tinne Tuytelaars, Thomas Verelst*\n\n**Abstract:** Modern convolutional neural networks apply the same operations on every pixel in an image. However, not all image regions are equally important. To address this inefficiency, we propose a method to dynamically apply convolutions conditioned on the input image. We introduce a residual block where a small gating branch learns which spatial positions should be evaluated. These discrete gating decisions are trained end-to-end using the Gumbel-Softmax trick, in combination with a sparsity criterion. Our experiments on CIFAR, ImageNet and MPII show that our method has better focus on the region of interest and better accuracy than existing methods, at a lower computational complexity. Moreover, we provide an efficient CUDA implementation of our dynamic convolutions using a gather-scatter approach, achieving a significant improvement in inference speed with MobileNetV2 residual blocks. On human pose estimation, a task that is inherently spatially sparse, the processing speed is increased by 60% with no loss in accuracy.\n\n**Conference:** dynamic-convolutions-exploiting-spatial-1\n\n**Published:** 2019-12-06\n\n\n\n#### 2. Dynamic Sparse Training with Structured Sparsity\n\n*From Search Query: dynamic sparsity*\n\n*Yani Ioannou, Mihai Nica, Utku Evci, Anna Golubeva, Mike Lasby*\n\n**Abstract:** Dynamic Sparse Training (DST) methods achieve state-of-the-art results in sparse neural network training, matching the generalization of dense models while enabling sparse training and inference. Although the resulting models are highly sparse and theoretically less computationally expensive, achieving speedups with unstructured sparsity on real-world hardware is challenging. In this work, we propose a sparse-to-sparse DST method, Structured RigL (SRigL), to learn a variant of fine-grained structured N:M sparsity by imposing a constant fan-in constraint. Using our empirical analysis of existing DST methods at high sparsity, we additionally employ a neuron ablation method which enables SRigL to achieve state-of-the-art sparse-to-sparse structured DST performance on a variety of Neural Network (NN) architectures. Using a 90% sparse linear layer, we demonstrate a real-world acceleration of 3.4x/2.5x on CPU for online inference and 1.7x/13.0x on GPU for inference with a batch size of 256 when compared to equivalent dense/unstructured (CSR) sparse layers, respectively.\n\n**Published:** 2023-05-03\n\n\n\n#### 3. FFT-based Dynamic Token Mixer for Vision\n\n*From Search Query: FFT-based attention*\n\n*Masato Taki, Yuki Tatsunami*\n\n**Abstract:** Multi-head-self-attention (MHSA)-equipped models have achieved notable performance in computer vision. Their computational complexity is proportional to quadratic numbers of pixels in input feature maps, resulting in slow processing, especially when dealing with high-resolution images. New types of token-mixer are proposed as an alternative to MHSA to circumvent this problem: an FFT-based token-mixer involves global operations similar to MHSA but with lower computational complexity. However, despite its attractive properties, the FFT-based token-mixer has not been carefully examined in terms of its compatibility with the rapidly evolving MetaFormer architecture. Here, we propose a novel token-mixer called Dynamic Filter and novel image recognition models, DFFormer and CDFFormer, to close the gaps above. The results of image classification and downstream tasks, analysis, and visualization show that our models are helpful. Notably, their throughput and memory efficiency when dealing with high-resolution image recognition is remarkable. Our results indicate that Dynamic Filter is one of the token-mixer options that should be seriously considered. The code is available at https://github.com/okojoalg/dfformer\n\n**Published:** 2023-03-07\n\n\n\n#### 4. Xiaoicesing 2: A High-Fidelity Singing Voice Synthesizer Based on Generative Adversarial Network\n\n*From Search Query: FFT-based attention*\n\n*Xing He, Chang Zeng, Chunhui Wang*\n\n**Abstract:** XiaoiceSing is a singing voice synthesis (SVS) system that aims at generating 48kHz singing voices. However, the mel-spectrogram generated by it is over-smoothing in middle- and high-frequency areas due to no special design for modeling the details of these parts. In this paper, we propose XiaoiceSing2, which can generate the details of middle- and high-frequency parts to better construct the full-band mel-spectrogram. Specifically, in order to alleviate this problem, XiaoiceSing2 adopts a generative adversarial network (GAN), which consists of a FastSpeech-based generator and a multi-band discriminator. We improve the feed-forward Transformer (FFT) block by adding multiple residual convolutional blocks in parallel with the self-attention block to balance the local and global features. The multi-band discriminator contains three sub-discriminators responsible for low-, middle-, and high-frequency parts of the mel-spectrogram, respectively. Each sub-discriminator is composed of several segment discriminators (SD) and detail discriminators (DD) to distinguish the audio from different aspects. The experiment on our internal 48kHz singing voice dataset shows XiaoiceSing2 significantly improves the quality of the singing voice over XiaoiceSing.\n\n**Conference:** xiaoicesing-2-a-high-fidelity-singing-voice-1\n\n**Published:** 2022-10-26\n\n\n\n#### 5. Reformer: The Efficient Transformer\n\n*From Search Query: efficient transformers*\n\n*\u0141ukasz Kaiser, Anselm Levskaya, Nikita Kitaev*\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2020-01-13\n\n\n\n#### 6. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n\n*From Search Query: efficient transformers*\n\n*Ping Luo, Jose M. Alvarez, Anima Anandkumar, Zhiding Yu, Wenhai Wang, Enze Xie*\n\n**Abstract:** We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-05-31\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher achieve the goal of enhancing the `FastTTTLinear` Generalized Attention Unit (GAU) with dynamic sparsity and FFT-based computation, here are some key insights and relevant information from the provided sources and additional context:\n\n### Novelty and Existing Research\n\n- The combination of dynamic sparsity and FFT-based computation in Transformer models or GAUs is a novel approach. While there are individual applications of dynamic sparsity and FFT in neural networks, combining these specifically for Transformer models, especially in the context of GAUs like `FastTTTLinear`, appears to be a new direction.\n\n### Dynamic Sparsity in Neural Networks\n\n- Dynamic sparsity has been explored in various neural network architectures to reduce computational and memory requirements. However, specific applications in Transformer models are less common. The concept of sparse deep learning, as mentioned in the context of fault diagnosis, highlights the potential benefits of sparsity but does not directly address its integration with FFT-based computations.\n\n### FFT-Based Computation in Neural Networks\n\n- FFT-based computations have been used to accelerate certain types of neural network operations. For example, the use of FFT in spiking neural networks (SNNs) to accelerate computations by leveraging parallel processing capabilities is a relevant example. The method described in the SNN paper involves using FFT to compute convolutional operations efficiently, which could be analogous to integrating FFT in attention mechanisms.\n\n### Combining Dynamic Sparsity and FFT\n\n- There is no direct precedent for combining dynamic sparsity with FFT-based computation in Transformer models or GAUs. However, the principles from both areas can be merged:\n  - **Dynamic Sparsity**: This can be implemented through learned sparsity patterns, which can selectively compute attention weights based on the input.\n  - **FFT-Based Computation**: This can be used to accelerate the computation of attention weights or other linear transformations within the GAU.\n\n### Practical Challenges and Implementation\n\n- **Training Stability and Convergence**: Integrating dynamic sparsity and FFT computations may introduce new challenges in maintaining training stability and ensuring convergence. The residual structure and normalization techniques used in other deep learning models (e.g., TCN and transformer models) could be adapted to mitigate these issues.\n- **Complexity of Implementation**: The implementation would require careful integration of sparsity patterns with FFT operations. This might involve modifying the existing `FastTTTLinear` architecture to incorporate these features efficiently.\n\n### Impact on Model Performance\n\n- **Accuracy and Perplexity**: The impact on accuracy and perplexity would depend on how effectively the dynamic sparsity and FFT computations are integrated. Reducing unnecessary computations through sparsity and accelerating others with FFT could potentially improve performance on long sequences.\n- **Robustness and Efficiency**: Dynamic sparsity can help in reducing the computational load, making the model more robust to input variations and noise. FFT-based computations can further enhance efficiency by allowing parallel processing, which is crucial for long-sequence tasks.\n\n### Comparison with Existing Research\n\n- The proposed `DynamicSparseFFTLinear` GAU would differ significantly from existing methods that either use static sparsity or do not leverage FFT computations. It would be important to compare this approach with other efficient Transformer models and GAUs that focus on long-sequence processing, such as those using self-attention mechanisms with multi-head attention layers.\n\n### Useful References and Excerpts\n\n- **Spiking Neural Networks (SNNs)**: The paper on SNNs provides insights into how FFT can be used to accelerate computations in neural networks, which could be adapted for Transformer models.\n- **Temporal Convolutional Networks (TCNs)**: The use of TCNs in predicting the remaining useful life of bearings highlights the effectiveness of residual structures and normalization in deep networks, which could be beneficial in integrating dynamic sparsity and FFT computations.\n\n### Future Search Plan\n\n- **Refine Search Keywords**: Use keywords like \"sparse FFT transformers,\" \"efficient attention with dynamic sparsity,\" or \"FFT-based computation in GAUs\" to find more targeted research.\n- **Review Related Work**: Carefully examine papers and design proposals that discuss similar enhancements to Transformer models or GAUs, focusing on efficiency, scalability, and performance metrics.\n\nBy synthesizing these insights, the researcher can develop a comprehensive understanding of how to integrate dynamic sparsity and FFT-based computations into the `FastTTTLinear` GAU, addressing the key aspects of novelty, feasibility, and performance impact.\n",
            "analysis": "**Initial Analysis:**\n\nThe proposal aims to enhance the **FastTTTLinear** GAU by introducing **dynamic sparsity** and **efficient FFT-based computation**. The goals are to:\n\n- Improve performance on long sequences.\n- Reduce memory and computational requirements.\n- Combine selective computation through learned sparsity patterns with hardware-efficient FFT implementations.\n\n**Key Aspects Requiring Further Investigation:**\n\n1. **Novelty:**\n   - Has dynamic sparsity been combined with FFT-based computation in Transformer models or GAUs before?\n   - Are there existing models or research that propose similar enhancements to FastTTTLinear?\n\n2. **Feasibility and Implementation:**\n   - Practical challenges in integrating dynamic sparsity and FFT into FastTTTLinear.\n   - Impact on training stability and convergence.\n   - Complexity of implementation and required modifications.\n\n3. **Impact on Model Performance:**\n   - Effects on accuracy and perplexity in language modeling tasks.\n   - Change in robustness to input variations or noise.\n   - Improvements in efficiency (memory usage, computation time) and scalability with respect to sequence length.\n\n4. **Comparison with Existing Research:**\n   - Similarities or differences with previous design proposals or research papers.\n   - Established methods for efficient long-sequence processing and how this proposal relates.\n\n---\n\n**Search Queries:**\n\n```keywords dynamic sparsity```\n\n```keywords FFT-based attention```\n\n```keywords efficient transformers```\n\n```description Seeking research papers, implementations, or design proposals that involve integrating dynamic sparsity and FFT-based computations into Transformer models or similar architectures. Specifically interested in works that enhance efficiency and scalability for long-sequence processing using these techniques. Also looking for any previous design proposals similar to 'DynamicSparseFFTLinear' or modifications to FastTTTLinear incorporating dynamic sparsity and FFT computations.```\n\n---\n\n**Analysis Note:**\n\n1. **Summary of the Analysis:**\n\n   - The proposal introduces **DynamicSparseFFTLinear**, enhancing FastTTTLinear with dynamic sparsity and FFT-based computation.\n   - Aims to improve efficiency and scalability for long-sequence processing in language models.\n   - Key areas for investigation include the novelty of combining these techniques, feasibility, and potential impact on performance.\n\n2. **Useful References with Excerpts:**\n\n   *(To be updated after reviewing search results.)*\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Dynamic Sparsity in Transformers:**\n     - Investigate how dynamic sparsity has been previously applied to Transformer models.\n     - Understand mechanisms for learning sparsity patterns and their impact on attention mechanisms.\n\n   - **FFT-Based Computation in Neural Networks:**\n     - Explore existing applications of FFT in accelerating neural network computations.\n     - Assess how FFT can be integrated into attention mechanisms or linear layers.\n\n   - **Combining Dynamic Sparsity and FFT:**\n     - Determine if the combination of these two techniques is novel or has been explored.\n     - Analyze potential synergies or conflicts between dynamic sparsity and FFT computations.\n\n   - **Modifications to FastTTTLinear:**\n     - Review any prior modifications or enhancements to FastTTTLinear or similar GAUs.\n     - Compare the proposed design with existing methods to assess novelty.\n\n4. **Future Search Plan or Next Steps:**\n\n   - **Review Search Results:**\n     - Carefully examine the returned papers and design proposals for related work.\n     - Note any similar approaches or key differences.\n\n   - **Refine Search If Necessary:**\n     - If initial results are inconclusive, adjust keywords to focus on specific aspects, such as \"sparse FFT transformers\" or \"efficient attention with dynamic sparsity.\"\n\n   - **Assess Novelty and Feasibility:**\n     - Based on findings, evaluate the uniqueness of the proposal.\n     - Identify any implementation challenges reported in the literature.\n\n   - **Prepare for In-Depth Review:**\n     - Gather sufficient information to assess the proposal thoroughly.\n     - Plan to analyze the theoretical soundness and potential impact on performance metrics.\n\n5. **List of References:**\n\n   *(To be compiled with precise citations after reviewing search results.)*\n\n---"
        },
        {
            "ready": false,
            "query": [
                "sparse FFT transformer",
                "dynamic sparse attention",
                "efficient attention FFT"
            ],
            "detail": "Looking for research papers, models, or implementations that combine dynamic sparsity with FFT-based computations in Transformer models or attention mechanisms. Specifically interested in methods that enhance efficiency and scalability for long-sequence processing by integrating FFT operations into sparse attention mechanisms. Also seeking any recent works that discuss challenges and solutions in combining dynamic sparsity and FFT in deep learning architectures.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nLooking for research papers, models, or implementations that combine dynamic sparsity with FFT-based computations in Transformer models or attention mechanisms. Specifically interested in methods that enhance efficiency and scalability for long-sequence processing by integrating FFT operations into sparse attention mechanisms. Also seeking any recent works that discuss challenges and solutions in combining dynamic sparsity and FFT in deep learning architectures.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 1.00)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 1.00)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n#### 2. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 1.00)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 2/41 (Score: 1.00)*\n\n```\nIn this work, we introduce SparseK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SparseK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available. ## 1 Introduction\n\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges $[1,20,19]$, owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices. Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods $[39,61]$ allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually $\\operatorname{cost} O(n \\log n)$ to maintain clusters. Ainslie et al. [1]\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_81cbe08ae077927ce965g-02.jpg?height=549&width=1261&top_left_y=254&top_left_x=432)\n\nFigure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query ( $\\tau(\\mathbf{u})$ ) such that the sum of normalized scores is $k$, which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency. incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SofTTOPK for variable-length context associated with different queries requires quadratic time in total. To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows. ## Incremental KV Selection. The SPARSEK operator (\u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results. Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (\u00a73.2). Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency. We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments\non language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention. ## 2 Related Work\n\nLong-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence $[17,35,36,12]$ or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions $[15,27,42]$. Sparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns $[34,13]$, or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27].\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 1.00)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 1.00)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 4. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.99)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 2/38 (Score: 0.99)*\n\n```\nFor these applications, the causal self-attention-which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0 \\times$ and $3.3 \\times$ for sequences of respectively $8 k$ and $16 k$ tokens. ## 1 Introduction\n\nMany methods have been developed to mitigate the quadratic cost of self-attention in Transformers (Vaswani et al., 2017). Some methods attempt to linearize the attention (Beltagy et al., 2020; Wang et al., 2020) by for instance linearizing the softmax operator to take advantage of the associativity of matrix products (Katharopoulos et al., 2020). Other methods rely on a predefined sparse masking of the attention matrix, e.g. to constrain the attention to a local temporal neighborhood (Zaheer et al., 2020; Child et al., 2019). While the structure is fixed, it is assumed that information from arbitrary locations in the sequence can still flow through this structure over several layers. All those methods impose static implicit or explicit constraints over the attention matrix. Another promising line of work consists in computing a dynamic modulation of a sub-part of the attention matrix. They are based, for instance, on dropping keys and queries (Kim et al., 2022) or using geometric hashing of the keys and queries to identify linear cost sub-blocks of the attention matrix that carry most of the weight (Kitaev et al., 2020). [^0]The promising theoretical computational complexity of these methods contrasts with the fact that today's most successfully deployed practical models instead rely on vanilla attention, in part thanks to the efficiency of FlashAttention (Dao et al., 2022). This implementation is mathematically identical to the vanilla attention proposed by Vaswani et al. (2017) in their seminal paper, but trades in additional compute for less memory I/O. While still avoiding a memory footprint quadratic with the sequence length, it delivers practical speedups of over $5 \\times$ compared to a naive implementation. Using an attention layer in an autoregressive model-which has been key in the recent remarkable AI breakthroughs-requires to make it causal. This is achieved by applying a mask to the attention matrix, so that information cannot flow from the future to the past during training. While FlashAttention can deal with vanilla causal masks, it does not provide enough flexibility to be used for situations where the causal attention mask is not perfectly regular, that is, lower triangular. This in particular prevents using it for models that dynamically drop keys and queries or rely on geometric hashing, which results in irregular causal structures as illustrated in Fig.\n```\n\n#### 5. An Attention Free Transformer (Avg. Score: 0.99)\n\n*Shuangfei Zhai, Walter A. Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, J. Susskind*\n\n**Published in:** arXiv.org (2021)\t**Cited by** 90  (*Influential: 10*)\n\n**TL;DR:** Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention, is introduced and demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.\n\n**Abstract:** We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.99)*\n\n```\nIn $A C L, 2019$. [24] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. ArXiv, abs/2003.05997, 2020. [25] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. ArXiv, abs/1901.10430, 2019. [26] Yi Tay, Dara Bahri, L. Yang, Donald Metzler, and D. Juan. Sparse sinkhorn attention. ArXiv, abs/2002.11296, 2020. [27] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: sparse FFT transformer, dynamic sparse attention, efficient attention FFT\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Sparse Universal Transformer\n\n*From Search Query: sparse FFT transformer*\n\n*Shawn Tan, Yikang Shen, Zhenfang Chen, Aaron Courville, Chuang Gan*\n\n**TL;DR:** This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability.\n\n**Abstract:** The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers. Empirical evidence shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, scaling UT parameters is much more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to reduce UT's computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT'14 and strong generalization results on formal language tasks (Logical inference and CFQ). The new halting mechanism also enables around 50\\% reduction in computation during inference with very little performance decrease on formal language tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 2. Flow-Guided Sparse Transformer for Video Deblurring\n\n*From Search Query: sparse FFT transformer*\n\n*Jing Lin, Yuanhao Cai, Xiaowan Hu, Haoqian Wang, Youliang Yan, X. Zou, Henghui Ding, Yulun Zhang, R. Timofte, L. Gool*\n\n**TL;DR:** This paper proposes a novel framework, Flow-Guided Sparse Transformer (FGST), for video deblurring, which outperforms state-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields more visually pleasing results in real videodeblurring.\n\n**Abstract:** Exploiting similar and sharper scene patches in spatio-temporal neighborhoods is critical for video deblurring. However, CNN-based methods show limitations in capturing long-range dependencies and modeling non-local self-similarity. In this paper, we propose a novel framework, Flow-Guided Sparse Transformer (FGST), for video deblurring. In FGST, we customize a self-attention module, Flow-Guided Sparse Window-based Multi-head Self-Attention (FGSW-MSA). For each $query$ element on the blurry reference frame, FGSW-MSA enjoys the guidance of the estimated optical flow to globally sample spatially sparse yet highly related $key$ elements corresponding to the same scene patch in neighboring frames. Besides, we present a Recurrent Embedding (RE) mechanism to transfer information from past frames and strengthen long-range temporal dependencies. Comprehensive experiments demonstrate that our proposed FGST outperforms state-of-the-art (SOTA) methods on both DVD and GOPRO datasets and even yields more visually pleasing results in real video deblurring. Code and pre-trained models are publicly available at https://github.com/linjing7/VR-Baseline\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 54  (*Influential: 7*)\n\n#### 3. Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator\n\n*From Search Query: sparse FFT transformer*\n\n*Ziwei He, Meng-Da Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, Zhouhan Lin*\n\n**TL;DR:** Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models, and achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA.\n\n**Abstract:** The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. \\footnote{Our code is publicly available at \\url{https://github.com/LUMIA-Group/FourierTransformer}}\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 4. Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\n\n*From Search Query: dynamic sparse attention*\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, F. Fleuret*\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention\u2014which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2 . 0 \u00d7 and 3 . 3 \u00d7 for sequences of respectively 8 k and 16 k tokens.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 5. Sparse Attention with Learning to Hash\n\n*From Search Query: dynamic sparse attention*\n\n*Zhiqing Sun, Yiming Yang, Shinjae Yoo*\n\n**TL;DR:** A new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively is proposed, which is applicable to the wide range of pre-trained Transformer models.\n\n**Abstract:** Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 6. Sparse Sinkhorn Attention\n\n*From Search Query: dynamic sparse attention*\n\n*Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, Da-Cheng Juan*\n\n**TL;DR:** This work introduces a meta sorting network that learns to generate latent permutations over sequences and is able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module.\n\n**Abstract:** We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 303  (*Influential: 37*)\n\n#### 7. Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding\n\n*From Search Query: efficient attention FFT*\n\n*Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, Tie-Yan Liu*\n\n**TL;DR:** A novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention using Fast Fourier Transform (FFT), and demonstrates that properly using relative positional encoding can mitigate the training instability problem of vanilla Kernelized attention.\n\n**Abstract:** The attention module, which is a crucial component in Transformer, cannot scale efficiently to long sequences due to its quadratic complexity. Many works focus on approximating the dot-then-exponentiate softmax function in the original attention, leading to sub-quadratic or even linear-complexity Transformer architectures. However, we show that these methods cannot be applied to more powerful attention modules that go beyond the dot-then-exponentiate style, e.g., Transformers with relative positional encoding (RPE). Since in many state-of-the-art models, relative positional encoding is used as default, designing efficient Transformers that can incorporate RPE is appealing. In this paper, we propose a novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention. Based upon the observation that relative positional encoding forms a Toeplitz matrix, we mathematically show that kernelized attention with RPE can be calculated efficiently using Fast Fourier Transform (FFT). With FFT, our method achieves $\\mathcal{O}(n\\log n)$ time complexity. Interestingly, we further demonstrate that properly using relative positional encoding can mitigate the training instability problem of vanilla kernelized attention. On a wide range of tasks, we empirically show that our models can be trained from scratch without any optimization issues. The learned model performs better than many efficient Transformer variants and is faster than standard Transformer in the long-sequence regime.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 45  (*Influential: 4*)\n\n#### 8. Training data-efficient image transformers & distillation through attention\n\n*From Search Query: efficient attention FFT*\n\n*Hugo Touvron, M. Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv'e J'egou*\n\n**TL;DR:** This work produces a competitive convolution-free transformer by training on Imagenet only and introduces a teacher-student strategy specific to transformers that relies on a distillation token ensuring that the student learns from the teacher through attention.\n\n**Abstract:** Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 5607  (*Influential: 912*)\n\n#### 9. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: efficient attention FFT*\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 346  (*Influential: 60*)\n\n### 2 related papers from ArXiv\n\n#### 1. ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference\n\n*From Search Query: dynamic sparse attention*\n\n*Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen*\n\n**Abstract:** With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.\n\n**Published:** 2024-10-28T19:08:12Z  (*Updated: 2024-10-28T19:08:12Z*)\n\n\n\n#### 2. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via\n  Dynamic Sparse Attention\n\n*From Search Query: dynamic sparse attention*\n\n*Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu*\n\n**Abstract:** The computational challenges of Large Language Model (LLM) inference remain a\nsignificant barrier to their widespread deployment, especially as prompt\nlengths continue to increase. Due to the quadratic complexity of the attention\ncomputation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens\n(i.e., the pre-filling stage) on a single A100 GPU. Existing methods for\nspeeding up prefilling often fail to maintain acceptable accuracy or efficiency\nwhen applied to long-context LLMs. To address this gap, we introduce MInference\n(Milliontokens Inference), a sparse calculation method designed to accelerate\npre-filling of long-sequence processing. Specifically, we identify three unique\npatterns in long-context attention matrices-the A-shape, Vertical-Slash, and\nBlock-Sparsethat can be leveraged for efficient sparse computation on GPUs. We\ndetermine the optimal pattern for each attention head offline and dynamically\nbuild sparse indices based on the assigned pattern during inference. With the\npattern and sparse indices, we perform efficient sparse attention calculations\nvia our optimized GPU kernels to significantly reduce the latency in the\npre-filling stage of long-context LLMs. Our proposed technique can be directly\napplied to existing LLMs without any modifications to the pre-training setup or\nadditional fine-tuning. By evaluating on a wide range of downstream tasks,\nincluding InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models\nincluding LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we\ndemonstrate that MInference effectively reduces inference latency by up to 10x\nfor pre-filling on an A100, while maintaining accuracy. Our code is available\nat https://aka.ms/MInference.\n\n**Published:** 2024-07-02T17:59:56Z  (*Updated: 2024-10-30T14:53:22Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\n\n*From Search Query: sparse FFT transformer*\n\n*Christopher R\u00e9, Eric Nguyen, Hermann Kumbong, Daniel Y. Fu*\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Published:** 2023-11-10\n\n\n\n#### 2. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\n\n*From Search Query: sparse FFT transformer*\n\n*Christopher R\u00e9, Albert Gu, Matthew Eichhorn, Atri Rudra, Tri Dao*\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \\log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to $1024$. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points -- the first time a structured approach has done so -- with 4X faster inference speed and 40X fewer parameters.\n\n**Published:** 2019-03-14\n\n\n\n#### 3. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention\n\n*From Search Query: dynamic sparse attention*\n\n*Fran\u00e7ois Fleuret, Martin Jaggi, Daniele Paliotta, Matteo Pagliardini*\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n**Published:** 2023-06-01\n\n\n\n#### 4. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: efficient attention FFT*\n\n*Christopher R\u00e9, Atri Rudra, Armin W. Thomas, Khaled K. Saab, Daniel Y. Fu, Tri Dao*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28\n\n\n\n#### 5. FFT-based Dynamic Token Mixer for Vision\n\n*From Search Query: efficient attention FFT*\n\n*Masato Taki, Yuki Tatsunami*\n\n**Abstract:** Multi-head-self-attention (MHSA)-equipped models have achieved notable performance in computer vision. Their computational complexity is proportional to quadratic numbers of pixels in input feature maps, resulting in slow processing, especially when dealing with high-resolution images. New types of token-mixer are proposed as an alternative to MHSA to circumvent this problem: an FFT-based token-mixer involves global operations similar to MHSA but with lower computational complexity. However, despite its attractive properties, the FFT-based token-mixer has not been carefully examined in terms of its compatibility with the rapidly evolving MetaFormer architecture. Here, we propose a novel token-mixer called Dynamic Filter and novel image recognition models, DFFormer and CDFFormer, to close the gaps above. The results of image classification and downstream tasks, analysis, and visualization show that our models are helpful. Notably, their throughput and memory efficiency when dealing with high-resolution image recognition is remarkable. Our results indicate that Dynamic Filter is one of the token-mixer options that should be seriously considered. The code is available at https://github.com/okojoalg/dfformer\n\n**Published:** 2023-03-07\n\n\n\n\n\n---\n## Web search results\n\n ### Understanding the Goal and Intent of the Researcher\n\nThe researcher aims to design a novel autoregressive language model (LM) block that outperforms current state-of-the-art models by integrating two key techniques: **dynamic sparsity** and **FFT-based computation** within Transformer architectures. Here are the key aspects of the researcher's intent and the most useful information to support this goal:\n\n## Key Techniques\n\n### Dynamic Sparsity\n- Dynamic sparsity involves reducing unnecessary computations by learning sparse attention patterns or pruning contextual information. This technique has been explored in several works:\n  - **Roy et al. (2020)**: Introduced the Routing Transformer, which learns dynamic sparse attention patterns to avoid attending to unrelated content.\n  - **Anagnostidis et al. (2023)**: Proposed dynamic context pruning to reduce memory and computational requirements during inference.\n  - **Yin et al. (2023)**: Focused on translating unstructured dynamic sparsity to hardware-friendly channel-level sparsity.\n\n### FFT-Based Computation\n- FFT has been used to accelerate computations in Transformers, especially for long sequences:\n  - **He et al. (2023)**: Proposed the Fourier Transformer, using FFT to remove sequence redundancy and improve efficiency.\n  - **Luo et al. (2021)**: Developed methods to accelerate attention calculation using FFT, particularly with relative positional encoding.\n\n## Integration of Techniques\n- The researcher is interested in combining these two techniques to enhance efficiency and scalability for long-sequence processing. There is no direct evidence of prior work that integrates both dynamic sparsity and FFT-based computation in Transformer models, making this a novel approach.\n\n## Useful Information and References\n\n### Efficiency and Scalability\n- Combining dynamic sparsity and FFT can potentially reduce computational load and memory usage. For example, using FFT to accelerate computations can be synergistic with dynamic sparsity, which reduces the number of computations needed.\n\n### Implementation Challenges\n- Implementing dynamic sparsity requires efficient learning and application of sparsity masks during training and inference. Integrating FFT computations into attention mechanisms may introduce complexities regarding training stability and compatibility with existing architectures.\n\n### Related Works and Techniques\n- **Fast Fourier Transform Convolution**: This technique, as described in the FCSwinU model, can be adapted for efficient feature extraction and processing in Transformers. It leverages FFT to extract global information efficiently.\n- **Temporal Convolutional Networks (TCNs)**: While not directly related to Transformers, TCNs demonstrate how to efficiently handle long sequences using dilation causal convolution and residual modules, which could inspire similar designs in Transformer architectures.\n\n## Future Search Plan\n\n### Refine Search Queries\n- Focus on keywords like \"sparse FFT transformer,\" \"dynamic sparse attention,\" and \"efficient attention FFT\" to find recent publications that might have attempted similar integrations.\n\n### Investigate Specific Implementations\n- Search for code repositories, preprints, or conference papers that propose similar ideas. For example, looking into the implementation details of the Fourier Transformer or dynamic context pruning models could provide insights into how to combine these techniques effectively.\n\n### Assess Potential Challenges\n- Research discussing the challenges of integrating dynamic sparsity or FFT individually can inform potential issues when combining them. For instance, the challenges in maintaining training stability and ensuring hardware efficiency when using FFT in attention mechanisms are crucial to consider.\n\n### Consider Related Fields\n- Exploring works in signal processing or computer vision where similar techniques might have been combined can provide additional insights. For example, the use of FFT in other domains could offer novel ways to apply it in Transformer models.\n\n## Conclusion\n\nThe integration of dynamic sparsity and FFT-based computation in Transformer models is a promising but novel approach. By leveraging the efficiency gains from both techniques, the researcher can potentially develop a more efficient and scalable autoregressive LM block. The key will be to address the implementation challenges and ensure that the combined approach maintains or improves the model's performance and robustness.\n",
            "analysis": "**Analysis Note:**\n\n1. **Summary of the Analysis:**\n\n   Based on the search results, combining **dynamic sparsity** and **FFT-based computation** in Transformer models or GAUs appears to be a novel approach. While dynamic sparsity and FFT-based methods have individually been explored to enhance efficiency and scalability in neural networks, there is limited evidence of prior work integrating both techniques within Transformer architectures, especially for long-sequence processing.\n\n2. **Useful References with Excerpts:**\n\n   - **Roy et al. (2020)**: *Efficient Content-Based Sparse Attention with Routing Transformers*.\n\n     *Excerpt*: \"Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest... This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention.\"\n\n   - **Anagnostidis et al. (2023)**: *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*.\n\n     *Excerpt*: \"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process.\"\n\n   - **Yin et al. (2023)**: *Dynamic Sparsity Is Channel-Level Sparsity Learner*.\n\n     *Excerpt*: \"We propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity during one end-to-end training process... The resulting small sparse networks can be directly accelerated by commodity hardware.\"\n\n   - **He et al. (2023)**: *Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator*.\n\n     *Excerpt*: \"We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator... Our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space.\"\n\n   - **Luo et al. (2021)**: *Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding*.\n\n     *Excerpt*: \"We propose a novel way to accelerate attention calculation for Transformers with RPE on top of the kernelized attention using Fast Fourier Transform (FFT)... We empirically show that our models can be trained from scratch without any optimization issues.\"\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Dynamic Sparsity in Transformers:**\n     - Dynamic sparsity techniques have been used to improve the efficiency of Transformers by reducing unnecessary computations.\n     - Roy et al. introduced the Routing Transformer, which learns dynamic sparse attention patterns to avoid attending to unrelated content.\n     - Anagnostidis et al. proposed dynamic context pruning, effectively reducing memory and computational requirements during inference.\n     - Yin et al. focused on translating unstructured dynamic sparsity to hardware-friendly channel-level sparsity, enhancing practical efficiency.\n\n   - **FFT-Based Computation in Transformers:**\n     - FFT has been utilized to accelerate computations in Transformers, particularly for long sequences.\n     - He et al. proposed the Fourier Transformer, using FFT to remove sequence redundancy and improve efficiency.\n     - Luo et al. developed methods to accelerate attention calculation using FFT, especially with relative positional encoding.\n\n   - **Integration of Dynamic Sparsity and FFT in Transformers:**\n     - There is no direct evidence of prior work that combines dynamic sparsity with FFT-based computation in Transformer models or GAUs.\n     - The proposal to enhance FastTTTLinear with both dynamic sparsity and FFT-based computation appears to be novel.\n     - Potential synergies include using sparsity to reduce computational load and FFT to efficiently handle the remaining computations.\n\n   - **Feasibility and Implementation Challenges:**\n     - Implementing dynamic sparsity requires efficient ways to learn and apply sparsity masks during training and inference.\n     - Integrating FFT computations into attention mechanisms may introduce complexities, especially regarding training stability and compatibility with existing architectures.\n     - Combining both techniques will necessitate careful architectural design to maintain model performance and ensure hardware efficiency.\n\n   - **Impact on Model Performance:**\n     - Potential for significant improvements in efficiency and scalability for long-sequence processing.\n     - Possible challenges in maintaining or improving accuracy and robustness due to the complexities introduced by combining these methods.\n     - The proposal aligns with ongoing research trends focusing on efficient Transformers and sparse attention mechanisms.\n\n4. **Future Search Plan or Next Steps:**\n\n   - **Refine Search Queries:**\n     - Focus on keywords that directly relate to the integration of dynamic sparsity and FFT in Transformers.\n     - Look for very recent publications (2023-2024) that might not have been indexed in initial searches.\n\n   - **Investigate Specific Implementations:**\n     - Search for code repositories, preprints, or conference papers that propose similar ideas.\n     - Examine whether any existing Transformer variants have attempted to combine these techniques.\n\n   - **Assess Potential Challenges:**\n     - Look for research discussing the challenges of integrating dynamic sparsity or FFT individually, which may inform potential issues when combining them.\n\n   - **Consider Related Fields:**\n     - Explore works in other domains (e.g., signal processing, computer vision) where similar techniques might have been combined.\n\n5. **List of References:**\n\n   - Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). **Efficient Content-Based Sparse Attention with Routing Transformers**. *Transactions of the Association for Computational Linguistics*, 9, 53-68.\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). **Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**. *Advances in Neural Information Processing Systems*.\n\n   - Yin, L., Li, G., Fang, M., Shen, L., Huang, T., Wang, Z., Menkovski, V., Ma, X., Pechenizkiy, M., & Liu, S. (2023). **Dynamic Sparsity Is Channel-Level Sparsity Learner**. *Advances in Neural Information Processing Systems*.\n\n   - He, Z., Yang, M., Feng, M., Yin, J., Wang, X., Leng, J., & Lin, Z. (2023). **Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**. *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics*.\n\n   - Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S., Ke, G., Wang, L., & Liu, T.-Y. (2021). **Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding**. *Advances in Neural Information Processing Systems*, 34, 13516\u201313528.\n\n---\n\n**New Search Queries:**\n\n1. **Keywords:**\n\n   ```keywords sparse FFT transformer```\n\n   ```keywords dynamic sparse attention```\n\n   ```keywords efficient attention FFT```\n\n2. **Internal Library Search Description:**\n\n   ```description Looking for research papers, models, or implementations that combine dynamic sparsity with FFT-based computations in Transformer models or attention mechanisms. Specifically interested in methods that enhance efficiency and scalability for long-sequence processing by integrating FFT operations into sparse attention mechanisms. Also seeking any recent works that discuss challenges and solutions in combining dynamic sparsity and FFT in deep learning architectures.```"
        },
        {
            "ready": false,
            "query": [
                "sparse fft attention",
                "sparse fft transformers",
                "dynamic sparse fft neural networks"
            ],
            "detail": "Seeking recent research papers, preprints, or implementations that explore integrating sparse FFT computations into Transformers or neural networks, particularly combining dynamic sparsity with FFT-based methods. Interested in methods that leverage sparse FFT algorithms within attention mechanisms or other neural network layers to improve efficiency in long-sequence processing. Looking for any works that discuss challenges and solutions in implementing sparse FFT in sparse neural computations or combining dynamic sparsity with FFT operations.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSeeking recent research papers, preprints, or implementations that explore integrating sparse FFT computations into Transformers or neural networks, particularly combining dynamic sparsity with FFT-based methods. Interested in methods that leverage sparse FFT algorithms within attention mechanisms or other neural network layers to improve efficiency in long-sequence processing. Looking for any works that discuss challenges and solutions in implementing sparse FFT in sparse neural computations or combining dynamic sparsity with FFT operations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Block-Recurrent Transformers (Avg. Score: 0.95)\n\n*DeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, Behnam Neyshabur*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 66  (*Influential: 9*)\n\n**TL;DR:** N/A\n\n**Abstract:** We introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to sequence length. Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block in order to make efficient use of accelerator hardware. The cell itself is strikingly simple. It is merely a transformer layer: it uses self-attention and cross-attention to efficiently compute a recurrent function over a large set of state vectors and tokens. Our design was inspired in part by LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM cell up by several orders of magnitude. Our implementation of recurrence has the same cost in both computation time and parameter count as a conventional transformer layer, but offers dramatically improved perplexity in language modeling tasks over very long sequences. Our model out-performs a long-range Transformer XL baseline by a wide margin, while running twice as fast. We demonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source code. Our code has been released as open source.\n\n##### *Relevant Chunk: No. 11/53 (Score: 0.95)*\n\n```\nZong, F. Xia, W. Li, and R. Navigli, eds.), 2021. [13] H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, and B. Dai, \"Combiner: Full attention transformer with sparse computation cost,\" in Advances in Neural Information Processing Systems (A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), 2021. [14] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, \"Linformer: Self-attention with linear complexity,\" CoRR, vol. abs/2006.04768, 2020. [15] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap, \"Compressive transformers for long-range sequence modelling,\" in ICLR, 2020. [16] Z. Dai, G. Lai, Y. Yang, and Q. Le, \"Funnel-transformer: Filtering out sequential redundancy for efficient language processing,\" in NeurIPS, 2020.\n```\n\n#### 2. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.95)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 13/30 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2304.07327, 2023. Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers, 2022. Heejun Lee, Minki Kang, Youngwan Lee, and Sung Ju Hwang. Sparse token transformer with attention back tracking. In The Eleventh International Conference on Learning Representations, 2023. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019. Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022. Andr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo. Sparse and continuous attention mechanisms. Advances in Neural Information Processing Systems, 33:20989-21001, 2020. Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.94)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.95)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 14/46 (Score: 0.92)*\n\n```\narXiv preprint arXiv:2204.02311, 2022. [18] Eleanor Chu and Alan George. Inside the FFT black box: serial and parallel fast Fourier transform algorithms. CRC press, 1999. [19] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965. [20] Fiona Cunningham, James E Allen, Jamie Allen, Jorge Alvarez-Jarreta, M Ridwan Amode, Irina M Armean, Olanrewaju Austine-Orimoloye, Andrey G Azov, If Barnes, Ruth Bennett, et al. Ensembl 2022. Nucleic acids research, 50(D1):D988-D995, 2022. [21] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, $33: 4271-4282,2020$\n[22] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [23] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training.\n```\n\n#### 4. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.93)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 0.93)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n\n\n---\n## Found 12 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: sparse fft attention, sparse fft transformers, dynamic sparse fft neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. SEA: Sparse Linear Attention with Estimated Attention Mask\n\n*From Search Query: sparse fft attention*\n\n*Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang*\n\n**TL;DR:** SEA is proposed: Sparse linear attention with an Estimated Attention mask, which estimates the attention matrix with linear complexity via kernel-based linear attention, then subsequently creates a sparse attention matrix with a top-k selection to perform a sparse attention operation.\n\n**Abstract:** The transformer architecture has driven breakthroughs in recent years on tasks which require modeling pairwise relationships between sequential elements, as is the case in natural language understanding. However, long seqeuences pose a problem due to the quadratic complexity of the attention operation. Previous research has aimed to lower the complexity by sparsifying or linearly approximating the attention matrix. Yet, these approaches cannot straightforwardly distill knowledge from a teacher's attention matrix and often require complete retraining from scratch. Furthermore, previous sparse and linear approaches lose interpretability if they cannot produce full attention matrices. To address these challenges, we propose SEA: Sparse linear attention with an Estimated Attention mask. SEA estimates the attention matrix with linear complexity via kernel-based linear attention, then subsequently creates a sparse attention matrix with a top-k selection to perform a sparse attention operation. For language modeling tasks (Wikitext2), previous linear and sparse attention methods show roughly two-fold worse perplexity scores over the quadratic OPT-1.3B baseline, while SEA achieves better perplexity than OPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable attention matrix. We believe that our work will have a large practical impact, as it opens the possibility of running large transformers on resource-limited devices with less memory.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 2. Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\n\n*From Search Query: sparse fft attention*\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, F. Fleuret*\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention\u2014which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2 . 0 \u00d7 and 3 . 3 \u00d7 for sequences of respectively 8 k and 16 k tokens.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 3. Sparse Token Transformer with Attention Back Tracking\n\n*From Search Query: sparse fft attention*\n\n*Heejun Lee, Minki Kang, Youngwan Lee, Sung Ju Hwang*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 4. Exphormer: Sparse Transformers for Graphs\n\n*From Search Query: sparse fft transformers*\n\n*Hamed Shirzad, A. Velingker, B. Venkatachalam, Danica J. Sutherland, A. Sinop*\n\n**TL;DR:** It is shown that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets.\n\n**Abstract:** Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that Exphormer can scale to datasets on larger graphs than shown in previous graph transformer architectures. Code can be found at \\url{https://github.com/hamed1375/Exphormer}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 67  (*Influential: 15*)\n\n#### 5. White-Box Transformers via Sparse Rate Reduction\n\n*From Search Query: sparse fft transformers*\n\n*Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, B. Haeffele, Y. Ma*\n\n**TL;DR:** This paper contends that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces, and shows that the standard transformer block can be derived from alternating optimization on complementary parts of this objective.\n\n**Abstract:** In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at \\url{https://github.com/Ma-Lab-Berkeley/CRATE}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 50  (*Influential: 4*)\n\n#### 6. Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization\n\n*From Search Query: dynamic sparse fft neural networks*\n\n*H. Mostafa, Xin Wang*\n\n**TL;DR:** This work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network, and outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget.\n\n**Abstract:** Modern deep neural networks are typically highly overparameterized. Pruning techniques are able to remove a significant fraction of network parameters with little loss in accuracy. Recently, techniques based on dynamic reallocation of non-zero parameters have emerged, allowing direct training of sparse networks without having to pre-train a large dense model. Here we present a novel dynamic sparse reparameterization method that addresses the limitations of previous techniques such as high computational cost and the need for manual configuration of the number of free parameters allocated to each layer. We evaluate the performance of dynamic reallocation methods in training deep convolutional networks and show that our method outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget, on par with accuracies obtained by iteratively pruning a pre-trained dense model. We further investigated the mechanisms underlying the superior generalization performance of the resultant sparse networks. We found that neither the structure, nor the initialization of the non-zero parameters were sufficient to explain the superior performance. Rather, effective learning crucially depended on the continuous exploration of the sparse network structure space during training. Our work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 285  (*Influential: 33*)\n\n#### 7. State Transition of Dendritic Spines Improves Learning of Sparse Spiking Neural Networks\n\n*From Search Query: dynamic sparse fft neural networks*\n\n*Yanqing Chen, Zhaofei Yu, Wei Fang, Zhengyu Ma, Tiejun Huang, Yonghong Tian*\n\n**TL;DR:** This work organizes SNN pruning techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights, which yields sparse deep networks on the large-scale dataset while maintaining state-of-the-art low performance loss.\n\n**Abstract:** Spiking Neural Networks (SNNs) are considered a promising alternative to Arti\ufb01cial Neural Networks (ANNs) for their event-driven computing paradigm when deployed on energy-ef\ufb01cient neuromorphic hardware. Recently, deep SNNs have shown breathtaking performance improvement through cutting-edge training strategy and \ufb02exi-ble structure, which also scales up the number of parameters and computational burdens in a single network. Inspired by the state transition of dendritic spines in the \ufb01lopodial model of spinogen-esis, we model different states of SNN weights, facilitating weight optimization for pruning. Furthermore, the pruning speed can be regulated by using different functions describing the growing threshold of state transition. We organize these techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights. Our approach yields sparse deep networks on the large-scale dataset (SEW ResNet18 on ImageNet) while maintaining state-of-the-art low performance loss ( \u223c 3% at 88.8% sparsity) compared to existing pruning methods on directly trained SNNs. Moreover, we \ufb01nd out pruning speed regulation while learning is crucial to avoiding disastrous performance degradation at the \ufb01nal stages of training, which may shed light on future work on SNN pruning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 32  (*Influential: 2*)\n\n#### 8. Implicit Bias of the Step Size in Linear Diagonal Neural Networks\n\n*From Search Query: dynamic sparse fft neural networks*\n\n*M. S. Nacson, Kavya Ravichandran, N. Srebro, Daniel Soudry*\n\n**TL;DR:** This work shows how using large step size for non-centered data can change the implicit bias from a \u201dkernel\u201d type behavior to a \u201drich\u201d (sparsity-inducing) regime \u2014 even when gradient flow, studied in previous works, would not escape the \u201dkernel\u201d regime.\n\n**Abstract:** Focusing on diagonal linear networks as a model for understanding the implicit bias in underdeter-mined models, we show how the gradient descent step size can have a large qualitative effect on the implicit bias, and thus on generalization ability. In particular, we show how using large step size for non-centered data can change the implicit bias from a \u201dkernel\u201d type behavior to a \u201drich\u201d (sparsity-inducing) regime \u2014 even when gradient flow, studied in previous works, would not escape the \u201dkernel\u201d regime. We do so by using dynamic stability, proving that convergence to dynamically stable global minima entails a bound on some weighted \u2113 1 -norm of the linear predictor, i.e. a \u201drich\u201d regime. We prove this leads to good generalization in a sparse regression setting.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 42  (*Influential: 4*)\n\n### 4 related papers from Papers with Code\n\n#### 1. PRISTA-Net: Deep Iterative Shrinkage Thresholding Network for Coded Diffraction Patterns Phase Retrieval\n\n*From Search Query: sparse fft attention*\n\n*Jianping Zhang, Yin Yang, Xiaohong Fan, Aoxu Liu*\n\n**Abstract:** The problem of phase retrieval (PR) involves recovering an unknown image from limited amplitude measurement data and is a challenge nonlinear inverse problem in computational imaging and image processing. However, many of the PR methods are based on black-box network models that lack interpretability and plug-and-play (PnP) frameworks that are computationally complex and require careful parameter tuning. To address this, we have developed PRISTA-Net, a deep unfolding network (DUN) based on the first-order iterative shrinkage thresholding algorithm (ISTA). This network utilizes a learnable nonlinear transformation to address the proximal-point mapping sub-problem associated with the sparse priors, and an attention mechanism to focus on phase information containing image edges, textures, and structures. Additionally, the fast Fourier transform (FFT) is used to learn global features to enhance local information, and the designed logarithmic-based loss function leads to significant improvements when the noise level is low. All parameters in the proposed PRISTA-Net framework, including the nonlinear transformation, threshold parameters, and step size, are learned end-to-end instead of being manually set. This method combines the interpretability of traditional methods with the fast inference ability of deep learning and is able to handle noise at each iteration during the unfolding stage, thus improving recovery quality. Experiments on Coded Diffraction Patterns (CDPs) measurements demonstrate that our approach outperforms the existing state-of-the-art methods in terms of qualitative and quantitative evaluations. Our source codes are available at \\emph{https://github.com/liuaxou/PRISTA-Net}.\n\n**Published:** 2023-09-08\n\n\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\n\n*From Search Query: sparse fft transformers*\n\n*Christopher R\u00e9, Eric Nguyen, Hermann Kumbong, Daniel Y. Fu*\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Published:** 2023-11-10\n\n\n\n#### 3. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\n\n*From Search Query: sparse fft transformers*\n\n*Christopher R\u00e9, Albert Gu, Matthew Eichhorn, Atri Rudra, Tri Dao*\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \\log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to $1024$. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points -- the first time a structured approach has done so -- with 4X faster inference speed and 40X fewer parameters.\n\n**Published:** 2019-03-14\n\n\n\n#### 4. Robust Data-Driven Discovery of Partial Differential Equations under Uncertainties\n\n*From Search Query: dynamic sparse fft neural networks*\n\n*Anonymous*\n\n**Abstract:** Robust physics (e.g., governing equations and laws) discovery is of great interest for many engineering fields and explainable machine learning. A critical challenge compared with general training is that the term and format of governing equations is not known as a prior. In addition, significant measurement noise and complex algorithm hyperparameter tuning usually reduces the robustness of existing methods. A robust data-driven method is proposed in this study for identifying the governing Partial Differential Equations (PDEs) of a given system from noisy data. The proposed method is based on the concept of Progressive Sparse Identification of PDEs (PSI-PDE or $\\psi$-PDE). Special focus is on the handling of data with huge uncertainties (e.g., 50$\\%$ noise level). Neural Network modeling and fast Fourier transform (FFT) are implemented to reduce the influence of noise in sparse regression. Following this, candidate terms from the prescribed library are progressively selected and added to the learned PDEs, which automatically promotes parsimony with respect to the number of terms in PDEs as well as their complexity. Next, the significance of each learned terms is further evaluated and the coefficients of PDE terms are optimized by minimizing the L2 residuals. Results of numerical case studies indicate that the governing PDEs of many canonical dynamical systems can be correctly identified using the proposed $\\psi$-PDE method with highly noisy data. One great benefit of proposed algorithm is that it avoids complex algorithm modification and hyperparameter tuning in most existing methods. Limitations of the proposed method and major findings are presented.\n\n**Published:** 2021-01-31\n\n\n\n\n\n---\n## Web search results\n\n The researcher's goal is to design a novel autoregressive language model (LM) block that integrates **dynamic sparsity** and **efficient FFT-based computation** to enhance the performance of the existing `FastTTTLinear` Generalized Attention Unit (GAU). Here are the key points and the most useful information to help achieve this goal:\n\n## Integration of Dynamic Sparsity and FFT-Based Computation\n\n### Dynamic Sparsity\n- Dynamic sparsity techniques have been used to reduce unnecessary computations in Transformers, improving efficiency. Key methods include:\n  - Content-based sparse attention (Roy et al., 2020),\n  - Dynamic context pruning (Anagnostidis et al., 2023),\n  - Channel-level sparsity learning (Yin et al., 2023).\n  These methods focus on learning sparse attention patterns to reduce memory and computational requirements.\n\n### FFT-Based Computation\n- FFT has been used to accelerate computations in neural networks, particularly in Transformers.\n  - The Fourier Transformer (He et al., 2023) uses FFT to remove sequence redundancy, improving efficiency for long sequences.\n  - FlashFFTConv (Fu et al., 2023) optimizes FFT convolutions and introduces sparse convolution algorithms by skipping blocks in the matrix decomposition.\n\n## Combining Dynamic Sparsity and FFT-Based Computation\n\n### Novelty and Feasibility\n- There is no direct evidence of prior work combining dynamic sparsity with FFT-based computation in Transformer models or GAUs. This makes the proposal novel but also introduces implementation challenges.\n\n### Potential Synergy and Challenges\n- The synergy between dynamic sparsity and FFT operations could lead to significant efficiency gains, but it requires careful architectural design. FFT operations are structured and may not naturally align with dynamically changing sparsity patterns. Ensuring training stability and maintaining model performance is a key concern.\n\n## Useful References and Concepts\n\n### Sparse FFT Algorithms\n- Investigating sparse FFT algorithms and their applications in neural networks is crucial. For example, the use of FFT to accelerate computations in the context of attention mechanisms or sequence modeling, as seen in the Fourier Transformer and FlashFFTConv, could be adapted to incorporate dynamic sparsity.\n\n### Recent Work and Preprints\n- Searching for the latest publications (late 2023 and 2024) on preprint servers like arXiv can help find cutting-edge studies that might have explored similar ideas recently.\n\n### Implementations in Related Fields\n- Exploring domains like signal processing or computer vision where sparse FFT and dynamic sparsity might have been combined can provide transferable concepts or techniques. For instance, the use of FFT in other domains, such as in the prediction of bearing remaining useful life (RUL) using TCN and transformer models, shows the versatility of FFT in handling sequence data efficiently.\n\n### Hardware Considerations\n- Researching hardware-efficient implementations of sparse FFT and their compatibility with GPU architectures is essential. The PVA hardware accelerator, for example, can handle math computations efficiently, including FFT, which could be relevant for optimizing the computational pipeline.\n\n## Key Search Queries and Internal Library Search\n\n### Keywords\n```keywords sparse fft attention```\n```keywords sparse fft transformers```\n```keywords dynamic sparse fft neural networks```\n\n### Internal Library Search Description\n```description Seeking recent research papers, preprints, or implementations that explore integrating sparse FFT computations into Transformers or neural networks, particularly combining dynamic sparsity with FFT-based methods. Interested in methods that leverage sparse FFT algorithms within attention mechanisms or other neural network layers to improve efficiency in long-sequence processing. Looking for any works that discuss challenges and solutions in implementing sparse FFT in sparse neural computations or combining dynamic sparsity with FFT operations.```\n\n## Conclusion\n\nTo achieve the researcher's goal, it is crucial to:\n\n- **Explore sparse FFT algorithms** and their integration into attention mechanisms or sequence modeling.\n- **Search for recent work** on preprint servers and in related fields to find any cutting-edge studies that might have combined dynamic sparsity with FFT-based methods.\n- **Assess hardware considerations** and the feasibility of implementing such a system on current GPU architectures.\n- **Prepare for in-depth review** by compiling findings to evaluate the theoretical soundness, feasibility, and potential impact of the proposal.\n\nBy focusing on these areas, the researcher can comprehensively evaluate and develop a novel autoregressive LM block that integrates dynamic sparsity and FFT-based computation, potentially leading to significant improvements in efficiency and scalability for long-sequence processing.\n",
            "analysis": "**Analysis Note:**\n\n1. **Summary of the Analysis:**\n\n   The proposal aims to enhance the **FastTTTLinear** GAU by integrating **dynamic sparsity** and **efficient FFT-based computation**. Specifically, it introduces **DynamicSparseFFTLinear**, which combines learned sparsity patterns with hardware-efficient FFT implementations to improve performance on long sequences while reducing memory and computational requirements.\n\n   Based on the search results from internal and external sources:\n\n   - **Dynamic Sparsity** and **FFT-based computations** have individually been explored in Transformer models and attention mechanisms to improve efficiency and scalability for long-sequence processing.\n   - **Dynamic Sparsity** has been used to reduce unnecessary computations by learning sparse attention patterns.\n   - **FFT-based Computation** has been applied to accelerate attention calculations and convolution operations in neural networks.\n   - There is **limited evidence** of prior work that combines both dynamic sparsity and FFT-based computation within Transformer architectures or GAUs.\n   - The proposal appears to be **novel** in integrating these two techniques for enhancing FastTTTLinear.\n\n2. **All Useful References with Excerpts:**\n\n   - **Roy et al. (2020): Efficient Content-Based Sparse Attention with Routing Transformers**\n\n     *Excerpt*: \"Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest... It combines the modeling flexibility of prior work on content-based sparse attention with efficiency gains from approaches based on local, temporal sparse attention.\"\n\n   - **He et al. (2023): Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**\n\n     *Excerpt*: \"We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequences using the ready-made Fast Fourier Transform (FFT) operator... Our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space.\"\n\n   - **Pagliardini et al. (2023): Faster Causal Attention Over Large Sequences Through Sparse Flash Attention**\n\n     *Excerpt*: \"We extend FlashAttention to accommodate a large class of attention sparsity patterns... This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\"\n\n   - **Lou et al. (2024): Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**\n\n     *Excerpt*: \"We propose SPARSEK Attention, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance... Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query.\"\n\n   - **Fu et al. (2023): FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**\n\n     *Excerpt*: \"We study how to optimize the FFT convolution... Propose FlashFFTConv, which uses a matrix decomposition that computes the FFT using matrix multiply units... Present two sparse convolution algorithms which can be implemented by skipping blocks in the matrix decomposition.\"\n\n   - **Anagnostidis et al. (2023): Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**\n\n     *Excerpt*: \"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process.\"\n\n   - **Sun et al. (2024): ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference**\n\n     *Excerpt*: \"We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences... We demonstrate that it can support up to 6\u00d7 larger batch sizes and boost throughput by up to 3.04\u00d7 on an A100 GPU without sacrificing accuracy.\"\n\n   - **Yin et al. (2023): Dynamic Sparsity Is Channel-Level Sparsity Learner**\n\n     *Excerpt*: \"We propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity during one end-to-end training process... The resulting small sparse networks can be directly accelerated by commodity hardware.\"\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Dynamic Sparsity in Transformers:**\n     - Dynamic sparsity techniques have been effectively used to improve the efficiency of Transformers by reducing unnecessary computations.\n     - Key methods include content-based sparse attention (Roy et al., 2020), dynamic context pruning (Anagnostidis et al., 2023), and channel-level sparsity learning (Yin et al., 2023).\n     - These methods learn to focus computation on the most relevant parts of the input, reducing memory and computational requirements.\n\n   - **FFT-Based Computation in Transformers:**\n     - FFT has been leveraged to accelerate computations in Transformers and other neural network architectures.\n     - The Fourier Transformer (He et al., 2023) uses FFT to remove sequence redundancy, improving efficiency for long sequences.\n     - FlashFFTConv (Fu et al., 2023) optimizes FFT convolutions and introduces sparse convolution algorithms by skipping blocks in the matrix decomposition.\n\n   - **Combination of Dynamic Sparsity and FFT-Based Computation:**\n     - There is **no direct evidence** of prior work combining dynamic sparsity with FFT-based computation in Transformer models or GAUs.\n     - Existing FFT-based methods primarily focus on dense computations or do not incorporate learned sparsity patterns in conjunction with FFT operations.\n     - The potential synergy between dynamic sparsity and FFT operations could lead to significant efficiency gains but may also introduce implementation challenges.\n\n   - **Feasibility and Implementation Challenges:**\n     - Integrating dynamic sparsity with FFT-based computation requires careful architectural design.\n     - FFT operations are inherently structured and may not naturally align with dynamically changing sparsity patterns.\n     - Sparse FFT algorithms exist but are less commonly used and may be more complex to implement efficiently on current hardware.\n     - Ensuring training stability and maintaining model performance while integrating both techniques is a key concern.\n\n   - **Impact on Model Performance:**\n     - If successfully implemented, the proposal could significantly improve the efficiency and scalability of models for long-sequence processing.\n     - It may reduce memory footprint and computation time without compromising, or potentially even improving, accuracy and robustness.\n     - Aligns with current research trends aiming to make Transformers more efficient for longer contexts.\n\n   - **Novelty Assessment:**\n     - The proposal appears to be novel, as existing literature does not report combining dynamic sparsity with FFT-based computation in this manner.\n     - This novelty could contribute valuable advancements to the field but also requires thorough investigation to confirm feasibility.\n\n4. **Future Search Plan or Next Steps:**\n\n   - **Explore Sparse FFT Algorithms:**\n     - Investigate research on sparse FFT algorithms and their applications in neural networks.\n     - Look for any methods that apply sparse FFT in the context of attention mechanisms or sequence modeling.\n\n   - **Search for Very Recent Work:**\n     - Focus on the latest publications (late 2023 and 2024), as new research may have explored similar ideas recently.\n     - Use preprint servers like arXiv to find cutting-edge studies.\n\n   - **Investigate Implementations in Related Fields:**\n     - Explore domains like signal processing or computer vision where sparse FFT and dynamic sparsity might have been combined.\n     - Look for transferable concepts or techniques that could inform the proposal.\n\n   - **Assess Practicality and Hardware Considerations:**\n     - Research hardware-efficient implementations of sparse FFT and their compatibility with GPU architectures.\n     - Examine any reported implementation challenges and solutions in integrating FFT with sparse computations.\n\n   - **Prepare for In-Depth Review:**\n     - Compile findings from the additional searches to complete the analysis.\n     - Ensure that sufficient information is available to assess the theoretical soundness, feasibility, and potential impact of the proposal.\n\n5. **List of References:**\n\n   - Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). **Efficient Content-Based Sparse Attention with Routing Transformers**. *Transactions of the Association for Computational Linguistics*, 9, 53-68.\n\n   - He, Z., Yang, M., Feng, M., Yin, J., Wang, X., Leng, J., & Lin, Z. (2023). **Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator**. *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics*.\n\n   - Pagliardini, M., Paliotta, D., Jaggi, M., & Fleuret, F. (2023). **Faster Causal Attention Over Large Sequences Through Sparse Flash Attention**. *Advances in Neural Information Processing Systems*.\n\n   - Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). **Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers**. *arXiv preprint*.\n\n   - Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). **FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores**. *arXiv preprint*.\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). **Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers**. *Advances in Neural Information Processing Systems*.\n\n   - Sun, H., Chang, L.-W., Bao, W., Zheng, S., Zheng, N., Liu, X., Dong, H., Chi, Y., & Chen, B. (2024). **ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference**. *arXiv preprint*.\n\n   - Yin, L., Li, G., Fang, M., Shen, L., Huang, T., Wang, Z., Menkovski, V., Ma, X., Pechenizkiy, M., & Liu, S. (2023). **Dynamic Sparsity Is Channel-Level Sparsity Learner**. *Advances in Neural Information Processing Systems*.\n\n---\n\n**New Search Queries:**\n\n1. **Keywords:**\n\n   ```keywords sparse fft attention```\n\n   ```keywords sparse fft transformers```\n\n   ```keywords dynamic sparse fft neural networks```\n\n2. **Internal Library Search Description:**\n\n   ```description Seeking recent research papers, preprints, or implementations that explore integrating sparse FFT computations into Transformers or neural networks, particularly combining dynamic sparsity with FFT-based methods. Interested in methods that leverage sparse FFT algorithms within attention mechanisms or other neural network layers to improve efficiency in long-sequence processing. Looking for any works that discuss challenges and solutions in implementing sparse FFT in sparse neural computations or combining dynamic sparsity with FFT operations.```\n\n---\n\n**Note:**\n\nI will proceed to review the results from these new search queries to further assess the novelty and feasibility of the proposal. This will help ensure a comprehensive evaluation before formulating the final review."
        }
    ],
    "rating": 4.0,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.8715419999999999,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 5.4354000000000005,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "# DynamicSparseFFTLinear: Enhancing FastTTTLinear with Dynamic Sparsity and Efficient FFT Computation\n\n## Motivation\n\nCurrent language models face significant challenges in efficiently processing long sequences while maintaining expressiveness. While FastTTTLinear addresses some efficiency concerns through gated linear attention, it still struggles with:\n\n1. High memory requirements for long sequences\n2. Computational overhead from dense operations\n3. Limited ability to adapt computation based on input importance\n4. Inefficient hardware utilization\n\nThis proposal aims to enhance FastTTTLinear by introducing dynamic sparsity and efficient FFT-based computation to improve scalability while maintaining or improving model quality.\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **Memory Efficiency**:\n   - Full attention matrices consume significant memory\n   - State representations not optimized for sparsity\n   - Inefficient memory access patterns\n\n2. **Computational Overhead**:\n   - Dense operations performed on all inputs\n   - Limited ability to skip irrelevant computations\n   - Hardware resources not optimally utilized\n\n3. **Adaptability**:\n   - Fixed computation patterns regardless of input\n   - Limited ability to focus on important features\n   - Inefficient handling of varying sequence lengths\n\n### Opportunities for Enhancement\n\n1. **Dynamic Sparsity**:\n   - Input-dependent computation patterns\n   - Selective feature processing\n   - Efficient memory management\n\n2. **FFT-based Computation**:\n   - Hardware-efficient implementations\n   - Improved parallel processing\n   - Reduced computational complexity\n\n3. **Hardware Optimization**:\n   - Memory hierarchy awareness\n   - Efficient tensor operations\n   - Improved cache utilization\n\n## Core Idea and Philosophy\n\nThe core idea is to enhance FastTTTLinear by:\n\n1. Introducing dynamic sparsity through learned importance scoring\n2. Implementing efficient FFT-based computation for long-range dependencies\n3. Optimizing memory access patterns for hardware efficiency\n4. Maintaining test-time training capabilities\n\n## Design Plan\n\n### Mathematical Formulation\n\nLet X \u2208 \u211d^(B\u00d7L\u00d7D) be the input sequence, where:\n- B: batch size\n- L: sequence length\n- D: embedding dimension\n\nThe enhanced computation flow is:\n\n1. **Importance Scoring**:\n   ```\n   S = \u03c3(W_s \u00b7 X + b_s)  # S \u2208 \u211d^(B\u00d7L\u00d71)\n   ```\n   where \u03c3 is sigmoid activation\n\n2. **Dynamic Sparsity Mask**:\n   ```\n   M = TopK(S, k=\u03b1L)  # M \u2208 \u211d^(B\u00d7L\u00d71), \u03b1 \u2208 (0,1]\n   ```\n   where \u03b1 controls sparsity ratio\n\n3. **FFT-based Processing**:\n   ```\n   X_f = FFT(X \u00b7 M)  # X_f \u2208 \u2102^(B\u00d7L\u00d7D)\n   Y_f = ComplexConv(X_f, W_f)  # Y_f \u2208 \u2102^(B\u00d7L\u00d7D)\n   Y = IFFT(Y_f)  # Y \u2208 \u211d^(B\u00d7L\u00d7D)\n   ```\n\n4. **Gated Output**:\n   ```\n   G = \u03c3(W_g \u00b7 Y + b_g)  # G \u2208 \u211d^(B\u00d7L\u00d7D)\n   O = G \u2299 Y  # O \u2208 \u211d^(B\u00d7L\u00d7D)\n   ```\n\n### Implementation\n\n```python\nclass DynamicSparseFFTLinear(GAUBase):\n    def __init__(self, embed_dim, block_loc, kwarg_all, \n                 sparsity_ratio=0.8, device=None, dtype=None):\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.sparsity_ratio = sparsity_ratio\n        \n        # Importance scoring\n        self.score_net = nn.Linear(embed_dim, 1)\n        \n        # FFT parameters\n        self.fft_conv = ComplexConv(embed_dim, embed_dim)\n        \n        # Gating\n        self.gate_net = nn.Linear(embed_dim, embed_dim)\n        \n        # Layer norm\n        self.norm = nn.LayerNorm(embed_dim)\n        \n    def _forward(self, X, **Z):\n        # Compute importance scores\n        scores = torch.sigmoid(self.score_net(X))\n        \n        # Create sparsity mask\n        k = int(X.shape[1] * self.sparsity_ratio)\n        mask = torch.zeros_like(scores)\n        topk_indices = scores.topk(k, dim=1).indices\n        mask.scatter_(1, topk_indices, 1.0)\n        \n        # Apply mask and FFT\n        X_masked = X * mask\n        X_f = torch.fft.rfft(X_masked, dim=1)\n        \n        # FFT convolution\n        Y_f = self.fft_conv(X_f)\n        Y = torch.fft.irfft(Y_f, dim=1)\n        \n        # Gating\n        gates = torch.sigmoid(self.gate_net(Y))\n        output = gates * Y\n        \n        # Normalization\n        output = self.norm(output)\n        \n        return output, Z\n\nclass ComplexConv(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.weight_real = nn.Parameter(torch.randn(in_dim, out_dim))\n        self.weight_imag = nn.Parameter(torch.randn(in_dim, out_dim))\n        \n    def forward(self, x_f):\n        # Complex convolution in frequency domain\n        real = x_f.real @ self.weight_real - x_f.imag @ self.weight_imag\n        imag = x_f.real @ self.weight_imag + x_f.imag @ self.weight_real\n        return torch.complex(real, imag)\n```\n\n### Memory Optimization\n\n1. **Efficient State Management**:\n   - Store only non-zero elements using sparse tensors\n   - Implement block-based processing for better cache utilization\n   - Use hierarchical state compression for long sequences\n\n2. **Hardware-Aware Implementation**:\n   - Optimize memory access patterns for GPU memory hierarchy\n   - Implement efficient parallel processing\n   - Use specialized CUDA kernels for key operations\n\n### Integration with Test-Time Training\n\n1. **Maintain Adaptability**:\n   - Update importance scores during inference\n   - Adjust sparsity patterns based on input\n   - Preserve gradient flow for test-time updates\n\n2. **Efficient Updates**:\n   - Implement sparse gradient updates\n   - Use incremental FFT computation\n   - Optimize memory usage during updates\n\n## Theoretical Analysis\n\n### Computational Complexity\n\n1. **Time Complexity**:\n   - Importance scoring: O(BLD)\n   - FFT computation: O(BL log L)\n   - Overall: O(BL(log L + D))\n\n2. **Memory Complexity**:\n   - Sparse storage: O(\u03b1BLD)\n   - FFT workspace: O(BLD)\n   - Total: O(BLD(\u03b1 + 1))\n\n### Stability Analysis\n\n1. **Gradient Flow**:\n   - Stable through skip connections\n   - Controlled by gating mechanism\n   - Preserved through sparse updates\n\n2. **Numerical Stability**:\n   - FFT operations well-conditioned\n   - Normalized intermediate states\n   - Bounded importance scores\n\n## Implementation Guidelines\n\n1. **Initialization**:\n```python\ndef init_parameters(module):\n    if isinstance(module, nn.Linear):\n        nn.init.xavier_uniform_(module.weight)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\n```\n\n2. **Training Configuration**:\n```python\nconfig = {\n    'learning_rate': 1e-4,\n    'warmup_steps': 1000,\n    'sparsity_ratio': 0.8,\n    'fft_block_size': 1024,\n    'gradient_clip': 1.0\n}\n```\n\n3. **Memory Management**:\n```python\ndef optimize_memory(tensors, max_block_size):\n    blocks = []\n    for t in tensors:\n        if t.shape[1] > max_block_size:\n            blocks.extend(t.split(max_block_size, dim=1))\n        else:\n            blocks.append(t)\n    return blocks\n```\n\n## Conclusion\n\nDynamicSparseFFTLinear enhances FastTTTLinear by introducing:\n1. Dynamic sparsity for efficient computation\n2. FFT-based processing for improved scalability\n3. Hardware-aware optimizations for better performance\n4. Maintained test-time training capabilities\n\nThese improvements enable:\n- Reduced memory requirements\n- Improved computational efficiency\n- Better handling of long sequences\n- Enhanced model adaptability\n\n## References\n\n1. Fu, D. Y., et al. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n2. Lou, C., et al. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv preprint.\n\n3. Anagnostidis, S., et al. (2023). Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. Neural Information Processing Systems.\n\n4. Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n5. Yin, L., et al. (2023). Dynamic Sparsity Is Channel-Level Sparsity Learner. Neural Information Processing Systems.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "o1_preview",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "Enhancing FastTTTLinear with dynamic sparsity and efficient FFT-based computation to improve performance on long sequences while reducing memory and computational requirements. The design combines selective computation through learned sparsity patterns with hardware-efficient FFT implementations for improved scalability.",
    "ideation": null,
    "modelname": "dynamicsparsefftlm",
    "suggestions": null,
    "user_input": ""
}