{
    "variantname": "HierarchicalSharedRMSNorm",
    "review": "The HierShare-GPT proposal presents an innovative approach to enhancing normalization in language models through hierarchical parameter sharing and adaptive computation. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel integration of hierarchical parameter organization with adaptive selection\n- Well-grounded mathematical formulation for parameter sharing and selection\n- Clear differentiation from existing normalization approaches\n- Thoughtful consideration of hardware efficiency\n\n2. Design Clarity:\n- Well-structured mathematical formulation\n- Clear architectural components and interactions\n- Detailed implementation guidelines\n- Comprehensive theoretical analysis\n\n3. Efficiency Considerations:\n- Strategic parameter sharing to reduce memory overhead\n- Cache-aware computation design\n- Hardware-efficient memory access patterns\n- Clear focus on practical implementation efficiency\n\n4. Adaptability:\n- Dynamic parameter selection based on input characteristics\n- Multi-level parameter organization\n- Efficient sharing across similar contexts\n- Clear scaling strategy for different model sizes\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex interaction between hierarchical levels\n- Additional overhead from selection network\n- Potential challenges in training stability\n- Integration complexity with existing architectures\n\n2. Memory Management:\n- Cache pressure from parameter tree structure\n- Memory overhead from selection network\n- Complex memory access patterns\n- Potential cache thrashing issues\n\n3. Computational Overhead:\n- Additional computation for parameter selection\n- Tree traversal computational cost\n- Multiple parallel normalization paths\n- Dynamic selection overhead\n\n4. Training Dynamics:\n- Stability with shared parameters\n- Impact on gradient flow through hierarchy\n- Potential challenges in optimization\n- Need for careful initialization strategies\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than standard hierarchical normalization\n2. More efficient than adaptive layer normalization\n3. Novel integration of tree-structured sharing\n4. Unique approach to dynamic parameter selection\n\nHowever, it shares some concepts with:\n1. Adaptive Layer Normalization's dynamic parameters\n2. Hierarchical Layer Normalization's multi-level structure\n3. Parameter-efficient transformers' sharing strategies",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient attention mechanisms state space models",
            "detail": "Find papers discussing efficient attention mechanisms and state space models, particularly focusing on:\n1. Linear or subquadratic attention mechanisms\n2. State space models for sequence modeling\n3. Hybrid approaches combining attention with other mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing efficient attention mechanisms and state space models, particularly focusing on:\n1. Linear or subquadratic attention mechanisms\n2. State space models for sequence modeling\n3. Hybrid approaches combining attention with other mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 1.00)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 1.00)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 1.00)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 3. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 1.00)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 3/35 (Score: 1.00)*\n\n```\nFinally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code ${ }^{1}$ and pre-trained model checkpoints ${ }^{2}$ are publicly available. ## 2 Background\n\n### 2.1 Attention Mechanism\n\nSuppose the input to the layer is $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, where $L$ is the sequence length and $d$ is the embedding dimension, then the attention mechanism outputs\n\n$$\n\\operatorname{Attn}(\\mathbf{X})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}$. Here $\\mathbf{W}_{q}, \\mathbf{W}_{k}, \\mathbf{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix $\\mathbf{A}=$ $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\in \\mathbb{R}^{L \\times L}$. Then, $\\mathbf{A}_{i j}$ captures the alignment between the $i$-th and the $j$-th input tokens. ### 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a 1-dimensional input signal $u(t)$ to a $d_{s}$-dimensional latent state $x(t)$, after which $x(t)$ is mapped to a 1-dimensional output signal $y(t)$. Concretely,\n\n$$\nx^{\\prime}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nHere, $\\mathbf{A} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{B} \\in \\mathbb{R}^{d_{s}}$ and $\\mathbf{C} \\in \\mathbb{R}^{d_{s}}$. Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$\n\n[^1]cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, highorder polynomial projection operators) are proposed to initialize A. The HiPPO matrices are designed such that the state $x(t)$ at time $t$ can memorize the history of the input $u(t)$ up to time $t$. Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, where $L$ is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size $\\Delta$, such that\n\n$$\n\\begin{aligned}\n& x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k} \\\\\n& \\text { where } \\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1}(\\mathbf{I}+\\Delta / 2 \\cdot \\mathbf{A}) \\\\\n& \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1} \\Delta \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n\\end{aligned}\n$$\n\nWe unroll the above recurrent representation, after which we have\n\n$$\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\cdots+\\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C B}} u_{k}\n$$\n\nThis can be written as a convolutional representation $y=\\overline{\\mathbf{K}} * u$, where the convolution kernel\n\n$$\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\cdots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nHere, \" $*$ \" is the discrete convolution operator, $u$ represents the input sequence $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, and $y$ represents the corresponding output sequence $\\left(y_{0}, y_{1}, \\cdots, y_{L}\\right)$.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 5. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient attention mechanisms state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 3. Efficient Attention via Control Variates\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 4. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 5. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Mega: Moving Average Equipped Gated Attention\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Luke Zettlemoyer, Jonathan May, Graham Neubig, Liangke Gui, Junxian He, Xiang Kong, Chunting Zhou, Xuezhe Ma*\n\n**Abstract:** The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.\n\n**Published:** 2022-09-21\n\n\n\n#### 2. Linformer: Self-Attention with Linear Complexity\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Madian Khabsa, Hao Ma, Sinong Wang, Belinda Z. Li, Han Fang*\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n**Published:** 2020-06-08\n\n\n\n#### 3. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: efficient attention mechanisms state space models*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient attention mechanisms and state space models, here are some key findings and references that align with your analysis and goals:\n\n## Efficient Attention Mechanisms\n\n### Linear or Subquadratic Attention Mechanisms\n- The paper \"Attention Is All You Need\" introduced the transformer architecture, but it suffers from quadratic complexity in sequence length. Recent works have aimed to address this:\n  - The concept of **causal self-attention** and its limitations are discussed, but new models like **Taipan** introduce selective attention and sliding window attention to reduce computational complexity. Taipan achieves a sparser attention weight map, enhancing efficiency for long sequences.\n  - **Hyena Hierarchy** proposes using interleaved long convolutions and data-controlled gating as an alternative to traditional attention, achieving subquadratic complexity. This approach could be integrated into autoregressive models to handle long-range dependencies more efficiently[Analysis Note].\n\n### Hybrid Approaches Combining Attention with Other Mechanisms\n- **Taipan** combines the efficiency of state space models (SSMs) with strategically placed selective attention layers. This hybrid approach optimizes resource allocation by applying attention only to tokens that benefit from it, thus balancing computational efficiency and expressiveness.\n- **Mamba-2** generalizes linear attention using structured state-space models (SSMs), which balances computational efficiency and recall accuracy. This model integrates attention mechanisms with SSMs, offering improved performance in memory-intensive tasks.\n\n## State Space Models for Sequence Modeling\n\n### State Space Models (SSMs)\n- SSMs have emerged as a promising approach for sequence modeling, particularly in attention-free architectures. They offer improved computational efficiency and the ability to handle long-range dependencies effectively:\n  - SSMs maintain linear complexity, making them scalable for long sequences. They also integrate concepts from both recurrent and convolutional networks, positioning them as a competitive alternative to traditional transformer architectures, especially in resource-constrained environments.\n  - The **Mamba-2** architecture and **Taipan** model leverage SSMs to enhance the efficiency of sequence processing while capturing important long-range dependencies.\n\n## Hybrid Approaches and Efficiency Considerations\n\n### Combining Multiple Processing Mechanisms\n- Hybrid architectures that combine different processing mechanisms show significant promise:\n  - Integrating SSMs with attention mechanisms, as seen in **Taipan** and **Mamba-2**, allows for efficient processing of long sequences while maintaining the ability to capture long-range dependencies. This approach optimizes resource allocation and improves computational efficiency.\n  - The use of **selective attention** and **sliding window attention** in **Taipan** reduces the computational demands associated with full attention mechanisms, making it more efficient for handling extensive in-context retrieval tasks.\n\n### Memory and Computational Efficiency\n- SSMs are particularly beneficial for their linear complexity and context-aware selectivity, which enhance their predictive capabilities and scalability:\n  - **State Space Models** can handle high-dimensional data and maintain linear complexity, making them efficient for long sequences and resource-constrained environments.\n  - The **attention budget constraint** in **Taipan** ensures that attention is applied only where necessary, optimizing both computational and memory efficiency.\n\nBy incorporating these insights, researchers can develop more efficient autoregressive language models that balance computational efficiency with model expressiveness, handle long-range dependencies effectively, and scale well with larger models and more data.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis\n\nThe seed design HieraNorm-AttnGPT introduces hierarchical adaptive multi-head attention with dynamic layer normalization. From analyzing the seed design and initial references, several key areas emerge for potential innovation:\n\na) Attention Mechanisms:\n- Current designs struggle with quadratic complexity in sequence length\n- Need for more efficient ways to handle long-range dependencies\n- Opportunity to explore alternative attention formulations\n\nb) Normalization:\n- Traditional normalization techniques lack adaptability\n- Room for improvement in gradient stability\n- Need for more efficient normalization approaches\n\nc) Memory and Context:\n- Challenges in handling very long sequences\n- Need for better context management\n- Opportunity to improve memory efficiency\n\n2. Useful References with Excerpts\n\nFrom Hyena Hierarchy paper:\n\"Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible.\"\n\nKey insight: Hyena proposes using interleaved long convolutions and data-controlled gating as an alternative to attention, achieving subquadratic complexity.\n\nFrom Normalized Attention Without Probability Cage:\n\"We highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization.\"\n\nKey insight: Questions fundamental assumptions about attention normalization and suggests alternatives to softmax.\n\nFrom Soaring from 4K to 400K:\n\"Activation Beacon condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window.\"\n\nKey insight: Novel approach to extending context window through activation condensation.\n\n3. Key Insights and Analysis\n\na) Attention Alternatives:\n- Convolution-based approaches (like Hyena) show promise for efficient sequence processing\n- Removing probability constraints on attention (as in Normalized Attention) could increase expressiveness\n- Activation condensation techniques could help with long-range dependencies\n\nb) Efficiency Considerations:\n- Need to balance computational efficiency with model expressiveness\n- Opportunity to combine multiple approaches (e.g., convolutions + selective attention)\n- Memory efficiency is as important as computational efficiency\n\nc) Architectural Innovation:\n- Potential for hybrid architectures combining different processing mechanisms\n- Importance of maintaining stability while improving efficiency\n- Need for scalable solutions that work well at different model sizes\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in efficient attention mechanisms beyond standard transformers\n2. Novel approaches to sequence compression and information condensation\n3. Hybrid architectures combining multiple processing mechanisms\n4. Memory-efficient normalization techniques\n\n5. References\n\n[1] Poli, M., Massaroli, S., Nguyen, E. Q., Fu, D. Y., Dao, T., Baccus, S., ... & R\u00e9, C. (2023). Hyena Hierarchy: Towards Larger Convolutional Language Models. International Conference on Machine Learning.\n\n[2] Richter, O., & Wattenhofer, R. (2020). Normalized Attention Without Probability Cage. arXiv preprint arXiv:2005.09561.\n\n[3] Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., & Dou, Z. (2024). Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon. arXiv preprint arXiv:2401.03462.\n\n[4] Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., ... & Neyshabur, B. (2022). Exploring Length Generalization in Large Language Models. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "selective state space models content-aware mechanisms",
            "detail": "Find papers discussing:\n1. Content-aware state space models\n2. Selective mechanisms in sequence modeling\n3. Methods for making SSMs adaptive to input content",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Content-aware state space models\n2. Selective mechanisms in sequence modeling\n3. Methods for making SSMs adaptive to input content\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 1.00)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n##### *Relevant Chunk: No. 25/74 (Score: 1.00)*\n\n```\nThis validates our core motivation in Sections 3.1 and 3.3. ## 5 Discussion\n\nWe discuss related work, limitations, and some future directions. Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models. Table 9: (Ablations: Expressivity of $\\Delta$. ) The selection mechanism of $\\Delta$ constructs it with a projection of the input. Projecting it even to dim. 1 provides a large increase in performance; increasing it further provides further improvements at the cost of a modest increase in parameters. State size fixed to $N=16$. | Size of $\\Delta$ ProJ. | Params (M) | Perplexity |\n| :--- | :--- | :--- |\n| - | 358.9 | 9.12 |\n| 1 | 359.1 | 8.97 |\n| 2 | 359.3 | 8.97 |\n| 4 | 359.7 | 8.91 |\n| 8 | 360.5 | 8.83 |\n| 16 | 362.1 | 8.84 |\n| 32 | 365.2 | 8.80 |\n| 64 | 371.5 | 8.71 |\n\nTable 10: (Ablations: SSM state dimension.) (Top) Constant $\\boldsymbol{B}$ and $\\boldsymbol{C}$ (Bottom) Selective $B$ and $C$. Increasing the SSM state dimension $N$, which can be viewed as an expansion factor on the dimension of the recurrent state, can significantly improve performance for a negligible cost in parameters/FLOPs, but only when $\\boldsymbol{B}$ and $\\boldsymbol{C}$ are also selective. Size of $\\Delta$ projection fixed to 64 . No Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally defined as discretizations of continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeoff in more detail. Downstream Affordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as fine-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and affordances. Scaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper. ## 6 Conclusion\n\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for different domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model backbone. ## Acknowledgments\n\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft. ## References\n\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. \"Unitary Evolution Recurrent Neural Networks\". In: The International Conference on Machine Learning (ICML). 2016, pp. 1120-1128. [2] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \"Effective Gene Expression Prediction from Sequence by Integrating Long-range Interactions\".\n```\n\n##### *Relevant Chunk: No. 10/74 (Score: 0.99)*\n\n```\nSection 4.3.2). Boundary Resetting. In settings where multiple independent sequences are stitched together, Transformers can keep them separate by instantiating a particular attention mask, while LTI models will bleed information between the sequences. Selective SSMs can also reset their state at boundaries (e.g. $\\Delta_{t} \\rightarrow \\infty$, or Theorem 1 when $g_{t} \\rightarrow 1$ ). These settings may occur artificially (e.g. packing documents together to improve hardware utilization) or naturally (e.g. episode boundaries in reinforcement learning (Lu et al. 2023)). Additionally, we elaborate on effects of each selective parameter. Interpretation of $\\Delta$. In general, $\\Delta$ controls the balance between how much to focus or ignore the current input $x_{t}$. It generalizes RNN gates (e.g. $g_{t}$ in Theorem 1): mechanically, a large $\\Delta$ resets the state $h$ and focuses on the current input $x$, while a small $\\Delta$ persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as a continuous system discretized by a timestep $\\Delta$, and in this context the intuition is that large $\\Delta \\rightarrow \\infty$ represents the system focusing on the current input for longer (thus \"selecting\" it and forgetting its current state) while a small $\\Delta \\rightarrow 0$ represents a transient input that is ignored. Interpretation of $A$. We remark that while the $A$ parameter could also be selective, it ultimately affects the model only through its interaction with $\\Delta$ via $\\bar{A}=\\exp (\\Delta A)$ (the discretization (4)). Thus selectivity in $\\Delta$ is enough to ensure selectivity in $(\\bar{A}, \\bar{B})$, and is the main source of improvement. We hypothesize that making $A$ selective in addition to (or instead of) $\\Delta$ would have similar performance, and leave it out for simplicity. Interpretation of $B$ and $C$. As discussed in Section 3.1, the most important property of selectivity is filtering out irrelevant information so that a sequence model's context can be compressed into an efficient state. In an SSM, modifying $B$ and $C$ to be selective allows finer-grained control over whether to let an input $x_{t}$ into the state $h_{t}$, or the state into the output $y_{t}$. These can be interpreted as allowing the model to modulate the recurrent dynamics based on content (input) and context (hidden states) respectively. ### 3.6 Additional Model Details\n\nReal vs. Complex. Most prior SSMs use complex numbers in their state $h$, which is necessary for strong performance on many tasks in perceptual modalities (Gu, Goel, and R\u00e9 2022). However, it has been empirically observed that completely real-valued SSMs seem to work fine, and possibly even better, in some settings (Ma et al. 2023). We use real values as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeoff is related to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous modalities (e.g.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective state space models content-aware mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space\n\n*From Search Query: selective state space models content-aware mechanisms*\n\n*Katja Schwarz, Seung Wook Kim, Jun Gao, Sanja Fidler, Andreas Geiger, Karsten Kreis*\n\n**TL;DR:** This work first train an autoencoder that infers a compressed latent representation that additionally captures the images' underlying 3D structure and enables not only reconstruction but also novel view synthesis, enabling synthesis of high-quality 3D-consistent image samples, outperforming recent state-of-the-art GAN-based methods.\n\n**Abstract:** Modern learning-based approaches to 3D-aware image synthesis achieve high photorealism and 3D-consistent viewpoint changes for the generated images. Existing approaches represent instances in a shared canonical space. However, for in-the-wild datasets a shared canonical system can be difficult to define or might not even exist. In this work, we instead model instances in view space, alleviating the need for posed images and learned camera distributions. We find that in this setting, existing GAN-based methods are prone to generating flat geometry and struggle with distribution coverage. We hence propose WildFusion, a new approach to 3D-aware image synthesis based on latent diffusion models (LDMs). We first train an autoencoder that infers a compressed latent representation, which additionally captures the images' underlying 3D structure and enables not only reconstruction but also novel view synthesis. To learn a faithful 3D representation, we leverage cues from monocular depth prediction. Then, we train a diffusion model in the 3D-aware latent space, thereby enabling synthesis of high-quality 3D-consistent image samples, outperforming recent state-of-the-art GAN-based methods. Importantly, our 3D-aware LDM is trained without any direct supervision from multiview images or 3D geometry and does not require posed images or learned pose or camera distributions. It directly learns a 3D representation without relying on canonical camera coordinates. This opens up promising research avenues for scalable 3D-aware image synthesis and 3D content creation from in-the-wild image data. See https://katjaschwarz.github.io/wildfusion for videos of our 3D results.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 2. VoxGRAF: Fast 3D-Aware Image Synthesis with Sparse Voxel Grids\n\n*From Search Query: selective state space models content-aware mechanisms*\n\n*Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao, Andreas Geiger*\n\n**TL;DR:** The results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization and allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.\n\n**Abstract:** State-of-the-art 3D-aware generative models rely on coordinate-based MLPs to parameterize 3D radiance fields. While demonstrating impressive results, querying an MLP for every sample along each ray leads to slow rendering. Therefore, existing approaches often render low-resolution feature maps and process them with an upsampling network to obtain the final image. Albeit efficient, neural rendering often entangles viewpoint and content such that changing the camera pose results in unwanted changes of geometry or appearance. Motivated by recent results in voxel-based novel view synthesis, we investigate the utility of sparse voxel grid representations for fast and 3D-consistent generative modeling in this paper. Our results demonstrate that monolithic MLPs can indeed be replaced by 3D convolutions when combining sparse voxel grids with progressive growing, free space pruning and appropriate regularization. To obtain a compact representation of the scene and allow for scaling to higher voxel resolutions, our model disentangles the foreground object (modeled in 3D) from the background (modeled in 2D). In contrast to existing approaches, our method requires only a single forward pass to generate a full 3D scene. It hence allows for efficient rendering from arbitrary viewpoints while yielding 3D consistent results with high visual fidelity.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 132  (*Influential: 12*)\n\n#### 3. KagNet: Knowledge-Aware Graph Networks for Commonsense Reasoning\n\n*From Search Query: selective state space models content-aware mechanisms*\n\n*Bill Yuchen Lin, Xinyue Chen, Jamin Chen, Xiang Ren*\n\n**TL;DR:** This paper proposes a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences.\n\n**Abstract:** Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 434  (*Influential: 71*)\n\n#### 4. Structure-Aware Transformer Policy for Inhomogeneous Multi-Task Reinforcement Learning\n\n*From Search Query: selective state space models content-aware mechanisms*\n\n*Sunghoon Hong, Deunsol Yoon, Kee-Eung Kim*\n\n**TL;DR:** It is empirically show that the morphological information is crucial for modular reinforcement learning, substantially outperforming prior state-of-the-art methods on multi-task learning as well as transfer learning settings with different state and action space dimensions.\n\n**Abstract:** Modular Reinforcement Learning, where the agent is assumed to be morphologically structured as a graph, for example composed of limbs and joints, aims to learn a policy that is transferable to a structurally similar but different agent. Compared to traditional Multi-Task Reinforcement Learning, this promising approach allows us to cope with inhomogeneous tasks where the state and action space dimensions differ across tasks. Graph Neural Networks are a natural model for representing the pertinent policies, but a recent work has shown that their multihop message passing mechanism is not ideal for conveying important information to other modules and thus a transformer model without morphological information was proposed. In this work, we argue that the morphological information is still very useful and propose a transformer policy model that effectively encodes such information. Specifically, we encode the morphological information in terms of the traversal-based positional embedding and the graph-based relational embedding. We empirically show that the morphological information is crucial for modular reinforcement learning, substantially outperforming prior state-of-the-art methods on multi-task learning as well as transfer learning settings with different state and action space dimensions.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 19  (*Influential: 6*)\n\n#### 5. Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs\n\n*From Search Query: selective state space models content-aware mechanisms*\n\n*Chengjin Xu, Fenglong Su, Jens Lehmann*\n\n**TL;DR:** This paper proposes a novel Time-aware Entity Alignment approach based on Graph Neural Networks (TEA-GNN), which significantly outperforms the state-of-the-art methods due to the inclusion of time information.\n\n**Abstract:** Entity alignment aims to identify equivalent entity pairs between different knowledge graphs (KGs). Recently, the availability of temporal KGs (TKGs) that contain time information created the need for reasoning over time in such TKGs. Existing embedding-based entity alignment approaches disregard time information that commonly exists in many large-scale KGs, leaving much room for improvement. In this paper, we focus on the task of aligning entity pairs between TKGs and propose a novel Time-aware Entity Alignment approach based on Graph Neural Networks (TEA-GNN). We embed entities, relations and timestamps of different KGs into a vector space and use GNNs to learn entity representations. To incorporate both relation and time information into the GNN structure of our model, we use a self-attention mechanism which assigns different weights to different nodes with orthogonal transformation matrices computed from embeddings of the relevant relations and timestamps in a neighborhood. Experimental results on multiple real-world TKG datasets show that our method significantly outperforms the state-of-the-art methods due to the inclusion of time information.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 43  (*Influential: 7*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using selective state space models and content-aware mechanisms, here are some key papers and concepts that align with your requirements:\n\n## Content-Aware State Space Models\n\n1. **S7: Selective and Simplified State Space Layers for Sequence Modeling**\n   This paper introduces S7, a state space model that incorporates input-dependent dynamics, allowing it to adapt state transitions based on the input content. S7 combines the efficiency of recurrent architectures with the adaptive processing capabilities of more complex models, enabling content-based reasoning and adaptive filtering.\n\n## Selective Mechanisms in Sequence Modeling\n\n1. **Mamba and Spatial-Mamba**\n   The Mamba model and its extension, Spatial-Mamba, utilize selective state space mechanisms to capture long-range dependencies efficiently. Mamba makes the SSM parameters functions of the input, addressing the weakness of traditional SSMs in content-based reasoning. Spatial-Mamba further enhances this by introducing structure-aware state fusion to capture spatial dependencies in visual tasks.\n\n## Methods for Making SSMs Adaptive to Input Content\n\n1. **Dynamic State Transitions in S7**\n   S7 introduces dynamic state transitions that depend on the input content, allowing the model to selectively update internal states based on the relevance of the input. This approach improves the model's ability to handle diverse temporal dependencies and filter out irrelevant information.\n\n2. **Input-Dependent Filtering in Mamba and Spatial-Mamba**\n   Both Mamba and Spatial-Mamba incorporate input-dependent filtering mechanisms. Mamba lets the SSM parameters be functions of the input, and Spatial-Mamba uses structure-aware state fusion to adapt to the input content, particularly in visual tasks.\n\n## Hybrid Architectures Combining SSMs and Attention\n\n1. **Efficient Long Sequence Modeling via State Space Augmented Transformer**\n   This work combines state space models with transformer architectures to achieve efficient long sequence modeling. The integration of SSMs with attention mechanisms helps in reducing computational complexity while maintaining model expressiveness (referenced in the analysis note).\n\n## Key Insights\n\n- **Linear Complexity and Efficiency**: SSMs offer linear complexity in sequence length, making them more efficient than traditional attention mechanisms, especially for long sequences.\n- **Content-Aware Processing**: Making SSMs content-aware through selective mechanisms significantly improves their performance in handling diverse input content.\n- **Hybrid Architectures**: Combining SSMs with attention mechanisms or other hierarchical structures can provide a balance between efficiency and model capability (referenced in the analysis note).\n\nThese papers and concepts provide a solid foundation for designing autoregressive language models that are efficient, adaptive to input content, and scalable.\n",
            "analysis": "Analysis Note: Research on Efficient Attention and State Space Models\n\n1. Summary of Analysis\n\nThe search results reveal several key trends and opportunities in LM block design:\n\na) Evolution of Attention Mechanisms:\n- Traditional attention faces quadratic complexity challenges\n- Linear and subquadratic alternatives are emerging\n- Hybrid approaches combining attention with other mechanisms show promise\n\nb) State Space Models (SSMs):\n- Emerging as powerful alternatives to pure attention\n- Offer linear complexity while maintaining model expressiveness\n- Can be enhanced with selective mechanisms for better performance\n\nc) Hybrid Architectures:\n- Combining SSMs with attention mechanisms shows strong results\n- Hardware-efficient implementations are crucial\n- Need for balancing efficiency with model capability\n\n2. Useful References with Excerpts\n\nFrom \"Transformers are SSMs\":\n\"While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale.\"\n\nKey insight: SSMs can match or exceed Transformer performance while being more efficient.\n\nFrom \"CHELA\":\n\"To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern\"\n\nKey insight: Combining linear attention with SSMs can provide both efficiency and effectiveness.\n\nFrom \"Mamba\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n\nKey insight: Making SSMs content-aware through selective mechanisms improves their performance.\n\n3. Key Insights and Analysis\n\na) SSM Advantages:\n- Linear complexity in sequence length\n- Efficient hardware implementation possible\n- Can be made content-aware through selective mechanisms\n\nb) Attention Evolution:\n- Linear attention variants offer efficiency gains\n- Need for maintaining expressiveness while reducing complexity\n- Hardware-aware implementations crucial for real performance\n\nc) Integration Opportunities:\n- Combining SSMs with selective attention mechanisms\n- Using hierarchical structures for efficiency\n- Hardware-aware algorithm design\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for making SSMs content-aware\n2. Hardware-efficient implementations of hybrid architectures\n3. Methods for combining hierarchical processing with SSMs\n\n5. References\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. International Conference on Machine Learning.\n\n[2] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[4] Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint.\n\n[5] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hierarchical attention selective state space models",
            "detail": "Find papers discussing:\n1. Hierarchical attention mechanisms and their integration with state space models\n2. Multi-scale processing in language models\n3. Efficient parameter sharing strategies for hybrid architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical attention mechanisms and their integration with state space models\n2. Multi-scale processing in language models\n3. Efficient parameter sharing strategies for hybrid architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.95)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.91)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.91)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.67)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 7/30 (Score: 0.67)*\n\n```\nV. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 77-87, 2017. [6] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [8] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [9] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [11] Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052, 2022. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding.\n```\n\n#### 4. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.24)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.24)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n#### 5. Retentive network: a successor to transformer for large language models (Avg. Score: 0.07)\n\n*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 143  (*Influential: 18*)\n\n**TL;DR:** This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n##### *Relevant Chunk: No. 12/21 (Score: 0.07)*\n\n```\narXiv preprint arXiv:2101.00027, 2020. [GGR21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. $\\left[\\mathrm{HCP}^{+}\\right.$21] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021. $\\left[\\mathrm{HDW}^{+} 23\\right]$ Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023. [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv: Learning, 2016. [HS97] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735-1780, November 1997. [HSD ${ }^{+}$22a] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv, abs/2206.06336, 2022. $\\left[\\mathrm{HSD}^{+}\\right.$22b] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical attention selective state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 3. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 4. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Paul Mattes, Rainer Schlosser, R. Herbrich*\n\n**TL;DR:** Hieros is a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space that allows for more efficient training than RNN- based world models and more efficient imagination than Transformer-based world models.\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 273  (*Influential: 21*)\n\n### 3 related papers from Papers with Code\n\n#### 1. A variational selection mechanism for article comment generation\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Jiayong Liu, Jinqiao Dai, Pengsen Cheng, Jiamiao Liu*\n\n**Abstract:** Article comment generation is a new and challenging task in natural language processing, which has recently received much attention from researchers. In the article comment generation, there are apparent distinctions and different perspectives on the comments of a single article. However, current researches ignore the one-to-many relationship between articles and comments, resulting in a lack of diversity and coherence in generated comments. To solve this problem, a variational selection mechanism model (VSMM) is proposed in our research. In this model, we construct a Gaussian mixture prior network to capture a richer latent space and generate comments with more diversity and informativeness. At the same time, VSMM maps latent variables into different semantic spaces through the selection mechanism to capture one-to-many relationships. Then we introduce a discriminator to distinguish whether the selected content is consistent with the reference comment content, thus improving the coherence of the generated comments. In addition, a hierarchical encoder with attention is introduced in the VSMM model, which can effectively solve the problem of long document encoding. Furthermore, we propose a multi-category article comment dataset to align closely with practical applications. Experiments on three datasets demonstrate that VSMM outperforms existing state-of-the-art comment generation methods in terms of diversity in single and multiple comment generations. Moreover, VSMM can generate fluent, diverse, and coherent comments on multi-category and topic-rich datasets.\n\n**Proceeding:** expert-systems-with-applications-2023-9\n\n**Published:** 2023-09-12\n\n\n\n#### 2. Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Frank Guerin, Chenghua Lin, Tyler Loakman, Hongbo Zhang, Chen Tang*\n\n**Abstract:** Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at https://github.com/tangg555/SaBART.\n\n**Published:** 2023-06-28\n\n\n\n#### 3. Mamba-Spike: Enhancing the Mamba Architecture with a Spiking Front-End for Efficient Temporal Data Processing\n\n*From Search Query: hierarchical attention selective state space models*\n\n*Feng Liu, Jiahao Qin*\n\n**Abstract:** The field of neuromorphic computing has gained significant attention in recent years, aiming to bridge the gap between the efficiency of biological neural networks and the performance of artificial intelligence systems. This paper introduces Mamba-Spike, a novel neuromorphic architecture that integrates a spiking front-end with the Mamba backbone to achieve efficient and robust temporal data processing. The proposed approach leverages the event-driven nature of spiking neural networks (SNNs) to capture and process asynchronous, time-varying inputs, while harnessing the power of the Mamba backbone's selective state spaces and linear-time sequence modeling capabilities to model complex temporal dependencies effectively. The spiking front-end of Mamba-Spike employs biologically inspired neuron models, along with adaptive threshold and synaptic dynamics. These components enable efficient spatiotemporal feature extraction and encoding of the input data. The Mamba backbone, on the other hand, utilizes a hierarchical structure with gated recurrent units and attention mechanisms to capture long-term dependencies and selectively process relevant information. To evaluate the efficacy of the proposed architecture, a comprehensive empirical study is conducted on both neuromorphic datasets, including DVS Gesture and TIDIGITS, and standard datasets, such as Sequential MNIST and CIFAR10-DVS. The results demonstrate that Mamba-Spike consistently outperforms state-of-the-art baselines, achieving higher accuracy, lower latency, and improved energy efficiency. Moreover, the model exhibits robustness to various input perturbations and noise levels, highlighting its potential for real-world applications. The code will be available at https://github.com/ECNU-Cross-Innovation-Lab/Mamba-Spike.\n\n**Published:** 2024-08-04\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using hierarchical attention mechanisms, selective state space models, and efficient parameter sharing strategies, here are some key findings and references that align with the researcher's goals:\n\n## Hierarchical Attention Mechanisms and Integration with State Space Models\n\n- The paper on Taipan architecture discusses the integration of selective attention with state space models (SSMs). Taipan combines the efficiency of Mamba (a state space model) with strategically placed selective attention layers. This approach allows for the efficient processing of long sequences while capturing important long-range dependencies, which is crucial for hierarchical attention mechanisms.\n\n- The concept of State Space Duality (SSD) in Mamba2, which simplifies the matrix A into a scalar, can be applied to both linear and quadratic forms. This simplification can be integrated with hierarchical attention to enhance the model's ability to handle complex dependencies efficiently.\n\n## Multi-Scale Processing in Language Models\n\n- The Mamba architecture and its variants, such as SC-MAMBA2, demonstrate the capability for multi-scale processing. SC-MAMBA2 integrates state-space models with the MAMBA framework, enabling efficient and scalable modeling of large sequences. This approach can be adapted to language models to handle sequences at multiple scales, from short to very long contexts.\n\n- The Taipan architecture also supports multi-scale processing by using a combination of selective attention and sliding window attention. This allows the model to capture both local and long-range dependencies efficiently, which is essential for handling sequences of varying lengths.\n\n## Efficient Parameter Sharing Strategies for Hybrid Architectures\n\n- The DenseMamba model, mentioned in the analysis, highlights the use of dense connections between layers to enhance information flow without sacrificing efficiency. This approach can be applied to hybrid architectures combining SSMs and hierarchical attention, ensuring that fine-grained information is retained while maintaining training parallelizability and inference efficiency[Analysis Note].\n\n- The ACMamba paper discusses an end-to-end lightweight network based on selective state space models for image restoration. While focused on image restoration, the concept of using input-independent parameters (B, C, and \u2206) in SSMs can be adapted to language models to improve parameter sharing efficiency in hybrid architectures.\n\n- The Heracles model, which combines a global state space model with a local convolutional network and attention-based token interaction, provides insights into stabilizing scaling and improving local detail capture. This hybrid approach can be applied to language models to balance computational efficiency and model capability, ensuring efficient parameter sharing and hardware-aware implementations.\n\nIn summary, the integration of selective state space models with hierarchical attention mechanisms, as seen in Taipan and SC-MAMBA2, offers a promising direction for improving autoregressive language models. The use of dense connections and efficient parameter sharing strategies, as discussed in DenseMamba and ACMamba, can further enhance the performance and efficiency of these hybrid architectures.\n",
            "analysis": "Analysis Note: Research on Selective State Space Models and Hybrid Architectures\n\n1. Summary of Analysis\n\nThe search results reveal promising directions for enhancing the seed design through:\n\na) Selective State Space Models (SSMs):\n- Enable content-based reasoning while maintaining linear complexity\n- Allow adaptive processing based on input content\n- Provide efficient mechanisms for handling long sequences\n\nb) Hybrid Architectures:\n- Potential to combine benefits of SSMs with hierarchical attention\n- Balance between computational efficiency and model capability\n- Opportunities for hardware-aware implementations\n\n2. Useful References with Excerpts\n\nFrom Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\nKey insight: Making SSM parameters input-dependent enables content-aware processing.\n\n\"Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.\"\n\nKey insight: Linear scaling with sequence length while maintaining performance.\n\nFrom DenseMamba:\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nKey insight: Dense connections between layers can enhance information flow without sacrificing efficiency.\n\n3. Key Insights and Analysis\n\na) Selective Mechanisms:\n- Input-dependent parameter generation enables content-aware processing\n- Selective state updates allow efficient information filtering\n- Balance between persistence and forgetting of information\n\nb) Architectural Considerations:\n- Dense connections can enhance information flow\n- Hardware-aware implementation crucial for real performance gains\n- Need for efficient parameter sharing strategies\n\nc) Integration Opportunities:\n- Potential to combine selective SSMs with hierarchical attention\n- Opportunity for multi-scale processing through layer hierarchy\n- Possibility of adaptive computation paths\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for combining hierarchical attention with SSMs\n2. Methods for efficient parameter sharing in hybrid architectures\n3. Hardware-efficient implementations of selective mechanisms\n\n5. References\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n[2] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Dao, T., Fu, D. Y., Saab, K. K., Thomas, A., Rudra, A., & R\u00e9, C. (2023). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "efficient parameter sharing hierarchical models",
            "detail": "Find papers discussing:\n1. Efficient parameter sharing techniques in deep learning models\n2. Methods for reducing parameter redundancy in hierarchical architectures\n3. Hardware-efficient implementations of parameter sharing",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Efficient parameter sharing techniques in deep learning models\n2. Methods for reducing parameter redundancy in hierarchical architectures\n3. Hardware-efficient implementations of parameter sharing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.92)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.92)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n#### 2. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.38)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 24/28 (Score: 0.38)*\n\n```\narXiv preprint arXiv:1911.05507, 2019. [31] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Selim Sandal and Ismail Akturk. Zero-shot rtl code generation with attention sink augmented large language models.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.36)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.36)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation (Avg. Score: 0.25)\n\n*Ta-Chung Chi, Ting-Han Fan, P. Ramadge, Alexander I. Rudnicky*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 39  (*Influential: 7*)\n\n**TL;DR:** KERPLE is proposed, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences using conditionally positive definite (CPD) kernels, and it is shown that a CPD kernel can be transformed into a PD kernel by adding a constant offset.\n\n**Abstract:** Relative positional embeddings (RPE) have received considerable attention since RPEs effectively model the relative distance among tokens and enable length extrapolation. We propose KERPLE, a framework that generalizes relative position embedding for extrapolation by kernelizing positional differences. We achieve this goal using conditionally positive definite (CPD) kernels, a class of functions known for generalizing distance metrics. To maintain the inner product interpretation of self-attention, we show that a CPD kernel can be transformed into a PD kernel by adding a constant offset. This offset is implicitly absorbed in the Softmax normalization during self-attention. The diversity of CPD kernels allows us to derive various RPEs that enable length extrapolation in a principled way. Experiments demonstrate that the logarithmic variant achieves excellent extrapolation performance on three large language modeling datasets. Our implementation and pretrained checkpoints are released at~\\url{https://github.com/chijames/KERPLE.git}.\n\n##### *Relevant Chunk: No. 26/37 (Score: 0.25)*\n\n```\narXiv preprint arXiv:2101.00027, 2020. Sid Black, Stella Biderman, Alex Andonian, Quentin Anthony, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Jason Phang, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in pytorch, 2021. URL http://github.com/eleutherai/ gpt-neox\n\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages $3505-3506,2020$. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training.\n```\n\n#### 5. Linear Attention Sequence Parallelism (Avg. Score: 0.19)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.19)*\n\n```\nCoRR, abs/2112.05682, 2021. URL https://arxiv.org/abs/2112.05682. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models, 2020. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Sun, W., Qin, Z., Sun, W., Li, S., Li, D., Shen, X., Qiao, Y., and Zhong, Y. CO2: Efficient distributed training with full communication-computation overlap.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient parameter sharing hierarchical models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Efficient Hierarchical Domain Adaptation for Pretrained Language Models\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Alexandra Chronopoulou, Matthew E. Peters, Jesse Dodge*\n\n**TL;DR:** This paper introduces a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach based on the observation that textual domains are partially overlapping, and represents domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights.\n\n**Abstract:** The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 38  (*Influential: 2*)\n\n#### 2. DiffEnc: Variational Diffusion with a Learned Encoder\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Beatrix M. G. Nielsen, Anders Christensen, Andrea Dittadi, Ole Winther*\n\n**TL;DR:** This work introduces a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss, and proposes a proposed framework, DiffEnc, which achieves a statistically significant improvement in likelihood on CIFAR-10.\n\n**Abstract:** Diffusion models may be viewed as hierarchical variational autoencoders (VAEs) with two improvements: parameter sharing for the conditional distributions in the generative process and efficient computation of the loss as independent terms over the hierarchy. We consider two changes to the diffusion model that retain these advantages while adding flexibility to the model. Firstly, we introduce a data- and depth-dependent mean function in the diffusion process, which leads to a modified diffusion loss. Our proposed framework, DiffEnc, achieves a statistically significant improvement in likelihood on CIFAR-10. Secondly, we let the ratio of the noise variance of the reverse encoder process and the generative process be a free weight parameter rather than being fixed to 1. This leads to theoretical insights: For a finite depth hierarchy, the evidence lower bound (ELBO) can be used as an objective for a weighted diffusion loss approach and for optimizing the noise schedule specifically for inference. For the infinite-depth hierarchy, on the other hand, the weight parameter has to be 1 to have a well-defined ELBO.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 3. Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Peiyu Liu, Ze-Feng Gao, Yushuo Chen, Xin Zhao, Ji-Rong Wen*\n\n**Abstract:** ,\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. Efficient Neural Architecture Search via Parameter Sharing\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Hieu Pham, M. Guan, Barret Zoph, Quoc V. Le, J. Dean*\n\n**TL;DR:** Efficient Neural Architecture Search is a fast and inexpensive approach for automatic model design that establishes a new state-of-the-art among all methods without post-training processing and delivers strong empirical performances using much fewer GPU-hours.\n\n**Abstract:** We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. In ENAS, a controller learns to discover neural network architectures by searching for an optimal subgraph within a large computational graph. The controller is trained with policy gradient to select a subgraph that maximizes the expected reward on the validation set. Meanwhile the model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Thanks to parameter sharing between child models, ENAS is fast: it delivers strong empirical performances using much fewer GPU-hours than all existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On the Penn Treebank dataset, ENAS discovers a novel architecture that achieves a test perplexity of 55.8, establishing a new state-of-the-art among all methods without post-training processing. On the CIFAR-10 dataset, ENAS designs novel architectures that achieve a test error of 2.89%, which is on par with NASNet (Zoph et al., 2018), whose test error is 2.65%.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 2623  (*Influential: 483*)\n\n#### 5. UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Haoyu Lu, Mingyu Ding, Yuqi Huo, Guoxing Yang, Zhiwu Lu, M. Tomizuka, Wei Zhan*\n\n**TL;DR:** UniAdapter is proposed, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models and shows that in most cases, UniAdapter not only outperforms the state-of-the-arts, but even beats the full fine-tuning strategy.\n\n**Abstract:** Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs. This paper proposes UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models. Specifically, adapters are distributed to different modalities and their interactions, with the total number of tunable parameters reduced by partial weight sharing. The unified and knowledge-sharing design enables powerful cross-modal representations that can benefit various downstream tasks, requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive experiments on 6 cross-modal downstream benchmarks (including video-text retrieval, image-text retrieval, VideoQA, and VQA) show that in most cases, UniAdapter not only outperforms the state-of-the-arts, but even beats the full fine-tuning strategy. Particularly, on the MSRVTT retrieval task, UniAdapter achieves 49.7% recall@1 with 2.2% model parameters, outperforming the latest competitors by 2.0%. The code and models are available at https://github.com/RERV/UniAdapter.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 22  (*Influential: 5*)\n\n### 3 related papers from Papers with Code\n\n#### 1. JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Haoyi Xiong, Dejing Dou, daxiang dong, Zhiwei Zha, Guobao Yang, Wenlin Wang, Guangxing Chen, Hao Xiong, Xiaochao Liao, Jiang Li, Qian Gao, Hao liu*\n\n**Abstract:** In modern internet industries, deep learning based recommender systems have became an indispensable building block for a wide spectrum of applications, such as search engine, news feed, and short video clips. However, it remains challenging to carry the well-trained deep models for online real-time inference serving, with respect to the time-varying web-scale traffics from billions of users, in a cost-effective manner. In this work, we present JIZHI - a Model-as-a-Service system - that per second handles hundreds of millions of online inference requests to huge deep models with more than trillions of sparse parameters, for over twenty real-time recommendation services at Baidu, Inc. In JIZHI, the inference workflow of every recommendation request is transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the pipeline refers to a staged computation or I/O intensive task processor. With traffics of real-time inference requests arrived, each modularized processor can be run in a fully asynchronized way and managed separately. Besides, JIZHI introduces heterogeneous and hierarchical storage to further accelerate the online inference process by reducing unnecessary computations and potential data access latency induced by ultra-sparse model parameters. Moreover, an intelligent resource manager has been deployed to maximize the throughput of JIZHI over the shared infrastructure by searching the optimal resource allocation plan from historical logs and fine-tuning the load shedding policies over intermediate system feedback. Extensive experiments have been done to demonstrate the advantages of JIZHI from the perspectives of end-to-end service latency, system-wide throughput, and resource consumption. JIZHI has helped Baidu saved more than ten million US dollars in hardware and utility costs while handling 200% more traffics without sacrificing inference efficiency.\n\n**Published:** 2021-06-03\n\n\n\n#### 2. Meta-learning Extractors for Music Source Separation\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Jason Naradowsky, Aditya Ganeshan, David Samuel*\n\n**Abstract:** We propose a hierarchical meta-learning-inspired model for music source separation (Meta-TasNet) in which a generator model is used to predict the weights of individual extractor models. This enables efficient parameter-sharing, while still allowing for instrument-specific parameterization. Meta-TasNet is shown to be more effective than the models trained independently or in a multi-task setting, and achieve performance comparable with state-of-the-art methods. In comparison to the latter, our extractors contain fewer parameters and have faster run-time performance. We discuss important architectural considerations, and explore the costs and benefits of this approach.\n\n**Published:** 2020-02-17\n\n\n\n#### 3. Multilingual Hierarchical Attention Networks for Document Classification\n\n*From Search Query: efficient parameter sharing hierarchical models*\n\n*Andrei Popescu-Belis, Nikolaos Pappas*\n\n**Abstract:** Hierarchical attention networks have recently achieved remarkable performance\nfor document classification in a given language. However, when multilingual\ndocument collections are considered, training such models separately for each\nlanguage entails linear parameter growth and lack of cross-language transfer.\nLearning a single multilingual model with fewer parameters is therefore a\nchallenging but potentially beneficial objective. To this end, we propose\nmultilingual hierarchical attention networks for learning document structures,\nwith shared encoders and/or shared attention mechanisms across languages, using\nmulti-task learning and an aligned semantic space as input. We evaluate the\nproposed models on multilingual document classification with disjoint label\nsets, on a large dataset which we provide, with 600k news documents in 8\nlanguages, and 5k labels. The multilingual models outperform monolingual ones\nin low-resource as well as full-resource settings, and use fewer parameters,\nthus confirming their computational efficiency and the utility of\ncross-language transfer.\n\n**Conference:** multilingual-hierarchical-attention-networks-1\n\n**Published:** 2017-07-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on efficient parameter sharing, reducing parameter redundancy in hierarchical architectures, and hardware-efficient implementations, here are some key findings and techniques from the provided sources:\n\n## Efficient Parameter Sharing Techniques\n\n- The concept of **Relaxed Recursive Transformers** introduced in the papers is particularly relevant. This approach involves sharing parameters across layers while using **Low-Rank Adaptation (LoRA)** to add flexibility. LoRA allows for small, targeted adjustments to the shared parameters, enabling the model to adapt to different types of data without significantly increasing the number of parameters. This technique reduces the model size and deployment costs while maintaining performance.\n\n## Methods for Reducing Parameter Redundancy in Hierarchical Architectures\n\n- **Recursive Transformers** leverage parameter sharing by reusing the same set of weights across different layers. This approach drastically reduces the number of parameters needed, making the model more efficient and compact. The **Relaxed Recursive Transformers** further enhance this by introducing LoRA, which balances the benefits of parameter sharing with the need for layer-specific adaptation.\n\n- In the context of **Multi-Task Learning (MTL)**, adaptive parameter sharing strategies can be applied to hierarchical architectures. Techniques such as hard and soft parameter sharing, and learning which layers to share based on task transferability, can be effective in reducing redundancy and improving generalizability.\n\n## Hardware-Efficient Implementations of Parameter Sharing\n\n- The **Relaxed Recursive Transformers** not only reduce the number of parameters but also enable more efficient inference through **Continuous Depth-wise Batching**. This new inference paradigm, when paired with early exiting, can lead to significant gains in inference throughput (up to 2-3 times).\n\n- The use of **LoRA** in these models also contributes to hardware efficiency by minimizing the additional parameters needed for adaptation. This approach ensures that the model remains compact and efficient in terms of memory usage and computational requirements.\n\n## Additional Insights\n\n- **Hierarchical State Space Models (HiSS)** and **DenseMamba** models, mentioned in the analysis, highlight the importance of hierarchical structures and dense connections for efficient sequence processing. Integrating these concepts with the parameter sharing techniques of Recursive Transformers could further enhance model efficiency and capability[Analysis Note].\n\nBy combining these techniques, researchers can develop autoregressive language models that are not only more efficient in terms of parameter usage but also more capable and scalable, aligning with the goals of reducing parameter redundancy, improving hardware efficiency, and enhancing overall model performance.\n",
            "analysis": "Analysis Note: Research on Hybrid Architectures and Efficient Integration\n\n1. Summary of Analysis\n\nThe search results reveal promising directions for enhancing the seed design through:\n\na) Integration of SSMs with Hierarchical Attention:\n- SSMs offer linear complexity and efficient sequence processing\n- Hierarchical structures can improve both efficiency and model capability\n- Selective mechanisms enable content-aware processing\n\nb) Multi-Scale Processing:\n- Hierarchical architectures enable processing at different scales\n- Dense connections can enhance information flow\n- Need for efficient parameter sharing across scales\n\nc) Hardware-Efficient Implementation:\n- Importance of IO-aware computation\n- Need for efficient parameter sharing strategies\n- Balance between model capability and computational efficiency\n\n2. Useful References with Excerpts\n\nFrom \"Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\":\n\"HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. [...] outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE.\"\n\nKey insight: Hierarchical stacking of SSMs can significantly improve performance.\n\nFrom \"Hungry Hungry Hippos\":\n\"We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. [...] H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers.\"\n\nKey insight: SSMs need enhancement for token comparison and recall capabilities.\n\nFrom \"DenseMamba\":\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nKey insight: Dense connections can improve information flow without sacrificing efficiency.\n\n3. Key Insights and Analysis\n\na) Architectural Integration:\n- Hierarchical SSMs show promise for efficient sequence processing\n- Dense connections can enhance information flow across layers\n- Need for selective mechanisms to handle different types of dependencies\n\nb) Efficiency Considerations:\n- IO-aware computation crucial for real performance gains\n- Parameter sharing can reduce memory requirements\n- Need for efficient implementation of hierarchical structures\n\nc) Model Capabilities:\n- SSMs need enhancement for token comparison\n- Hierarchical structures can improve long-range dependencies\n- Dense connections help maintain fine-grained information\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient parameter sharing in hierarchical architectures\n2. Methods for combining attention with SSMs at different scales\n3. Hardware-efficient implementations of hierarchical models\n\n5. References\n\n[1] Bhirangi, R. M., Wang, C., Pattabiraman, V., Majidi, C., Gupta, A., Hellebrekers, T., & Pinto, L. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. International Conference on Machine Learning.\n\n[2] Dao, T., Fu, D. Y., Saab, K. K., Thomas, A., Rudra, A., & R\u00e9, C. (2022). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. International Conference on Learning Representations.\n\n[3] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[4] Lu, P., Wang, S., Rezagholizadeh, M., Liu, B., & Kobyzev, I. (2023). Efficient Classification of Long Documents via State-Space Models. Conference on Empirical Methods in Natural Language Processing.\n\n[5] Smith, J. T. H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "hierarchical normalization transformers, memory efficient parameter sharing, adaptive computation language models",
            "detail": "Search for papers discussing:\n1. Hierarchical normalization implementations in transformer architectures:\n   - Layer-wise vs. hierarchical normalization\n   - Impact on gradient flow\n   - Memory efficiency considerations\n2. Parameter sharing strategies in language models:\n   - Tree-structured sharing approaches\n   - Dynamic parameter allocation\n   - Memory-computation trade-offs\n3. Adaptive computation mechanisms:\n   - Dynamic parameter selection\n   - Resource allocation strategies\n   - Training stability considerations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hierarchical normalization implementations in transformer architectures:\n   - Layer-wise vs. hierarchical normalization\n   - Impact on gradient flow\n   - Memory efficiency considerations\n2. Parameter sharing strategies in language models:\n   - Tree-structured sharing approaches\n   - Dynamic parameter allocation\n   - Memory-computation trade-offs\n3. Adaptive computation mechanisms:\n   - Dynamic parameter selection\n   - Resource allocation strategies\n   - Training stability considerations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.48)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.48)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.09)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.09)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 3. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.07)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 11/63 (Score: 0.07)*\n\n```\nE. Hinton. Layer normalization. stat, 1050:21, 2016. [5] D. A. M. Barrington, K. Compton, H. Straubing, and D. Th\u00e9rien. Regular languages in nc1. Journal of Computer and System Sciences, 44(3):478-499, 1992. [6] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096-7116, 2020. [7] D. Blasi, R. Cotterell, L. Wolf-Sonkin, S. Stoll, B. Bickel, and M. Baroni. On the distribution of deep clausal embeddings: A large cross-linguistic study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3938-3943, 2019. [8] J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2016. [9] D. Chiang and P. Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654-7664, 2022. [10] D. Chiang, P. Cholak, and A. Pillay. Tighter bounds on the expressivity of transformer encoders.\n```\n\n#### 4. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.03)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.03)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n#### 5. Staircase Attention for Recurrent Processing of Sequences (Avg. Score: 0.02)\n\n*Da Ju, Stephen Roller, Sainbayar Sukhbaatar, J. Weston*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 10  (*Influential: 2*)\n\n**TL;DR:** Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, and to provide improved modeling power compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.\n\n**Abstract:** Attention mechanisms have become a standard tool for sequence modeling tasks, in particular by stacking self-attention layers over the entire input sequence as in the Transformer architecture. In this work we introduce a novel attention procedure called staircase attention that, unlike self-attention, operates across the sequence (in time) recurrently processing the input by adding another step of processing. A step in the staircase comprises of backward tokens (encoding the sequence so far seen) and forward tokens (ingesting a new part of the sequence), or an extreme Ladder version with a forward step of zero that simply repeats the Transformer on each step of the ladder, sharing the weights. We thus describe a family of such models that can trade off performance and compute, by either increasing the amount of recurrence through time, the amount of sequential processing via recurrence in depth, or both. Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot, due to this recurrence. Further, it is shown to provide improved modeling power for the same size model (number of parameters) compared to self-attentive Transformers on large language modeling and dialogue tasks, yielding significant perplexity gains.\n\n##### *Relevant Chunk: No. 9/21 (Score: 0.02)*\n\n```\narXiv preprint arXiv:1804.09849, 2018. [14] Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. Modeling recurrence for transformer. arXiv preprint arXiv:1904.03092, 2019. [15] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In ACL (1), pages 2978-2988. Association for Computational Linguistics, 2019. [16] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [17] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2019. [18] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [19] Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. [20] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021. [21] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical normalization transformers, memory efficient parameter sharing, adaptive computation language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Grokking of Hierarchical Structure in Vanilla Transformers\n\n*From Search Query: hierarchical normalization transformers*\n\n*Shikhar Murty, Pratyusha Sharma, Jacob Andreas, Christopher D. Manning*\n\n**TL;DR:** This work shows that transformer language models can learn to generalize hierarchically after training for extremely long periods\u2014far beyond the point when in-domain accuracy has saturated.\n\n**Abstract:** For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods\u2014far beyond the point when in-domain accuracy has saturated. We call this phenomenon structural grokking. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of CITATION. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 4*)\n\n#### 2. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical normalization transformers*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 38  (*Influential: 3*)\n\n#### 3. CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers\n\n*From Search Query: hierarchical normalization transformers*\n\n*Ming Ding, Wendi Zheng, Wenyi Hong, Jie Tang*\n\n**TL;DR:** This work pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution in the new text-to-image system, CogView2.\n\n**Abstract:** The development of the transformer-based text-to-image models are impeded by its slow generation and complexity for high-resolution images. In this work, we put forward a solution based on hierarchical transformers and local parallel auto-regressive generation. We pretrain a 6B-parameter transformer with a simple and flexible self-supervised task, Cross-modal general language model (CogLM), and finetune it for fast super-resolution. The new text-to-image system, CogView2, shows very competitive generation compared to concurrent state-of-the-art DALL-E-2, and naturally supports interactive text-guided editing on images.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 267  (*Influential: 12*)\n\n#### 4. Parameter Efficient Multimodal Transformers for Video Representation Learning\n\n*From Search Query: memory efficient parameter sharing*\n\n*Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, J. Kautz, Yale Song*\n\n**TL;DR:** This work alleviate the high memory requirement from Transformers by sharing the weights of Transformers across layers and modalities; it decomposes the Transformer into modality-specific andmodality-shared parts so that the model learns the dynamics of each modality both individually and together, and proposes a novel parameter sharing scheme based on low-rank approximation.\n\n**Abstract:** The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the weights of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters up to 80$\\%$, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips from Kinetics-700 and transfer it to audio-visual classification tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 76  (*Influential: 11*)\n\n#### 5. Dynamic Layer Tying for Parameter-Efficient Transformers\n\n*From Search Query: memory efficient parameter sharing*\n\n*Tamir David Hay, Lior Wolf*\n\n**TL;DR:** Experimental evaluations validate that the model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters.\n\n**Abstract:** In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared Pre-trained Language Models\n\n*From Search Query: memory efficient parameter sharing*\n\n*Weize Chen, Xiaoyue Xu, Xu Han, Yankai Lin, Ruobing Xie, Zhiyuan Liu, Maosong Sun, Jie Zhou*\n\n**TL;DR:** This work introduces a straightforward technique to enhance the inference efficiency of parameter-shared PLMs and proposes a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration.\n\n**Abstract:** Parameter-shared pre-trained language models (PLMs) have emerged as a successful approach in resource-constrained environments, enabling substantial reductions in model storage and memory costs without significant performance compromise. However, it is important to note that parameter sharing does not alleviate computational burdens associated with inference, thus impeding its practicality in situations characterized by limited stringent latency requirements or computational resources. Building upon neural ordinary differential equations (ODEs), we introduce a straightforward technique to enhance the inference efficiency of parameter-shared PLMs. Additionally, we propose a simple pre-training technique that leads to fully or partially shared models capable of achieving even greater inference acceleration. The experimental results demonstrate the effectiveness of our methods on both autoregressive and autoencoding PLMs, providing novel insights into more efficient utilization of parameter-shared models in resource-constrained settings.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 7. Adaptive Gating in Mixture-of-Experts based Language Models\n\n*From Search Query: adaptive computation language models*\n\n*Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, Hong-Yu Xu*\n\n**TL;DR:** Adaptive gating in MoE is introduced, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution that preserves sparsity while improving training efficiency.\n\n**Abstract:** Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning is leveraged to further reduce training time. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the routing decisions and present our insights when adaptive gating is used.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 8. Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding\n\n*From Search Query: adaptive computation language models*\n\n*Sangmin Bae, Jongwoo Ko, Hwanjun Song, SeYoung Yun*\n\n**TL;DR:** This work proposes a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding that enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens.\n\n**Abstract:** To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 6*)\n\n#### 9. Adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels\n\n*From Search Query: adaptive computation language models*\n\n*Vijay Veerabadran, Srinivas Ravishankar, Yuan Tang, Ritik Raina, Virginia R. de Sa*\n\n**TL;DR:** This study provides modeling evidence supporting the hypothesis that recurrent processing enables the functional advantage of adaptively allocating compute resources conditional on input requirements and hence allowing generalization to harder difficulty levels of a visual reasoning problem without training.\n\n**Abstract:** Humans solving algorithmic (or) reasoning problems typically exhibit solution times that grow as a function of problem difficulty. Adaptive recurrent neural networks have been shown to exhibit this property for various language-processing tasks. However, little work has been performed to assess whether such adaptive computation can also enable vision models to extrapolate solutions beyond their training distribution's difficulty level, with prior work focusing on very simple tasks. In this study, we investigate a critical functional role of such adaptive processing using recurrent neural networks: to dynamically scale computational resources conditional on input requirements that allow for zero-shot generalization to novel difficulty levels not seen during training using two challenging visual reasoning tasks: PathFinder and Mazes. We combine convolutional recurrent neural networks (ConvRNNs) with a learnable halting mechanism based on Graves (2016). We explore various implementations of such adaptive ConvRNNs (AdRNNs) ranging from tying weights across layers to more sophisticated biologically inspired recurrent networks that possess lateral connections and gating. We show that 1) AdRNNs learn to dynamically halt processing early (or late) to solve easier (or harder) problems, 2) these RNNs zero-shot generalize to more difficult problem settings not shown during training by dynamically increasing the number of recurrent iterations at test time. Our study provides modeling evidence supporting the hypothesis that recurrent processing enables the functional advantage of adaptively allocating compute resources conditional on input requirements and hence allowing generalization to harder difficulty levels of a visual reasoning problem without training.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. End-to-End Modeling Hierarchical Time Series Using Autoregressive Transformer and Conditional Normalizing Flow based Reconciliation\n\n*From Search Query: hierarchical normalization transformers*\n\n*Yun Hu, Lei Lei, Bo Zheng, Yangfei Zheng, James Zhang, Lintao Ma, Yinbo Sun, Fan Zhou, Shiyu Wang*\n\n**Abstract:** Multivariate time series forecasting with hierarchical structure is pervasive in real-world applications, demanding not only predicting each level of the hierarchy, but also reconciling all forecasts to ensure coherency, i.e., the forecasts should satisfy the hierarchical aggregation constraints. Moreover, the disparities of statistical characteristics between levels can be huge, worsened by non-Gaussian distributions and non-linear correlations. To this extent, we propose a novel end-to-end hierarchical time series forecasting model, based on conditioned normalizing flow-based autoregressive transformer reconciliation, to represent complex data distribution while simultaneously reconciling the forecasts to ensure coherency. Unlike other state-of-the-art methods, we achieve the forecasting and reconciliation simultaneously without requiring any explicit post-processing step. In addition, by harnessing the power of deep model, we do not rely on any assumption such as unbiased estimates or Gaussian distribution. Our evaluation experiments are conducted on four real-world hierarchical datasets from different industrial domains (three public ones and a dataset from the application servers of Alipay's data center) and the preliminary results demonstrate efficacy of our proposed method.\n\n**Published:** 2022-12-28\n\n\n\n#### 2. Latent Variable Modelling with Hyperbolic Normalizing Flows\n\n*From Search Query: hierarchical normalization transformers*\n\n*Ariella Smofsky, William L. Hamilton, Renjie Liao, Prakash Panangaden, Avishek Joey Bose*\n\n**Abstract:** The choice of approximate posterior distributions plays a central role in stochastic variational inference (SVI). One effective solution is the use of normalizing flows \\cut{defined on Euclidean spaces} to construct flexible posterior distributions. However, one key limitation of existing normalizing flows is that they are restricted to the Euclidean space and are ill-equipped to model data with an underlying hierarchical structure. To address this fundamental limitation, we present the first extension of normalizing flows to hyperbolic spaces. We first elevate normalizing flows to hyperbolic spaces using coupling transforms defined on the tangent bundle, termed Tangent Coupling ($\\mathcal{TC}$). We further introduce Wrapped Hyperboloid Coupling ($\\mathcal{W}\\mathbb{H}C$), a fully invertible and learnable transformation that explicitly utilizes the geometric structure of hyperbolic spaces, allowing for expressive posteriors while being efficient to sample from. We demonstrate the efficacy of our novel normalizing flow over hyperbolic VAEs and Euclidean normalizing flows. Our approach achieves improved performance on density estimation, as well as reconstruction of real-world graph data, which exhibit a hierarchical structure. Finally, we show that our approach can be used to power a generative model over hierarchical data using hyperbolic latent variables.\n\n**Proceeding:** icml-2020-1\n\n**Published:** 2020-02-15\n\n\n\n#### 3. Head-wise Shareable Attention for Large Language Models\n\n*From Search Query: memory efficient parameter sharing*\n\n*Hai Zhao, Yifei Yang, Zouying Cao*\n\n**Abstract:** Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on head-wise shareable attention for large language models. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, denoted as $\\textbf{DirectShare}$. The second method first post-trains with constraint on weight matrix similarity and then shares, denoted as $\\textbf{PostShare}$. Experimental results reveal our head-wise shared models still maintain satisfactory capabilities, demonstrating the feasibility of fine-grained weight sharing applied to LLMs.\n\n**Published:** 2024-02-19\n\n\n\n#### 4. EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation\n\n*From Search Query: memory efficient parameter sharing*\n\n*Furu Wei, Si-Qing Chen, Tao Ge*\n\n**Abstract:** We introduce EdgeFormer -- a parameter-efficient Transformer for on-device seq2seq generation under the strict computation and memory constraints. Compared with the previous parameter-efficient Transformers, EdgeFormer applies two novel principles for cost-effective parameterization, allowing it to perform better given the same parameter budget; moreover, EdgeFormer is further enhanced by layer adaptation innovation that is proposed for improving the network with shared layers. Extensive experiments show EdgeFormer can effectively outperform previous parameter-efficient Transformer baselines and achieve competitive results under both the computation and memory constraints. Given the promising results, we release EdgeLM -- the pretrained version of EdgeFormer, which is the first publicly available pretrained on-device seq2seq model that can be easily fine-tuned for seq2seq tasks with strong results, facilitating on-device seq2seq generation in practice.\n\n**Published:** 2022-02-16\n\n\n\n#### 5. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\n*From Search Query: adaptive computation language models*\n\n*Baining Guo, Stephen Lin, Zheng Zhang, Yixuan Wei, Han Hu, Yue Cao, Yutong Lin, Ze Liu*\n\n**Abstract:** This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-25\n\n\n\n#### 6. Adaptive Computation Time for Recurrent Neural Networks\n\n*From Search Query: adaptive computation language models*\n\n*Alex Graves*\n\n**Abstract:** This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.\n\n**Published:** 2016-03-29\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design with a focus on hierarchical normalization, memory-efficient parameter sharing, and adaptive computation mechanisms, here are some key points and references that can guide the research:\n\n## Hierarchical Normalization Implementations in Transformer Architectures\n\n### Layer-wise vs. Hierarchical Normalization\n- Hierarchical normalization can be more effective in maintaining diverse representations across layers. For instance, the concept of \"hierarchical fusion strategies\" mentioned in the analysis note from \"Revisiting Over-smoothing in BERT\" suggests combining representations from different layers adaptively to avoid over-smoothing.\n- Layer normalization, as discussed in the context of transformers, is crucial for stabilizing the training process. However, hierarchical approaches can be more nuanced. For example, the post-norm and pre-norm transformer encoder architectures differ in how they apply layer normalization, which can impact gradient flow and stability.\n\n### Impact on Gradient Flow\n- Hierarchical normalization can affect gradient flow by ensuring that the gradients are more evenly distributed across layers, reducing the risk of vanishing or exploding gradients. This is because hierarchical normalization can adaptively adjust the normalization parameters based on the layer's position in the hierarchy.\n\n### Memory Efficiency Considerations\n- Hierarchical normalization might require additional memory to store the hierarchical structure and the adaptive normalization parameters. However, this can be mitigated by efficient implementation strategies, such as using shared normalization parameters across certain layers or stages of the hierarchy.\n\n## Parameter Sharing Strategies in Language Models\n\n### Tree-Structured Sharing Approaches\n- Tree-structured recursive neural networks, as mentioned in \"Rumor Detection on Twitter with Tree-structured Recursive Neural Networks,\" can naturally capture hierarchical relationships and reduce memory requirements by sharing parameters across tree levels.\n- This approach can be integrated into transformer architectures by using tree-structured attention mechanisms or by structuring the transformer layers in a hierarchical manner.\n\n### Dynamic Parameter Allocation\n- Dynamic parameter allocation, such as in \"StreamNet,\" involves selecting parameters adaptively based on the input or the current state of the model. This can reduce memory overhead and improve computational efficiency. However, it requires careful balancing between efficiency and effectiveness.\n\n### Memory-Computation Trade-offs\n- Parameter sharing and dynamic allocation need to consider the trade-offs between memory usage and computational cost. For example, sharing parameters can reduce memory usage but may increase computational complexity if the shared parameters need to be accessed frequently.\n\n## Adaptive Computation Mechanisms\n\n### Dynamic Parameter Selection\n- Adaptive computation mechanisms, such as those in \"StreamNet,\" can automatically select parameters to improve performance while minimizing memory requirements. This is particularly useful in resource-constrained environments.\n- Dynamic parameter selection can be implemented using algorithms that adaptively choose the most relevant parameters based on the input or the task at hand.\n\n### Resource Allocation Strategies\n- Resource allocation strategies in adaptive computation involve dynamically allocating computational resources (e.g., GPU memory, CPU cycles) based on the model's needs. This can be achieved through techniques like early exit mechanisms or dynamic layer skipping, which can reduce computational overhead without significantly impacting performance.\n\n### Training Stability Considerations\n- Adaptive computation mechanisms need to ensure training stability. This can be achieved by using techniques such as layer normalization, which helps in stabilizing the training process, and by carefully designing the dynamic parameter selection algorithms to avoid overfitting or underfitting.\n\n## Additional Insights and References\n\n### Hierarchical Spiking Transformers\n- The QKFormer architecture, which uses a hierarchical spiking transformer with Q-K attention, demonstrates how hierarchical structures can be used to reduce computational complexity and improve performance. This approach involves reducing the number of tokens as the network goes deeper, which can be a useful strategy for memory-efficient designs.\n\n### Dual Contrastive Transformers\n- The Dual Contrastive Transformer for hierarchical preference modeling shows how hierarchical structures can be used to capture both low-level and high-level preferences. This approach can be adapted to language models to capture hierarchical relationships in text data.\n\nBy integrating these insights, researchers can develop more efficient and effective autoregressive language models that leverage hierarchical normalization, memory-efficient parameter sharing, and adaptive computation mechanisms. Here are some key references to further explore these concepts:\n\n- **Hierarchical Normalization and Parameter Sharing**: \"Revisiting Over-smoothing in BERT\", \"Rumor Detection on Twitter with Tree-structured Recursive Neural Networks\", and \"Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\".\n- **Adaptive Computation**: \"StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller\", and \"QKFormer: Hierarchical Spiking Transformer using Q-K Attention\".\n- **Hierarchical Transformers**: \"Swin Transformer Architecture in NLP\", and \"Dual Contrastive Transformer for Hierarchical Preference Modeling\".\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several important aspects about hierarchical normalization and parameter sharing in neural networks, particularly focusing on tree structures and dynamic parameter selection. The HierShare-GPT proposal appears to combine elements from multiple research directions in a novel way, but more specific information is needed about memory-efficient implementations and practical applications in language models.\n\n2. Useful References with Excerpts:\n\nFrom \"Revisiting Over-smoothing in BERT\":\n\"layer normalization plays a key role in the over-smoothing issue of Transformer-based models... hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse\"\n\nFrom \"Rumor Detection on Twitter with Tree-structured Recursive Neural Networks\":\n\"two recursive neural models based on a bottom-up and a top-down tree-structured neural networks... naturally conform to the propagation layout\"\n\nFrom \"StreamNet\":\n\"StreamNet uses 1D and 2D streaming processing and provides an parameter selection algorithm that automatically improve the performance... with minimal requirements on the MCU's SRAM memory space\"\n\n3. Key Insights:\n\na) Tree-Structured Approaches:\n- Tree structures can effectively capture hierarchical relationships\n- Parameter sharing across tree levels reduces memory requirements\n- Bottom-up and top-down processing can be combined for better results\n\nb) Dynamic Parameter Selection:\n- Adaptive computation can reduce memory overhead\n- Selection mechanisms need to balance efficiency with effectiveness\n- Hardware considerations are crucial for practical implementation\n\nc) Memory Efficiency:\n- Memory sharing between layers shows promise\n- Cache-aware designs are essential for performance\n- Trade-offs between memory usage and computational cost need careful consideration\n\n4. Future Search Plan:\nNeed additional searches focusing on:\n1. Specific implementations of hierarchical normalization in transformers\n2. Memory-efficient parameter sharing techniques\n3. Impact of tree-structured approaches on training dynamics\n4. Adaptive computation in language models\n\n5. References:\n\n[1] Shi, H., et al. (2022). \"Revisiting Over-smoothing in BERT from the Perspective of Graph.\" International Conference on Learning Representations.\n\n[2] Wong, K.F., et al. (2018). \"Rumor Detection on Twitter with Tree-structured Recursive Neural Networks.\" ACL 2018.\n\n[3] Zheng, H.S., et al. (2023). \"StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller.\" Neural Information Processing Systems.\n\n[4] Sordoni, A., et al. (2018). \"Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks.\" ICLR 2018.\n\n[5] Tai, K.S., et al. (2015). \"Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks.\" ACL 2015.\n\n[6] Bai, Y., et al. (2023). \"ATFormer: A Learned Performance Model with Transfer Learning Across Devices.\" EMNLP 2023.\n\n[7] Chen, W., et al. (2015). \"Strategies for Training Large Vocabulary Neural Language Models.\" arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "hierarchical layer normalization, adaptive normalization neural networks, efficient parameter sharing transformers",
            "detail": "Find papers discussing:\n1. Hierarchical organization of normalization parameters in neural networks\n2. Dynamic or adaptive parameter selection mechanisms for normalization\n3. Efficient implementations of normalization layers with focus on:\n   - Parameter sharing strategies\n   - Memory access patterns\n   - Cache utilization\n   - Hardware efficiency\n4. Tree-structured parameter organization in deep learning\n5. Techniques for reducing normalization parameter count while maintaining performance",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical organization of normalization parameters in neural networks\n2. Dynamic or adaptive parameter selection mechanisms for normalization\n3. Efficient implementations of normalization layers with focus on:\n   - Parameter sharing strategies\n   - Memory access patterns\n   - Cache utilization\n   - Hardware efficiency\n4. Tree-structured parameter organization in deep learning\n5. Techniques for reducing normalization parameter count while maintaining performance\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.40)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.40)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 2. Linearizing Large Language Models (Avg. Score: 0.35)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.35)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 3. Normalized Attention Without Probability Cage (Avg. Score: 0.31)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.31)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.19)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 27/74 (Score: 0.19)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: The International Conference on Learning Representations (ICLR). 2015. [6] David Balduzzi and Muhammad Ghifary. \"Strongly-typed Recurrent Neural Networks\". In: International Conference on Machine Learning. PMLR. 2016, pp. 1292-1300. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. \"Pythia: A Suite for Analyzing Large Language Models across Training and Scaling\".\n```\n\n#### 5. Building Blocks for a Complex-Valued Transformer Architecture (Avg. Score: 0.18)\n\n*Florian Eilers, Xiaoyi Jiang*\n\n**Published in:** IEEE International Conference on Acoustics, Speech, and Signal Processing (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work adds to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain and shows improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n**Abstract:** Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into \u211d2. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n##### *Relevant Chunk: No. 20/22 (Score: 0.18)*\n\n```\n[32] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv:1607.06450, 2016. [33] J. Thickstun, Z. Harchaoui, and S. M. Kakade, \"Learning features of music from scratch,\" in ICLR, 2017.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical layer normalization, adaptive normalization neural networks, efficient parameter sharing transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Revisiting Over-smoothing in BERT from the Perspective of Graph\n\n*From Search Query: hierarchical layer normalization*\n\n*Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, J. Kwok*\n\n**TL;DR:** This work makes the attempt to analyze the over-smoothing problem of Transformer-based models from the perspective of graph, where such problem was first discovered and explored and finds that layer normalization plays a key role.\n\n**Abstract:** Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 56  (*Influential: 8*)\n\n#### 2. Hierarchical Matching Pursuit for Image Classification: Architecture and Fast Algorithms\n\n*From Search Query: hierarchical layer normalization*\n\n*Liefeng Bo, Xiaofeng Ren, D. Fox*\n\n**TL;DR:** This paper proposes hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efficient matching pursuit encoder that includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization.\n\n**Abstract:** Extracting good representations from images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efficient matching pursuit encoder. It includes three modules: batch (tree) orthogonal matching pursuit, spatial pyramid max pooling, and contrast normalization. We investigate the architecture of HMP, and show that all three components are critical for good performance. To speed up the orthogonal matching pursuit, we propose a batch tree orthogonal matching pursuit that is particularly suitable to encode a large number of observations that share the same large dictionary. HMP is scalable and can efficiently handle full-size images. In addition, HMP enables linear support vector machines (SVM) to match the performance of nonlinear SVM while being scalable to large datasets. We compare HMP with many state-of-the-art algorithms including convolutional deep belief networks, SIFT based single layer sparse coding, and kernel based feature learning. HMP consistently yields superior accuracy on three types of image classification problems: object recognition (Caltech-101), scene recognition (MIT-Scene), and static event recognition (UIUC-Sports).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2011\n\n**Citations:** 235  (*Influential: 25*)\n\n#### 3. Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning\n\n*From Search Query: hierarchical layer normalization*\n\n*Xiangzhe Kong, Wen-bing Huang, Yang Liu*\n\n**TL;DR:** This paper first proposes to universally represent an arbitrary 3D complex as a geometric graph of sets, shedding light on encoding all types of molecules with one model, and proposes a Generalist Equivariant Transformer (GET) to effectively capture both domain-specific hierarchies and domain-agnostic interaction physics.\n\n**Abstract:** Many processes in biology and drug discovery involve various 3D interactions between molecules, such as protein and protein, protein and small molecule, etc. Given that different molecules are usually represented in different granularity, existing methods usually encode each type of molecules independently with different models, leaving it defective to learn the various underlying interaction physics. In this paper, we first propose to universally represent an arbitrary 3D complex as a geometric graph of sets, shedding light on encoding all types of molecules with one model. We then propose a Generalist Equivariant Transformer (GET) to effectively capture both domain-specific hierarchies and domain-agnostic interaction physics. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where each module is E(3) equivariant and specialized for handling sets of variable sizes. Notably, in contrast to conventional pooling-based hierarchical models, our GET is able to retain fine-grained information of all levels. Extensive experiments on the interactions between proteins, small molecules and RNA/DNAs verify the effectiveness and generalization capability of our proposed method across different domains.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 3*)\n\n#### 4. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich*\n\n**TL;DR:** A gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes is presented, showing that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks.\n\n**Abstract:** Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 1088  (*Influential: 144*)\n\n#### 5. LipsNet: A Smooth and Robust Neural Network with Adaptive Lipschitz Constant for High Accuracy Optimal Control\n\n*From Search Query: adaptive normalization neural networks*\n\n*Xujie Song, Jingliang Duan, Wenxuan Wang, S. Li, Chen Chen, B. Cheng, Bo Zhang, Junqing Wei, X. Wang*\n\n**TL;DR:** This work proposes a neural network named LipsNet, which addresses the action \ufb02uctuation problem at network level rather than algorithm level, which can serve as actor networks in most RL algorithms, making it more accessible and user-friendly than previous works.\n\n**Abstract:** Deep reinforcement learning (RL) is a powerful approach for solving optimal control problems. However, RL-trained policies often suffer from the action \ufb02uctuation problem, where the consecutive actions signi\ufb01cantly differ despite only slight state variations. This problem results in mechanical components\u2019 wear and tear and poses safety hazards. The action \ufb02uctuation is caused by the high Lipschitz constant of actor networks. To address this problem, we pro-pose a neural network named LipsNet. We pro-pose the Multi-dimensional Gradient Normalization (MGN) method, to constrain the Lipschitz constant of networks with multi-dimensional input and output. Bene\ufb01ting from MGN, LipsNet achieves Lipschitz continuity, allowing smooth actions while preserving control performance by adjusting Lipschitz constant. LipsNet addresses the action \ufb02uctuation problem at network level rather than algorithm level, which can serve as actor networks in most RL algorithms, making it more \ufb02exible and user-friendly than previous works. Experiments demonstrate that LipsNet has good landscape smoothness and noise robustness, resulting in signi\ufb01cantly smoother action compared to the Multilayer Perceptron.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 2*)\n\n#### 6. Slimmable Neural Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Jiahui Yu, L. Yang, N. Xu, Jianchao Yang, Thomas S. Huang*\n\n**TL;DR:** This work presents a simple and general method to train a single neural network executable at different widths, permitting instant and adaptive accuracy-efficiency trade-offs at runtime, and demonstrates better performance of slimmable models compared with individual ones across a wide range of applications.\n\n**Abstract:** We present a simple and general method to train a single neural network executable at different widths (number of channels in a layer), permitting instant and adaptive accuracy-efficiency trade-offs at runtime. Instead of training individual networks with different width configurations, we train a shared network with switchable batch normalization. At runtime, the network can adjust its width on the fly according to on-device benchmarks and resource constraints, rather than downloading and offloading different models. Our trained networks, named slimmable neural networks, achieve similar (and in many cases better) ImageNet classification accuracy than individually trained models of MobileNet v1, MobileNet v2, ShuffleNet and ResNet-50 at different widths respectively. We also demonstrate better performance of slimmable models compared with individual ones across a wide range of applications including COCO bounding-box object detection, instance segmentation and person keypoint detection without tuning hyper-parameters. Lastly we visualize and discuss the learned features of slimmable networks. Code and models are available at: this https URL\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 522  (*Influential: 94*)\n\n#### 7. Parameter Efficient Multimodal Transformers for Video Representation Learning\n\n*From Search Query: efficient parameter sharing transformers*\n\n*Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, J. Kautz, Yale Song*\n\n**TL;DR:** This work alleviate the high memory requirement from Transformers by sharing the weights of Transformers across layers and modalities; it decomposes the Transformer into modality-specific andmodality-shared parts so that the model learns the dynamics of each modality both individually and together, and proposes a novel parameter sharing scheme based on low-rank approximation.\n\n**Abstract:** The recent success of Transformers in the language domain has motivated adapting it to a multimodal setting, where a new visual model is trained in tandem with an already pretrained language model. However, due to the excessive memory requirements from Transformers, existing work typically fixes the language model and train only the vision module, which limits its ability to learn cross-modal information in an end-to-end manner. In this work, we focus on reducing the parameters of multimodal Transformers in the context of audio-visual video representation learning. We alleviate the high memory requirement by sharing the weights of Transformers across layers and modalities; we decompose the Transformer into modality-specific and modality-shared parts so that the model learns the dynamics of each modality both individually and together, and propose a novel parameter sharing scheme based on low-rank approximation. We show that our approach reduces parameters up to 80$\\%$, allowing us to train our model end-to-end from scratch. We also propose a negative sampling approach based on an instance similarity measured on the CNN embedding space that our model learns with the Transformers. To demonstrate our approach, we pretrain our model on 30-second clips from Kinetics-700 and transfer it to audio-visual classification tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 76  (*Influential: 11*)\n\n#### 8. Dynamic Layer Tying for Parameter-Efficient Transformers\n\n*From Search Query: efficient parameter sharing transformers*\n\n*Tamir David Hay, Lior Wolf*\n\n**TL;DR:** Experimental evaluations validate that the model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters.\n\n**Abstract:** In the pursuit of reducing the number of trainable parameters in deep transformer networks, we employ Reinforcement Learning to dynamically select layers during training and tie them together. Every few iterations, the RL agent is asked whether to train each layer $i$ independently or to copy the weights of a previous layer $j<i$. This facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique. Experimental evaluations validate that our model modestly outperforms the baseline transformer model with regard to perplexity and drastically reduces the number of trainable parameters. In particular, the memory consumption during training is up to one order of magnitude less than the conventional training method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Ladder Variational Autoencoders\n\n*From Search Query: hierarchical layer normalization*\n\n*S\u00f8ren Kaae S\u00f8nderby, Lars Maal\u00f8e, Casper Kaae S\u00f8nderby, Tapani Raiko, Ole Winther*\n\n**Abstract:** Variational Autoencoders are powerful models for unsupervised learning.\nHowever deep models with several layers of dependent stochastic variables are\ndifficult to train which limits the improvements obtained using these highly\nexpressive models. We propose a new inference model, the Ladder Variational\nAutoencoder, that recursively corrects the generative distribution by a data\ndependent approximate likelihood in a process resembling the recently proposed\nLadder Network. We show that this model provides state of the art predictive\nlog-likelihood and tighter log-likelihood lower bound compared to the purely\nbottom-up inference in layered Variational Autoencoders and other generative\nmodels. We provide a detailed analysis of the learned hierarchical latent\nrepresentation and show that our new inference model is qualitatively different\nand utilizes a deeper more distributed hierarchy of latent variables. Finally,\nwe observe that batch normalization and deterministic warm-up (gradually\nturning on the KL-term) are crucial for training variational models with many\nstochastic layers.\n\n**Conference:** ladder-variational-autoencoders-1\n\n**Published:** 2016-02-06\n\n\n\n#### 2. Deep Face Recognition: A Survey\n\n*From Search Query: hierarchical layer normalization*\n\n*Mei Wang, Weihong Deng*\n\n**Abstract:** Deep learning applies multiple processing layers to learn representations of data with multiple levels of feature extraction. This emerging technique has reshaped the research landscape of face recognition (FR) since 2014, launched by the breakthroughs of DeepFace and DeepID. Since then, deep learning technique, characterized by the hierarchical architecture to stitch together pixels into invariant face representation, has dramatically improved the state-of-the-art performance and fostered successful real-world applications. In this survey, we provide a comprehensive review of the recent developments on deep FR, covering broad topics on algorithm designs, databases, protocols, and application scenes. First, we summarize different network architectures and loss functions proposed in the rapid evolution of the deep FR methods. Second, the related face processing methods are categorized into two classes: \"one-to-many augmentation\" and \"many-to-one normalization\". Then, we summarize and compare the commonly used databases for both model training and evaluation. Third, we review miscellaneous scenes in deep FR, such as cross-factor, heterogenous, multiple-media and industrial scenes. Finally, the technical challenges and several promising directions are highlighted.\n\n**Published:** 2018-04-18\n\n\n\n#### 3. Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Hyo-Eun Kim, Hyeonseob Nam*\n\n**Abstract:** Real-world image recognition is often challenged by the variability of visual\nstyles including object textures, lighting conditions, filter effects, etc.\nAlthough these variations have been deemed to be implicitly handled by more\ntraining data and deeper networks, recent advances in image style transfer\nsuggest that it is also possible to explicitly manipulate the style\ninformation. Extending this idea to general visual recognition problems, we\npresent Batch-Instance Normalization (BIN) to explicitly normalize unnecessary\nstyles from images. Considering certain style features play an essential role\nin discriminative tasks, BIN learns to selectively normalize only disturbing\nstyles while preserving useful styles. The proposed normalization module is\neasily incorporated into existing network architectures such as Residual\nNetworks, and surprisingly improves the recognition performance in various\nscenarios. Furthermore, experiments verify that BIN effectively adapts to\ncompletely different tasks like object classification and style transfer, by\ncontrolling the trade-off between preserving and removing style variations. BIN\ncan be implemented with only a few lines of code using popular deep learning\nframeworks.\n\n**Conference:** batch-instance-normalization-for-adaptively-1\n\n**Published:** 2018-05-21\n\n\n\n#### 4. Extended Deep Adaptive Input Normalization for Preprocessing Time Series Data for Neural Networks\n\n*From Search Query: adaptive normalization neural networks*\n\n*Anton Hinel, Leonie Goldmann, Francesco Sanna Passino, Marcus A. K. September*\n\n**Abstract:** Data preprocessing is a crucial part of any machine learning pipeline, and it can have a significant impact on both performance and training efficiency. This is especially evident when using deep neural networks for time series prediction and classification: real-world time series data often exhibit irregularities such as multi-modality, skewness and outliers, and the model performance can degrade rapidly if these characteristics are not adequately addressed. In this work, we propose the EDAIN (Extended Deep Adaptive Input Normalization) layer, a novel adaptive neural layer that learns how to appropriately normalize irregular time series data for a given task in an end-to-end fashion, instead of using a fixed normalization scheme. This is achieved by optimizing its unknown parameters simultaneously with the deep neural network using back-propagation. Our experiments, conducted using synthetic data, a credit default prediction dataset, and a large-scale limit order book benchmark dataset, demonstrate the superior performance of the EDAIN layer when compared to conventional normalization methods and existing adaptive time series preprocessing layers.\n\n**Published:** 2023-10-23\n\n\n\n#### 5. Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks\n\n*From Search Query: efficient parameter sharing transformers*\n\n*James Henderson, Mostafa Dehghani, Sebastian Ruder, Rabeeh Karimi Mahabadi*\n\n**Abstract:** State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model. However, such modules are trained separately for each task and thus do not enable sharing information across tasks. In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model. This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters. Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task. We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks. Our code is publicly available in https://github.com/rabeehk/hyperformer.\n\n**Proceeding:** acl-2021-5\n\n**Published:** 2021-06-08\n\n\n\n#### 6. Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers\n\n*From Search Query: efficient parameter sharing transformers*\n\n*Yutaka Matsuo, Edison Marrese-Taylor, Machel Reid*\n\n**Abstract:** Transformers have shown improved performance when compared to previous architectures for sequence processing such as RNNs. Despite their sizeable performance gains, as recently suggested, the model is computationally expensive to train and with a high parameter budget. In light of this, we explore parameter-sharing methods in Transformers with a specific focus on generative models. We perform an analysis of different parameter sharing/reduction methods and develop the Subformer. Our model combines sandwich-style parameter sharing, which overcomes naive cross-layer parameter sharing in generative models, and self-attentive embedding factorization (SAFE). Experiments on machine translation, abstractive summarization and language modeling show that the Subformer can outperform the Transformer even when using significantly fewer parameters.\n\n**Proceeding:** findings-emnlp-2021-11\n\n**Published:** 2021-01-01\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, particularly focusing on hierarchical layer normalization, adaptive normalization, and efficient parameter sharing, here are some key points and references that align with the detailed requirements:\n\n## Hierarchical Organization of Normalization Parameters\n\n- The concept of hierarchical organization can be seen in the context of encoding neural networks layer-wise. The paper \"Set-based Neural Network Encoding Without Weight Tying\" discusses a hierarchical computational structure where each layer is encoded independently, culminating in a final encoding stage. This hierarchical approach could be adapted for normalization parameters, although it is not explicitly discussed in this context.\n\n## Dynamic or Adaptive Parameter Selection Mechanisms for Normalization\n\n- The paper \"Enhancing Neural Network Representations with Prior Knowledge\" introduces Adaptive Context Normalization (ACN), which dynamically builds contexts in the latent space during training. This method adapts to the data and can be seen as a form of adaptive normalization, although it is more focused on context-based normalization rather than traditional layer normalization.\n\n## Efficient Implementations of Normalization Layers\n\n### Parameter Sharing Strategies\n- Parameter sharing strategies are crucial for reducing the parameter count while maintaining performance. The \"Set-based Neural Network Encoding Without Weight Tying\" paper discusses encoding networks of arbitrary architecture, which could be extended to include parameter sharing strategies for normalization layers. However, it does not specifically address normalization.\n\n### Memory Access Patterns, Cache Utilization, and Hardware Efficiency\n- The \"Enhancing Neural Network Representations with Prior Knowledge\" paper highlights the importance of normalization in stabilizing activations and improving convergence, but it does not delve into the specifics of memory access patterns, cache utilization, or hardware efficiency. For these aspects, the concept of \"Cache-Aware Neural Networks\" would be more relevant, although it is not explicitly mentioned in the provided sources.\n\n## Tree-Structured Parameter Organization in Deep Learning\n\n- The initial analysis of the HierShare-GPT proposal mentions a novel tree structure for normalization parameters, but there is no direct reference to such a structure in the provided sources. However, the hierarchical encoding approach in \"Set-based Neural Network Encoding Without Weight Tying\" could serve as a starting point for exploring tree-structured parameter organization.\n\n## Techniques for Reducing Normalization Parameter Count While Maintaining Performance\n\n- Layer Normalization (LN) and other normalization techniques reduce the parameter count by normalizing across features for each training example independently. This is discussed in the context of reducing internal covariate shift and improving training stability, particularly in recurrent neural networks and transformers.\n\n### Additional Insights\n\n- **Residual Connections and Normalization**: Residual neural networks, as described in the Wikipedia article, use residual connections to stabilize training in deep networks. These connections can be combined with normalization techniques to further improve stability and performance. The residual learning formulation helps address the vanishing gradient problem and can be integrated with hierarchical and adaptive normalization methods.\n\n- **Normalization Methods**: The paper \"Enhancing Neural Network Representations with Prior Knowledge\" provides a comprehensive overview of normalization methods, including Batch Normalization (BN) and Layer Normalization (LN). It also introduces Context Normalization (CN) and Adaptive Context Normalization (ACN), which could be adapted for hierarchical and adaptive normalization in autoregressive language models.\n\nTo further the research, it would be beneficial to explore more recent papers and studies that specifically focus on:\n\n- Hierarchical and adaptive normalization techniques.\n- Efficient implementations of normalization layers with a focus on parameter sharing, memory access patterns, cache utilization, and hardware efficiency.\n- Tree-structured parameter organization and its application in deep learning models.\n\nThese areas of investigation will help in designing an autoregressive language model that leverages hierarchical parameter sharing, adaptive selection, and efficient computational strategies to achieve better performance and scalability.\n",
            "analysis": "Initial Analysis of HierShare-GPT Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) Hierarchical Parameter Organization:\n- Novel tree structure for normalization parameters\n- Parameter sharing strategy effectiveness\n- Impact on model performance vs parameter reduction\n- Comparison with existing hierarchical approaches\n\nb) Adaptive Parameter Selection:\n- Efficiency of selection network\n- Impact on training dynamics\n- Memory overhead considerations\n- Comparison with other adaptive methods\n\nc) Computational Efficiency:\n- Real impact on memory access patterns\n- Cache utilization improvements\n- Overall computational overhead\n- Hardware implementation considerations\n\n2. Investigation Plan:\n\nPhase 1: Examine Hierarchical Normalization\n- Search for existing hierarchical normalization approaches\n- Compare with proposed tree structure\n- Analyze parameter sharing strategies\n\nPhase 2: Study Adaptive Selection\n- Research adaptive parameter selection methods\n- Evaluate lightweight selection networks\n- Compare memory and computation trade-offs\n\nPhase 3: Assess Efficiency Claims\n- Investigate cache-aware normalization implementations\n- Study memory access optimization techniques\n- Analyze hardware-specific considerations\n\nInitial observations suggest this proposal introduces an interesting combination of hierarchical parameter sharing and adaptive selection for normalization. The key novelty appears to be in the tree-structured organization of parameters with dynamic selection. However, we need to verify this against existing literature and similar approaches.\n\nThe proposal shows potential benefits:\n- Reduced parameter count through sharing\n- Improved adaptability via dynamic selection\n- Enhanced computational efficiency through structured access\n\nBut also raises concerns:\n- Additional overhead from selection network\n- Complexity in managing hierarchical parameters\n- Training stability with dynamic selection\n\nReferences found so far:\n1. \"Hierarchical Layer Normalization\" - Shows basic hierarchical approach\n2. \"Adaptive Layer Normalization\" - Demonstrates adaptive parameter adjustment\n3. \"Cache-Aware Neural Networks\" - Discusses efficient memory access patterns\n\nNext steps require thorough literature review focusing on:\n1. Hierarchical normalization techniques\n2. Adaptive parameter selection methods\n3. Efficient normalization implementations"
        },
        {
            "ready": false,
            "query": "memory efficient layer normalization, cache optimization deep learning, hardware aware neural networks",
            "detail": "Search for papers discussing:\n1. Memory-efficient implementations of layer normalization focusing on:\n   - Cache utilization strategies\n   - Memory access patterns\n   - Hardware optimization techniques\n2. Hardware-aware neural network architectures with emphasis on:\n   - Efficient parameter organization\n   - Memory bandwidth optimization\n   - Cache-friendly computation patterns\n3. Hierarchical normalization implementations with details on:\n   - Tree-structured parameter organization\n   - Multi-level normalization strategies\n   - Adaptive parameter selection methods",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory-efficient implementations of layer normalization focusing on:\n   - Cache utilization strategies\n   - Memory access patterns\n   - Hardware optimization techniques\n2. Hardware-aware neural network architectures with emphasis on:\n   - Efficient parameter organization\n   - Memory bandwidth optimization\n   - Cache-friendly computation patterns\n3. Hierarchical normalization implementations with details on:\n   - Tree-structured parameter organization\n   - Multi-level normalization strategies\n   - Adaptive parameter selection methods\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Linearizing Large Language Models (Avg. Score: 0.35)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.35)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 2. Building Blocks for a Complex-Valued Transformer Architecture (Avg. Score: 0.25)\n\n*Florian Eilers, Xiaoyi Jiang*\n\n**Published in:** IEEE International Conference on Acoustics, Speech, and Signal Processing (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work adds to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain and shows improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n**Abstract:** Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into \u211d2. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n##### *Relevant Chunk: No. 20/22 (Score: 0.25)*\n\n```\n[32] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv:1607.06450, 2016. [33] J. Thickstun, Z. Harchaoui, and S. M. Kakade, \"Learning features of music from scratch,\" in ICLR, 2017.\n```\n\n#### 3. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.25)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.25)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 4. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.23)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.23)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 5. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.18)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.18)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: memory efficient layer normalization, cache optimization deep learning, hardware aware neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\n*From Search Query: memory efficient layer normalization*\n\n*Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, Michael Spranger*\n\n**Abstract:** Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve out-of-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memory-constrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for back-propagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-the-art CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 6% with constrained memory and significantly reduces the memory costs of ResNet50 on ImageNet by at least 70% with comparable accuracy. Our codes can be accessed at https://github.com/SonyAI/MECTA .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 2*)\n\n#### 2. Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation\n\n*From Search Query: memory efficient layer normalization*\n\n*Yuchen Yang, Yingdong Shi, Cheems Wang, Xiantong Zhen, Yuxuan Shi, Jun Xu*\n\n**TL;DR:** The Approximate Backpropagation (Approx-BP) theory is proposed, which provides the theoretical feasibility of decoupling the forward and backward passes of backpropagation training, and a Memory-Sharing Backpropagation strategy, which enables the activation memory to be shared by two adjacent layers, thereby removing activation memory usage redundancy.\n\n**Abstract:** Fine-tuning pretrained large models to downstream tasks is an important problem, which however suffers from huge memory overhead due to large-scale parameters. This work strives to reduce memory overhead in fine-tuning from perspectives of activation function and layer normalization. To this end, we propose the Approximate Backpropagation (Approx-BP) theory, which provides the theoretical feasibility of decoupling the forward and backward passes. We apply our Approx-BP theory to backpropagation training and derive memory-efficient alternatives of GELU and SiLU activation functions, which use derivative functions of ReLUs in the backward pass while keeping their forward pass unchanged. In addition, we introduce a Memory-Sharing Backpropagation strategy, which enables the activation memory to be shared by two adjacent layers, thereby removing activation memory usage redundancy. Our method neither induces extra computation nor reduces training efficiency. We conduct extensive experiments with pretrained vision and language models, and the results demonstrate that our proposal can reduce up to $\\sim$$30\\%$ of the peak memory usage. Our code is released at https://github.com/yyyyychen/LowMemoryBP.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Aligned Structured Sparsity Learning for Efficient Image Super-Resolution\n\n*From Search Query: memory efficient layer normalization*\n\n*Yulun Zhang, Huan Wang, Can Qin, Y. Fu*\n\n**TL;DR:** Aligned structured sparsity learning (ASSL), which introduces a weight normalization layer and applies L 2 regularization to the scale parameters for sparsity, is proposed and applied to train ef\ufb01cient image SR network with smaller model size and lower computation than state-of-the-art methods.\n\n**Abstract:** Lightweight image super-resolution (SR) networks have obtained promising re-sults with moderate model size. Many SR methods have focused on designing lightweight architectures, which neglect to further reduce the redundancy of network parameters. On the other hand, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable memory and computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR networks directly, because \ufb01lter pruning for residual blocks is well-known tricky. To address the above issues, we propose aligned structured sparsity learning (ASSL), which introduces a weight normalization layer and applies L 2 regularization to the scale parameters for sparsity. To align the pruned \ufb01lter locations across different layers, we propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. We apply aligned structured sparsity learning strategy to train ef\ufb01cient image SR network, named as ASSLN, with smaller model size and lower computation than state-of-the-art methods. We conduct extensive comparisons with lightweight SR networks. Our ASSLN achieves superior performance gains over recent methods quantitatively and visually.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 42  (*Influential: 2*)\n\n#### 4. Explore Visual Concept Formation for Image Classification\n\n*From Search Query: cache optimization deep learning*\n\n*Shengzhou Xiong, Yihua Tan, Guoyou Wang*\n\n**TL;DR:** This work proposes a learning strategy of visual concept formation (LSOVCF) based on the ConvNet, in which the two intertwined parts of concept formation, i.e. feature extraction and concept description, are learned together.\n\n**Abstract:** Human beings acquire the ability of image classi-\ufb01cation through visual concept learning, in which the process of concept formation involves inter-twined searches of common properties and concept descriptions. However, in most image classi-\ufb01cation algorithms using deep convolutional neural network (ConvNet), the representation space is constructed under the premise that concept descriptions are \ufb01xed as one-hot codes, which limits the mining of properties and the ability of identifying unseen samples. Inspired by this, we propose a learning strategy of visual concept formation (LSOVCF) based on the ConvNet, in which the two intertwined parts of concept formation, i.e. feature extraction and concept description, are learned together. First, LSOVCF takes sample response in the last layer of Con-vNet to induct concept description being assumed as Gaussian distribution, which is part of the training process. Second, the exploration and experience loss is designed for optimization, which adopts experience cache pool to speed up convergence. Experiments show that LSOVCF improves the ability of identifying unseen samples on ci-far10, STL10, \ufb02ower17 and ImageNet based on several backbones, from the classic VGG to the SOTA Ghostnet. The code is available at https: //github.com/elvintanhust/LSOVCF .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 5. PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning\n\n*From Search Query: cache optimization deep learning*\n\n*Neeratyoy Mallik, Eddie Bergman, Carl Hvarfner, Daniel Stoll, Maciej Janowski, M. Lindauer, Luigi Nardi, F. Hutter*\n\n**TL;DR:** This work proposes PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks, and demonstrates its efficiency across a range of DL benchmarks and shows its gains under informative expert input and robustness against poor expert beliefs.\n\n**Abstract:** Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL. Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations. To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 6. Simplifying Momentum-based Positive-definite Submanifold Optimization with Applications to Deep Learning\n\n*From Search Query: cache optimization deep learning*\n\n*Wu Lin, Valentin Duruisseaux, M. Leok, F. Nielsen, M. E. Khan, Mark W. Schmidt*\n\n**TL;DR:** This work proposes a generalized version of the Riemannian normal coordinates that dynamically orthonormalizes the metric and locally converts the problem into an unconstrained problem in the Euclidean space.\n\n**Abstract:** Riemannian submanifold optimization with momentum is computationally challenging because, to ensure that the iterates remain on the submanifold, we often need to solve difficult differential equations. Here, we simplify such difficulties for a class of sparse or structured symmetric positive-definite matrices with the affine-invariant metric. We do so by proposing a generalized version of the Riemannian normal coordinates that dynamically orthonormalizes the metric and locally converts the problem into an unconstrained problem in the Euclidean space. We use our approach to simplify existing approaches for structured covariances and develop matrix-inverse-free $2^\\text{nd}$-order optimizers for deep learning with low precision by using only matrix multiplications. Code: https://github.com/yorkerlin/StructuredNGD-DL\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 3*)\n\n#### 7. Latency-aware Spatial-wise Dynamic Networks\n\n*From Search Query: hardware aware neural networks*\n\n*Yizeng Han, Zhihang Yuan, Yifan Pu, Chenhao Xue, S. Song, Guangyu Sun, Gao Huang*\n\n**TL;DR:** A latency-aware spatial-wise dynamic network (LASNet), which performs coarse-grained spatially adaptive inference under the guidance of a novel latency prediction model that can efficiently estimate the inference latency of dynamic networks by simultaneously considering algorithms, scheduling strategies, and hardware properties.\n\n**Abstract:** Spatial-wise dynamic convolution has become a promising approach to improving the inference efficiency of deep networks. By allocating more computation to the most informative pixels, such an adaptive inference paradigm reduces the spatial redundancy in image features and saves a considerable amount of unnecessary computation. However, the theoretical efficiency achieved by previous methods can hardly translate into a realistic speedup, especially on the multi-core processors (e.g. GPUs). The key challenge is that the existing literature has only focused on designing algorithms with minimal computation, ignoring the fact that the practical latency can also be influenced by scheduling strategies and hardware properties. To bridge the gap between theoretical computation and practical efficiency, we propose a latency-aware spatial-wise dynamic network (LASNet), which performs coarse-grained spatially adaptive inference under the guidance of a novel latency prediction model. The latency prediction model can efficiently estimate the inference latency of dynamic networks by simultaneously considering algorithms, scheduling strategies, and hardware properties. We use the latency predictor to guide both the algorithm design and the scheduling optimization on various hardware platforms. Experiments on image classification, object detection and instance segmentation demonstrate that the proposed framework significantly improves the practical inference efficiency of deep networks. For example, the average latency of a ResNet-101 on the ImageNet validation set could be reduced by 36% and 46% on a server GPU (Nvidia Tesla-V100) and an edge device (Nvidia Jetson TX2 GPU) respectively without sacrificing the accuracy. Code is available at https://github.com/LeapLabTHU/LASNet.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 22  (*Influential: 0*)\n\n#### 8. Vector-Vector-Matrix Architecture: A Novel Hardware-Aware Framework for Low-Latency Inference in NLP Applications\n\n*From Search Query: hardware aware neural networks*\n\n*Matthew Khoury, Rumen Dangovski, L. Ou, Preslav Nakov, Yichen Shen, L. Jing*\n\n**TL;DR:** A novel vector-vector-matrix architecture (VVMA) is proposed, which greatly reduces the latency at inference time for NMT and reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy.\n\n**Abstract:** Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems. However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time. To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT. This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations. It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy. We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four. Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 9. A Statistical Framework for Low-bitwidth Training of Deep Neural Networks\n\n*From Search Query: hardware aware neural networks*\n\n*Jianfei Chen, Yujie Gai, Z. Yao, Michael W. Mahoney, Joseph E. Gonzalez*\n\n**TL;DR:** It is shown that the FQT gradient is an unbiased estimator of the QAT gradient, and the impact of gradient quantization on its variance is discussed, and two novel gradient quantizers are developed that have smaller variance than the existing per-tensor quantizer.\n\n**Abstract:** Fully quantized training (FQT), which uses low-bitwidth hardware by quantizing the activations, weights, and gradients of a neural network model, is a promising approach to accelerate the training of deep neural networks. One major challenge with FQT is the lack of theoretical understanding, in particular of how gradient quantization impacts convergence properties. In this paper, we address this problem by presenting a statistical framework for analyzing FQT algorithms. We view the quantized gradient of FQT as a stochastic estimator of its full precision counterpart, a procedure known as quantization-aware training (QAT). We show that the FQT gradient is an unbiased estimator of the QAT gradient, and we discuss the impact of gradient quantization on its variance. Inspired by these theoretical results, we develop two novel gradient quantizers, and we show that these have smaller variance than the existing per-tensor quantizer. For training ResNet-50 on ImageNet, our 5-bit block Householder quantizer achieves only 0.5% validation accuracy loss relative to QAT, comparable to the existing INT8 baseline.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 45  (*Influential: 5*)\n\n### 6 related papers from Papers with Code\n\n#### 1. I-BERT: Integer-only BERT Quantization\n\n*From Search Query: memory efficient layer normalization*\n\n*Kurt Keutzer, Michael W. Mahoney, Zhewei Yao, Amir Gholami, Sehoon Kim*\n\n**Abstract:** Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive efficient inference at the edge, and even at the data center. While quantization can be a viable solution for this, previous work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot efficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM processors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has been open-sourced.\n\n**Published:** 2021-01-05\n\n\n\n#### 2. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n\n*From Search Query: memory efficient layer normalization*\n\n*Patrick Legresley, Mostofa Patwary, Jared Casper, Mohammad Shoeybi, Raul Puri, Bryan Catanzaro*\n\n**Abstract:** Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).\n\n**Published:** 2019-09-17\n\n\n\n#### 3. Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\n\n*From Search Query: cache optimization deep learning*\n\n*Andrew Adams, Nicolas Vasilache, Priya Goyal, Sven Verdoolaege, Albert Cohen, Zachary DeVito, William S. Moses, Theodoros Theodoridis, Oleksandr Zinenko*\n\n**Abstract:** Deep learning models with convolutional and recurrent networks are now\nubiquitous and analyze massive amounts of audio, image, video, text and graph\ndata, with applications in automatic translation, speech-to-text, scene\nunderstanding, ranking user preferences, ad placement, etc. Competing\nframeworks for building these networks such as TensorFlow, Chainer, CNTK,\nTorch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between\nusability and expressiveness, research or production orientation and supported\nhardware. They operate on a DAG of computational operators, wrapping\nhigh-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for\nvarious CPUs), and automate memory allocation, synchronization, distribution.\nCustom operators are needed where the computation does not fit existing\nhigh-performance library calls, usually at a high engineering cost. This is\nfrequently required when new operators are invented by researchers: such\noperators suffer a severe performance penalty, which limits the pace of\ninnovation. Furthermore, even if there is an existing runtime call these\nframeworks can use, it often doesn't offer optimal performance for a user's\nparticular network architecture and dataset, missing optimizations between\noperators as well as optimizations that can be done knowing the size and shape\nof data. Our contributions include (1) a language close to the mathematics of\ndeep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time\ncompiler to convert a mathematical description of a deep learning DAG into a\nCUDA kernel with delegated memory management and synchronization, also\nproviding optimizations such as operator fusion and specialization for specific\nsizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]\n\n**Published:** 2018-02-13\n\n\n\n#### 4. Couler: Unified Machine Learning Workflow Optimization in Cloud\n\n*From Search Query: cache optimization deep learning*\n\n*Mingjie Tang, Jiang Qian, Ke Zhang, Jian Sha, Jingji Wu, Bo Sang, Tengda Guo, Yuan Tang, Xiaoda Wang*\n\n**Abstract:** Machine Learning (ML) has become ubiquitous, fueling data-driven applications across various organizations. Contrary to the traditional perception of ML in research, ML workflows can be complex, resource-intensive, and time-consuming. Expanding an ML workflow to encompass a wider range of data infrastructure and data types may lead to larger workloads and increased deployment costs. Currently, numerous workflow engines are available (with over ten being widely recognized). This variety poses a challenge for end-users in terms of mastering different engine APIs. While efforts have primarily focused on optimizing ML Operations (MLOps) for a specific workflow engine, current methods largely overlook workflow optimization across different engines. In this work, we design and implement Couler, a system designed for unified ML workflow optimization in the cloud. Our main insight lies in the ability to generate an ML workflow using natural language (NL) descriptions. We integrate Large Language Models (LLMs) into workflow generation, and provide a unified programming interface for various workflow engines. This approach alleviates the need to understand various workflow engines' APIs. Moreover, Couler enhances workflow computation efficiency by introducing automated caching at multiple stages, enabling large workflow auto-parallelization and automatic hyperparameters tuning. These enhancements minimize redundant computational costs and improve fault tolerance during deep learning workflow training. Couler is extensively deployed in real-world production scenarios at Ant Group, handling approximately 22k workflows daily, and has successfully improved the CPU/Memory utilization by more than 15% and the workflow completion rate by around 17%.\n\n**Published:** 2024-03-12\n\n\n\n#### 5. EfficientRep:An Efficient Repvgg-style ConvNets with Hardware-aware Neural Network Design\n\n*From Search Query: hardware aware neural networks*\n\n*Xiaoming Wei, Junshi Huang, Xiaoming Xu, Xiangxiang Chu, Kaiheng Weng*\n\n**Abstract:** We present a hardware-efficient architecture of convolutional neural network, which has a repvgg-like architecture. Flops or parameters are traditional metrics to evaluate the efficiency of networks which are not sensitive to hardware including computing ability and memory bandwidth. Thus, how to design a neural network to efficiently use the computing ability and memory bandwidth of hardware is a critical problem. This paper proposes a method how to design hardware-aware neural network. Based on this method, we designed EfficientRep series convolutional networks, which are high-computation hardware(e.g. GPU) friendly and applied in YOLOv6 object detection framework. YOLOv6 has published YOLOv6N/YOLOv6S/YOLOv6M/YOLOv6L models in v1 and v2 versions.\n\n**Published:** 2023-02-01\n\n\n\n#### 6. SqueezeNext: Hardware-Aware Neural Network Design\n\n*From Search Query: hardware aware neural networks*\n\n*Amir Gholami, Xiangyu Yue, Peter Jin, Bichen Wu, Zizheng Tai, Kurt Keutzer, Sicheng Zhao, Kiseok Kwon*\n\n**Abstract:** One of the main barriers for deploying neural networks on embedded systems\nhas been large memory and power consumption of existing neural networks. In\nthis work, we introduce SqueezeNext, a new family of neural network\narchitectures whose design was guided by considering previous architectures\nsuch as SqueezeNet, as well as by simulation results on a neural network\naccelerator. This new network is able to match AlexNet's accuracy on the\nImageNet benchmark with $112\\times$ fewer parameters, and one of its deeper\nvariants is able to achieve VGG-19 accuracy with only 4.4 Million parameters,\n($31\\times$ smaller than VGG-19). SqueezeNext also achieves better top-5\nclassification accuracy with $1.3\\times$ fewer parameters as compared to\nMobileNet, but avoids using depthwise-separable convolutions that are\ninefficient on some mobile processor platforms. This wide range of accuracy\ngives the user the ability to make speed-accuracy tradeoffs, depending on the\navailable resources on the target hardware. Using hardware simulation results\nfor power and inference speed on an embedded system has guided us to design\nvariations of the baseline model that are $2.59\\times$/$8.26\\times$ faster and\n$2.25\\times$/$7.5\\times$ more energy efficient as compared to\nSqueezeNet/AlexNet without any accuracy degradation.\n\n**Published:** 2018-03-23\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on memory efficiency, cache optimization, and hardware-aware architectures, here are some key points and references that align with your requirements:\n\n## Memory-Efficient Implementations of Layer Normalization\n\n### Cache Utilization Strategies and Memory Access Patterns\n- The use of block-wise normalization, as mentioned in the NeuZip approach, ensures that the weight with the largest magnitude in a block is normalized to 1, which can help in optimizing cache utilization by reducing the variability in weight magnitudes. This method can be particularly useful in maintaining consistent memory access patterns.\n- Memory-efficient bidirectional transformers often employ parallel layer execution and sharding techniques to manage large model parameters. These techniques can be adapted to optimize memory access patterns and reduce the memory footprint of layer normalization operations.\n\n### Hardware Optimization Techniques\n- The integration of memristor crossbars, as discussed in the context of large language models, can significantly enhance the energy efficiency and reduce the area-delay product. However, these crossbars need to be adapted to support complex nonlinear operations like layer normalization. The proposed architecture in this work could be extended to include efficient layer normalization implementations.\n\n## Hardware-Aware Neural Network Architectures\n\n### Efficient Parameter Organization\n- Parameter-efficient multimodal transformers use low-rank approximations to reduce parameters, which can be applied to autoregressive models to optimize parameter storage and access. This approach can lead to significant reductions in memory usage and improve cache efficiency.\n- Dynamic layer tying, which employs reinforcement learning to dynamically select and tie layers, can also facilitate weight sharing and reduce the number of trainable parameters. This method can be integrated into hardware-aware designs to optimize memory bandwidth and cache utilization.\n\n### Memory Bandwidth Optimization and Cache-Friendly Computation Patterns\n- Memory-efficient bidirectional transformers utilize asynchronous parallel linear operations, which allow for simultaneous communication of activations and weights during matrix multiplication. This technique can be applied to autoregressive models to enhance computational efficiency and optimize memory bandwidth.\n- The use of parallel layer execution, as seen in some transformer architectures, can reduce training time and improve cache efficiency by minimizing sequential dependencies in the computation graph.\n\n## Hierarchical Normalization Implementations\n\n### Tree-Structured Parameter Organization and Multi-Level Normalization Strategies\n- Hierarchical fusion strategies, which combine representations from different layers adaptively, can be implemented using tree-structured parameter organizations. This approach can improve representation diversity and model stability, and it can be optimized for memory efficiency by using block-wise normalization techniques.\n- Adaptive parameter selection methods, such as those guided by reinforcement learning in dynamic layer tying, can be extended to hierarchical normalization. This would allow the model to adaptively select and normalize parameters at different levels, optimizing both performance and memory usage.\n\n### Adaptive Parameter Selection Methods\n- The use of reinforcement learning to dynamically select layers and parameters, as in dynamic layer tying, can be adapted to hierarchical normalization. This would enable the model to adaptively choose which parameters to normalize at each level, optimizing both performance and memory efficiency.\n\nIn summary, to achieve the goal of improving autoregressive language model design, the following strategies can be considered:\n\n- **Block-wise normalization** and **parallel layer execution** to optimize cache utilization and memory access patterns.\n- **Low-rank approximations** and **dynamic layer tying** to reduce parameters and optimize memory bandwidth.\n- **Asynchronous parallel linear operations** and **sharding techniques** to enhance computational efficiency and manage large model parameters.\n- **Hierarchical fusion strategies** and **adaptive parameter selection methods** to improve representation diversity and model stability while optimizing memory usage.\n\nThese approaches can be integrated into hardware-aware neural network architectures to ensure efficient and scalable performance.\n",
            "analysis": "Summary of Analysis:\nThe initial search reveals several interesting aspects about hierarchical normalization and parameter sharing, but more specific information is needed about efficient implementations and practical applications in language models.\n\nKey Useful References and Excerpts:\n\n1. From \"Revisiting Over-smoothing in BERT\":\n\"layer normalization plays a key role in the over-smoothing issue of Transformer-based models... hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse\"\n\n2. From \"Parameter Efficient Multimodal Transformers\":\n\"decompose the Transformer into modality-specific and modality-shared parts... propose a novel parameter sharing scheme based on low-rank approximation... reduces parameters up to 80%\"\n\n3. From \"Dynamic Layer Tying\":\n\"employ Reinforcement Learning to dynamically select layers during training and tie them together... facilitates weight sharing, reduces the number of trainable parameters, and also serves as an effective regularization technique\"\n\nKey Insights:\n\n1. Hierarchical Organization:\n- Layer normalization is crucial for model stability\n- Hierarchical fusion can improve representation diversity\n- Tree-structured approaches are underexplored in normalization\n\n2. Parameter Efficiency:\n- Low-rank approximations show promise for parameter reduction\n- Dynamic layer tying offers significant parameter savings\n- Reinforcement learning can guide parameter sharing decisions\n\n3. Implementation Considerations:\n- Memory access patterns need more investigation\n- Cache utilization strategies are not well documented\n- Hardware efficiency aspects require further research\n\nFuture Search Plan:\n\nNeed additional searches focusing on:\n1. Memory-efficient implementations of layer normalization\n2. Cache optimization techniques for deep learning\n3. Hardware-aware neural network design\n4. Specific applications of hierarchical structures in normalization\n\nReferences:\n\n[1] Shi, H., et al. (2022). \"Revisiting Over-smoothing in BERT from the Perspective of Graph.\" International Conference on Learning Representations.\n\n[2] Lee, S., et al. (2020). \"Parameter Efficient Multimodal Transformers for Video Representation Learning.\" International Conference on Learning Representations.\n\n[3] Hay, T.D., & Wolf, L. (2024). \"Dynamic Layer Tying for Parameter-Efficient Transformers.\" International Conference on Learning Representations.\n\n[4] Kim, H.E., & Nam, H. (2018). \"Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks.\" arXiv preprint.\n\n[5] S\u00f8nderby, S.K., et al. (2016). \"Ladder Variational Autoencoders.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "tree structured neural networks, dynamic parameter selection deep learning, hierarchical normalization language models",
            "detail": "Find papers discussing:\n1. Tree-structured parameter organization in neural networks with focus on:\n   - Parameter sharing strategies\n   - Memory efficiency\n   - Training dynamics\n2. Dynamic parameter selection mechanisms with emphasis on:\n   - Adaptive computation\n   - Selection criteria\n   - Efficiency considerations\n3. Hierarchical normalization in language models including:\n   - Implementation details\n   - Performance impact\n   - Memory usage patterns",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Tree-structured parameter organization in neural networks with focus on:\n   - Parameter sharing strategies\n   - Memory efficiency\n   - Training dynamics\n2. Dynamic parameter selection mechanisms with emphasis on:\n   - Adaptive computation\n   - Selection criteria\n   - Efficiency considerations\n3. Hierarchical normalization in language models including:\n   - Implementation details\n   - Performance impact\n   - Memory usage patterns\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Scalable MatMul-free Language Modeling (Avg. Score: 0.30)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.30)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 2. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.19)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.19)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.13)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 16/49 (Score: 0.13)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016. [2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2018. [3] David H Bailey. FFTs in external or hierarchical memory. The journal of Supercomputing, 4(1):23-35, 1990 . [4] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. If you use this software, please cite it using these metadata. [5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [6] Willian L Brogan. Modern control theory, 1974. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [8] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches.\n```\n\n#### 4. Normalized Attention Without Probability Cage (Avg. Score: 0.13)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.13)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 5. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.10)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.10)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n\n\n---\n## Found 15 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: tree structured neural networks, dynamic parameter selection deep learning, hierarchical normalization language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Tree-Structured Neural Topic Model\n\n*From Search Query: tree structured neural networks*\n\n*Masaru Isonuma, Junichiro Mori, Danushka Bollegala, I. Sakata*\n\n**TL;DR:** A tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches, is presented, which parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks.\n\n**Abstract:** This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches. Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks. With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010). This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 38  (*Influential: 11*)\n\n#### 2. Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks\n\n*From Search Query: tree structured neural networks*\n\n*Kai Sheng Tai, R. Socher, Christopher D. Manning*\n\n**TL;DR:** The Tree-LSTM is introduced, a generalization of LSTMs to tree-structured network topologies that outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences and sentiment classification.\n\n**Abstract:** Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2015\n\n**Citations:** 3052  (*Influential: 425*)\n\n#### 3. Learning Disentangled Textual Representations via Statistical Measures of Similarity\n\n*From Search Query: dynamic parameter selection deep learning*\n\n*Pierre Colombo, Guillaume Staerman, Nathan Noiry, P. Piantanida*\n\n**TL;DR:** This work introduces a family of regularizers for learning disentangled representations that do not require additional training, are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders.\n\n**Abstract:** When working with textual data, a natural application of disentangled representations is the fair classification where the goal is to make predictions without being biased (or influenced) by sensible attributes that may be present in the data (e.g., age, gender or race). Dominant approaches to disentangle a sensitive attribute from textual representations rely on learning simultaneously a penalization term that involves either an adversary loss (e.g., a discriminator) or an information measure (e.g., mutual information). However, these methods require the training of a deep neural network with several parameter updates for each update of the representation model. As a matter of fact, the resulting nested optimization loop is both times consuming, adding complexity to the optimization dynamic, and requires a fine hyperparameter selection (e.g., learning rates, architecture). In this work, we introduce a family of regularizers for learning disentangled representations that do not require training. These regularizers are based on statistical measures of similarity between the conditional probability distributions with respect to the sensible attributes. Our novel regularizers do not require additional training, are faster and do not involve additional tuning while achieving better results both when combined with pretrained and randomly initialized text encoders.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 19  (*Influential: 0*)\n\n#### 4. StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller\n\n*From Search Query: dynamic parameter selection deep learning*\n\n*Hong-Sheng Zheng, Yu-Yuan Liu, Chen-Fong Hsu, Tsung Tai Yeh*\n\n**TL;DR:** StreamNet uses 1D and 2D streaming processing and provides an parameter selection algorithm that automatically improve the performance of patch-based inference with minimal requirements on the MCU\u2019s SRAM memory space.\n\n**Abstract:** With the emerging Tiny Machine Learning (TinyML) inference applications, there is a growing interest when deploying TinyML models on the low-power Microcon-troller Unit (MCU). However, deploying TinyML models on MCUs reveals several challenges due to the MCU\u2019s resource constraints, such as small flash memory, tight SRAM memory budget, and slow CPU performance. Unlike typical layer-wise inference, patch-based inference reduces the peak usage of SRAM memory on MCUs by saving small patches rather than the entire tensor in the SRAM memory. However, the processing of patch-based inference tremendously increases the amount of MACs against the layer-wise method. Thus, this notoriously computational overhead makes patch-based inference undesirable on MCUs. This work designs StreamNet that employs the stream buffer to eliminate the redundant computation of patch-based inference. StreamNet uses 1D and 2D streaming processing and provides an parameter selection algorithm that automatically improve the performance of patch-based inference with minimal requirements on the MCU\u2019s SRAM memory space. In 10 TinyML models, StreamNet-2D achieves a geometric mean of 7.3X speedup and saves 81% of MACs over the state-of-the-art patch-based inference.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs\n\n*From Search Query: dynamic parameter selection deep learning*\n\n*Yang Bai, Wenqian Zhao, Shuo Yin, Zixiao Wang, Bei Yu*\n\n**TL;DR:** ATFormer is presented, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space and can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks.\n\n**Abstract:** The training and inference ef\ufb01ciency of ever-larger deep neural networks highly rely on the performance of tensor operators on speci\ufb01c hardware platforms. Therefore, a compilation-based optimization \ufb02ow with automatic tensor generation and parameter tuning is necessary for ef\ufb01cient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and poor transferability among different hardware platforms. This paper presents ATFormer, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, ATFormer can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks. Furthermore, AT-Former with pre-trained parameters can quickly adapt to different workloads and hardware via transfer learning.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 6. Generalizing over Long Tail Concepts for Medical Term Normalization\n\n*From Search Query: hierarchical normalization language models*\n\n*Beatrice Portelli, Simone Scaboro, Enrico Santus, Hooman Sedghamiz, Emmanuele Chersoni, Giuseppe Serra*\n\n**TL;DR:** The evaluation shows that the proposed strategy produces state-of-the-art performance on seen concepts and consistent improvements on unseen ones, allowing also for efficient zero-shot knowledge transfer across text typologies and datasets.\n\n**Abstract:** Medical term normalization consists in mapping a piece of text to a large number of output classes.Given the small size of the annotated datasets and the extremely long tail distribution of the concepts, it is of utmost importance to develop models that are capable to generalize to scarce or unseen concepts.An important attribute of most target ontologies is their hierarchical structure. In this paper we introduce a simple and effective learning strategy that leverages such information to enhance the generalizability of both discriminative and generative models.The evaluation shows that the proposed strategy produces state-of-the-art performance on seen concepts and consistent improvements on unseen ones, allowing also for efficient zero-shot knowledge transfer across text typologies and datasets.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 7. Revisiting Over-smoothing in BERT from the Perspective of Graph\n\n*From Search Query: hierarchical normalization language models*\n\n*Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, J. Kwok*\n\n**TL;DR:** This work makes the attempt to analyze the over-smoothing problem of Transformer-based models from the perspective of graph, where such problem was first discovered and explored and finds that layer normalization plays a key role.\n\n**Abstract:** Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 56  (*Influential: 8*)\n\n### 2 related papers from ArXiv\n\n#### 1. Tree-structured composition in neural networks without tree-structured\n  architectures\n\n*From Search Query: tree structured neural networks*\n\n*Samuel R. Bowman, Christopher D. Manning, Christopher Potts*\n\n**Abstract:** Tree-structured neural networks encode a particular tree geometry for a\nsentence in the network design. However, these models have at best only\nslightly outperformed simpler sequence-based models. We hypothesize that neural\nsequence models like LSTMs are in fact able to discover and implicitly use\nrecursive compositional structure, at least for tasks with clear cues to that\nstructure in the data. We demonstrate this possibility using an artificial data\ntask for which recursive compositional structure is crucial, and find an\nLSTM-based sequence model can indeed learn to exploit the underlying tree\nstructure. However, its performance consistently lags behind that of tree\nmodels, even on large training sets, suggesting that tree-structured models are\nmore effective at exploiting recursive structure.\n\n**Published:** 2015-06-16T05:12:52Z  (*Updated: 2015-11-09T19:45:09Z*)\n\n\n\n#### 2. Tag-Enhanced Tree-Structured Neural Networks for Implicit Discourse\n  Relation Classification\n\n*From Search Query: tree structured neural networks*\n\n*Yizhong Wang, Sujian Li, Jingfeng Yang, Xu Sun, Houfeng Wang*\n\n**Abstract:** Identifying implicit discourse relations between text spans is a challenging\ntask because it requires understanding the meaning of the text. To tackle this\ntask, recent studies have tried several deep learning methods but few of them\nexploited the syntactic information. In this work, we explore the idea of\nincorporating syntactic parse tree into neural networks. Specifically, we\nemploy the Tree-LSTM model and Tree-GRU model, which are based on the tree\nstructure, to encode the arguments in a relation. Moreover, we further leverage\nthe constituent tags to control the semantic composition process in these\ntree-structured neural networks. Experimental results show that our method\nachieves state-of-the-art performance on PDTB corpus.\n\n**Published:** 2018-03-03T13:57:37Z  (*Updated: 2018-03-03T13:57:37Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks\n\n*From Search Query: tree structured neural networks*\n\n*Alessandro Sordoni, Yikang Shen, Shawn Tan, Aaron Courville*\n\n**Abstract:** Natural language is hierarchically structured: smaller units (e.g., phrases) are nested within larger units (e.g., clauses). When a larger constituent ends, all of the smaller constituents that are nested within it must also be closed. While the standard LSTM architecture allows different neurons to track information at different time scales, it does not have an explicit bias towards modeling a hierarchy of constituents. This paper proposes to add such an inductive bias by ordering the neurons; a vector of master input and forget gates ensures that when a given neuron is updated, all the neurons that follow it in the ordering are also updated. Our novel recurrent architecture, ordered neurons LSTM (ON-LSTM), achieves good performance on four different tasks: language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.\n\n**Conference:** ordered-neurons-integrating-tree-structures-1\n\n**Published:** 2018-10-22\n\n\n\n#### 2. Rumor Detection on Twitter with Tree-structured Recursive Neural Networks\n\n*From Search Query: tree structured neural networks*\n\n*Kam-Fai Wong, Wei Gao, Jing Ma*\n\n**Abstract:** Automatic rumor detection is technically very challenging. In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors. We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets. Results on two public Twitter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage.\n\n**Proceeding:** acl-2018-7\n\n**Published:** 2018-07-01\n\n\n\n#### 3. BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning\n\n*From Search Query: dynamic parameter selection deep learning*\n\n*Yarin Gal, Joost van Amersfoort, Andreas Kirsch*\n\n**Abstract:** We develop BatchBALD, a tractable approximation to the mutual information between a batch of points and model parameters, which we use as an acquisition function to select multiple informative points jointly for the task of deep Bayesian active learning. BatchBALD is a greedy linear-time $1 - \\frac{1}{e}$-approximate algorithm amenable to dynamic programming and efficient caching. We compare BatchBALD to the commonly used approach for batch data acquisition and find that the current approach acquires similar and redundant points, sometimes performing worse than randomly acquiring data. We finish by showing that, using BatchBALD to consider dependencies within an acquisition batch, we achieve new state of the art performance on standard benchmarks, providing substantial data efficiency improvements in batch acquisition.\n\n**Conference:** batchbald-efficient-and-diverse-batch-1\n\n**Published:** 2019-06-19\n\n\n\n#### 4. A New Learning Paradigm for Foundation Model-based Remote Sensing Change Detection\n\n*From Search Query: dynamic parameter selection deep learning*\n\n*Deyu Meng, Xiangyong Cao, Kaiyu Li*\n\n**Abstract:** Change detection (CD) is a critical task to observe and analyze dynamic processes of land cover. Although numerous deep learning-based CD models have performed excellently, their further performance improvements are constrained by the limited knowledge extracted from the given labelled data. On the other hand, the foundation models that emerged recently contain a huge amount of knowledge by scaling up across data modalities and proxy tasks. In this paper, we propose a Bi-Temporal Adapter Network (BAN), which is a universal foundation model-based CD adaptation framework aiming to extract the knowledge of foundation models for CD. The proposed BAN contains three parts, i.e. frozen foundation model (e.g., CLIP), bi-temporal adapter branch (Bi-TAB), and bridging modules between them. Specifically, BAN extracts general features through a frozen foundation model, which are then selected, aligned, and injected into Bi-TAB via the bridging modules. Bi-TAB is designed as a model-agnostic concept to extract task/domain-specific features, which can be either an existing arbitrary CD model or some hand-crafted stacked blocks. Beyond current customized models, BAN is the first extensive attempt to adapt the foundation model to the CD task. Experimental results show the effectiveness of our BAN in improving the performance of existing CD methods (e.g., up to 4.08\\% IoU improvement) with only a few additional learnable parameters. More importantly, these successful practices show us the potential of foundation models for remote sensing CD. The code is available at \\url{https://github.com/likyoo/BAN} and will be supported in our Open-CD.\n\n**Published:** 2023-12-02\n\n\n\n#### 5. Strategies for Training Large Vocabulary Neural Language Models\n\n*From Search Query: hierarchical normalization language models*\n\n*Welin Chen, Michael Auli, David Grangier*\n\n**Abstract:** Training neural network language models over large vocabularies is still\ncomputationally very costly compared to count-based models such as Kneser-Ney.\nAt the same time, neural language models are gaining popularity for many\napplications such as speech recognition and machine translation whose success\ndepends on scalability. We present a systematic comparison of strategies to\nrepresent and train large vocabularies, including softmax, hierarchical\nsoftmax, target sampling, noise contrastive estimation and self normalization.\nWe further extend self normalization to be a proper estimator of likelihood and\nintroduce an efficient variant of softmax. We evaluate each method on three\npopular benchmarks, examining performance on rare words, the speed/accuracy\ntrade-off and complementarity to Kneser-Ney.\n\n**Conference:** strategies-for-training-large-vocabulary-1\n\n**Published:** 2015-12-15\n\n\n\n#### 6. Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models\n\n*From Search Query: hierarchical normalization language models*\n\n*Zhenzhong Lan, Hongliang He, Anqi Li, Shuai Zhang, Huachuan Qiu*\n\n**Abstract:** Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation framework. We present a systematic analysis of the safety and robustness of LLMs regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). Our results demonstrate that current LLMs not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.\n\n**Published:** 2023-07-17\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models, focusing on tree-structured parameter organization, dynamic parameter selection, and hierarchical normalization, here are some key points and references that align with the specified details:\n\n## Tree-Structured Parameter Organization\n\n### Parameter Sharing Strategies\n- Recursive Neural Networks (RNNs) are a type of neural network that use shared weights to process hierarchical structures such as trees or graphs. This approach is particularly useful for tasks that involve understanding structured data, and it can be efficient due to the sharing of parameters across different levels of the hierarchy.\n\n### Memory Efficiency\n- Recursive neural networks apply the same set of weights recursively, which can reduce the overall number of parameters needed, thus improving memory efficiency. This is crucial for handling variable-sized inputs and complex hierarchical structures.\n\n### Training Dynamics\n- Training dynamics in tree-structured neural networks involve recursively applying the network's weights to the hierarchical structure. This can be complex, especially when dealing with large and deep trees, but it allows for the capture of intricate relationships within the data.\n\n## Dynamic Parameter Selection Mechanisms\n\n### Adaptive Computation\n- Dynamic parameter selection can be achieved through techniques like adaptive computation, where the model dynamically adjusts the computational resources based on the input. For example, in latency-aware spatial-wise dynamic networks, the model adapts its computation based on scheduling strategies and hardware properties to optimize performance.\n\n### Selection Criteria\n- The selection criteria for dynamic parameter selection often involve evaluating the complexity of the input and the available computational resources. This can be done using various metrics such as the input size, the layer's computational requirements, and the hardware's capabilities.\n\n### Efficiency Considerations\n- Efficiency considerations include minimizing memory overhead and optimizing cache utilization. Techniques like memory-sharing backpropagation can significantly reduce the memory overhead by sharing activation memory between layers.\n\n## Hierarchical Normalization in Language Models\n\n### Implementation Details\n- Hierarchical normalization can be implemented by applying normalization techniques at different levels of the hierarchical structure. For example, in transformer-based models, layer normalization can be applied at each layer, and additional normalization techniques can be applied within the attention mechanisms to handle hierarchical inputs.\n\n### Performance Impact\n- Hierarchical normalization can improve the performance of language models by stabilizing the training process and reducing the impact of vanishing or exploding gradients. This is particularly important in deep models where gradients can accumulate and cause instability.\n\n### Memory Usage Patterns\n- The memory usage patterns in models with hierarchical normalization depend on the specific normalization techniques used and the structure of the model. Efficient memory management, such as sharing intermediate cache for back-propagation, can significantly reduce memory overhead.\n\n## Additional Insights and References\n\n- **Parameter Sharing and Training Stability**: Studies on recursive neural networks and tree-structured models highlight the importance of parameter sharing for memory efficiency and training stability. However, the impact of shared parameters on training stability needs further examination.\n\n- **Hardware-Aware Design**: Optimizing models for hardware-specific properties, such as cache-friendly computation patterns and scheduling strategies, is crucial for real efficiency gains. This involves considering algorithms, scheduling strategies, and hardware properties simultaneously.\n\n- **Specific Applications of Hierarchical Normalization**: For language models, hierarchical normalization can be particularly beneficial in models that process hierarchical inputs, such as those involving syntactic or semantic analysis. Implementing normalization at multiple levels can help in capturing complex relationships within the data.\n\nBy focusing on these aspects, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage tree-structured parameter organization, dynamic parameter selection, and hierarchical normalization.\n",
            "analysis": "Summary of Analysis:\n\nThe search results provide valuable insights into memory-efficient normalization and hardware-aware design, but more specific information is needed about hierarchical parameter sharing and dynamic adaptation mechanisms.\n\nKey Useful References and Excerpts:\n\n1. From \"Reducing Fine-Tuning Memory Overhead\":\n\"Memory-Sharing Backpropagation strategy enables activation memory to be shared by two adjacent layers, removing activation memory usage redundancy\"\n\n2. From \"MECTA\":\n\"Major memory overhead comes from intermediate cache for back-propagation, which scales by batch size, channel, and layer number\"\n\n3. From \"Latency-aware Spatial-wise Dynamic Networks\":\n\"The practical latency can be influenced by scheduling strategies and hardware properties... requires simultaneously considering algorithms, scheduling strategies, and hardware properties\"\n\nKey Insights:\n\n1. Memory Efficiency:\n- Memory sharing between layers can significantly reduce overhead\n- Intermediate cache management is crucial for efficiency\n- Batch size and layer structure affect memory usage patterns\n\n2. Hardware Considerations:\n- Scheduling strategies significantly impact practical performance\n- Cache-friendly computation patterns are essential\n- Hardware-specific optimizations needed for real efficiency gains\n\n3. Parameter Sharing:\n- Tree-structured approaches need more investigation\n- Dynamic selection mechanisms require further study\n- Impact on training stability needs examination\n\n4. Implementation Aspects:\n- Memory access patterns crucial for efficiency\n- Cache utilization strategies need optimization\n- Hardware-specific considerations important\n\nFuture Search Plan:\n\nNeed additional searches focusing on:\n1. Tree-structured parameter organization in neural networks\n2. Dynamic parameter selection mechanisms\n3. Training stability with shared parameters\n4. Specific applications of hierarchical normalization in language models\n\nReferences:\n\n[1] Hong, J., et al. (2023). \"MECTA: Memory-Economic Continual Test-Time Model Adaptation.\" International Conference on Learning Representations.\n\n[2] Yang, Y., et al. (2024). \"Reducing Fine-Tuning Memory Overhead by Approximate and Memory-Sharing Backpropagation.\" International Conference on Machine Learning.\n\n[3] Han, Y., et al. (2022). \"Latency-aware Spatial-wise Dynamic Networks.\" Neural Information Processing Systems.\n\n[4] Keutzer, K., et al. (2021). \"I-BERT: Integer-only BERT Quantization.\"\n\n[5] Legresley, P., et al. (2019). \"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.\""
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.9347310000000002,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.0508790000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "RMSNorm",
    "proposal": "\n# HierShare-GPT: Efficient Parameter Sharing through Hierarchical Normalization\n\n## Motivation\n\nCurrent language models face several challenges with their normalization layers:\n1. Parameter inefficiency due to redundant normalization parameters across layers\n2. Limited adaptability to different input characteristics\n3. High memory and computational overhead\n4. Difficulty in maintaining stability across different sequence lengths and domains\n\nHierShare-GPT addresses these challenges by introducing a hierarchical shared normalization scheme that enables efficient parameter sharing while maintaining adaptability to different input contexts.\n\n## Related Work\n\n### Normalization in Language Models\n- RMSNorm provides efficient normalization without mean centering\n- Layer normalization helps stabilize training in deep networks\n- Adaptive normalization techniques show promise but often increase parameter count\n\n### Parameter Sharing\n- Recursive transformers demonstrate benefits of weight sharing across layers\n- Low-rank adaptations enable flexible parameter sharing\n- Hierarchical architectures allow efficient information flow\n\n### Efficient Computation\n- IO-aware implementations improve hardware efficiency\n- Selective computation reduces unnecessary operations\n- Cache-aware designs enhance memory efficiency\n\n## Problem Analysis\n\n### Key Challenges\n\n1. **Parameter Efficiency**:\n   - Standard normalization requires separate parameters for each layer\n   - Redundant parameters capture similar statistical patterns\n   - Need for efficient parameter sharing without sacrificing performance\n\n2. **Adaptability**:\n   - Different inputs require different normalization strategies\n   - Static normalization parameters limit model flexibility\n   - Balance needed between sharing and specialization\n\n3. **Computational Efficiency**:\n   - Memory access patterns affect performance\n   - Cache utilization crucial for efficiency\n   - Need to minimize data movement\n\n### Core Philosophy\n\nThe core idea is to organize normalization parameters in a hierarchical tree structure, enabling selective parameter sharing based on input characteristics while maintaining computational efficiency through careful memory management.\n\n## Design Plan\n\n### Selected GAU to Modify\n\n```selection\nRMSNorm\n```\n\n### Modifications and New Structures\n\n#### 1. Hierarchical Parameter Organization\n\nThe normalization parameters are organized in a tree structure:\n- Root contains shared base parameters\n- Internal nodes capture domain-specific patterns\n- Leaf nodes provide fine-grained adaptability\n\n```python\nclass HierarchicalSharedRMSNorm(nn.Module):\n    def __init__(self, embed_dim, num_levels=3, num_groups=4):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_levels = num_levels\n        self.num_groups = num_groups\n        \n        # Initialize hierarchical parameters\n        self.shared_weight = nn.Parameter(torch.ones(embed_dim))\n        self.level_weights = nn.ParameterList([\n            nn.Parameter(torch.ones(num_groups, embed_dim))\n            for _ in range(num_levels)\n        ])\n        \n        # Parameter selection network\n        self.selector = nn.Sequential(\n            nn.Linear(embed_dim, num_levels * num_groups),\n            nn.Softmax(dim=-1)\n        )\n```\n\n#### 2. Adaptive Parameter Selection\n\nA lightweight network determines which parameters to use based on input characteristics:\n```python\ndef select_parameters(self, x):\n    # Compute input statistics\n    stats = torch.mean(x, dim=1)  # [B, D]\n    \n    # Get selection weights\n    weights = self.selector(stats)  # [B, L*G]\n    weights = weights.view(-1, self.num_levels, self.num_groups)\n    \n    return weights\n```\n\n#### 3. Efficient Computation\n\nOptimize memory access patterns and cache utilization:\n```python\ndef forward(self, x, eps=1e-5):\n    # Compute RMS statistics\n    norm = torch.sqrt(torch.mean(x * x, dim=-1, keepdim=True) + eps)\n    \n    # Select parameters\n    weights = self.select_parameters(x)\n    \n    # Compute weighted parameters\n    effective_weight = self.shared_weight\n    for level in range(self.num_levels):\n        level_contribution = torch.sum(\n            weights[:, level, :].unsqueeze(-1) * \n            self.level_weights[level],\n            dim=1\n        )\n        effective_weight = effective_weight + level_contribution\n    \n    # Apply normalization\n    return x * effective_weight / norm\n```\n\n### Mathematical Formulation\n\n1. **Hierarchical Parameter Composition**:\n   \\[\n   w_{\\text{eff}} = w_{\\text{shared}} + \\sum_{l=1}^L \\sum_{g=1}^G \\alpha_{l,g}(x) w_{l,g}\n   \\]\n   where:\n   - \\(w_{\\text{eff}}\\) is the effective normalization parameter\n   - \\(w_{\\text{shared}}\\) is the shared base parameter\n   - \\(w_{l,g}\\) are group-specific parameters at each level\n   - \\(\\alpha_{l,g}(x)\\) are input-dependent selection weights\n\n2. **Normalization Operation**:\n   \\[\n   y = x \\cdot \\frac{w_{\\text{eff}}}{\\sqrt{\\mathbb{E}[x^2] + \\epsilon}}\n   \\]\n\n3. **Parameter Selection**:\n   \\[\n   \\alpha(x) = \\text{softmax}(f_{\\theta}(\\mathbb{E}[x]))\n   \\]\n   where \\(f_{\\theta}\\) is the selection network.\n\n## Implementation Guidelines\n\n### Step-by-Step Integration\n\n1. **Replace Standard RMSNorm**:\n   - Initialize hierarchical parameters\n   - Set up parameter selection network\n   - Implement efficient forward pass\n\n2. **Memory Management**:\n   - Cache frequently used parameters\n   - Optimize memory access patterns\n   - Use efficient matrix operations\n\n3. **Training Considerations**:\n   - Initialize shared parameters with pretrained values\n   - Gradually introduce hierarchical parameters\n   - Use gradient checkpointing if needed\n\n### Optimization Strategies\n\n1. **Parameter Efficiency**:\n   - Share parameters across similar contexts\n   - Use low-rank approximations where appropriate\n   - Prune redundant parameters\n\n2. **Computational Efficiency**:\n   - Batch similar computations\n   - Minimize memory transfers\n   - Utilize hardware-specific optimizations\n\n3. **Training Stability**:\n   - Use gradient clipping\n   - Implement warm-up periods\n   - Monitor parameter utilization\n\n## Theoretical Analysis\n\n### Computational Complexity\n- Time complexity: O(ND) where N is sequence length and D is embedding dimension\n- Memory complexity: O(LGD) where L is number of levels and G is groups per level\n- Additional overhead from parameter selection: O(BD) where B is batch size\n\n### Parameter Efficiency\n- Total parameters: O(LGD) compared to O(LD) in standard layer-wise normalization\n- Effective parameter count can be much lower due to sharing\n- Adaptive selection enables efficient use of parameters\n\n### Expected Benefits\n1. Reduced parameter count through sharing\n2. Better adaptation to different input types\n3. Improved computational efficiency\n4. Enhanced stability through hierarchical structure\n\n## Conclusion\n\nHierShare-GPT's hierarchical shared normalization scheme offers an efficient solution to the challenges of parameter redundancy and computational overhead in language models. By organizing normalization parameters in a hierarchical structure and employing selective sharing, it achieves:\n- Reduced parameter count through efficient sharing\n- Enhanced adaptability through input-dependent parameter selection\n- Improved computational efficiency through optimized memory access\n- Better stability through hierarchical organization\n\nThe design maintains the benefits of RMSNorm while significantly reducing parameter count and computational overhead, making it particularly suitable for large-scale language models.\n\n## References\n\n[1] Dao, T., Fu, D. Y., Saab, K. K., Thomas, A., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[2] Chronopoulou, A., Peters, M. E., & Dodge, J. (2021). Efficient Hierarchical Domain Adaptation for Pretrained Language Models. North American Chapter of the Association for Computational Linguistics.\n\n[3] Liu, P., Gao, Z. F., Chen, Y., Zhao, X., & Wen, J. R. (2023). Enhancing Scalability of Pre-trained Language Models via Efficient Parameter Sharing. Conference on Empirical Methods in Natural Language Processing.\n\n[4] Pham, H., Guan, M., Zoph, B., Le, Q. V., & Dean, J. (2018). Efficient Neural Architecture Search via Parameter Sharing. International Conference on Machine Learning.\n\n[5] Lu, H., Ding, M., Huo, Y., Yang, G., Lu, Z., Tomizuka, M., & Zhan, W. (2023). UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling. International Conference on Learning Representations.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "HierShare-GPT introduces a novel hierarchical shared normalization scheme that combines efficient parameter sharing with adaptive computation. By organizing normalization parameters in a hierarchical tree structure and employing selective parameter sharing based on input characteristics, it achieves superior performance while significantly reducing parameter count and computational overhead.",
    "ideation": null,
    "modelname": "hiershare_gpt",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of cache management strategies\n- Include concrete examples of parameter sharing patterns\n- Add guidelines for tree structure initialization\n- Specify requirements for different model scales\n\n2. Memory Analysis:\n- Include quantitative analysis of memory savings\n- Provide detailed cache utilization strategies\n- Address potential memory bottlenecks\n- Add guidelines for cache size selection\n\n3. Training Considerations:\n- Add specific guidance for handling gradient flow\n- Include stability analysis during training\n- Provide recommendations for hyperparameter tuning\n- Consider adaptive learning rate strategies\n\n4. Scalability:\n- Address interaction with model parallelism\n- Include analysis of performance at different scales\n- Provide benchmarking guidelines\n- Add recommendations for distributed training\n\n5. Documentation:\n- Add more detailed API specifications\n- Include example configurations\n- Provide debugging guidelines\n- Add performance optimization tips\n\n6. Hardware Considerations:\n- Specify hardware requirements\n- Include optimization strategies for different architectures\n- Provide memory hierarchy recommendations\n- Add cache tuning guidelines",
    "user_input": ""
}