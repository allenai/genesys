{
    "variantname": "DualStateFastTTTLinear",
    "review": "### Comprehensive Review of the SSMFastTTTLinear Proposal\n\n#### 1. Clarity\nThe proposal is clearly articulated, with well-defined objectives and motivations. It provides a detailed explanation of the integration of State Space Models (SSMs) into FastTTTLinear GAUs, including theoretical justifications and mathematical formulations. The design plan is comprehensive, outlining the steps for implementation and the expected benefits of the integration.\n\n#### 2. Innovation\nThe integration of SSMs with FastTTTLinear GAUs is innovative within the context of existing research. While SSMs have been used in other models like Mamba and SPADE, this specific combination with FastTTTLinear is novel. The proposal leverages the strengths of both SSMs and attention mechanisms, aiming to improve long-range dependency modeling while maintaining computational efficiency.\n\n#### 3. Feasibility\nThe proposed design is feasible and can be implemented within the existing framework of FastTTTLinear GAUs. The proposal provides a clear path for integration, including pseudo-code and implementation guidelines. However, attention must be given to potential challenges related to numerical stability and computational overhead.\n\n#### 4. Scalability\nThe design is expected to scale efficiently with larger models and longer sequences, as both SSMs and FastTTTLinear are designed for linear computational complexity. The proposal's focus on efficient state management and parallel processing further supports its scalability.\n\n#### 5. Accuracy and Robustness\nThe integration of SSMs is likely to enhance the model's ability to capture long-range dependencies, potentially improving accuracy on tasks requiring extended context understanding. The proposal also suggests that the model will be more robust to diverse inputs due to the combined modeling capabilities of SSMs and attention mechanisms.\n\n#### 6. Efficiency\nWhile the integration of SSMs may introduce additional computations, the proposal emphasizes maintaining computational efficiency through linear complexity and efficient state management. However, careful optimization is necessary to ensure that the added complexity does not negatively impact efficiency.\n\n### Strengths\n- **Innovative Integration**: The proposal introduces a novel combination of SSMs and FastTTTLinear GAUs, potentially enhancing long-sequence modeling capabilities.\n- **Theoretical Foundation**: The design is grounded in established research, with clear theoretical justifications for the integration.\n- **Scalability and Robustness**: The proposal addresses key challenges in long-sequence modeling, offering a scalable and robust solution.\n\n### Concerns\n- **Numerical Stability**: SSMs can suffer from numerical instability, which must be addressed through appropriate stabilization techniques.\n- **Implementation Complexity**: The integration increases architectural complexity, requiring careful planning and optimization.\n- **Computational Overhead**: Without careful optimization, the added complexity could impact computational efficiency.\n\n### Suggestions for Improvement\n1. **Address Numerical Stability**: Implement stabilization techniques such as reparameterization, layer-wise nonlinearity, and appropriate normalization (e.g., LayerNorm, RMSNorm).\n2. **Optimization Strategies**: Focus on optimizing the implementation to minimize computational overhead, possibly leveraging hardware-aware algorithms and efficient parallelization.\n3. **Empirical Validation**: Plan for rigorous empirical evaluation against benchmarks to validate the proposed benefits and identify any unforeseen issues.\n4. **Detailed Implementation Plan**: Provide more granular implementation details, including parameter initialization strategies and training protocols that have proven effective in prior work.\n\n### Final Rating\nBased on the analysis, the proposal is promising and offers a meaningful advancement in language model design through innovative integration of SSMs into FastTTTLinear GAUs. However, careful attention must be given to addressing potential challenges in implementation and ensuring efficiency.\n\n**Rating: 4.2**",
    "search_stack": [
        {
            "ready": false,
            "query": "adaptive memory neural networks, multi-scale attention transformers, hierarchical neural architectures",
            "detail": "Find papers and implementations discussing:\n1. Adaptive memory mechanisms in neural networks, particularly those that can dynamically adjust memory capacity or organization\n2. Multi-scale or hierarchical attention architectures that efficiently handle different ranges of dependencies\n3. Methods for combining multiple types of attention or memory mechanisms efficiently",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and implementations discussing:\n1. Adaptive memory mechanisms in neural networks, particularly those that can dynamically adjust memory capacity or organization\n2. Multi-scale or hierarchical attention architectures that efficiently handle different ranges of dependencies\n3. Methods for combining multiple types of attention or memory mechanisms efficiently\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.36)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.36)*\n\n```\narXiv preprint arXiv:1910.06611, 2019. Imanol Schlag, Tsendsuren Munkhdalai, and J\u00fcrgen Schmidhuber. Learning associative inference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355-9366. PMLR, 2021. J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131-139, 1992. Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. arXiv preprint arXiv:1812.01243, 2018. Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159-216, 1990. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015. Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In International Conference on Machine Learning, pp. 9902-9912. PMLR, 2021. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, $30,2017$. Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural networks, 1(4):339-356, 1988. Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory.\n```\n\n#### 2. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.33)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 31/54 (Score: 0.33)*\n\n```\n## A. 2 Memory in Machine Learning\n\nMemory in sequence models Sequential or temporal data in areas such as language, reinforcement learning, and continual learning can involve increasingly long dependencies. However, direct parametric modeling cannot handle inputs of unknown and potentially unbounded lengths. Many modern solutions such as attention [70] and dilated convolutions [5], are functions on finite windows, thus sidestepping the need for an explicit memory representation. While this suffices for certain tasks, these approaches can only process a finite context window instead of an entire sequence. Naively increasing the window length poses significant compute and memory challenges. This has spurred various approaches to extend this fixed context window subjected to compute and storage constraints [6, 15, 18, 42, 59, 60, 64, 74]. We instead focus on the core problem of online processing and memorization of continuous and discrete signals, and anticipate that the study of this foundational problem will be useful in improving a variety of models. Recurrent memory Recurrent neural networks are a natural tool for modeling sequential data online, with the appealing property of having unbounded context; in other words they can summarize history indefinitely. However, due to difficulties in the optimization process (vanishing/exploding gradients [56]), particular care must be paid to endow them with longer memory. The ubiquitous LSTM 34 and simplifications such as the GRU [17] control the update with gates to smooth the optimization process. With more careful parametrization, the addition of gates alone make RNNs significantly more robust and able to address long-term dependencies [31. Tallec and Ollivier [66] show that gates are in fact fundamental for recurrent dynamics by allowing time dilations. Many other approaches to endowing RNNs with better memory exist, such as noise injection [32] or non-saturating gates [9], which can suffer from instability issues.\n```\n\n#### 3. Attention with Bounded-memory Control (Avg. Score: 0.28)\n\n*Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, Noah A. Smith*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 21  (*Influential: 2*)\n\n**TL;DR:** This work shows that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and it outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.\n\n**Abstract:** Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.\n\n##### *Relevant Chunk: No. 4/39 (Score: 0.28)*\n\n```\n4. \u00a7B. 2 presents a detailed derivation. Connections to other prior works. Although starting from distinct motivations, $\\mathrm{ABC}_{\\text {MLP }}$ closely relates to hierarchical attention (HA; Yang et al., 2016). HA summarizes the context into higherlevel representations with a cascade of attention mechanisms, e.g., words to sentences, and then to documents. $\\mathrm{ABC}_{\\text {MLP }}$ applies two types of attention. The first learns context-agnostic pseudo-queries and attends to the same sequence for $n$ times in parallel, while the second retrieves from the memory with real queries. HA, in contrast, summarizes non-overlapping segments at each level. The learned pseudo-queries closely relate to the inducing point method in set attention (ISA; Lee et al., 2019). ISA applies a non-linear feedforward network between a cascade of two attention mod- ules. This precludes the outer-product memory computation and efficient recurrences in $A B C$. Another line of work \"linearizes\" attention through kernel tricks and also applies bounded memory: their feature map dimensions are analogous to memory sizes. They substitute the softmax with approximations (Peng et al., 2021; Choromanski et al., 2021), heuristically designed (Katharopoulos et al., 2020; Schlag et al., 2021), or learned (Kasai et al., 2021b) functions. $\\mathrm{ABC}_{\\text {MLP }}$ keeps the softmax, but over a smaller constant-sized context. This can be useful in practice: (1) ABC provides a unified perspective of several efficient attention methods, allowing for borrowing from existing wisdom to design new architectures; (2) it draws a close analogy to the canonical softmax attention, and is better-suited as its drop-in substitute in various application settings, as we will show in the experiments; (3) empirically, we find that $\\mathrm{ABC}_{\\text {MLP }}$ can get away with a much smaller memory size to retain the accuracy.\n```\n\n#### 4. \u221e-former: Infinite Memory Transformer (Avg. Score: 0.20)\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins*\n\n**Published in:** Volume 1 (2022)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** N/A\n\n##### *Relevant Chunk: No. 17/32 (Score: 0.20)*\n\n```\nIn Proc. $A C L$. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. 2020. Location Attention for Extrapolation to Longer Sequences. In Proc. ACL. Angela Fan, Claire Gardent, Chlo\u00e9 Braud, and Antoine Bordes. 2021. Augmenting Transformers with KNN-Based Composite Memory for Dialog. Transactions of the Association for Computational Linguistics. Ant\u00f3nio Farinhas, Andr\u00e9 F. T. Martins, and P. Aguiar. 2021. Multimodal Continuous Visual Attention Mechanisms. Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016. Improving Neural Language Models with a Continuous Cache. In Proc. ICLR. Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. 2015. Learning to transduce with unbounded memory. Proc. NeurIPS. Maosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaussian Transformer: A Lightweight Approach for Natural Language Inference.\n```\n\n#### 5. Repeat After Me: Transformers are Better than State Space Models at Copying (Avg. Score: 0.17)\n\n*Samy Jelassi, David Brandfonbrener, S. Kakade, Eran Malach*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 4*)\n\n**TL;DR:** It is proved that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state, and a fundamental gap between transformers and GSSMs on tasks of practical interest is suggested.\n\n**Abstract:** Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as\"generalized state space models\"(GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.17)*\n\n```\narXiv preprint arXiv:2301.10743, 2023. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dao, T., Fu, D., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Del\u00e9tang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L. K., Catt, E., Hutter, M., Legg, S., and Ortega, P. A. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098, 2022. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Edelman, B. L., Goel, S., Kakade, S., and Zhang, C. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pp. 5793-5831. PMLR, 2022. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800 gb dataset of diverse text for language modeling.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive memory neural networks, multi-scale attention transformers, hierarchical neural architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Pruning Deep Neural Networks from a Sparsity Perspective\n\n*From Search Query: adaptive memory neural networks*\n\n*Enmao Diao, G. Wang, Jiawei Zhan, Yuhong Yang, Jie Ding, V. Tarokh*\n\n**TL;DR:** The proposed PQ Index (PQI) is proposed to measure the potential compressibility of deep neural networks and used to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm that is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.\n\n**Abstract:** In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 2. Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks\n\n*From Search Query: adaptive memory neural networks*\n\n*Qi Xu, Yuyuan Gao, Jiangrong Shen, Yaxin Li, Xuming Ran, Huajin Tang, Gang Pan*\n\n**TL;DR:** This paper develops a recurrent spiking neural network model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns.\n\n**Abstract:** Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS). Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points. In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns. It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination. The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets. The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 3. Robust Training of Neural Networks using Scale Invariant Architectures\n\n*From Search Query: adaptive memory neural networks*\n\n*Zhiyuan Li, Srinadh Bhojanapalli, M. Zaheer, Sashank J. Reddi, Surinder Kumar*\n\n**TL;DR:** A scale invariant version of BERT is designed, called SIBERT, which when trained simply by vanilla SGD achieves performance comparable to BERT trained by adaptive methods like Adam on downstream tasks.\n\n**Abstract:** In contrast to SGD, adaptive gradient methods like Adam allow robust training of modern deep networks, especially large language models. However, the use of adaptivity not only comes at the cost of extra memory but also raises the fundamental question: can non-adaptive methods like SGD enjoy similar benefits? In this paper, we provide an affirmative answer to this question by proposing to achieve both robust and memory-efficient training via the following general recipe: (1) modify the architecture and make it scale invariant, i.e. the scale of parameter doesn't affect the output of the network, (2) train with SGD and weight decay, and optionally (3) clip the global gradient norm proportional to weight norm multiplied by $\\sqrt{\\tfrac{2\\lambda}{\\eta}}$, where $\\eta$ is learning rate and $\\lambda$ is weight decay. We show that this general approach is robust to rescaling of parameter and loss by proving that its convergence only depends logarithmically on the scale of initialization and loss, whereas the standard SGD might not even converge for many initializations. Following our recipe, we design a scale invariant version of BERT, called SIBERT, which when trained simply by vanilla SGD achieves performance comparable to BERT trained by adaptive methods like Adam on downstream tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 23  (*Influential: 1*)\n\n#### 4. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: multi-scale attention transformers*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 193  (*Influential: 28*)\n\n#### 5. Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting\n\n*From Search Query: multi-scale attention transformers*\n\n*Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang, Qingsong Wen, Bin Yang, Chenjuan Guo*\n\n**TL;DR:** This work proposes Pathformer, a multi-scale Transformer with adaptive pathways, which integrates both temporal resolution and temporal distance for multi-scale modeling and exhibits stronger generalization abilities under various transfer scenarios.\n\n**Abstract:** Transformers for time series forecasting mainly model time series from limited or fixed scales, making it challenging to capture different characteristics spanning various scales. We propose Pathformer, a multi-scale Transformer with adaptive pathways. It integrates both temporal resolution and temporal distance for multi-scale modeling. Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies. We further enrich the multi-scale Transformer with adaptive pathways, which adaptively adjust the multi-scale modeling process based on the varying temporal dynamics of the input, improving the accuracy and generalization of Pathformer. Extensive experiments on eleven real-world datasets demonstrate that Pathformer not only achieves state-of-the-art performance by surpassing all current models but also exhibits stronger generalization abilities under various transfer scenarios. The code is made available at https://github.com/decisionintelligence/pathformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 16  (*Influential: 1*)\n\n#### 6. HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction\n\n*From Search Query: multi-scale attention transformers*\n\n*Zalan Fabian, M. Soltanolkotabi*\n\n**TL;DR:** HUMUS-Net is proposed, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network that establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset.\n\n**Abstract:** In accelerated MRI reconstruction, the anatomy of a patient is recovered from a set of under-sampled and noisy measurements. Deep learning approaches have been proven to be successful in solving this ill-posed inverse problem and are capable of producing very high quality reconstructions. However, current architectures heavily rely on convolutions, that are content-independent and have difficulties modeling long-range dependencies in images. Recently, Transformers, the workhorse of contemporary natural language processing, have emerged as powerful building blocks for a multitude of vision tasks. These models split input images into non-overlapping patches, embed the patches into lower-dimensional tokens and utilize a self-attention mechanism that does not suffer from the aforementioned weaknesses of convolutional architectures. However, Transformers incur extremely high compute and memory cost when 1) the input image resolution is high and 2) when the image needs to be split into a large number of patches to preserve fine detail information, both of which are typical in low-level vision problems such as MRI reconstruction, having a compounding effect. To tackle these challenges, we propose HUMUS-Net, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network. HUMUS-Net extracts high-resolution features via convolutional blocks and refines low-resolution features via a novel Transformer-based multi-scale feature extractor. Features from both levels are then synthesized into a high-resolution output reconstruction. Our network establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset. We further demonstrate the performance of HUMUS-Net on two other popular MRI datasets and perform fine-grained ablation studies to validate our design.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 34  (*Influential: 3*)\n\n#### 7. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: hierarchical neural architectures*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 2*)\n\n#### 8. Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars\n\n*From Search Query: hierarchical neural architectures*\n\n*Simon Schrodi, Daniel Stoll, Binxin Ru, R. Sukthanker, T. Brox, F. Hutter*\n\n**TL;DR:** This work introduces a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature.\n\n**Abstract:** The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Code is available at https://github.com/automl/hierarchical_nas_construction.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 9. Differentiable hierarchical and surrogate gradient search for spiking neural networks\n\n*From Search Query: hierarchical neural architectures*\n\n*Kaiwei Che, Luziwei Leng, Kaixuan Zhang, Jianguo Zhang, Qinghu Meng, Jie Cheng, Qinghai Guo, Jianxing Liao*\n\n**TL;DR:** A spike-based differentiable hierarchical search (SpikeDHS) framework, where spike- based computation is realized on both the cell and the layer level search space, which finds effective SNN architectures under limited computation cost and exceeds the accuracy of specially designed ANNs meanwhile.\n\n**Abstract:** Spiking neural network (SNN) has been viewed as a potential candidate for the next generation of artificial intelligence with appealing characteristics such as sparse computation and inherent temporal dynamics. By adopting architectures of deep artificial neural networks (ANNs), SNNs are achieving competitive performances in benchmark tasks such as image classification. However, successful architectures of ANNs are not necessary ideal for SNN and when tasks become more diverse effective architectural variations could be critical. To this end, we develop a spike-based differentiable hierarchical search (SpikeDHS) framework, where spike-based computation is realized on both the cell and the layer level search space. Based on this framework, we find effective SNN architectures under limited computation cost. During the training of SNN, a suboptimal surrogate gradient function could lead to poor approximations of true gradients, making the network enter certain local minima. To address this problem, we extend the differential approach to surrogate gradient search where the SG function is efficiently optimized locally. Our models achieve state-of-the-art performances on classification of CIFAR10/100 and ImageNet with accuracy of 95.50%, 76.25% and 68.64%. On event-based deep stereo, our method finds optimal layer variation and surpasses the accuracy of specially designed ANNs meanwhile with 26 \u00d7 lower energy cost ( 6 . 7mJ ), demonstrating the advantage of SNN in processing highly sparse and dynamic signals. Codes are available at https://github.com/Huawei-BIC/SpikeDHS .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 31  (*Influential: 3*)\n\n### 1 related papers from ArXiv\n\n#### 1. Language Model Pre-training for Hierarchical Document Representations\n\n*From Search Query: hierarchical neural architectures*\n\n*Ming-Wei Chang, Kristina Toutanova, Kenton Lee, Jacob Devlin*\n\n**Abstract:** Hierarchical neural architectures are often used to capture long-distance\ndependencies and have been applied to many document-level tasks such as\nsummarization, document segmentation, and sentiment analysis. However,\neffective usage of such a large context can be difficult to learn, especially\nin the case where there is limited labeled data available. Building on the\nrecent success of language model pretraining methods for learning flat\nrepresentations of text, we propose algorithms for pre-training hierarchical\ndocument representations from unlabeled data. Unlike prior work, which has\nfocused on pre-training contextual token representations or context-independent\n{sentence/paragraph} representations, our hierarchical document representations\ninclude fixed-length sentence/paragraph representations which integrate\ncontextual information from the entire documents. Experiments on document\nsegmentation, document-level question answering, and extractive document\nsummarization demonstrate the effectiveness of the proposed pre-training\nalgorithms.\n\n**Published:** 2019-01-26T00:35:35Z  (*Updated: 2019-01-26T00:35:35Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. MoCA: Memory-Centric, Adaptive Execution for Multi-Tenant Deep Neural Networks\n\n*From Search Query: adaptive memory neural networks*\n\n*Yakun Sophia Shao, Borivoje Nikoli\u0107, Krste Asanovi\u0107, Vadim Vadimovich Nikiforov, Hasan Genc, Seah Kim*\n\n**Abstract:** Driven by the wide adoption of deep neural networks (DNNs) across different application domains, multi-tenancy execution, where multiple DNNs are deployed simultaneously on the same hardware, has been proposed to satisfy the latency requirements of different applications while improving the overall system utilization. However, multi-tenancy execution could lead to undesired system-level resource contention, causing quality-of-service (QoS) degradation for latency-critical applications. To address this challenge, we propose MoCA, an adaptive multi-tenancy system for DNN accelerators. Unlike existing solutions that focus on compute resource partition, MoCA dynamically manages shared memory resources of co-located applications to meet their QoS targets. Specifically, MoCA leverages the regularities in both DNN operators and accelerators to dynamically modulate memory access rates based on their latency targets and user-defined priorities so that co-located applications get the resources they demand without significantly starving their co-runners. We demonstrate that MoCA improves the satisfaction rate of the service level agreement (SLA) up to 3.9x (1.8x average), system throughput by 2.3x (1.7x average), and fairness by 1.3x (1.2x average), compared to prior work.\n\n**Published:** 2023-05-10\n\n\n\n#### 2. State-Frequency Memory Recurrent Neural Networks\n\n*From Search Query: adaptive memory neural networks*\n\n*Guo-Jun Qi, Hao Hu*\n\n**Abstract:** \n    Modeling temporal sequences plays a fundamental role in various modern applications and has drawn more and more attentions in the machine learning community. Among those efforts on improving the capability to represent temporal data, the Long Short-Term Memory (LSTM) has achieved great success in many areas. Although the LSTM can capture long-range dependency in the time domain, it does not explicitly model the pattern occurrences in the frequency domain that plays an important role in tracking and predicting data points over various time cycles. We propose the State-Frequency Memory (SFM), a novel recurrent architecture that allows to separate dynamic patterns across different frequency components and their impacts on modeling the temporal contexts of input sequences. By jointly decomposing memorized dynamics into state-frequency components, the SFM is able to offer a fine-grained analysis of temporal sequences by capturing the dependency of uncovered patterns in both time and frequency domains. Evaluations on several temporal modeling tasks demonstrate the SFM can yield competitive performances, in particular as compared with the state-of-the-art LSTM models.\n  \n\n**Proceeding:** icml-2017-8\n\n**Published:** 2017-08-01\n\n\n\n#### 3. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\n\n*From Search Query: multi-scale attention transformers*\n\n*Rameswar Panda, Quanfu Fan, Chun-Fu Chen*\n\n**Abstract:** The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at \\url{https://github.com/IBM/CrossViT}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-27\n\n\n\n#### 4. Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention\n\n*From Search Query: multi-scale attention transformers*\n\n*Ming Wu, Chuang Zhang, Haotian Yan*\n\n**Abstract:** Multi-scale representations are crucial for semantic segmentation. The community has witnessed the flourish of semantic segmentation convolutional neural networks (CNN) exploiting multi-scale contextual information. Motivated by that the vision transformer (ViT) is powerful in image classification, some semantic segmentation ViTs are recently proposed, most of them attaining impressive results but at a cost of computational economy. In this paper, we succeed in introducing multi-scale representations into semantic segmentation ViT via window attention mechanism and further improves the performance and efficiency. To this end, we introduce large window attention which allows the local window to query a larger area of context window at only a little computation overhead. By regulating the ratio of the context area to the query area, we enable the $\\textit{large window attention}$ to capture the contextual information at multiple scales. Moreover, the framework of spatial pyramid pooling is adopted to collaborate with $\\textit{the large window attention}$, which presents a novel decoder named $\\textbf{la}$rge $\\textbf{win}$dow attention spatial pyramid pooling (LawinASPP) for semantic segmentation ViT. Our resulting ViT, Lawin Transformer, is composed of an efficient hierachical vision transformer (HVT) as encoder and a LawinASPP as decoder. The empirical results demonstrate that Lawin Transformer offers an improved efficiency compared to the existing method. Lawin Transformer further sets new state-of-the-art performance on Cityscapes (84.4% mIoU), ADE20K (56.2% mIoU) and COCO-Stuff datasets. The code will be released at https://github.com/yan-hao-tian/lawin\n\n**Published:** 2022-01-05\n\n\n\n#### 5. Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n\n*From Search Query: hierarchical neural architectures*\n\n*Li Fei-Fei, Alan Yuille, Florian Schroff, Liang-Chieh Chen, Wei Hua, Hartwig Adam, Chenxi Liu*\n\n**Abstract:** Recently, Neural Architecture Search (NAS) has successfully identified neural\nnetwork architectures that exceed human designed ones on large-scale image\nclassification. In this paper, we study NAS for semantic image segmentation.\nExisting works often focus on searching the repeatable cell structure, while\nhand-designing the outer network structure that controls the spatial resolution\nchanges. This choice simplifies the search space, but becomes increasingly\nproblematic for dense image prediction which exhibits a lot more network level\narchitectural variations. Therefore, we propose to search the network level\nstructure in addition to the cell level structure, which forms a hierarchical\narchitecture search space. We present a network level search space that\nincludes many popular designs, and develop a formulation that allows efficient\ngradient-based architecture search (3 P100 GPU days on Cityscapes images). We\ndemonstrate the effectiveness of the proposed method on the challenging\nCityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our\narchitecture searched specifically for semantic image segmentation, attains\nstate-of-the-art performance without any ImageNet pretraining.\n\n**Conference:** auto-deeplab-hierarchical-neural-architecture-1\n\n**Published:** 2019-01-10\n\n\n\n#### 6. Hierarchical Neural Architecture Search for Deep Stereo Matching\n\n*From Search Query: hierarchical neural architectures*\n\n*ZongYuan Ge, Hongdong Li, Tom Drummond, Xiaojun Chang, Yuchao Dai, Mehrtash Harandi, Yiran Zhong, Xuelian Cheng*\n\n**Abstract:** To reduce the human efforts in neural network design, Neural Architecture Search (NAS) has been applied with remarkable success to various high-level vision tasks such as classification and semantic segmentation. The underlying idea for the NAS algorithm is straightforward, namely, to enable the network the ability to choose among a set of operations (e.g., convolution with different filter sizes), one is able to find an optimal architecture that is better adapted to the problem at hand. However, so far the success of NAS has not been enjoyed by low-level geometric vision tasks such as stereo matching. This is partly due to the fact that state-of-the-art deep stereo matching networks, designed by humans, are already sheer in size. Directly applying the NAS to such massive structures is computationally prohibitive based on the currently available mainstream computing resources. In this paper, we propose the first end-to-end hierarchical NAS framework for deep stereo matching by incorporating task-specific human knowledge into the neural architecture search framework. Specifically, following the gold standard pipeline for deep stereo matching (i.e., feature extraction -- feature volume construction and dense matching), we optimize the architectures of the entire pipeline jointly. Extensive experiments show that our searched network outperforms all state-of-the-art deep stereo matching architectures and is ranked at the top 1 accuracy on KITTI stereo 2012, 2015 and Middlebury benchmarks, as well as the top 1 on SceneFlow dataset with a substantial improvement on the size of the network and the speed of inference. The code is available at https://github.com/XuelianCheng/LEAStereo.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive memory mechanisms, multi-scale attention architectures, and efficient combinations of these components, here are some relevant findings and suggestions based on the provided sources and additional insights:\n\n## Adaptive Memory Mechanisms\n\n1. **MEMENTO Approach**:\n   The paper on \"Memory-Enhanced Neural Solvers for Efficient Adaptation\" introduces a method called MEMENTO, which leverages memory to adapt neural solvers at inference time. This approach dynamically updates the action distribution based on previous decisions, using a memory module to store and retrieve relevant data. This concept can be adapted to language models to enhance their ability to retain and adaptively filter information, especially in long-sequence modeling.\n\n2. **Adaptive Continuous Attractor Neural Networks (A-CANNs)**:\n   The review on A-CANNs discusses how these networks incorporate adaptation to destabilize otherwise stable attractor states, allowing for both reliable information representation and efficient state updates. This mechanism could be integrated into language models to create adaptive memory systems that can adjust their memory capacity or organization based on the complexity of the input sequences.\n\n## Multi-scale Attention Architectures\n\n1. **Hybrid Attention Mechanisms**:\n   The Mega paper introduces a gated attention mechanism with moving averages to incorporate position-aware local dependencies. This can be a starting point for developing multi-scale attention mechanisms. By combining different types of attention (e.g., local and global attention), you can efficiently handle different ranges of dependencies. For example, using local attention for short-range dependencies and global attention for long-range dependencies could enhance the model's expressiveness while maintaining efficiency.\n\n2. **Hierarchical Attention**:\n   Hierarchical neural architectures can be designed to incorporate multi-scale attention. For instance, using a hierarchical structure where different layers or modules focus on different scales of dependencies can help in efficiently handling various ranges of dependencies. This approach aligns with the idea of selective state updates and hierarchical state organization mentioned in the analysis note.\n\n## Combining Multiple Types of Attention or Memory Mechanisms\n\n1. **Combining Adaptive Memory and Attention**:\n   Integrating adaptive memory mechanisms like those in MEMENTO or A-CANNs with multi-scale attention architectures can enhance the model's ability to retain and process information efficiently. For example, using an adaptive memory module to store context information and then applying multi-scale attention to this context can improve the model's performance on long-sequence tasks.\n\n2. **Efficient Implementations**:\n   To maintain computational efficiency, techniques like linear attention variants and State Space Models (SSMs) can be combined with adaptive memory and multi-scale attention. The SUPRA method mentioned in the Linear LLMs paper, which uptrains existing transformers into RNNs, could be adapted to incorporate these advanced memory and attention mechanisms while keeping the computational complexity in check.\n\n## Additional Considerations\n\n- **Hopfield Networks**:\n  While not directly applicable to autoregressive language models, Hopfield networks provide insights into associative memory and content-addressable memory systems. These concepts could be adapted to create more sophisticated memory mechanisms in language models, particularly in how they store and retrieve information.\n\n- **Sensory-Memory Interacting Networks**:\n  The study on sensory-memory interacting networks highlights the importance of modular structures in efficiently encoding and maintaining information. This modular approach could be applied to language models by separating the model into different modules, each handling different aspects of memory and attention, thus improving overall performance and efficiency.\n\nBy integrating these concepts\u2014adaptive memory mechanisms, multi-scale attention architectures, and efficient hierarchical structures\u2014the researcher can develop an autoregressive language model that is more robust, efficient, and scalable.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe seed design SSMFastTTTLinear integrates State Space Models (SSMs) with FastTTTLinear GAU to improve long-sequence modeling. From analyzing the seed design and initial references, several key areas emerge for potential innovation:\n\na) Attention Mechanisms:\n- Current designs like Mega show promise in using moving average and gated attention\n- Linear attention variants offer efficiency but may sacrifice expressiveness\n- Opportunity exists for novel hybrid attention mechanisms\n\nb) Memory and State Management:\n- SSMs provide efficient state tracking but may have limitations in selective memory retention\n- Need for more sophisticated memory mechanisms that can adaptively filter and retain information\n\nc) Efficiency vs Expressiveness Trade-off:\n- Linear transformers sacrifice some modeling power for efficiency\n- Opportunity to develop mechanisms that maintain expressiveness while keeping computational efficiency\n\n2. Useful References with Excerpts\n\nFrom Mega paper:\n\"Mega introduces a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.\"\n\nKey implementation details:\n```python\ndef forward(self, x, encoder_padding_mask):\n    x, _ = self.mega_layer(x, encoder_padding_mask)\n    if self.nffn is not None:\n        x = self.nffn(x)\n    return x\n```\n\nFrom Linear LLMs paper:\n\"As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget.\"\n\nFrom Mamba paper:\n\"Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner.\"\n\n3. Key Insights and Analysis\n\na) Memory Management Innovation:\n- Current SSM approaches use fixed state dimensions\n- Opportunity to create adaptive state sizing based on sequence complexity\n- Potential for hierarchical state organization\n\nb) Attention Mechanism Enhancement:\n- Moving average attention shows promise for efficiency\n- Could combine with selective state updates for better expressiveness\n- Potential for multi-scale attention patterns\n\nc) Computational Efficiency:\n- Linear attention and SSMs both aim for O(n) complexity\n- Need to maintain efficiency while adding sophisticated features\n- Hardware considerations important for practical deployment\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in adaptive memory mechanisms\n2. Multi-scale attention architectures\n3. Efficient implementations of hierarchical structures in neural networks\n\n5. References\n\n1. Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., & Zettlemoyer, L. (2022). Mega: Moving Average Equipped Gated Attention. International Conference on Learning Representations.\n\n2. Mercat, J.P., Vasiljevic, I., Keh, S.S., Arora, K., Dave, A., Gaidon, A., & Kollar, T. (2024). Linearizing Large Language Models. arXiv preprint arXiv:2401.XXXXX.\n\n3. Ali, A., Zimerman, I., & Wolf, L. (2024). The Hidden Attention of Mamba Models. arXiv preprint arXiv:2401.XXXXX.\n\n4. Shao, N., Xiao, S., Liu, Z., & Zhang, P. (2024). Extensible Embedding: A Flexible Multipler For LLM's Context Length. arXiv preprint arXiv:2401.XXXXX."
        },
        {
            "ready": false,
            "query": "adaptive compression neural networks, hierarchical attention implementation, hardware efficient memory management",
            "detail": "Find papers and implementations discussing:\n1. Adaptive compression techniques for neural network memory\n2. Efficient implementations of hierarchical attention, particularly those combining local and global attention\n3. Hardware-efficient approaches to memory management in neural architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and implementations discussing:\n1. Adaptive compression techniques for neural network memory\n2. Efficient implementations of hierarchical attention, particularly those combining local and global attention\n3. Hardware-efficient approaches to memory management in neural architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.71)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.71)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Reformer: The Efficient Transformer (Avg. Score: 0.67)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.67)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n#### 3. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.60)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.60)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n#### 4. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.43)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.60)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.26)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive compression neural networks, hierarchical attention implementation, hardware efficient memory management\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Adaptive Estimators Show Information Compression in Deep Neural Networks\n\n*From Search Query: adaptive compression neural networks*\n\n*Ivan Chelombiev, Conor J. Houghton, Cian O\u2019Donnell*\n\n**TL;DR:** More robust mutual information estimation techniques are developed, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions, which explore compression in networks with a range of different activation functions.\n\n**Abstract:** To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 7*)\n\n#### 2. RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks\n\n*From Search Query: adaptive compression neural networks*\n\n*Edouard Yvinec, Arnaud Dapogny, M. Cord, K\u00e9vin Bailly*\n\n**TL;DR:** Red is presented, a data-free structured, unified approach to tackle structured pruning ofDeep Neural Networks by proposing a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors.\n\n**Abstract:** Deep Neural Networks (DNNs) are ubiquitous in today's computer vision land-scape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to re-train the model. In this paper, we present RED, a data-free structured, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 0*)\n\n#### 3. Entropy and mutual information in models of deep neural networks\n\n*From Search Query: adaptive compression neural networks*\n\n*Marylou Gabri\u00e9, Andre Manoel, Cl\u00e9ment Luneau, Jean Barbier, N. Macris, Florent Krzakala, L. Zdeborov\u00e1*\n\n**TL;DR:** It is concluded that, in the proposed setting, the relationship between compression and generalization remains elusive and an experiment framework with generative models of synthetic datasets is proposed, on which deep neural networks are trained with a weight constraint designed so that the assumption in (i) is verified during learning.\n\n**Abstract:** We examine a class of stochastic deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) we show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 167  (*Influential: 3*)\n\n#### 4. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention implementation*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4301  (*Influential: 554*)\n\n#### 5. HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level\n\n*From Search Query: hierarchical attention implementation*\n\n*Haoran Luo, E. Haihong, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin*\n\n**TL;DR:** A novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention, that addresses the issue of HKG multi-position prediction for the first time and achieves state-of-the-art performance in link prediction tasks on HKG standard datasets.\n\n**Abstract:** Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs\u2019 representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 6. D\u00e9j\u00e0Vu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving\n\n*From Search Query: hardware efficient memory management*\n\n*F. Strati, Sara Mcallister, Amar Phanishayee, Jakub Tarnawski, Ana Klimovic*\n\n**TL;DR:** This paper proposes and implements efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance, and highlights the efficacy of these solutions on a range of large models across cloud deployments.\n\n**Abstract:** Distributed LLM serving is costly and often underutilizes hardware accelerators due to three key challenges: bubbles in pipeline-parallel deployments caused by the bimodal latency of prompt and token processing, GPU memory overprovisioning, and long recovery times in case of failures. In this paper, we propose D\\'ej\\`aVu, a system to address all these challenges using a versatile and efficient KV cache streaming library (D\\'ej\\`aVuLib). Using D\\'ej\\`aVuLib, we propose and implement efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance. We highlight the efficacy of these solutions on a range of large models across cloud deployments.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 10  (*Influential: 2*)\n\n#### 7. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient memory management*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 8. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: hardware efficient memory management*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Context-adaptive neural network based prediction for image compression\n\n*From Search Query: adaptive compression neural networks*\n\n*Thierry Dumas, Aline Roumy, Christine Guillemot*\n\n**Abstract:** This paper describes a set of neural network architectures, called Prediction Neural Networks Set (PNNS), based on both fully-connected and convolutional neural networks, for intra image prediction. The choice of neural network for predicting a given image block depends on the block size, hence does not need to be signalled to the decoder. It is shown that, while fully-connected neural networks give good performance for small block sizes, convolutional neural networks provide better predictions in large blocks with complex textures. Thanks to the use of masks of random sizes during training, the neural networks of PNNS well adapt to the available context that may vary, depending on the position of the image block to be predicted. When integrating PNNS into a H.265 codec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained. These gains are on average 0.99% larger than those of prior neural network based methods. Unlike the H.265 intra prediction modes, which are each specialized in predicting a specific texture, the proposed PNNS can model a large set of complex textures.\n\n**Published:** 2018-07-17\n\n\n\n#### 2. Accelerating Deep Unsupervised Domain Adaptation with Transfer Channel Pruning\n\n*From Search Query: adaptive compression neural networks*\n\n*Jindong Wang, Chaohui Yu, Zijing Wu, Yiqiang Chen*\n\n**Abstract:** Deep unsupervised domain adaptation (UDA) has recently received increasing\nattention from researchers. However, existing methods are computationally\nintensive due to the computation cost of Convolutional Neural Networks (CNN)\nadopted by most work. To date, there is no effective network compression method\nfor accelerating these models. In this paper, we propose a unified Transfer\nChannel Pruning (TCP) approach for accelerating UDA models. TCP is capable of\ncompressing the deep UDA model by pruning less important channels while\nsimultaneously learning transferable features by reducing the cross-domain\ndistribution divergence. Therefore, it reduces the impact of negative transfer\nand maintains competitive performance on the target task. To the best of our\nknowledge, TCP is the first approach that aims at accelerating deep UDA models.\nTCP is validated on two benchmark datasets-Office-31 and ImageCLEF-DA with two\ncommon backbone networks-VGG16 and ResNet50. Experimental results demonstrate\nthat TCP achieves comparable or better classification accuracy than other\ncomparison methods while significantly reducing the computational cost. To be\nmore specific, in VGG16, we get even higher accuracy after pruning 26% floating\npoint operations (FLOPs); in ResNet50, we also get higher accuracy on half of\nthe tasks after pruning 12% FLOPs. We hope that TCP will open a new door for\nfuture research on accelerating transfer learning models.\n\n**Published:** 2019-03-25\n\n\n\n#### 3. DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection\n\n*From Search Query: hierarchical attention implementation*\n\n*Heung-Yeung Shum, Lionel M. Ni, Jun Zhu, Hang Su, Lei Zhang, Shilong Liu, Feng Li, Hao Zhang*\n\n**Abstract:** We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising anch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in this paper. DINO improves over previous DETR-like models in performance and efficiency by using a contrastive way for denoising training, a mixed query selection method for anchor initialization, and a look forward twice scheme for box prediction. DINO achieves $49.4$AP in $12$ epochs and $51.3$AP in $24$ epochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a significant improvement of $\\textbf{+6.0}$\\textbf{AP} and $\\textbf{+2.7}$\\textbf{AP}, respectively, compared to DN-DETR, the previous best DETR-like model. DINO scales well in both model size and data size. Without bells and whistles, after pre-training on the Objects365 dataset with a SwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017} ($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev} (\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO significantly reduces its model size and pre-training data size while achieving better results. Our code will be available at \\url{https://github.com/IDEACVR/DINO}.\n\n**Conference:** dino-detr-with-improved-denoising-anchor\n\n**Published:** 2022-03-07\n\n\n\n#### 4. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention implementation*\n\n*Pavlo Molchanov, Jan Kautz, Jose M. Alvarez, Andrew Tao, Hongxu Yin, Greg Heinrich, Ali Hatamizadeh*\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Published:** 2023-06-09\n\n\n\n#### 5. FedML Parrot: A Scalable Federated Learning System via Heterogeneity-aware Scheduling on Sequential and Hierarchical Training\n\n*From Search Query: hardware efficient memory management*\n\n*Chaoyang He, Salman Avestimehr, Alex Qiaozhong Liang, Yuxin Wang, Yonggang Zhang, Shaohuai Shi, Sunwoo Lee, Ryan Yide Ran, Xiaowen Chu, Zhenheng Tang*\n\n**Abstract:** Federated Learning (FL) enables collaborations among clients for train machine learning models while protecting their data privacy. Existing FL simulation platforms that are designed from the perspectives of traditional distributed training, suffer from laborious code migration between simulation and production, low efficiency, low GPU utility, low scalability with high hardware requirements and difficulty of simulating stateful clients. In this work, we firstly demystify the challenges and bottlenecks of simulating FL, and design a new FL system named as FedML \\texttt{Parrot}. It improves the training efficiency, remarkably relaxes the requirements on the hardware, and supports efficient large-scale FL experiments with stateful clients by: (1) sequential training clients on devices; (2) decomposing original aggregation into local and global aggregation on devices and server respectively; (3) scheduling tasks to mitigate straggler problems and enhance computing utility; (4) distributed client state manager to support various FL algorithms. Besides, built upon our generic APIs and communication interfaces, users can seamlessly transform the simulation into the real-world deployment without modifying codes. We evaluate \\texttt{Parrot} through extensive experiments for training diverse models on various FL datasets to demonstrate that \\texttt{Parrot} can achieve simulating over 1000 clients (stateful or stateless) with flexible GPU devices setting ($4 \\sim 32$) and high GPU utility, 1.2 $\\sim$ 4 times faster than FedScale, and 10 $\\sim$ 100 times memory saving than FedML. And we verify that \\texttt{Parrot} works well with homogeneous and heterogeneous devices in three different clusters. Two FL algorithms with stateful clients and four algorithms with stateless clients are simulated to verify the wide adaptability of \\texttt{Parrot} to different algorithms.\n\n**Published:** 2023-03-03\n\n\n\n#### 6. Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory Manycore Clusters\n\n*From Search Query: hardware efficient memory management*\n\n*Anonymous*\n\n**Abstract:** Systolic arrays and shared-L1-memory manycore clusters are commonly used architectural paradigms that offer different trade-offs to accelerate parallel workloads. While the first excel with regular dataflow at the cost of rigid architectures and complex programming models, the second are versatile and easy to program but require explicit dataflow management and synchronization. This work aims at enabling efficient systolic execution on shared-L1-memory manycore clusters. We devise a flexible architecture where small and energy-efficient RISC-V cores act as the systolic array's processing elements (PEs) and can form diverse, reconfigurable systolic topologies through queues mapped in the cluster's shared memory. We introduce two low-overhead RISC-V ISA extensions for efficient systolic execution, namely Xqueue and Queue-linked registers (QLRs), which support queue management in hardware. The Xqueue extension enables single-instruction access to shared-memory-mapped queues, while QLRs allow implicit and autonomous access to them, relieving the cores of explicit communication instructions. We demonstrate Xqueue and QLRs in MemPool, an open-source shared-memory cluster with 256 PEs, and analyze the hybrid systolic-shared-memory architecture's trade-offs on several DSP kernels with diverse arithmetic intensity. For an area increase of just 6%, our hybrid architecture can double MemPool's compute unit utilization, reaching up to 73%. In typical conditions (TT/0.80V/25{\\deg}C), in a 22 nm FDX technology, our hybrid architecture runs at 600 MHz with no frequency degradation and is up to 65% more energy efficient than the shared-memory baseline, achieving up to 208 GOPS/W, with up to 63% of power spent in the PEs.\n\n**Published:** 2024-02-20\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive compression techniques, efficient hierarchical attention implementations, and hardware-efficient memory management, here are some key findings and references:\n\n## Adaptive Compression Techniques for Neural Network Memory\n\n- The paper on \"FusionLLM: A Decentralized LLM Training System on Geo-distributed GPUs with Adaptive Compression\" introduces an adaptive compression technique specifically designed for decentralized training of large language models. It proposes an \"AdaTopK compressor\" that adaptively compresses intermediate activations and gradients at the slowest communication links, which can be beneficial for reducing memory and communication overheads in distributed training settings.\n\n## Efficient Implementations of Hierarchical Attention\n\n- The concept of \"Infini-attention\" from the paper \"Leave No Context Behind\" is relevant. It incorporates both masked local attention and long-term linear attention mechanisms in a single Transformer block, which can be seen as a hierarchical approach to attention. This method integrates compressive memory into the attention mechanism, allowing for efficient handling of both local and global context [Analysis Note].\n\n- The \"Pathformer\" model uses a multi-scale division approach, dividing the time series into different temporal resolutions using patches of various sizes. It performs dual attention over these patches to capture both global correlations and local details, which is a hierarchical attention mechanism suitable for capturing dependencies at various scales[Analysis Note].\n\n- The \"Mega\" model incorporates the inductive bias of position-aware local dependencies into a position-agnostic attention mechanism, which can be seen as a way to combine local and global attention efficiently[Analysis Note].\n\n## Hardware-Efficient Approaches to Memory Management\n\n- The \"FusionLLM\" system also addresses hardware efficiency by representing the model as a directed acyclic graph of operators (OP-DAG) and using an OP-Fence scheduler to cluster devices with similar bandwidths together. This approach helps in optimizing the use of hardware resources and reducing the straggler problem in distributed training.\n\n- The concept of \"bounded-memory control\" from the paper \"Attention with Bounded-memory Control\" is crucial for hardware-efficient memory management. It connects several efficient attention variants and ensures that the memory usage is controlled and efficient, which is essential for practical implementations[Analysis Note].\n\n- Hierarchical memory structures and dynamic adjustment of memory capacity, as hinted in the analysis note, can be achieved through mechanisms like those proposed in \"HiPPO: Recurrent Memory with Optimal Polynomial Projections,\" which uses a hierarchical memory structure for efficient information retention[Analysis Note].\n\nBy integrating these concepts\u2014adaptive compression, hierarchical attention mechanisms, and bounded-memory control\u2014the researchers can develop an autoregressive language model that is not only efficient in terms of memory usage but also scalable and robust.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nFrom analyzing the search results, several promising directions emerge for enhancing the SSMFastTTTLinear GAU:\n\na) Memory Management:\n- Adaptive memory mechanisms that can dynamically adjust capacity and organization\n- Hierarchical memory structures for efficient information retention\n- Integration of bounded-memory control for efficiency\n\nb) Multi-scale Processing:\n- Combining different attention ranges and types\n- Hierarchical architectures for capturing dependencies at various scales\n- Efficient implementation of multi-scale features\n\nc) Efficiency Considerations:\n- Bounded memory approaches for practical implementation\n- Linear-time complexity while maintaining expressiveness\n- Hardware-aware optimizations\n\n2. Useful References with Excerpts\n\nFrom \"Leave No Context Behind\":\n\"The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"\n\nFrom \"Attention with Bounded-memory Control\":\n\"ABC reveals new, unexplored possibilities... connects several efficient attention variants that would otherwise seem apart.\"\n\nFrom \"Pathformer\":\n\"Multi-scale division divides the time series into different temporal resolutions using patches of various sizes. Based on the division of each scale, dual attention is performed over these patches to capture global correlations and local details as temporal dependencies.\"\n\nFrom \"Mega\":\n\"Moving Average Equipped Gated Attention... incorporates inductive bias of position-aware local dependencies into the position-agnostic attention mechanism.\"\n\n3. Key Insights and Analysis\n\na) Memory Architecture:\n- Potential for hierarchical memory organization with different levels of compression\n- Dynamic adjustment of memory capacity based on input complexity\n- Integration of bounded memory control for efficiency\n\nb) Multi-scale Processing:\n- Combination of local and global attention mechanisms\n- Use of different patch sizes for multi-scale feature extraction\n- Adaptive pathway selection based on input characteristics\n\nc) Implementation Efficiency:\n- Need for careful balance between expressiveness and computational cost\n- Importance of bounded memory approaches\n- Potential for hardware-aware optimizations\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in adaptive compression techniques for neural networks\n2. Efficient implementations of hierarchical attention mechanisms\n3. Hardware-efficient memory management strategies in neural networks\n\n5. References\n\n1. Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv preprint.\n\n2. Peng, H., et al. (2021). Attention with Bounded-memory Control. Annual Meeting of the Association for Computational Linguistics.\n\n3. Chen, P., et al. (2024). Pathformer: Multi-scale Transformers with Adaptive Pathways for Time Series Forecasting. International Conference on Learning Representations.\n\n4. Ma, X., et al. (2022). Mega: Moving Average Equipped Gated Attention. International Conference on Learning Representations.\n\n5. Gu, A., et al. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. Neural Information Processing Systems.\n\n6. Jelassi, S., et al. (2024). Repeat After Me: Transformers are Better than State Space Models at Copying. arXiv preprint."
        },
        {
            "ready": false,
            "query": "adaptive state space models, hybrid attention mechanisms, hardware optimized memory management",
            "detail": "Find papers and implementations discussing:\n1. Adaptive state space models and their efficient implementations\n2. Hybrid attention mechanisms combining different types of attention\n3. Hardware-optimized memory management techniques specifically for state space models and attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and implementations discussing:\n1. Adaptive state space models and their efficient implementations\n2. Hybrid attention mechanisms combining different types of attention\n3. Hardware-optimized memory management techniques specifically for state space models and attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.95)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 0.95)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.94)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.94)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.91)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.91)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 4. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.87)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.87)*\n\n```\nFinally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code ${ }^{1}$ and pre-trained model checkpoints ${ }^{2}$ are publicly available. ## 2 Background\n\n### 2.1 Attention Mechanism\n\nSuppose the input to the layer is $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, where $L$ is the sequence length and $d$ is the embedding dimension, then the attention mechanism outputs\n\n$$\n\\operatorname{Attn}(\\mathbf{X})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}$. Here $\\mathbf{W}_{q}, \\mathbf{W}_{k}, \\mathbf{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix $\\mathbf{A}=$ $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\in \\mathbb{R}^{L \\times L}$. Then, $\\mathbf{A}_{i j}$ captures the alignment between the $i$-th and the $j$-th input tokens. ### 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a 1-dimensional input signal $u(t)$ to a $d_{s}$-dimensional latent state $x(t)$, after which $x(t)$ is mapped to a 1-dimensional output signal $y(t)$. Concretely,\n\n$$\nx^{\\prime}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nHere, $\\mathbf{A} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{B} \\in \\mathbb{R}^{d_{s}}$ and $\\mathbf{C} \\in \\mathbb{R}^{d_{s}}$. Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$\n\n[^1]cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, highorder polynomial projection operators) are proposed to initialize A. The HiPPO matrices are designed such that the state $x(t)$ at time $t$ can memorize the history of the input $u(t)$ up to time $t$. Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, where $L$ is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size $\\Delta$, such that\n\n$$\n\\begin{aligned}\n& x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k} \\\\\n& \\text { where } \\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1}(\\mathbf{I}+\\Delta / 2 \\cdot \\mathbf{A}) \\\\\n& \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1} \\Delta \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n\\end{aligned}\n$$\n\nWe unroll the above recurrent representation, after which we have\n\n$$\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\cdots+\\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C B}} u_{k}\n$$\n\nThis can be written as a convolutional representation $y=\\overline{\\mathbf{K}} * u$, where the convolution kernel\n\n$$\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\cdots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nHere, \" $*$ \" is the discrete convolution operator, $u$ represents the input sequence $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, and $y$ represents the corresponding output sequence $\\left(y_{0}, y_{1}, \\cdots, y_{L}\\right)$.\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.85)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.85)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive state space models, hybrid attention mechanisms, hardware optimized memory management\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models\n\n*From Search Query: adaptive state space models*\n\n*Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu*\n\n**TL;DR:** This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass, and introduces a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre- trained knowledge during the pruning process.\n\n**Abstract:** The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. TADAM: Task dependent adaptive metric for improved few-shot learning\n\n*From Search Query: adaptive state space models*\n\n*Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, Alexandre Lacoste*\n\n**TL;DR:** This work identifies that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms and proposes and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space.\n\n**Abstract:** Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100. Our code is publicly available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1237  (*Influential: 202*)\n\n#### 3. DMix: Adaptive Distance-aware Interpolative Mixup\n\n*From Search Query: adaptive state space models*\n\n*Ramit Sawhney, Megh Thakkar, Shrey Pandit, Ritesh Soun, Di Jin, Diyi Yang, Lucie Flek*\n\n**TL;DR:** DMix is proposed, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space that leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.\n\n**Abstract:** Interpolation-based regularisation methods such as Mixup, which generate virtual training samples, have proven to be effective for various tasks and modalities.We extend Mixup and propose DMix, an adaptive distance-aware interpolative Mixup that selects samples based on their diversity in the embedding space. DMix leverages the hyperbolic space as a similarity measure among input samples for a richer encoded representation.DMix achieves state-of-the-art results on sentence classification over existing data augmentation methods on 8 benchmark datasets across English, Arabic, Turkish, and Hindi languages while achieving benchmark F1 scores in 3 times less number of iterations.We probe the effectiveness of DMix in conjunction with various similarity measures and qualitatively analyze the different components.DMix being generalizable, can be applied to various tasks, models and modalities.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 2*)\n\n#### 4. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid attention mechanisms*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios\n\n*From Search Query: hybrid attention mechanisms*\n\n*Zhongzhan Huang, Mingfu Liang, Shan Zhong, Liang Lin*\n\n**Abstract:** We propose the attention-inspired numerical solver (AttNS), a concise method that helps the generalization and robustness issues faced by the AI-Hybrid numerical solver in solving differential equations due to limited data. AttNS is inspired by the effectiveness of attention modules in Residual Neural Networks (ResNet) in enhancing model generalization and robustness for conventional deep learning tasks. Drawing from the dynamical system perspective of ResNet, We seamlessly incorporate attention mechanisms into the design of numerical methods tailored for the characteristics of solving differential equations. Our results on benchmarks, ranging from high-dimensional problems to chaotic systems, show-case AttNS consistently enhancing various numerical solvers without any intricate model crafting. Finally, we analyze AttNS experimentally and theoretically, demonstrating its ability to achieve strong generalization and robustness while ensuring the convergence of the solver. This includes requiring less data compared to other advanced methods to achieve comparable generalization errors and better prevention of numerical explosion issues when solving differential equations.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention\n\n*From Search Query: hybrid attention mechanisms*\n\n*Ekta Sood, Simon Tannert, Philipp Mueller, A. Bulling*\n\n**TL;DR:** This work proposes a novel hybrid text saliency model that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Abstract:** A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing(NLP). We propose a novel hybrid text saliency model(TSM) that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 65  (*Influential: 4*)\n\n#### 7. RMM: Reinforced Memory Management for Class-Incremental Learning\n\n*From Search Query: hardware optimized memory management*\n\n*Yaoyao Liu, B. Schiele, Qianru Sun*\n\n**Abstract:** Class-Incremental Learning (CIL) [40] trains classi\ufb01ers under a strict memory budget: in each incremental phase, learning is done for new data, most of which is abandoned to free space for the next phase. The preserved data are exemplars used for replaying. However, existing methods use a static and ad hoc strategy for memory allocation, which is often sub-optimal. In this work, we propose a dynamic memory management strategy that is optimized for the incremental phases and different object classes. We call our method reinforced memory management (RMM), leveraging reinforcement learning. RMM training is not naturally compatible with CIL as the past, and future data are strictly non-accessible during the incremental phases. We solve this by training the policy function of RMM on pseudo CIL tasks, e.g., the tasks built on the data of the 0 -th phase, and then applying it to target tasks. RMM propagates two levels of actions: Level-1 determines how to split the memory between old and new classes, and Level-2 allocates memory for each speci\ufb01c class. In essence, it is an optimizable and general method for memory management that can be used in any replaying-based CIL method. For evaluation, we plug RMM into two top-performing baselines (LUCIR+AANets and POD+AANets [30]) and conduct experiments on three benchmarks (CIFAR-100, ImageNet-Subset, and ImageNet-Full). Our results show clear improvements, e.g., boosting POD+AANets by 3 . 6% , 4 . 4% , and 1 . 9% in the 25 -Phase settings of the above benchmarks, respectively. The code is available at https://class-il.mpi-inf.mpg.de/rmm\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 75  (*Influential: 4*)\n\n#### 8. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware optimized memory management*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 9. LLM in a flash: Efficient Large Language Model Inference with Limited Memory\n\n*From Search Query: hardware optimized memory management*\n\n*Keivan Alizadeh-Vahid, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, C. C. D. Mundo, Mohammad Rastegari, Mehrdad Farajtabar*\n\n**TL;DR:** The integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.\n\n**Abstract:** Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First,\"windowing\"strategically reduces data transfer by reusing previously activated neurons, and second,\"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 60  (*Influential: 8*)\n\n### 2 related papers from ArXiv\n\n#### 1. Semantic-Unit-Based Dilated Convolution for Multi-Label Text\n  Classification\n\n*From Search Query: hybrid attention mechanisms*\n\n*Junyang Lin, Qi Su, Pengcheng Yang, Shuming Ma, Xu Sun*\n\n**Abstract:** We propose a novel model for multi-label text classification, which is based\non sequence-to-sequence learning. The model generates higher-level semantic\nunit representations with multi-level dilated convolution as well as a\ncorresponding hybrid attention mechanism that extracts both the information at\nthe word-level and the level of the semantic unit. Our designed dilated\nconvolution effectively reduces dimension and supports an exponential expansion\nof receptive fields without loss of local information, and the\nattention-over-attention mechanism is able to capture more summary relevant\ninformation from the source context. Results of our experiments show that the\nproposed model has significant advantages over the baseline models on the\ndataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is\ncompetitive to the deterministic hierarchical models and it is more robust to\nclassifying low-frequency labels.\n\n**Published:** 2018-08-26T14:36:22Z  (*Updated: 2018-11-11T19:12:35Z*)\n\n\n\n#### 2. MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to\n  Enhance Medical Image Segmentation\n\n*From Search Query: hybrid attention mechanisms*\n\n*Gurucharan Marthi Krishna Kumar, Aman Chadha, Janine Mendola, Amir Shmuel*\n\n**Abstract:** Large Language Models (LLMs), known for their versatility in textual data,\nare increasingly being explored for their potential to enhance medical image\nsegmentation, a crucial task for accurate diagnostic imaging. This study\nexplores enhancing Vision Transformers (ViTs) for medical image segmentation by\nintegrating pre-trained LLM transformer blocks. Our approach, which\nincorporates a frozen LLM transformer block into the encoder of a ViT-based\nmodel, leads to substantial improvements in segmentation performance across\nvarious medical imaging modalities. We propose a Hybrid Attention Mechanism\nthat combines global and local feature learning with a Multi-Scale Fusion Block\nfor aggregating features across different scales. The enhanced model shows\nsignificant performance gains, including an average Dice score increase from\n0.74 to 0.79 and improvements in accuracy, precision, and the Jaccard Index.\nThese results demonstrate the effectiveness of LLM-based transformers in\nrefining medical image segmentation, highlighting their potential to\nsignificantly boost model accuracy and robustness. The source code and our\nimplementation are available at: https://bit.ly/3zf2CVs\n\n**Published:** 2024-10-03T14:50:33Z  (*Updated: 2024-10-04T14:19:33Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class-Incremental Learning\n\n*From Search Query: adaptive state space models*\n\n*Min Zhang, Liqiang Nie, Bernard Ghanem, Jianlong Wu, Yibo Yang, Xiaojie Li*\n\n**Abstract:** Few-shot class-incremental learning (FSCIL) confronts the challenge of integrating new classes into a model with minimal training samples while preserving the knowledge of previously learned classes. Traditional methods widely adopt static adaptation relying on a fixed parameter space to learn from data that arrive sequentially, prone to overfitting to the current session. Existing dynamic strategies require the expansion of the parameter space continually, leading to increased complexity. In this study, we explore the potential of Selective State Space Models (SSMs) for FSCIL, leveraging its dynamic weights and strong ability in sequence modeling to address these challenges. Concretely, we propose a dual selective SSM projector that dynamically adjusts the projection parameters based on the intermediate features for dynamic adaptation. The dual design enables the model to maintain the robust features of base classes, while adaptively learning distinctive feature shifts for novel classes. Additionally, we develop a class-sensitive selective scan mechanism to guide dynamic adaptation. It minimizes the disruption to base-class representations caused by training on novel data, and meanwhile, forces the selective scan to perform in distinct patterns between base and novel classes. Experiments on miniImageNet, CUB-200, and CIFAR-100 demonstrate that our framework outperforms the existing state-of-the-art methods. The code is available at \\url{https://github.com/xiaojieli0903/Mamba-FSCIL}.\n\n**Published:** 2024-07-08\n\n\n\n#### 2. Adaptive Probabilistic ODE Solvers Without Adaptive Memory Requirements\n\n*From Search Query: adaptive state space models*\n\n*Nicholas Kr\u00e4mer*\n\n**Abstract:** Despite substantial progress in recent years, probabilistic solvers with adaptive step sizes can still not solve memory-demanding differential equations -- unless we care only about a single point in time (which is far too restrictive; we want the whole time series). Counterintuitively, the culprit is the adaptivity itself: Its unpredictable memory demands easily exceed our machine's capabilities, making our simulations fail unexpectedly and without warning. Still, dropping adaptivity would abandon years of progress, which can't be the answer. In this work, we solve this conundrum. We develop an adaptive probabilistic solver with fixed memory demands building on recent developments in robust state estimation. Switching to our method (i) eliminates memory issues for long time series, (ii) accelerates simulations by orders of magnitude through unlocking just-in-time compilation, and (iii) makes adaptive probabilistic solvers compatible with scientific computing in JAX.\n\n**Published:** 2024-10-14\n\n\n\n#### 3. Hybrid intelligence for dynamic job-shop scheduling with deep reinforcement learning and attention mechanism\n\n*From Search Query: hybrid attention mechanisms*\n\n*Bo Yuan, Xiu Li, Rong Wang, Yuanzhi Dai, Zijun Liao, Yunhui Zeng*\n\n**Abstract:** The dynamic job-shop scheduling problem (DJSP) is a class of scheduling tasks that specifically consider the inherent uncertainties such as changing order requirements and possible machine breakdown in realistic smart manufacturing settings. Since traditional methods cannot dynamically generate effective scheduling strategies in face of the disturbance of environments, we formulate the DJSP as a Markov decision process (MDP) to be tackled by reinforcement learning (RL). For this purpose, we propose a flexible hybrid framework that takes disjunctive graphs as states and a set of general dispatching rules as the action space with minimum prior domain knowledge. The attention mechanism is used as the graph representation learning (GRL) module for the feature extraction of states, and the double dueling deep Q-network with prioritized replay and noisy networks (D3QPN) is employed to map each state to the most appropriate dispatching rule. Furthermore, we present Gymjsp, a public benchmark based on the well-known OR-Library, to provide a standardized off-the-shelf facility for RL and DJSP research communities. Comprehensive experiments on various DJSP instances confirm that our proposed framework is superior to baseline algorithms with smaller makespan across all instances and provide empirical justification for the validity of the various components in the hybrid framework.\n\n**Published:** 2022-01-03\n\n\n\n#### 4. A Hybrid Attention Mechanism for Weakly-Supervised Temporal Action Localization\n\n*From Search Query: hybrid attention mechanisms*\n\n*Richard Radke, Chengjiang Long, Ashraful Islam*\n\n**Abstract:** Weakly supervised temporal action localization is a challenging vision task due to the absence of ground-truth temporal locations of actions in the training videos. With only video-level supervision during training, most existing methods rely on a Multiple Instance Learning (MIL) framework to predict the start and end frame of each action category in a video. However, the existing MIL-based approach has a major limitation of only capturing the most discriminative frames of an action, ignoring the full extent of an activity. Moreover, these methods cannot model background activity effectively, which plays an important role in localizing foreground activities. In this paper, we present a novel framework named HAM-Net with a hybrid attention mechanism which includes temporal soft, semi-soft and hard attentions to address these issues. Our temporal soft attention module, guided by an auxiliary background class in the classification module, models the background activity by introducing an \"action-ness\" score for each video snippet. Moreover, our temporal semi-soft and hard attention modules, calculating two attention scores for each video snippet, help to focus on the less discriminative frames of an action to capture the full action boundary. Our proposed approach outperforms recent state-of-the-art methods by at least 2.2% mAP at IoU threshold 0.5 on the THUMOS14 dataset, and by at least 1.3% mAP at IoU threshold 0.75 on the ActivityNet1.2 dataset. Code can be found at: https://github.com/asrafulashiq/hamnet.\n\n**Published:** 2021-01-03\n\n\n\n#### 5. Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions\n\n*From Search Query: hardware optimized memory management*\n\n*Andrew Adams, Nicolas Vasilache, Priya Goyal, Sven Verdoolaege, Albert Cohen, Zachary DeVito, William S. Moses, Theodoros Theodoridis, Oleksandr Zinenko*\n\n**Abstract:** Deep learning models with convolutional and recurrent networks are now\nubiquitous and analyze massive amounts of audio, image, video, text and graph\ndata, with applications in automatic translation, speech-to-text, scene\nunderstanding, ranking user preferences, ad placement, etc. Competing\nframeworks for building these networks such as TensorFlow, Chainer, CNTK,\nTorch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between\nusability and expressiveness, research or production orientation and supported\nhardware. They operate on a DAG of computational operators, wrapping\nhigh-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for\nvarious CPUs), and automate memory allocation, synchronization, distribution.\nCustom operators are needed where the computation does not fit existing\nhigh-performance library calls, usually at a high engineering cost. This is\nfrequently required when new operators are invented by researchers: such\noperators suffer a severe performance penalty, which limits the pace of\ninnovation. Furthermore, even if there is an existing runtime call these\nframeworks can use, it often doesn't offer optimal performance for a user's\nparticular network architecture and dataset, missing optimizations between\noperators as well as optimizations that can be done knowing the size and shape\nof data. Our contributions include (1) a language close to the mathematics of\ndeep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time\ncompiler to convert a mathematical description of a deep learning DAG into a\nCUDA kernel with delegated memory management and synchronization, also\nproviding optimizations such as operator fusion and specialization for specific\nsizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]\n\n**Published:** 2018-02-13\n\n\n\n#### 6. Intel nGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning\n\n*From Search Query: hardware optimized memory management*\n\n*Sandeep Aswath Narayana, Christian Convey, Matthew Brookhart, Varun Kumar, Scott Cyphers, Robert Kimball, Jennifer Myers, Jaikrishnan Menon, Yixing Lao, Omar Kanawi, Leona Cook, Avijit Chakraborty, Anahita Bhiwandiwalla, Adam Procter, Will Constable, Tristan J. Webb, Christopher R. Lishka, Arjun K. Bansal, Nikolay Korovaiko, Jayaram Bobba, Jason Knight*\n\n**Abstract:** The Deep Learning (DL) community sees many novel topologies published each\nyear. Achieving high performance on each new topology remains challenging, as\neach requires some level of manual effort. This issue is compounded by the\nproliferation of frameworks and hardware platforms. The current approach, which\nwe call \"direct optimization\", requires deep changes within each framework to\nimprove the training performance for each hardware backend (CPUs, GPUs, FPGAs,\nASICs) and requires $\\mathcal{O}(fp)$ effort; where $f$ is the number of\nframeworks and $p$ is the number of platforms. While optimized kernels for\ndeep-learning primitives are provided via libraries like Intel Math Kernel\nLibrary for Deep Neural Networks (MKL-DNN), there are several compiler-inspired\nways in which performance can be further optimized. Building on our experience\ncreating neon (a fast deep learning library on GPUs), we developed Intel\nnGraph, a soon to be open-sourced C++ library to simplify the realization of\noptimized deep learning performance across frameworks and hardware platforms.\nInitially-supported frameworks include TensorFlow, MXNet, and Intel neon\nframework. Initial backends are Intel Architecture CPUs (CPU), the Intel(R)\nNervana Neural Network Processor(R) (NNP), and NVIDIA GPUs. Currently supported\ncompiler optimizations include efficient memory management and data layout\nabstraction. In this paper, we describe our overall architecture and its core\ncomponents. In the future, we envision extending nGraph API support to a wider\nrange of frameworks, hardware (including FPGAs and ASICs), and compiler\noptimizations (training versus inference optimizations, multi-node and\nmulti-device scaling via efficient sub-graph partitioning, and HW-specific\ncompounding of operations).\n\n**Published:** 2018-01-24\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive state space models, hybrid attention mechanisms, and hardware-optimized memory management, here are some key findings and references that align with your goals:\n\n## Adaptive State Space Models\n\n- The integration of state space models (SSMs) with other architectures has shown promising results in efficiency and scalability. For instance, the SC-MAMBA2 model combines SSMs with the MAMBA framework, enabling efficient and scalable modeling of large gene sequences and handling long context lengths effectively. This approach overcomes the computational efficiency limitations of traditional Transformer-based architectures.\n- The MatMamba model, which combines Matryoshka-style learning with Mamba2 state space models, allows for efficient and adaptive deployment across various model sizes. This model scales well for language and vision tasks and enables the flexible extraction of submodels, preserving the metric space of the original model.\n\n## Hybrid Attention Mechanisms\n\n- Hybrid attention mechanisms can be designed by combining different types of attention. For example, the concept of gated linear attention (GLA) can be integrated into Transformer architectures. GLA Transformers have been shown to perform competitively against other baselines, offering a balance between expressiveness and efficiency[Analysis Note, b) Attention Mechanisms].\n- Hierarchical attention structures can also be employed for multi-scale processing. This approach allows the model to capture dependencies at different scales, which is crucial for long-range dependency modeling[Analysis Note, b) Attention Mechanisms].\n\n## Hardware-Optimized Memory Management\n\n- Low-rank approximations and adaptive compression of the KV cache are effective techniques for reducing memory footprint while maintaining performance. The Loki approach to low-rank approximation for efficient sparse attention is a notable example[Analysis Note, 1. Singhania et al.].\n- Hardware-aware memory management strategies are crucial for practical deployment. Techniques such as prompt-token disaggregation, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance, as proposed in D\u00e9j\u00e0Vu, can significantly improve memory efficiency[Analysis Note, 3. Strati et al.].\n- Efficient convolution implementations, such as those described in \"Simple Hardware-Efficient Long Convolutions,\" can speed up convolutions and allow training on longer sequences, which is beneficial for sequence modeling tasks[Analysis Note, 4. Fu et al.].\n\n### Specific Implementations and References\n\n- **SC-MAMBA2**: This model integrates state space models with the MAMBA framework, enabling efficient modeling of large sequences and outperforming traditional Transformer-based models in single-cell transcriptomics tasks.\n- **MatMamba**: This model combines Matryoshka-style learning with Mamba2 state space models, allowing for adaptive deployment and efficient extraction of submodels. It scales well across various model sizes and tasks.\n- **Loki**: This method uses low-rank approximations for efficient sparse attention, reducing the memory footprint while maintaining performance[Analysis Note, 1. Singhania et al.].\n- **Gated Linear Attention (GLA) Transformers**: These models integrate gated mechanisms into linear attention, offering competitive performance and efficiency[Analysis Note, 2. Yang et al.].\n- **D\u00e9j\u00e0Vu**: This approach implements efficient prompt-token disaggregation, microbatch swapping, and state replication for fault-tolerant and efficient GPU memory management[Analysis Note, 3. Strati et al.].\n\nBy leveraging these advancements in adaptive state space models, hybrid attention mechanisms, and hardware-optimized memory management, researchers can develop more efficient, scalable, and performant autoregressive language models.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nFrom analyzing the search results, several promising directions emerge for enhancing the SSMFastTTTLinear GAU:\n\na) Memory Efficiency:\n- Loki's approach to low-rank approximation for efficient sparse attention\n- Adaptive KV cache compression techniques\n- Hardware-aware memory management strategies\n\nb) Attention Mechanisms:\n- Gated Linear Attention (GLA) with hardware-efficient implementation\n- Hierarchical attention structures for multi-scale processing\n- Efficient long-range dependency modeling\n\nc) Implementation Efficiency:\n- IO-aware algorithms for better hardware utilization\n- Bounded memory approaches\n- Efficient convolution implementations for long sequences\n\n2. Useful References with Excerpts\n\nFrom Loki paper:\n\"Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space.\"\n\nFrom GLA paper:\n\"The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\"\n\nFrom D\u00e9j\u00e0Vu:\n\"Using D\u00e9j\u00e0VuLib, we propose and implement efficient prompt-token disaggregation to reduce pipeline bubbles, microbatch swapping for efficient GPU memory management, and state replication for fault-tolerance.\"\n\nFrom Simple Hardware-Efficient Long Convolutions:\n\"FlashButterfly speeds up convolutions by 2.2\u00d7, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2\u00d7 faster than prior work.\"\n\n3. Key Insights and Analysis\n\na) Memory Management:\n- Low-rank approximations can significantly reduce memory footprint while maintaining performance\n- Adaptive compression of KV cache shows promise for efficient inference\n- Hardware-aware memory management crucial for practical deployment\n\nb) Attention Design:\n- Gated mechanisms can improve expressiveness while maintaining efficiency\n- Hierarchical structures enable multi-scale processing\n- Linear attention variants can be competitive with proper optimization\n\nc) Implementation Considerations:\n- IO-aware algorithms crucial for performance\n- Bounded memory approaches necessary for scalability\n- Hardware-efficient implementations can significantly improve throughput\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in adaptive state space models\n2. Efficient implementations of hybrid attention mechanisms\n3. Hardware-optimized memory management techniques for state space models\n\n5. References\n\n1. Singhania, P., Singh, S., He, S., Feizi, S., & Bhatele, A. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint.\n\n2. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n3. Strati, F., Mcallister, S., Phanishayee, A., Tarnawski, J., & Klimovic, A. (2024). D\u00e9j\u00e0Vu: KV-cache Streaming for Fast, Fault-tolerant Generative LLM Serving. International Conference on Machine Learning.\n\n4. Fu, D.Y., Epstein, E.L., Nguyen, E.N.D., Thomas, A., Zhang, M., Dao, T., Rudra, A., & R\u00e9, C. (2023). Simple Hardware-Efficient Long Convolutions for Sequence Modeling. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "state space duality implementation, parallel processing SSM, hybrid attention integration",
            "detail": "Find papers and implementations discussing:\n1. Practical implementations and applications of state space duality in neural networks\n2. Efficient parallel processing techniques specifically for state space models\n3. Methods for integrating and optimizing hybrid attention mechanisms in neural architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and implementations discussing:\n1. Practical implementations and applications of state space duality in neural networks\n2. Efficient parallel processing techniques specifically for state space models\n3. Methods for integrating and optimizing hybrid attention mechanisms in neural architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.98)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 1.00)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n##### *Relevant Chunk: No. 7/86 (Score: 0.96)*\n\n```\n2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms. ### 2.4 Overview: Structured State Space Duality\n\nWhile this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically. Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM (2). The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:\n\n- The structure on $A$ is further simplified from diagonal to scalar times identity structure. Each $A_{t}$ can also be identified with just a scalar in this case. - We use a larger head dimension $P$, compared to $P=1$ used in Mamba. Typically $P=\\{64,128\\}$ is chosen which is similar to conventions for modern Transformers. Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators. Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as\n\n$$\n\\left(L \\circ Q K^{\\top}\\right) \\cdot V \\quad L_{i j}= \\begin{cases}a_{i} \\times \\cdots \\times a_{j+1} & i \\geq j \\\\ 0 & i<j\\end{cases}\n$$\n\nwhere $a_{i}$ are input-dependent scalars bounded in $[0,1]$. Compared to standard softmax attention, there are two main differences\n\n- The softmax is dropped. - The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$. Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the \"attention sink\" phenomenon (Darcet et al. 2024; Xiao et al. 2024). More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time. More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4. Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form $Y=M X$ for a matrix $M_{\\theta} \\in \\mathbb{R}^{(T, T)}$ that depends on $\\theta=(A, B, C)$. In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$. Going beyond these, any algorithm for multiplication by $M$ can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs (Gu and Dao 2023); Listing 1 provides a complete implementation in a few lines of code.\n```\n\n#### 3. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.88)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.88)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 4. Long Range Language Modeling via Gated State Spaces (Avg. Score: 0.84)\n\n*Harsh Mehta, Ankit Gupta, Ashok Cutkosky, Behnam Neyshabur*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 134  (*Influential: 17*)\n\n**TL;DR:** This work proposes a new layer named Gated State Space (GSS) and shows that it trains significantly faster than the diagonal version of S4 on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement.\n\n**Abstract:** State space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further.\n\n##### *Relevant Chunk: No. 2/28 (Score: 0.84)*\n\n```\nwhere decoding every token requires attending to the whole past. The ideal model is parallelizable at training time but incurs a small constant cost (per decoded token) at inference time. This brings us to the final point. Due to the inherent convolution-recurrence equivalence of the state space model, it can be made to accumulate state and unroll like an RNN at inference time without any approximations. Despite these attractive properties, we found that current state space models (such as S4, DSS) run slower than we expected at training time on TPUs, our accelerator of choice. We take this opportunity to modify the architecture to reduce dimensionality of specific operations which we found to be bottlenecks. Our proposed changes borrow from a well-supported empirical observation around the effectiveness of gating units [Shazeer, 2020]. Specifically, Hua et al. [2022] observed that replacing the typical Feed-Forward layer in the Transformer with gating units allows for a reduced dimensionality when mixing tokens along the length dimension using self-attention. We extend the use of gating units to state space model family and observe that, even in our context, the use of gating units allows for a reduction in dimensionality when performing FFT operations, which we observed to be the main bottleneck behind slow training. Furthermore, somewhat contrary to observations made by S4 and DSS authors, we found the performance of the model on language modeling tasks to be much less sensitive to initialization. We found that only the scale and structural aspects of initialization of state space variables were important and not the exact values. We were able to successfully train the model while initializing the state space variables randomly. This departs significantly, at least in understanding, from the reliance of the design on the theory of HiPPO matrices, which led the S 4 model to employ several numerical linear algebra tricks to able to make it work. Combining both of these contributions, we propose a layer named Gated State Space (GSS) (Figure 1), which we empirically verified to be $2-3 \\times$ faster than DSS while keeping the perplexity on several language modeling benchmarks (Table 1). Going one step further, we also perform an apples-to-apples comparison with well-tuned and performant baselines reported in Block Recurrent Transformers [Hutchins et al., 2022], on several long range language modeling benchmarks over modalities such as English books, raw source code from Github and LaTeX source of ArXiv mathematics articles. As detailed in Table 2, while our GSS model currently lags behind on some tasks when compared in the fixed-parameter setting, it is fairly competitive in the fixed-compute setting where we measure compute as the exact amount of TPUv4 hours spent on a training run and serves as a fairly accurate proxy to the realistic cost of training that model. Furthermore, we also experimented with a hybrid model in which we sparingly interleave Transformer layers (having local attention) in a GSS stack to allow for a richer modeling of short range interactions. To our delight, this further improves performance at (roughly) no extra training cost, both in terms of parameters and compute. While in our experiments we train on sequences of length at most 4 k , we evaluated our GSS variants on a wide range of sequence lengths upto 65 k and found consistent generalization to longer inputs. Not only the performance doesn't degrade as the sequence length is increased but it gets significantly better, suggesting that GSS is effective at utilizing the extra context even though it was not trained with that much amount of context. At inference time, state space models including GSS are fairly efficient since decoding can happen in recurrent mode (as much as $60 \\times$ better in the case of $S 4$ [Gu et al., 2022a]). Though, the hybrid model which also uses local attention complicates this advantage a bit. In summary, we propose GSS, an alternative to $S 4$ and DSS which trains $2-3 \\times$ faster, is simple to implement and fairly competitive with well-tuned Transformer-based baselines on several long range language modeling benchmarks. ## 2 Related Work\n\nIn recent years, attention-based models have emerged as a dominant technique for sequence modeling, achieving remarkable improvements in a wide range of tasks, starting in NLP [Vaswani et al., 2017, Devlin et al., 2019, Radford et al., 2019, Liu et al., 2019], then moving to other classical machine learning areas such as computer vision [Dosovitskiy et al., 2021] and now to the physical sciences [Avsec et al., 2021, Jumper et al., 2021].\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: state space duality implementation, parallel processing SSM, hybrid attention integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space duality implementation*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 166  (*Influential: 39*)\n\n#### 2. Robustifying State-space Models for Long Sequences via Approximate Diagonalization\n\n*From Search Query: state space duality implementation*\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 3. Simplified State Space Layers for Sequence Modeling\n\n*From Search Query: state space duality implementation*\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 337  (*Influential: 32*)\n\n#### 4. SMR: State Memory Replay for Long Sequence Modeling\n\n*From Search Query: parallel processing SSM*\n\n*Biqing Qi, Junqi Gao, Kaiyan Zhang, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou*\n\n**TL;DR:** A novel non-recursive non-uniform sample processing strategy, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data, enables SSMs to stably model varying sampling points.\n\n**Abstract:** Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding\n\n*From Search Query: parallel processing SSM*\n\n*Sangmin Bae, Jongwoo Ko, Hwanjun Song, SeYoung Yun*\n\n**TL;DR:** This work proposes a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding that enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens.\n\n**Abstract:** To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 6*)\n\n#### 6. Blockwise Parallel Transformers for Large Context Models\n\n*From Search Query: parallel processing SSM*\n\n*Hao Liu, Pieter Abbeel*\n\n**TL;DR:** Blockwise Parallel transformers is presented, that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel transformers (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 1*)\n\n#### 7. Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data\n\n*From Search Query: hybrid attention integration*\n\n*Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang*\n\n**TL;DR:** This work innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data, and utilizes this framework in real-world industrial data to conduct extensive experiments on two types of QA systems with four representative methods.\n\n**Abstract:** Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems. In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (DSFT and RAG frameworks) with four representative methods: Markdown format, Template serialization, TPLM-based method, and LLM-based method. Based on the experimental results, we draw some empirical findings and explore the underlying reasons behind the success of some methods. We hope the findings of this work will provide a valuable reference for the academic and industrial communities in developing robust QA systems.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 8. Leveraging Local and Global Patterns for Self-Attention Networks\n\n*From Search Query: hybrid attention integration*\n\n*Mingzhou Xu, Derek F. Wong, Baosong Yang, Yue Zhang, Lidia S. Chao*\n\n**TL;DR:** The extensive analyses verify that the two types of contexts are complementary to each other, and the proposed hybrid attention mechanism gives highly effective improvements in their integration.\n\n**Abstract:** Self-attention networks have received increasing research attention. By default, the hidden states of each word are hierarchically calculated by attending to all words in the sentence, which assembles global information. However, several studies pointed out that taking all signals into account may lead to overlooking neighboring information (e.g. phrase pattern). To address this argument, we propose a hybrid attention mechanism to dynamically leverage both of the local and global information. Specifically, our approach uses a gating scalar for integrating both sources of the information, which is also convenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 40  (*Influential: 3*)\n\n#### 9. SMedBERT: A Knowledge-Enhanced Pre-trained Language Model with Structured Semantics for Medical Text Mining\n\n*From Search Query: hybrid attention integration*\n\n*Taolin Zhang, Zerui Cai, Chengyu Wang, Minghui Qiu, Bite Yang, Xiaofeng He*\n\n**TL;DR:** In SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbours of linked-entity, the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure.\n\n**Abstract:** Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding. For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text. In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbours of linked-entity. In SMedBERT, the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure. Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations. Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks. It also improves the performance of other tasks such as question answering, question matching and natural language inference.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 48  (*Influential: 3*)\n\n### 6 related papers from Papers with Code\n\n#### 1. RTNI - A symbolic integrator for Haar-random tensor networks\n\n*From Search Query: state space duality implementation*\n\n*Robert Koenig, Motohisa Fukuda, Ion Nechita*\n\n**Abstract:** We provide a computer algebra package called Random Tensor Network Integrator\n(RTNI). It allows to compute averages of tensor networks containing multiple\nHaar-distributed random unitary matrices and deterministic symbolic tensors.\nSuch tensor networks are represented as multigraphs, with vertices\ncorresponding to tensors or random unitaries and edges corresponding to tensor\ncontractions. Input and output spaces of random unitaries may be subdivided\ninto arbitrary tensor factors, with dimensions treated symbolically. The\nalgorithm implements the graphical Weingarten calculus and produces a weighted\nsum of tensor networks representing the average over the unitary group. We\nillustrate the use of this algorithmic tool on some examples from quantum\ninformation theory, including entropy calculations for random tensor network\nstates as considered in toy models for holographic duality. Mathematica and\nPython implementations are supplied.\n\n**Published:** 2019-02-22\n\n\n\n#### 2. From $SU(2)$ holonomies to holographic duality via tensor networks\n\n*From Search Query: state space duality implementation*\n\n*Anonymous*\n\n**Abstract:** Tensor networks provide a powerful tool for studying many-body quantum systems, particularly making quantum simulations more efficient. In this article, we construct a tensor network representation of the spin network states, which correspond to $SU(2)$ gauge-invariant discrete field theories. Importantly, the spin network states play a central role in the Loop Quantum Gravity (LQG) approach to the Planck scale physics. Our focus is on the Ising-type spin networks, which provide a basic model of quantum space in LQG. It is shown that the tensor network approach improves the previously introduced methods of constructing quantum circuits for the Ising spin networks by reducing the number of qubits involved. It is also shown that the tensor network approach is convenient for implementing holographic states involving the bulk-boundary conjecture, which contributes to establishing a link between LQG and holographic duality.\n\n**Published:** 2024-10-24\n\n\n\n#### 3. The Hidden Attention of Mamba Models\n\n*From Search Query: parallel processing SSM*\n\n*Lior Wolf, Itamar Zimerman, Ameen Ali*\n\n**Abstract:** The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains, including NLP, long-range sequence processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.\n\n**Published:** 2024-03-03\n\n\n\n#### 4. Efficient Long Sequence Modeling via State Space Augmented Transformer\n\n*From Search Query: parallel processing SSM*\n\n*Jianfeng Gao, Tuo Zhao, Eren Manavoglu, Denis Charles, Jian Jiao, Xiaodong Liu, Simiao Zuo*\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n**Published:** 2022-12-15\n\n\n\n#### 5. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: hybrid attention integration*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n#### 6. MobileNetV4 -- Universal Models for the Mobile Ecosystem\n\n*From Search Query: hybrid attention integration*\n\n*Andrew Howard, Daniele Moro, Tenghui Zhu, Vaibhav Aggarwal, Berkin Akin, Chengxi Ye, Colby Banbury, Weijun Wang, Fan Yang, Shixin Luo, Marco Fornoni, Manolis Delakis, Chas Leichner, Danfeng Qin*\n\n**Abstract:** We present the latest generation of MobileNets, known as MobileNetV4 (MNv4), featuring universally efficient architecture designs for mobile devices. At its core, we introduce the Universal Inverted Bottleneck (UIB) search block, a unified and flexible structure that merges Inverted Bottleneck (IB), ConvNext, Feed Forward Network (FFN), and a novel Extra Depthwise (ExtraDW) variant. Alongside UIB, we present Mobile MQA, an attention block tailored for mobile accelerators, delivering a significant 39% speedup. An optimized neural architecture search (NAS) recipe is also introduced which improves MNv4 search effectiveness. The integration of UIB, Mobile MQA and the refined NAS recipe results in a new suite of MNv4 models that are mostly Pareto optimal across mobile CPUs, DSPs, GPUs, as well as specialized accelerators like Apple Neural Engine and Google Pixel EdgeTPU - a characteristic not found in any other models tested. Finally, to further boost accuracy, we introduce a novel distillation technique. Enhanced by this technique, our MNv4-Hybrid-Large model delivers 87% ImageNet-1K accuracy, with a Pixel 8 EdgeTPU runtime of just 3.8ms.\n\n**Published:** 2024-04-16\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using state space models (SSMs), here are some key findings and resources that address the specified areas:\n\n### Practical Implementations and Applications of State Space Duality in Neural Networks\n\n- The concept of **State Space Duality (SSD)** is crucial for optimizing SSMs. The SSD framework, as discussed in \"Transformers are SSMs,\" allows for designing architectures like Mamba-2, which is a refinement of Mamba's selective SSM and is 2-8 times faster while maintaining competitiveness with Transformers on language modeling tasks[5|.\n  - The SSD framework provides a theoretical connection between SSMs and attention variants, enabling the design of more efficient and performant models.\n\n### Efficient Parallel Processing Techniques for State Space Models\n\n- **DenseMamba** highlights the importance of maintaining training parallelizability and inference efficiency. This model retains fine-grained information crucial for the final output and enhances information flow between layers through dense connections, which is beneficial for parallel processing[5|.\n  - Efficient parallel processing can be achieved by leveraging the structured nature of SSMs. For instance, the selective SSMs in Mamba and Mamba-2 are designed to be more efficient and scalable, allowing for better parallelization during training and inference.\n\n### Methods for Integrating and Optimizing Hybrid Attention Mechanisms\n\n- **Hybrid attention mechanisms** can combine the benefits of different attention types. For example, the **Gated Linear Attention (GLA) Transformer** integrates gated mechanisms with linear attention, performing competitively against other architectures like LLaMA and recent linear-time-inference baselines such as RetNet and Mamba[3|.\n  - The integration of hybrid attention can be optimized by considering hardware-aware strategies. For instance, the \"Gated Linear Attention Transformers with Hardware-Efficient Training\" paper discusses how to optimize training by adapting to hardware constraints, which can be applied to SSMs as well[3|.\n  - The use of **dense connections** in models like DenseMamba can also enhance the integration of hybrid attention mechanisms by improving information flow between layers, which is crucial for the effectiveness of attention mechanisms.\n\n### Additional Insights\n\n- **Hardware-aware algorithms** and **adaptive memory management** are essential for optimizing the performance of SSMs. The \"LLM in a flash\" paper provides insights into optimizing inference cost models, which can be applied to SSMs to reduce data transfer and optimize memory usage[4|.\n- **Selective mechanisms** in SSMs can enhance efficiency while maintaining performance. This is evident in the Mamba and Mamba-2 models, which use selective SSMs to achieve better scalability and performance.\n\nBy integrating these insights, researchers can develop more efficient, scalable, and performant autoregressive language models using state space models, optimized for parallel processing and hybrid attention mechanisms.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nAfter analyzing three rounds of search results, several promising directions emerge for enhancing the SSMFastTTTLinear GAU:\n\na) State Space Model Integration:\n- Mamba-2's refinements show how to make SSMs more efficient (2-8x faster)\n- State space duality (SSD) framework provides theoretical connections between SSMs and attention variants\n- DenseSSM demonstrates benefits of enhanced information flow between layers\n\nb) Memory and Attention Optimization:\n- Hardware-aware algorithms crucial for performance (FlashLinearAttention)\n- Hybrid attention mechanisms can combine benefits of different approaches\n- Adaptive memory management techniques show promise for efficiency\n\nc) Implementation Efficiency:\n- IO-aware algorithms essential for hardware utilization\n- Importance of efficient parallel processing\n- Need for careful memory management and caching strategies\n\n2. Useful References with Excerpts\n\nFrom \"Transformers are SSMs\":\n\"Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\"\n\nFrom \"DenseMamba\":\n\"DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom \"Gated Linear Attention\":\n\"The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\"\n\nFrom \"LLM in a flash\":\n\"Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks.\"\n\n3. Key Insights and Analysis\n\na) SSM Enhancement Opportunities:\n- State space duality provides theoretical framework for optimization\n- Dense connections between layers can improve information flow\n- Selective mechanisms can enhance efficiency while maintaining performance\n\nb) Memory Management Innovation:\n- Hardware-aware memory strategies crucial for performance\n- Adaptive caching and loading techniques show promise\n- Importance of considering memory hierarchy in design\n\nc) Attention Mechanism Improvements:\n- Hybrid approaches combining different attention types show promise\n- Gated mechanisms can improve efficiency while maintaining expressiveness\n- Linear attention variants can be competitive with proper optimization\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of state space duality in practice\n2. Efficient parallel processing techniques for SSMs\n3. Integration strategies for hybrid attention mechanisms\n\n5. References\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n3. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n4. Alizadeh-Vahid, K., et al. (2023). LLM in a flash: Efficient Large Language Model Inference with Limited Memory. Annual Meeting of the Association for Computational Linguistics.\n\n5. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n6. Zuo, S., et al. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint."
        },
        {
            "ready": false,
            "query": "SSM integration, FastTTTLinear optimization, long-sequence modeling",
            "detail": "Efficient integration of State Space Models with FastTTTLinear for enhanced long-sequence modeling",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nEfficient integration of State Space Models with FastTTTLinear for enhanced long-sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.99)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 0.99)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n#### 2. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.98)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 1/35 (Score: 0.98)*\n\n```\n# Efficient Long Sequence Modeling via State Space Augmented Transformer \n\nSimiao Zuo ${ }^{* \\ddagger}$, Xiaodong Liu ${ }^{* \\dagger \\wedge}$, Jian Jiao ${ }^{\\dagger \\diamond}$, Denis Charles ${ }^{\\diamond}$, Eren Manavoglu ${ }^{\\wedge}$,<br>Tuo Zhao ${ }^{\\ddagger}$ and Jianfeng Gao ${ }^{\\circ}$<br>${ }^{\\ddagger}$ Georgia Institute of Technology ${ }^{\\diamond}$ Microsoft\n\n\n#### Abstract\n\nTransformer models have achieved superior performance in various natural language processing tasks.\n```\n\n#### 3. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.95)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 7/51 (Score: 0.95)*\n\n```\nThis result represents an awkward reality for existing RNNs. On one hand, the main advantage of RNNs (vs. Transformers) is their linear (vs. quadratic) complexity. This asymptotic advantage is only realized in practice for long context, which according to Figure 3 is after 8 k . On the other hand, once context is long enough, existing RNNs such as Mamba struggle to actually take advantage of the extra information being conditioned on. The difficulty with long context is inherent to the very nature of RNN layers: Unlike self-attention, RNN layers have to compress context into a hidden state of fixed size. As a compression heuristic,\nthe update rule needs to discover the underlying structures and relationships among thousands or potentially millions of tokens. In this paper, we begin with the observation that self-supervised learning can compress a massive training set into the weights of a model such as an LLM, which often exhibits deep understanding about the semantic connections among its training data - exactly what we need from a compression heuristic. TTT layers. Motivated by this observation, we design a new class of sequence modeling layers where the hidden state is a model, and the update rule is a step of self-supervised learning. Because the process of updating the hidden state on a test sequence is equivalent to training a model at test time, this new class of layers is called Test-Time Training (TTT) layers. We introduce two simple instantiations within this class: TTT-Linear and TTT-MLP, where the hidden state is a linear model and a two-layer MLP, respectively. TTT layers can be integrated into any network architecture and optimized end-to-end, similar to RNNs layers and self-attention. Wall-clock time. While the TTT layer is already efficient in FLOPs, we propose two practical innovations to make it efficient in wall-clock time. First, similar to the standard practice of taking gradient steps on mini-batches of sequences during regular training for better parallelism, we use mini-batches of tokens during TTT. Second, we develop a dual form for operations inside each TTT mini-batch, to better take advantage of modern GPUs and TPUs. The dual form is equivalent in output to the naive implementation, but trains more than $5 \\times$ faster. As shown in Figure 3, TTT-Linear is faster than Transformer at 8 k context and matches Mamba. Evaluations and open problems. While we have highlighted some results for TTT-Linear at the beginning of the paper, Section 3 presents more comprehensive evaluations for both TTT-Linear and TTT-MLP, and open problems exposed by our evaluations. For example, our evaluations following the Chinchilla recipe [34] do not cleanly fit a linear scaling trend even for the Transformer baseline.\n```\n\n#### 4. Spectral State Space Models (Avg. Score: 0.95)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.97)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.92)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: SSM integration, FastTTTLinear optimization, long-sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Hierarchical Integration Diffusion Model for Realistic Image Deblurring\n\n*From Search Query: SSM integration*\n\n*Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, L. Kong, X. Yuan*\n\n**TL;DR:** The Hierarchical Integration Diffusion Model (HI-Diff) is proposed, which designs the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios.\n\n**Abstract:** Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 39  (*Influential: 6*)\n\n#### 2. BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations\n\n*From Search Query: SSM integration*\n\n*Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan*\n\n**TL;DR:** A comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations, and distinguishes between structured and unstructured knowledge, leading to more effective utilization of information.\n\n**Abstract:** Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at $\\href{https://github.com/QizhiPei/BioT5}{Github}$.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 40  (*Influential: 8*)\n\n#### 3. MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning\n\n*From Search Query: SSM integration*\n\n*Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li*\n\n**TL;DR:** This paper proposes a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities, and yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems.\n\n**Abstract:** The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 53  (*Influential: 4*)\n\n#### 4. Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n\n*From Search Query: FastTTTLinear optimization*\n\n*Rafael Rafailov, Archit Sharma, E. Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn*\n\n**TL;DR:** A new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form is introduced, allowing us to solve the standard RLHF problem with only a simple classification loss.\n\n**Abstract:** While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1933  (*Influential: 585*)\n\n#### 5. Symbolic Discovery of Optimization Algorithms\n\n*From Search Query: FastTTTLinear optimization*\n\n*Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le*\n\n**TL;DR:** Lion is a simple and effective optimization algorithm that requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function and is more memory-efficient than Adam as it only keeps track of the momentum.\n\n**Abstract:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 244  (*Influential: 40*)\n\n#### 6. Constrained Efficient Global Optimization of Expensive Black-box Functions\n\n*From Search Query: FastTTTLinear optimization*\n\n*Donald R. Jones, Matthias Schonlau, W. Welch*\n\n**TL;DR:** This work proposes CONFIG (CONstrained efFIcient Global Optimization), a simple and effective algorithm to solve the problem of constrained efficient global optimization, where both the objective and constraints are expensive black-box functions that can be learned with Gaussian processes.\n\n**Abstract:** We study the problem of constrained efficient global optimization, where both the objective and constraints are expensive black-box functions that can be learned with Gaussian processes. We propose CONFIG (CONstrained efFIcient Global Optimization), a simple and effective algorithm to solve it. Under certain regularity assumptions, we show that our algorithm enjoys the same cumulative regret bound as that in the unconstrained case and similar cumulative constraint violation upper bounds. For commonly used Matern and Squared Exponential kernels, our bounds are sublinear and allow us to derive a convergence rate to the optimal solution of the original constrained problem. In addition, our method naturally provides a scheme to declare infeasibility when the original black-box optimization problem is infeasible. Numerical experiments on sampled instances from the Gaussian process, artificial numerical problems, and a black-box building controller tuning problem all demonstrate the competitive performance of our algorithm. Compared to the other state-of-the-art methods, our algorithm significantly improves the theoretical guarantees, while achieving competitive empirical performance.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 2049  (*Influential: 134*)\n\n#### 7. What Makes Convolutional Models Great on Long Sequence Modeling?\n\n*From Search Query: long-sequence modeling*\n\n*Yuhong Li, Tianle Cai, Yi Zhang, De-huai Chen, Debadeepta Dey*\n\n**TL;DR:** A simple yet effective convolutional model called Structured Global Convolution (SGConv), which exhibits strong empirical performance over several tasks and shows the potential to improve both efficiency and performance when plugging SGConv into standard language and vision models.\n\n**Abstract:** Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependency efficiently. Attention overcomes this problem by aggregating global information but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. [2021] proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes. As a result, S4 is less intuitive and hard to use. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 82  (*Influential: 15*)\n\n#### 8. SMR: State Memory Replay for Long Sequence Modeling\n\n*From Search Query: long-sequence modeling*\n\n*Biqing Qi, Junqi Gao, Kaiyan Zhang, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou*\n\n**TL;DR:** A novel non-recursive non-uniform sample processing strategy, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data, enables SSMs to stably model varying sampling points.\n\n**Abstract:** Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 9. CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\n\n*From Search Query: long-sequence modeling*\n\n*Jinchao Zhang, Shuyang Jiang, Jiangtao Feng, Lin Zheng, Lingpeng Kong*\n\n**TL;DR:** This paper proposes Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self,Noncausal cross, and causal cross attentions, and sheds light on the fundamental problems of efficient attentions.\n\n**Abstract:** Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: SSM integration*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Mamba-FETrack: Frame-Event Tracking via State Space Model\n\n*From Search Query: SSM integration*\n\n*Bo Jiang, Xiao Wang, Zhe Wu, Shuai Wang, Shiao Wang, Ju Huang*\n\n**Abstract:** RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream). Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets. However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism. This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking. Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams. Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network. The fused features will be fed into the tracking head for target object localization. Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker. Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking. The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}.\n\n**Published:** 2024-04-28\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: long-sequence modeling*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 4. Efficiently Modeling Long Sequences with Structured State Spaces\n\n*From Search Query: long-sequence modeling*\n\n*Christopher R\u00e9, Karan Goel, Albert Gu*\n\n**Abstract:** A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.\n\n**Conference:** efficiently-modeling-long-sequences-with\n\n**Published:** 2021-10-31\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating State Space Models (SSMs) with FastTTTLinear, several key points and strategies can be considered based on the provided sources and analysis:\n\n## Theoretical Justification and Efficiency\n\n- **State Space Models (SSMs)**: SSMs have been shown to be highly efficient for long-sequence modeling, offering linear computational complexity and constant memory requirements. The Mamba model, which integrates selective SSMs, achieves fast inference (5 times higher throughput than Transformers) and linear scaling in sequence length, making it a strong candidate for integration.\n\n## Implementation Strategies\n\n- **Selective State Spaces**: The Mamba model's use of selective state spaces, where SSM parameters are functions of the input, allows for the selective propagation or forgetting of information based on the current token. This approach can be adapted to integrate with FastTTTLinear, ensuring content-based reasoning and efficient handling of long sequences.\n- **Hardware-Aware Parallel Algorithm**: Mamba's design includes a hardware-aware parallel algorithm in recurrent mode, which can be crucial for optimizing the integration with FastTTTLinear. This ensures that the model can be computed efficiently even with the added complexity of SSMs.\n\n## Addressing Local Pattern Shortcuts\n\n- **Global Selection Mechanism**: To mitigate the issue of local pattern shortcuts in SSMs, introducing a global selection module can be beneficial. This module helps in handling tasks that require distributed key information, which is essential for robust long-sequence modeling.\n\n## Optimization and Scalability\n\n- **Efficient Discretization**: For integrating SSMs with discrete data, efficient discretization techniques such as the zero-order hold (ZOH) method are essential. This process transforms continuous SSMs into discrete forms suitable for modern computational frameworks.\n- **Parallel Processing**: Techniques like the max-min boundary compression (PMBC) strategy can facilitate efficient inference and parallel processing, which is crucial for handling long sequences efficiently.\n\n## Empirical Results and Case Studies\n\n- **Performance Evaluation**: Studies on Mamba and similar SSM-based models have shown that these models can outperform or match the performance of similarly sized Transformers and even larger ones in certain tasks. This includes language modeling and various synthetic tasks, providing strong empirical support for the integration.\n\n## Future Directions\n\n- **Code Examples and Implementation Details**: Examining the codebases of Mamba and other related models can provide practical insights into how to integrate SSMs with FastTTTLinear. This includes looking into the specific implementation strategies and optimization techniques used in these models.\n- **Optimization Techniques**: Investigating optimization techniques such as those used in Mamba (e.g., hardware-aware parallel algorithms) and other models (e.g., PMBC strategy in SPikE-SSM) can further enhance the computational efficiency of the integrated model.\n\nBy combining these insights, researchers can develop an autoregressive language model that leverages the efficiency and scalability of SSMs, the computational advantages of FastTTTLinear, and the robustness to handle long sequences and diverse inputs.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal to integrate State Space Models (SSMs) with FastTTTLinear GAUs is supported by extensive research highlighting the effectiveness of SSMs in long-sequence modeling. The integration aims to enhance the model's ability to capture long-range dependencies while maintaining computational efficiency. The search results provide insights into the theoretical foundation, implementation complexity, and potential benefits of such integration.\n\n### Useful References and Excerpts\n\n1. **Simplified State Space Layers for Sequence Modeling**\n   - \"Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks.\"\n   - This reference highlights the success of SSMs in long-range sequence modeling, supporting the proposal's motivation.\n\n2. **Efficient Long Sequence Modeling via State Space Augmented Transformer**\n   - \"The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\"\n   - This paper discusses the integration of SSMs with attention mechanisms, similar to the proposal.\n\n3. **Spectral State Space Models**\n   - \"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n   - This reference provides a novel approach to SSMs that could inform the proposed integration.\n\n4. **Learning to (Learn at Test Time): RNNs with Expressive Hidden States**\n   - \"With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time.\"\n   - This supports the efficiency claims of FastTTTLinear, which is part of the proposal.\n\n5. **Mamba: Linear-Time Sequence Modeling with Selective State Spaces**\n   - \"Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences.\"\n   - This reference highlights the efficiency and scalability of SSMs, which are crucial for the proposed integration.\n\n### Key Insights and Detailed Analysis\n\n- **Theoretical Justification**: The use of SSMs for global context modeling is theoretically sound, as they can efficiently transform input sequences into latent state representations.\n- **Implementation Complexity**: Integrating SSMs with FastTTTLinear GAUs may introduce computational overhead, but this can be managed through efficient discretization techniques.\n- **Scalability and Robustness**: SSMs have demonstrated scalability in handling large sequence lengths and robustness to diverse inputs, making them suitable for the proposed integration.\n\n### Future Search Plan\n\n- **Implementation Details**: Search for specific implementation strategies and code examples that demonstrate how to integrate SSMs with FastTTTLinear GAUs. This could involve looking into the Mamba and MatMamba codebases and related research papers.\n- **Empirical Results**: Look for empirical results and case studies that evaluate the performance of similar integrations. This includes examining the results from Mamba, MatMamba, and other SSM-based models on various benchmarks and tasks.\n- **Optimization Techniques**: Investigate optimization techniques that can further enhance the computational efficiency of the integrated model. This might involve exploring hardware-aware parallel algorithms, efficient discretization methods, and other optimization strategies.\n\n### References\n\n1. Smith, J., Warrington, A., & Linderman, S. W. (2022). Simplified State Space Layers for Sequence Modeling. International Conference on Learning Representations.\n2. Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv.org.\n3. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv.org.\n4. Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G., Dubois, Y., Chen, X., Wang, X., Koyejo, S., Hashimoto, T., & Guestrin, C. (2024). Learning to (Learn at Test Time): RNNs with Expressive Hidden States. arXiv.org.\n5. Dao, T., & Gu, A. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.org."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "State Space Models, FastTTTLinear, Autoregressive Models",
            "detail": "Integration of State Space Models with FastTTTLinear GAUs for long-sequence modeling",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nIntegration of State Space Models with FastTTTLinear GAUs for long-sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.99)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 0.99)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n#### 2. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.99)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 15/37 (Score: 0.99)*\n\n```\nAdvances in neural information processing systems, 33:1474-1487, 2020. [16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [17] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. [18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572-585, 2021. [19] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections. International Conference on Learning Representations, 2023. [20] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. International Conference on Learning Representations, 2023. [21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training (Avg. Score: 0.99)\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 43  (*Influential: 9*)\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n##### *Relevant Chunk: No. 25/51 (Score: 0.99)*\n\n```\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. 2023. Gu, A., Goel, K., and R'e, C. Efficiently modeling long sequences with structured state spaces. International Conference On Learning Representations, 2021a. Gu, A., Johnson, I., Goel, K., Saab, K. K., Dao, T., Rudra, A., and R'e, C. Combining recurrent, convolutional, and continuous-time models with linear state-space layers. Neural Information Processing Systems, 2021b. URL https://arxiv.org/abs/2110.13985v1. Gu, A., Goel, K., and R\u00e9, C. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR\n2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Gupta, A. and Berant, J. Diagonal state spaces are as effective as structured state spaces. ARXIV.ORG, 2022. doi: 10.48550/arXiv.2203.14343. Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. Hinton, G. E. and Plaut, D. C. Using fast weights to deblur old memories. In Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177-186, 1987. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. Hooker, S. The hardware lottery. Communications of the ACM, 64:58-65, 2020. Hua, W., Dai, Z., Liu, H., and Le, Q. V. Transformer quality in linear time. In Chaudhuri, K., Jegelka, S., Song, L., Szepesv\u00e1ri, C., Niu, G., and Sabato, S. (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 9099-9117. PMLR, 2022. Irie, K., Schlag, I., Csord\u00e1s, R., and Schmidhuber, J. Going beyond linear transformers with recurrent fast weight programmers. Advances in Neural Information Processing Systems, 34:7703-7717, 2021. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. 1., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. ArXiv preprint, abs/2310.06825, 2023. Kacham, P., Mirrokni, V., and Zhong, P. Polysketchformer: Fast transformers via sketching polynomial kernels, 2023. Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G., Pappas, N., Mao, Y., Chen, W., and Smith, N. A. Finetuning pretrained transformers into RNNs. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 10630-10643, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main. 830 . Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Long Range Language Modeling via Gated State Spaces (Avg. Score: 0.99)\n\n*Harsh Mehta, Ankit Gupta, Ashok Cutkosky, Behnam Neyshabur*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 134  (*Influential: 17*)\n\n**TL;DR:** This work proposes a new layer named Gated State Space (GSS) and shows that it trains significantly faster than the diagonal version of S4 on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement.\n\n**Abstract:** State space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further.\n\n##### *Relevant Chunk: No. 1/28 (Score: 0.99)*\n\n```\n# Long Range Language Modeling via Gated State Spaces \n\nHarsh Mehta ${ }^{1 *} \\quad$ Ankit Gupta $^{2} \\quad$ Ashok Cutkosky ${ }^{3} \\quad$ Behnam Neyshabur ${ }^{1}$\n\n\n#### Abstract\n\nState space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further. ## 1 Introduction\n\nModeling long range dependencies on sequential data is a crucial step towards closing the gap with human-level performance on many tasks. Attention based models like Transformer [Vaswani et al., 2017] have proven to be a strong choice of backbone architecture for a considerable number of tasks across modalities and scale [Devlin et al., 2019, Brown et al., 2020, Dosovitskiy et al., 2021]. Vanilla Multi-Head-Attention famously incurs $\\Omega\\left(L^{2}\\right)$ penalty in modeling a sequence of length $L$. This is prohibitive at best for tasks where the model is required to capture long range dependencies from various parts of the input. Over the years, a variety of improvements have been proposed to alleviate this quadratic complexity (cf. [Tay et al., 2020]). On a somewhat orthogonal direction, attention-free models based on state spaces, such as $\\mathrm{S} 4[\\mathrm{Gu}$ et al., 2022a] and DSS [Gupta et al., 2022], have shown remarkable improvements on Long Range Arena (LRA) [Tay et al., 2021], a benchmark designed with long range modeling as its focus and consists of diverse tasks with $1 \\mathrm{k}-16 \\mathrm{k}$ sequence length across modalities. These models require careful initialization, originally borrowing ideas from the theory of HiPPO matrices [Voelker et al., 2019, Gu et al., 2020], to achieve good results on LRA. In this work, we explore and extend the use of state space models by focusing solely on the task of autoregressive sequence modeling [Brown et al., 2020, Rae et al., 2021, Chowdhery et al., 2022, Zhang et al., 2022, Hoffmann et al., 2022, Srivastava et al., 2022]. Several key properties endowed by the state space model family makes it particularly attractive, to at least fully explore it, in the context of language modeling. First, it reduces the $\\Omega\\left(L^{2}\\right)$ complexity on input sequence length to $O(L \\log L)$. This complexity results from the use of Fast Fourier Transform (FFT) [Cooley and Tukey, 1965] for performing convolutions. We will describe this in detail in later sections. Second, the state space model is fully parallelizable in the length dimension. This is an arguably subtle but an important property at training time. Note that transformers are also fully parallelizable, a worthy advantage over traditional RNNs for modeling sequences, which otherwise incurs only an $O(L)$ penalty. While this parallelism is useful at training time, it may also be a curse at inference time\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_5ab9ed67021bd72a4442g-02.jpg?height=470&width=563&top_left_y=242&top_left_x=369)\n\n```\ndef gss(x, F=4096, L=4096, \\(E=1024, \\mathrm{H}=256\\) ):\n    shortcut, \\(\\mathrm{x}=\\mathrm{x}, \\operatorname{norm}(\\mathrm{x})\\)\n    \\(\\mathrm{v}=\\) dense(x, F, activation='gelu')\n    \\(\\mathrm{u}=\\) dense(x, H, activation='gelu')\n    \\(\\mathrm{y}=\\operatorname{dss}(u, \\mathrm{H}, \\mathrm{L})\\)\n    \\# yh1,..,yhL are linear in uh1,..,uhL\n    \\(\\mathrm{uc}=\\) dense(y, F)\n    \\(0=\\) dense(uc \\(*\\) v, E)\n    return o + shortcut\n```\n\nFigure 1: (a) Our proposed Gated State Space (GSS) layer, (b) Pseudocode for GSS (full implementation in \u00a7A.2).\n```\n\n#### 5. Spectral State Space Models (Avg. Score: 0.98)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.98)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 14 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: State Space Models, FastTTTLinear, Autoregressive Models\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: State Space Models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 2. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: State Space Models*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 3. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: State Space Models*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 216  (*Influential: 38*)\n\n#### 4. Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study\n\n*From Search Query: Autoregressive Models*\n\n*Boxin Wang, Wei Ping, P. Xu, Lawrence C. McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar, Bryan Catanzaro*\n\n**TL;DR:** A comprehensive study on a scalable pre-trained retrieval-augmented LM of RETRO, which outperforms GPT on text generation with much less degeneration, moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database, and highlights the promising direction of pretraining autoregressive LMs with retrieval as future foundation models.\n\n**Abstract:** Large decoder-only language models (LMs) can be largely improved in terms of perplexity by retrieval (e.g., RETRO), but its impact on text generation quality and downstream task accuracy is unclear. Thus, it is still an open question: shall we pretrain large autoregressive LMs with retrieval? To answer it, we perform a comprehensive study on a scalable pre-trained retrieval-augmented LM (i.e., RETRO) compared with standard GPT and retrieval-augmented GPT incorporated at fine-tuning or inference stages. We first provide the recipe to reproduce RETRO up to 9.5B parameters while retrieving a text corpus with 330B tokens. Based on that, we have the following novel findings: i) RETRO outperforms GPT on text generation with much less degeneration (i.e., repetition), moderately higher factual accuracy, and slightly lower toxicity with a nontoxic retrieval database. ii) On the LM Evaluation Harness benchmark, RETRO largely outperforms GPT on knowledge-intensive tasks, but is on par with GPT on other tasks. Furthermore, we introduce a simple variant of the model, RETRO++, which largely improves open-domain QA results of original RETRO (e.g., EM score +8.6 on Natural Question) and significantly outperforms retrieval-augmented GPT in both fine-tuning and zero-shot evaluation settings. Our findings highlight the promising direction of pretraining autoregressive LMs with retrieval as future foundation models. We release our code and model at: https://github.com/NVIDIA/Megatron-LM/blob/main/tools/retro/README.md\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 50  (*Influential: 4*)\n\n#### 5. Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding\n\n*From Search Query: Autoregressive Models*\n\n*Sangmin Bae, Jongwoo Ko, Hwanjun Song, SeYoung Yun*\n\n**TL;DR:** This work proposes a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding that enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens.\n\n**Abstract:** To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 6*)\n\n#### 6. Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images\n\n*From Search Query: Autoregressive Models*\n\n*R. Child*\n\n**TL;DR:** This work presents a hierarchical VAE that, for the first time, outperforms the PixelCNN in log-likelihood on all natural image benchmarks and visualize the generative process and show the VAEs learn efficient hierarchical visual representations.\n\n**Abstract:** We present a hierarchical VAE that, for the first time, outperforms the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that VAEs can actually implement autoregressive models, and other, more efficient generative models, if made sufficiently deep. Despite this, autoregressive models have traditionally outperformed VAEs. We test if insufficient depth explains the performance gap by by scaling a VAE to greater stochastic depth than previously explored and evaluating it on CIFAR-10, ImageNet, and FFHQ. We find that, in comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. We visualize the generative process and show the VAEs learn efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 306  (*Influential: 64*)\n\n### 4 related papers from ArXiv\n\n#### 1. LOCOST: State-Space Models for Long Document Abstractive Summarization\n\n*From Search Query: State Space Models*\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n\n**Published:** 2024-01-31T15:33:37Z  (*Updated: 2024-03-25T12:52:42Z*)\n\n\n\n#### 2. Mimetic Initialization Helps State Space Models Learn to Recall\n\n*From Search Query: State Space Models*\n\n*Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, Srinadh Bhojanapalli*\n\n**Abstract:** Recent work has shown that state space models such as Mamba are significantly\nworse than Transformers on recall-based tasks due to the fact that their state\nsize is constant with respect to their input sequence length. But in practice,\nstate space models have fairly large state sizes, and we conjecture that they\nshould be able to perform much better at these tasks than previously reported.\nWe investigate whether their poor copying and recall performance could be due\nin part to training difficulties rather than fundamental capacity constraints.\nBased on observations of their \"attention\" maps, we propose a structured\ninitialization technique that allows state space layers to more readily mimic\nattention. Across a variety of architecture settings, our initialization makes\nit substantially easier for Mamba to learn to copy and do associative recall\nfrom scratch.\n\n**Published:** 2024-10-14T23:17:46Z  (*Updated: 2024-10-14T23:17:46Z*)\n\n\n\n#### 3. ENGINE: Energy-Based Inference Networks for Non-Autoregressive Machine\n  Translation\n\n*From Search Query: Autoregressive Models*\n\n*Lifu Tu, Richard Yuanzhe Pang, Sam Wiseman, Kevin Gimpel*\n\n**Abstract:** We propose to train a non-autoregressive machine translation model to\nminimize the energy defined by a pretrained autoregressive model. In\nparticular, we view our non-autoregressive translation system as an inference\nnetwork (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher\nenergy. This contrasts with the popular approach of training a\nnon-autoregressive model on a distilled corpus consisting of the beam-searched\noutputs of such a teacher model. Our approach, which we call ENGINE\n(ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive\nresults on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the\nperformance of autoregressive models.\n\n**Published:** 2020-05-02T15:06:47Z  (*Updated: 2020-05-12T20:44:24Z*)\n\n\n\n#### 4. Autoregressive Knowledge Distillation through Imitation Learning\n\n*From Search Query: Autoregressive Models*\n\n*Alexander Lin, Jeremy Wohlwend, Howard Chen, Tao Lei*\n\n**Abstract:** The performance of autoregressive models on natural language generation tasks\nhas dramatically improved due to the adoption of deep, self-attentive\narchitectures. However, these gains have come at the cost of hindering\ninference speed, making state-of-the-art models cumbersome to deploy in\nreal-world, time-sensitive settings. We develop a compression technique for\nautoregressive models that is driven by an imitation learning perspective on\nknowledge distillation. The algorithm is designed to address the exposure bias\nproblem. On prototypical language generation tasks such as translation and\nsummarization, our method consistently outperforms other distillation\nalgorithms, such as sequence-level knowledge distillation. Student models\ntrained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those\ntrained from scratch, while increasing inference speed by up to 14 times in\ncomparison to the teacher model.\n\n**Published:** 2020-09-15T17:43:02Z  (*Updated: 2020-10-29T00:40:45Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: State Space Models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: State Space Models*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n#### 3. GPT-NeoX-20B: An Open-Source Autoregressive Language Model\n\n*From Search Query: Autoregressive Models*\n\n*Samuel Weinbach, Ben Wang, Jonathan Tow, Laria Reynolds, Shivanshu Purohit, USVSN Sai Prashanth, Michael Pieler, Jason Phang, Kyle McDonell, Connor Leahy, Horace He, Laurence Golding, Leo Gao, Quentin Anthony, Eric Hallahan, Stella Biderman, Sid Black*\n\n**Abstract:** We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \\model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.\n\n**Conference:** gpt-neox-20b-an-open-source-autoregressive\n\n**Published:** 2022-04-14\n\n\n\n#### 4. Autoregressive Diffusion Models\n\n*From Search Query: Autoregressive Models*\n\n*Tim Salimans, Rianne van den Berg, Ben Poole, Jasmijn Bastings, Alexey A. Gritsenko, Emiel Hoogeboom*\n\n**Abstract:** We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.\n\n**Conference:** autoregressive-diffusion-models-1\n\n**Published:** 2021-10-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating State Space Models (SSMs) with FastTTTLinear GAUs, here are some key points and references that can guide the researchers:\n\n## Novelty and Related Work\n\n- The integration of SSMs with attention mechanisms has been explored in models like SC-MAMBA2. This model combines state-space models with the MAMBA framework, enabling efficient and scalable modeling of long sequences, such as gene sequences. This approach shows promise in handling large-scale data efficiently, which could be adapted to language modeling.\n\n- Deep state-space models (Deep SSMs) have been shown to perform in-context learning on autoregressive tasks, similar to transformers. These models use local self-attention and multiplicative interactions, which are crucial for capturing long-range dependencies and could be integrated with FastTTTLinear GAUs.\n\n## Theoretical Justification\n\n- The theoretical soundness of using SSMs for global context modeling is supported by their ability to transform input sequences into latent state representations efficiently. This is achieved through discrete state-space formulations, which maintain linear computational complexity. The use of local self-attention within SSMs enhances their expressive power, making them suitable for capturing both local and global dependencies.\n\n- The hybrid synergy between SSMs and attention mechanisms is theoretically justified by the ability of SSMs to act as gradient accumulators, which can be applied to the parameters of implicit regression models. This synergy can enhance the learning capabilities of the model, especially in autoregressive tasks.\n\n## Implementation Complexity and Efficiency\n\n- The integration of SSMs with FastTTTLinear GAUs is expected to introduce some computational overhead, but this can be managed through efficient discretization techniques and the use of convolutional implementations. For example, SC-MAMBA2 uses a bidirectional modeling approach within state-space modules, which helps in maintaining efficiency while handling long sequences.\n\n- The gating mechanism in SSMs, such as those used in SC-MAMBA2, can be designed to minimize additional computational overhead. The state-space duality concept introduced in Mamba2 simplifies the matrix transformations, reducing the computational complexity.\n\n## Scalability and Robustness\n\n- SSMs have shown scalability in handling large sequence lengths, as demonstrated by SC-MAMBA2, which can model full gene sequences encompassing thousands of genes. This scalability is crucial for language models that need to handle long-range dependencies efficiently.\n\n- The robustness of SSMs in handling diverse and noisy inputs is enhanced by their ability to capture intricate variations and regulatory elements. This is particularly relevant in biological data but can be generalized to language modeling, where robustness to variant inputs is essential.\n\n## Additional Considerations\n\n- The work on Deep SSMs highlights the importance of local self-attention and multiplicative interactions in recurrent architectures. These features can be incorporated into the FastTTTLinear GAUs to enhance their expressive power and ability to capture long-range dependencies.\n\n- The use of state-space models in other contexts, such as the Conditional Autoregressive F-Riesz model, demonstrates their flexibility and adaptability. This adaptability can be leveraged to improve the robustness and scalability of autoregressive language models.\n\nBy integrating these insights, the researchers can design a novel autoregressive language model that leverages the strengths of SSMs and FastTTTLinear GAUs, potentially achieving better performance, efficiency, and scalability.\n",
            "analysis": "### Initial Analysis\n\nThe proposal for **SSMFastTTTLinear** aims to enhance the FastTTTLinear GAU by integrating State Space Models (SSMs) to improve long-sequence modeling efficiency and expressiveness. This integration is expected to capture both local and global dependencies without significantly increasing computational overhead.\n\n### Key Aspects for Investigation\n\n1. **Novelty and Related Work**:\n   - The proposal claims novelty in integrating SSMs with FastTTTLinear GAUs. It is crucial to verify if similar integrations have been explored in existing literature.\n   - Examine the effectiveness of SSMs in capturing long-range dependencies and their integration with attention mechanisms in other models like Mamba and SPADE.\n\n2. **Theoretical Justification**:\n   - Assess the theoretical soundness of using SSMs for global context modeling and their linear computational complexity.\n   - Investigate the hybrid synergy between SSMs and attention mechanisms, as proposed in the design.\n\n3. **Implementation Complexity and Efficiency**:\n   - Evaluate the potential computational overhead introduced by integrating SSMs and how it compares to existing models.\n   - Consider the efficiency of the proposed gating mechanism and its impact on the overall model performance.\n\n4. **Scalability and Robustness**:\n   - Investigate how well the design scales with increasing model sizes and sequence lengths.\n   - Explore the robustness of the proposed design in handling diverse and noisy inputs.\n\n### Investigation Plan\n\n- Conduct a broad search to identify existing research on integrating SSMs with attention mechanisms, focusing on models like Mamba, SPADE, and others.\n- Perform a detailed search in the internal vector store to find specific papers discussing the theoretical and practical aspects of SSMs in language models.\n- Compare the proposal against previous design proposals and similar models to assess its novelty and potential impact.\n\n### Novelty Check\n\nThe proposal appears novel in its specific integration of SSMs into FastTTTLinear GAUs, as this combination is not evident in the provided references or previous design proposals. However, similar concepts have been explored in other contexts, such as Mamba and SPADE, which combine SSMs with attention mechanisms.\n\n### References\n\n1. Gu, A., Goel, K., & R\u00e9, C. (2021). Efficiently Modeling Long Sequences with Structured State Spaces. ICLR.\n2. Zuo, S., et al. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv.\n3. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv.\n4. Yang, Z., et al. (2016). Hierarchical Attention Networks for Document Classification. NAACL."
        },
        {
            "ready": false,
            "query": "SSM integration, FastTTTLinear optimization, long-sequence modeling",
            "detail": "Efficient integration of State Space Models with FastTTTLinear for enhanced long-sequence modeling",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nEfficient integration of State Space Models with FastTTTLinear for enhanced long-sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.99)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 0.99)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n#### 2. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.98)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 1/35 (Score: 0.98)*\n\n```\n# Efficient Long Sequence Modeling via State Space Augmented Transformer \n\nSimiao Zuo ${ }^{* \\ddagger}$, Xiaodong Liu ${ }^{* \\dagger \\wedge}$, Jian Jiao ${ }^{\\dagger \\diamond}$, Denis Charles ${ }^{\\diamond}$, Eren Manavoglu ${ }^{\\wedge}$,<br>Tuo Zhao ${ }^{\\ddagger}$ and Jianfeng Gao ${ }^{\\circ}$<br>${ }^{\\ddagger}$ Georgia Institute of Technology ${ }^{\\diamond}$ Microsoft\n\n\n#### Abstract\n\nTransformer models have achieved superior performance in various natural language processing tasks.\n```\n\n#### 3. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.95)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 7/51 (Score: 0.95)*\n\n```\nThis result represents an awkward reality for existing RNNs. On one hand, the main advantage of RNNs (vs. Transformers) is their linear (vs. quadratic) complexity. This asymptotic advantage is only realized in practice for long context, which according to Figure 3 is after 8 k . On the other hand, once context is long enough, existing RNNs such as Mamba struggle to actually take advantage of the extra information being conditioned on. The difficulty with long context is inherent to the very nature of RNN layers: Unlike self-attention, RNN layers have to compress context into a hidden state of fixed size. As a compression heuristic,\nthe update rule needs to discover the underlying structures and relationships among thousands or potentially millions of tokens. In this paper, we begin with the observation that self-supervised learning can compress a massive training set into the weights of a model such as an LLM, which often exhibits deep understanding about the semantic connections among its training data - exactly what we need from a compression heuristic. TTT layers. Motivated by this observation, we design a new class of sequence modeling layers where the hidden state is a model, and the update rule is a step of self-supervised learning. Because the process of updating the hidden state on a test sequence is equivalent to training a model at test time, this new class of layers is called Test-Time Training (TTT) layers. We introduce two simple instantiations within this class: TTT-Linear and TTT-MLP, where the hidden state is a linear model and a two-layer MLP, respectively. TTT layers can be integrated into any network architecture and optimized end-to-end, similar to RNNs layers and self-attention. Wall-clock time. While the TTT layer is already efficient in FLOPs, we propose two practical innovations to make it efficient in wall-clock time. First, similar to the standard practice of taking gradient steps on mini-batches of sequences during regular training for better parallelism, we use mini-batches of tokens during TTT. Second, we develop a dual form for operations inside each TTT mini-batch, to better take advantage of modern GPUs and TPUs. The dual form is equivalent in output to the naive implementation, but trains more than $5 \\times$ faster. As shown in Figure 3, TTT-Linear is faster than Transformer at 8 k context and matches Mamba. Evaluations and open problems. While we have highlighted some results for TTT-Linear at the beginning of the paper, Section 3 presents more comprehensive evaluations for both TTT-Linear and TTT-MLP, and open problems exposed by our evaluations. For example, our evaluations following the Chinchilla recipe [34] do not cleanly fit a linear scaling trend even for the Transformer baseline.\n```\n\n#### 4. Spectral State Space Models (Avg. Score: 0.95)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.97)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.92)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: SSM integration, FastTTTLinear optimization, long-sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Hierarchical Integration Diffusion Model for Realistic Image Deblurring\n\n*From Search Query: SSM integration*\n\n*Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, L. Kong, X. Yuan*\n\n**TL;DR:** The Hierarchical Integration Diffusion Model (HI-Diff) is proposed, which designs the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios.\n\n**Abstract:** Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 39  (*Influential: 6*)\n\n#### 2. BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations\n\n*From Search Query: SSM integration*\n\n*Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan*\n\n**TL;DR:** A comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations, and distinguishes between structured and unstructured knowledge, leading to more effective utilization of information.\n\n**Abstract:** Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at $\\href{https://github.com/QizhiPei/BioT5}{Github}$.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 40  (*Influential: 8*)\n\n#### 3. MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning\n\n*From Search Query: SSM integration*\n\n*Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li*\n\n**TL;DR:** This paper proposes a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities, and yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems.\n\n**Abstract:** The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 53  (*Influential: 4*)\n\n#### 4. Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n\n*From Search Query: FastTTTLinear optimization*\n\n*Rafael Rafailov, Archit Sharma, E. Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn*\n\n**TL;DR:** A new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form is introduced, allowing us to solve the standard RLHF problem with only a simple classification loss.\n\n**Abstract:** While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1933  (*Influential: 585*)\n\n#### 5. One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization\n\n*From Search Query: FastTTTLinear optimization*\n\n*Minghua Liu, Chao Xu, Haian Jin, Ling Chen, T. MukundVarma, Zexiang Xu, Hao Su*\n\n**TL;DR:** A novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass and favors better geometry, generates more 3D consistent results, and adheres more closely to the input image is proposed.\n\n**Abstract:** Single image 3D reconstruction is an important but challenging task that requires extensive knowledge of our natural world. Many existing methods solve this problem by optimizing a neural radiance field under the guidance of 2D diffusion models but suffer from lengthy optimization time, 3D inconsistency results, and poor geometry. In this work, we propose a novel method that takes a single image of any object as input and generates a full 360-degree 3D textured mesh in a single feed-forward pass. Given a single image, we first use a view-conditioned 2D diffusion model, Zero123, to generate multi-view images for the input view, and then aim to lift them up to 3D space. Since traditional reconstruction methods struggle with inconsistent multi-view predictions, we build our 3D reconstruction module upon an SDF-based generalizable neural surface reconstruction method and propose several critical training strategies to enable the reconstruction of 360-degree meshes. Without costly optimizations, our method reconstructs 3D shapes in significantly less time than existing methods. Moreover, our method favors better geometry, generates more 3D consistent results, and adheres more closely to the input image. We evaluate our approach on both synthetic data and in-the-wild images and demonstrate its superiority in terms of both mesh quality and runtime. In addition, our approach can seamlessly support the text-to-3D task by integrating with off-the-shelf text-to-image diffusion models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 305  (*Influential: 42*)\n\n#### 6. Symbolic Discovery of Optimization Algorithms\n\n*From Search Query: FastTTTLinear optimization*\n\n*Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le*\n\n**TL;DR:** Lion is a simple and effective optimization algorithm that requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function and is more memory-efficient than Adam as it only keeps track of the momentum.\n\n**Abstract:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 244  (*Influential: 40*)\n\n#### 7. What Makes Convolutional Models Great on Long Sequence Modeling?\n\n*From Search Query: long-sequence modeling*\n\n*Yuhong Li, Tianle Cai, Yi Zhang, De-huai Chen, Debadeepta Dey*\n\n**TL;DR:** A simple yet effective convolutional model called Structured Global Convolution (SGConv), which exhibits strong empirical performance over several tasks and shows the potential to improve both efficiency and performance when plugging SGConv into standard language and vision models.\n\n**Abstract:** Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependency efficiently. Attention overcomes this problem by aggregating global information but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. [2021] proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes. As a result, S4 is less intuitive and hard to use. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 82  (*Influential: 15*)\n\n#### 8. SMR: State Memory Replay for Long Sequence Modeling\n\n*From Search Query: long-sequence modeling*\n\n*Biqing Qi, Junqi Gao, Kaiyan Zhang, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou*\n\n**TL;DR:** A novel non-recursive non-uniform sample processing strategy, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data, enables SSMs to stably model varying sampling points.\n\n**Abstract:** Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 9. CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\n\n*From Search Query: long-sequence modeling*\n\n*Jinchao Zhang, Shuyang Jiang, Jiangtao Feng, Lin Zheng, Lingpeng Kong*\n\n**TL;DR:** This paper proposes Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self,Noncausal cross, and causal cross attentions, and sheds light on the fundamental problems of efficient attentions.\n\n**Abstract:** Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: SSM integration*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Mamba-FETrack: Frame-Event Tracking via State Space Model\n\n*From Search Query: SSM integration*\n\n*Bo Jiang, Xiao Wang, Zhe Wu, Shuai Wang, Shiao Wang, Ju Huang*\n\n**Abstract:** RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream). Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets. However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism. This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking. Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams. Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network. The fused features will be fed into the tracking head for target object localization. Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker. Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking. The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}.\n\n**Published:** 2024-04-28\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: long-sequence modeling*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 4. Efficiently Modeling Long Sequences with Structured State Spaces\n\n*From Search Query: long-sequence modeling*\n\n*Christopher R\u00e9, Karan Goel, Albert Gu*\n\n**Abstract:** A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.\n\n**Conference:** efficiently-modeling-long-sequences-with\n\n**Published:** 2021-10-31\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by integrating State Space Models (SSMs) with FastTTTLinear, several key points and strategies can be considered based on the provided analysis and additional insights from the search results:\n\n## Efficient Long-Sequence Modeling with SSMs\n- SSMs have been shown to be highly effective in modeling long-range dependencies, which is crucial for autoregressive language models. Studies like \"Long Range Language Modeling via Gated State Spaces\" and \"Simplified State Space Layers for Sequence Modeling\" highlight the success of SSMs in capturing long-range dependencies efficiently[Analysis References 1 & 4].\n\n## Computational Efficiency and Optimization\n- To manage the computational overhead of integrating SSMs with FastTTTLinear, efficient discretization techniques can be employed. For instance, the continuous formulation of SSMs can be discretized using rules such as zero-order hold, as mentioned in the spike-SSM paper. This approach helps in adapting SSMs to real-world discrete data while maintaining computational efficiency.\n\n## Hybrid Architectures\n- Combining the strengths of SSMs and other models, such as Transformers, can enhance performance and efficiency. For example, the PoinTramba model integrates Transformers to capture detailed dependencies within point groups, while using SSMs to model relationships between groups. This hybrid approach can be adapted for language models to leverage the best of both worlds.\n\n## Scalability and Distributed Training\n- For large-scale models, distributed training strategies are essential. Using data parallel and model parallel approaches, as described in the Amazon SageMaker documentation, can help in scaling the model efficiently. Techniques like automatic model splitting, pipeline execution scheduling, and mixed precision training can reduce memory consumption and improve training speed.\n\n## Specific Implementation Strategies\n- The integration can benefit from strategies like max-min boundary compression (PMBC) proposed in the spike-SSM paper. This strategy facilitates efficient inference and parallel training by handling long-range dependencies effectively. Implementing such strategies can optimize the performance of the integrated model.\n\n## Robustness and Local Feature Extraction\n- Ensuring the model is robust to diverse inputs and captures both local and global structures is crucial. Techniques like axis-wise reordering and sequence replication, as used in PointMamba and Point Cloud Mamba, can be adapted to improve the model's ability to capture local details while maintaining global context.\n\n## Future Directions\n- Conducting further research on specific implementation strategies, such as optimizing the state matrix and vectors in SSMs, and exploring empirical results from similar integrations can provide practical insights into the benefits and challenges of the proposed integration.\n- Investigating novel formulations for SSMs, such as spectral state space models, could offer additional avenues for improving the model's performance and efficiency[Analysis References 5].\n\nBy focusing on these areas, researchers can develop an autoregressive language model that integrates SSMs with FastTTTLinear efficiently, enhancing its ability to model long-range dependencies, maintain computational efficiency, and scale effectively.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal to integrate State Space Models (SSMs) with FastTTTLinear GAUs is supported by existing research that highlights the effectiveness of SSMs in long-sequence modeling. The integration aims to enhance the model's ability to capture long-range dependencies while maintaining computational efficiency. The search results provide insights into the theoretical foundation, implementation complexity, and potential benefits of such integration.\n\n### Useful References and Excerpts\n\n1. **Simplified State Space Layers for Sequence Modeling**\n   - \"Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks.\"\n   - This reference highlights the success of SSMs in long-range sequence modeling, supporting the proposal's motivation.\n\n2. **Robustifying State-space Models for Long Sequences via Approximate Diagonalization**\n   - \"State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks.\"\n   - This paper discusses the robustness and efficiency of SSMs, which are crucial for the proposed integration.\n\n3. **Gated Linear Attention Transformers with Hardware-Efficient Training**\n   - \"The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer.\"\n   - This reference provides insights into the effectiveness of gated linear attention, which is a component of the FastTTTLinear GAU.\n\n4. **Long Range Language Modeling via Gated State Spaces**\n   - \"State space models have shown to be effective at modeling long range dependencies, especially on sequence classification tasks.\"\n   - This supports the proposal's claim that SSMs can enhance long-range dependency modeling.\n\n5. **Spectral State Space Models**\n   - \"A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm.\"\n   - This reference provides a novel approach to SSMs that could inform the proposed integration.\n\n### Key Insights and Detailed Analysis\n\n- **Theoretical Justification**: The use of SSMs for global context modeling is theoretically sound, as they can efficiently transform input sequences into latent state representations.\n- **Implementation Complexity**: Integrating SSMs with FastTTTLinear GAUs may introduce computational overhead, but this can be managed through efficient discretization techniques.\n- **Scalability and Robustness**: SSMs have demonstrated scalability in handling large sequence lengths and robustness to diverse inputs, making them suitable for the proposed integration.\n\n### Future Search Plan\n\n- Conduct further searches on specific implementation strategies for integrating SSMs with FastTTTLinear GAUs, focusing on computational efficiency and optimization techniques.\n- Explore additional case studies or empirical results from similar integrations to assess practical benefits and challenges.\n\n### References\n\n1. Smith, J., Warrington, A., & Linderman, S. W. (2022). Simplified State Space Layers for Sequence Modeling. International Conference on Learning Representations.\n2. Yu, A., Nigmetov, A., Morozov, D., Mahoney, M. W., & Erichson, N. B. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization. arXiv.org.\n3. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv.org.\n4. Mehta, H., Gupta, A., Cutkosky, A., & Neyshabur, B. (2022). Long Range Language Modeling via Gated State Spaces. International Conference on Learning Representations.\n5. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv.org."
        },
        {
            "ready": false,
            "query": "SSM integration, FastTTTLinear optimization, long-sequence modeling",
            "detail": "Efficient integration of State Space Models with FastTTTLinear for enhanced long-sequence modeling",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nEfficient integration of State Space Models with FastTTTLinear for enhanced long-sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.99)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 0.99)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n#### 2. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.98)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 1/35 (Score: 0.98)*\n\n```\n# Efficient Long Sequence Modeling via State Space Augmented Transformer \n\nSimiao Zuo ${ }^{* \\ddagger}$, Xiaodong Liu ${ }^{* \\dagger \\wedge}$, Jian Jiao ${ }^{\\dagger \\diamond}$, Denis Charles ${ }^{\\diamond}$, Eren Manavoglu ${ }^{\\wedge}$,<br>Tuo Zhao ${ }^{\\ddagger}$ and Jianfeng Gao ${ }^{\\circ}$<br>${ }^{\\ddagger}$ Georgia Institute of Technology ${ }^{\\diamond}$ Microsoft\n\n\n#### Abstract\n\nTransformer models have achieved superior performance in various natural language processing tasks.\n```\n\n#### 3. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.95)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 7/51 (Score: 0.95)*\n\n```\nThis result represents an awkward reality for existing RNNs. On one hand, the main advantage of RNNs (vs. Transformers) is their linear (vs. quadratic) complexity. This asymptotic advantage is only realized in practice for long context, which according to Figure 3 is after 8 k . On the other hand, once context is long enough, existing RNNs such as Mamba struggle to actually take advantage of the extra information being conditioned on. The difficulty with long context is inherent to the very nature of RNN layers: Unlike self-attention, RNN layers have to compress context into a hidden state of fixed size. As a compression heuristic,\nthe update rule needs to discover the underlying structures and relationships among thousands or potentially millions of tokens. In this paper, we begin with the observation that self-supervised learning can compress a massive training set into the weights of a model such as an LLM, which often exhibits deep understanding about the semantic connections among its training data - exactly what we need from a compression heuristic. TTT layers. Motivated by this observation, we design a new class of sequence modeling layers where the hidden state is a model, and the update rule is a step of self-supervised learning. Because the process of updating the hidden state on a test sequence is equivalent to training a model at test time, this new class of layers is called Test-Time Training (TTT) layers. We introduce two simple instantiations within this class: TTT-Linear and TTT-MLP, where the hidden state is a linear model and a two-layer MLP, respectively. TTT layers can be integrated into any network architecture and optimized end-to-end, similar to RNNs layers and self-attention. Wall-clock time. While the TTT layer is already efficient in FLOPs, we propose two practical innovations to make it efficient in wall-clock time. First, similar to the standard practice of taking gradient steps on mini-batches of sequences during regular training for better parallelism, we use mini-batches of tokens during TTT. Second, we develop a dual form for operations inside each TTT mini-batch, to better take advantage of modern GPUs and TPUs. The dual form is equivalent in output to the naive implementation, but trains more than $5 \\times$ faster. As shown in Figure 3, TTT-Linear is faster than Transformer at 8 k context and matches Mamba. Evaluations and open problems. While we have highlighted some results for TTT-Linear at the beginning of the paper, Section 3 presents more comprehensive evaluations for both TTT-Linear and TTT-MLP, and open problems exposed by our evaluations. For example, our evaluations following the Chinchilla recipe [34] do not cleanly fit a linear scaling trend even for the Transformer baseline.\n```\n\n#### 4. Spectral State Space Models (Avg. Score: 0.95)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.97)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.92)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: SSM integration, FastTTTLinear optimization, long-sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations\n\n*From Search Query: SSM integration*\n\n*Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, Rui Yan*\n\n**TL;DR:** A comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations, and distinguishes between structured and unstructured knowledge, leading to more effective utilization of information.\n\n**Abstract:** Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability of capturing underlying relations and properties of bio-entities. Our code is available at $\\href{https://github.com/QizhiPei/BioT5}{Github}$.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 40  (*Influential: 8*)\n\n#### 2. Hierarchical Integration Diffusion Model for Realistic Image Deblurring\n\n*From Search Query: SSM integration*\n\n*Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, L. Kong, X. Yuan*\n\n**TL;DR:** The Hierarchical Integration Diffusion Model (HI-Diff) is proposed, which designs the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios.\n\n**Abstract:** Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 39  (*Influential: 6*)\n\n#### 3. MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning\n\n*From Search Query: SSM integration*\n\n*Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, Hongsheng Li*\n\n**TL;DR:** This paper proposes a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities, and yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems.\n\n**Abstract:** The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-art scores among open-source LLMs on the MATH (45.2%) and GSM8K (83.9%) datasets, substantially outperforming other open-source alternatives. Notably, the MathCoder model not only surpasses ChatGPT-3.5 and PaLM-2 on GSM8K and MATH but also outperforms GPT-4 on the competition-level MATH dataset. The dataset and models will be released at https://github.com/mathllm/MathCoder.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 53  (*Influential: 4*)\n\n#### 4. Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n\n*From Search Query: FastTTTLinear optimization*\n\n*Rafael Rafailov, Archit Sharma, E. Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn*\n\n**TL;DR:** A new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form is introduced, allowing us to solve the standard RLHF problem with only a simple classification loss.\n\n**Abstract:** While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1933  (*Influential: 585*)\n\n#### 5. Symbolic Discovery of Optimization Algorithms\n\n*From Search Query: FastTTTLinear optimization*\n\n*Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le*\n\n**TL;DR:** Lion is a simple and effective optimization algorithm that requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function and is more memory-efficient than Adam as it only keeps track of the momentum.\n\n**Abstract:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 244  (*Influential: 40*)\n\n#### 6. Constrained Efficient Global Optimization of Expensive Black-box Functions\n\n*From Search Query: FastTTTLinear optimization*\n\n*Donald R. Jones, Matthias Schonlau, W. Welch*\n\n**TL;DR:** This work proposes CONFIG (CONstrained efFIcient Global Optimization), a simple and effective algorithm to solve the problem of constrained efficient global optimization, where both the objective and constraints are expensive black-box functions that can be learned with Gaussian processes.\n\n**Abstract:** We study the problem of constrained efficient global optimization, where both the objective and constraints are expensive black-box functions that can be learned with Gaussian processes. We propose CONFIG (CONstrained efFIcient Global Optimization), a simple and effective algorithm to solve it. Under certain regularity assumptions, we show that our algorithm enjoys the same cumulative regret bound as that in the unconstrained case and similar cumulative constraint violation upper bounds. For commonly used Matern and Squared Exponential kernels, our bounds are sublinear and allow us to derive a convergence rate to the optimal solution of the original constrained problem. In addition, our method naturally provides a scheme to declare infeasibility when the original black-box optimization problem is infeasible. Numerical experiments on sampled instances from the Gaussian process, artificial numerical problems, and a black-box building controller tuning problem all demonstrate the competitive performance of our algorithm. Compared to the other state-of-the-art methods, our algorithm significantly improves the theoretical guarantees, while achieving competitive empirical performance.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 2049  (*Influential: 134*)\n\n#### 7. What Makes Convolutional Models Great on Long Sequence Modeling?\n\n*From Search Query: long-sequence modeling*\n\n*Yuhong Li, Tianle Cai, Yi Zhang, De-huai Chen, Debadeepta Dey*\n\n**TL;DR:** A simple yet effective convolutional model called Structured Global Convolution (SGConv), which exhibits strong empirical performance over several tasks and shows the potential to improve both efficiency and performance when plugging SGConv into standard language and vision models.\n\n**Abstract:** Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependency efficiently. Attention overcomes this problem by aggregating global information but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. [2021] proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes. As a result, S4 is less intuitive and hard to use. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 82  (*Influential: 15*)\n\n#### 8. SMR: State Memory Replay for Long Sequence Modeling\n\n*From Search Query: long-sequence modeling*\n\n*Biqing Qi, Junqi Gao, Kaiyan Zhang, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou*\n\n**TL;DR:** A novel non-recursive non-uniform sample processing strategy, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data, enables SSMs to stably model varying sampling points.\n\n**Abstract:** Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 9. CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling\n\n*From Search Query: long-sequence modeling*\n\n*Jinchao Zhang, Shuyang Jiang, Jiangtao Feng, Lin Zheng, Lingpeng Kong*\n\n**TL;DR:** This paper proposes Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self,Noncausal cross, and causal cross attentions, and sheds light on the fundamental problems of efficient attentions.\n\n**Abstract:** Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tasks, CAB validates efficient attentions in eight backbone networks to show their generalization across neural architectures. We conduct exhaustive experiments to benchmark the performances of nine widely-used efficient attention architectures designed with different philosophies on CAB. Extensive experimental results also shed light on the fundamental problems of efficient attentions, such as efficiency length against vanilla attention, performance consistency across attention patterns, the benefit of attention mechanisms, and interpolation/extrapolation on long-context language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: SSM integration*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Mamba-FETrack: Frame-Event Tracking via State Space Model\n\n*From Search Query: SSM integration*\n\n*Bo Jiang, Xiao Wang, Zhe Wu, Shuai Wang, Shiao Wang, Ju Huang*\n\n**Abstract:** RGB-Event based tracking is an emerging research topic, focusing on how to effectively integrate heterogeneous multi-modal data (synchronized exposure video frames and asynchronous pulse Event stream). Existing works typically employ Transformer based networks to handle these modalities and achieve decent accuracy through input-level or feature-level fusion on multiple datasets. However, these trackers require significant memory consumption and computational complexity due to the use of self-attention mechanism. This paper proposes a novel RGB-Event tracking framework, Mamba-FETrack, based on the State Space Model (SSM) to achieve high-performance tracking while effectively reducing computational costs and realizing more efficient tracking. Specifically, we adopt two modality-specific Mamba backbone networks to extract the features of RGB frames and Event streams. Then, we also propose to boost the interactive learning between the RGB and Event features using the Mamba network. The fused features will be fed into the tracking head for target object localization. Extensive experiments on FELT and FE108 datasets fully validated the efficiency and effectiveness of our proposed tracker. Specifically, our Mamba-based tracker achieves 43.5/55.6 on the SR/PR metric, while the ViT-S based tracker (OSTrack) obtains 40.0/50.9. The GPU memory cost of ours and ViT-S based tracker is 13.98GB and 15.44GB, which decreased about $9.5\\%$. The FLOPs and parameters of ours/ViT-S based OSTrack are 59GB/1076GB and 7MB/60MB, which decreased about $94.5\\%$ and $88.3\\%$, respectively. We hope this work can bring some new insights to the tracking field and greatly promote the application of the Mamba architecture in tracking. The source code of this work will be released on \\url{https://github.com/Event-AHU/Mamba_FETrack}.\n\n**Published:** 2024-04-28\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: long-sequence modeling*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 4. Efficiently Modeling Long Sequences with Structured State Spaces\n\n*From Search Query: long-sequence modeling*\n\n*Christopher R\u00e9, Karan Goel, Albert Gu*\n\n**Abstract:** A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.\n\n**Conference:** efficiently-modeling-long-sequences-with\n\n**Published:** 2021-10-31\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating State Space Models (SSMs) with FastTTTLinear, several key points and strategies can be considered based on the provided sources and analysis:\n\n## Theoretical Foundation and Effectiveness of SSMs\n- State Space Models (SSMs) have been shown to be highly effective in long-sequence modeling due to their ability to transform input sequences into latent state representations efficiently. This is highlighted in the context of Mamba, which uses SSMs to achieve linear computational complexity and constant memory requirements, outperforming or matching Transformers in various tasks.\n\n## Integration with FastTTTLinear\n- The integration of SSMs with FastTTTLinear GAUs can leverage the efficiency of FastTTTLinear in handling long contexts. FastTTTLinear has been shown to be faster than Transformers at certain context lengths and matches Mamba in wall-clock time, indicating its potential for efficient long-sequence modeling.\n\n## Efficient Implementation and Optimization\n- To manage the computational overhead of integrating SSMs with FastTTTLinear, efficient discretization techniques can be employed. For instance, the use of zero-order hold (ZOH) for discretizing continuous SSM formulations can make the model more suitable for modern computational frameworks.\n- The Mamba model's hardware-aware parallel algorithm in recurrent mode can be adapted to optimize the integration, ensuring fast inference and linear scaling in sequence length.\n\n## Addressing Local Pattern Shortcuts\n- One of the challenges in using SSMs like Mamba is their reliance on local pattern shortcuts, which can hinder their ability to handle distributed key information. Introducing a global selection mechanism, as proposed in the analysis of Mamba, can mitigate this issue and improve performance on tasks requiring long-context reasoning.\n\n## Scalability and Robustness\n- SSMs have demonstrated scalability in handling large sequence lengths and robustness to diverse inputs. The MatMamba model, which combines the Matryoshka approach with Mamba2, allows for efficient deployment at different model sizes, making it a practical option for flexible and scalable modeling.\n\n## Practical Implementation Strategies\n- The SC-MAMBA2 model provides an example of how SSMs can be integrated with specific domain requirements, such as modeling large gene sequences. This integration involves unique design modifications and a bidirectional modeling approach within the state-space modules, which can be adapted for language modeling tasks.\n\n### Future Search Plan\n\n- **Implementation Details**: Search for specific implementation strategies and code examples that demonstrate how to integrate SSMs with FastTTTLinear GAUs. This could involve looking into the Mamba and MatMamba codebases and related research papers.\n- **Empirical Results**: Look for empirical results and case studies that evaluate the performance of similar integrations. This includes examining the results from Mamba, MatMamba, and other SSM-based models on various benchmarks and tasks.\n- **Optimization Techniques**: Investigate optimization techniques that can further enhance the computational efficiency of the integrated model. This might involve exploring hardware-aware parallel algorithms, efficient discretization methods, and other optimization strategies.\n\nBy focusing on these areas, researchers can develop a more efficient and scalable autoregressive language model that leverages the strengths of both SSMs and FastTTTLinear.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal to integrate State Space Models (SSMs) with FastTTTLinear GAUs is supported by existing research that highlights the effectiveness of SSMs in long-sequence modeling. The integration aims to enhance the model's ability to capture long-range dependencies while maintaining computational efficiency. The search results provide insights into the theoretical foundation, implementation complexity, and potential benefits of such integration.\n\n### Useful References and Excerpts\n\n1. **Simplified State Space Layers for Sequence Modeling**\n   - \"Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks.\"\n   - This reference highlights the success of SSMs in long-range sequence modeling, supporting the proposal's motivation.\n\n2. **Efficient Long Sequence Modeling via State Space Augmented Transformer**\n   - \"The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\"\n   - This paper discusses the integration of SSMs with attention mechanisms, similar to the proposal.\n\n3. **Spectral State Space Models**\n   - \"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n   - This reference provides a novel approach to SSMs that could inform the proposed integration.\n\n4. **Learning to (Learn at Test Time): RNNs with Expressive Hidden States**\n   - \"With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time.\"\n   - This supports the efficiency claims of FastTTTLinear, which is part of the proposal.\n\n### Key Insights and Detailed Analysis\n\n- **Theoretical Justification**: The use of SSMs for global context modeling is theoretically sound, as they can efficiently transform input sequences into latent state representations.\n- **Implementation Complexity**: Integrating SSMs with FastTTTLinear GAUs may introduce computational overhead, but this can be managed through efficient discretization techniques.\n- **Scalability and Robustness**: SSMs have demonstrated scalability in handling large sequence lengths and robustness to diverse inputs, making them suitable for the proposed integration.\n\n### Future Search Plan\n\n- Conduct further searches on specific implementation strategies for integrating SSMs with FastTTTLinear GAUs, focusing on computational efficiency and optimization techniques.\n- Explore additional case studies or empirical results from similar integrations to assess practical benefits and challenges.\n\n### References\n\n1. Smith, J., Warrington, A., & Linderman, S. W. (2022). Simplified State Space Layers for Sequence Modeling. International Conference on Learning Representations.\n2. Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv.org.\n3. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv.org.\n4. Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G., Dubois, Y., Chen, X., Wang, X., Koyejo, S., Hashimoto, T., & Guestrin, C. (2024). Learning to (Learn at Test Time): RNNs with Expressive Hidden States. arXiv.org."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 1.0434,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.7558425,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "\n# DualStateFastTTTLinear: Enhancing FastTTTLinear with Dual State Space Modeling\n\n## Motivation\n\nModern language models face significant challenges in efficiently modeling long-range dependencies while maintaining computational efficiency. While the FastTTTLinear GAU has shown promise through its integration of gated linear attention, there remain opportunities for improvement in:\n\n1. **Long-range Dependency Modeling**: Current approaches struggle to effectively capture dependencies across very long sequences\n2. **Adaptive Processing**: Fixed architectures may not optimally handle varying sequence lengths and input distributions\n3. **Computational Efficiency**: Need for better parallel processing and memory management strategies\n4. **State Management**: Current state space models lack sophisticated mechanisms for selective information retention\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **State Space Management**:\n   - Single-state representations limit the model's ability to capture multiple types of dependencies\n   - Fixed state dimensions may not optimally adapt to varying sequence complexities\n   - Limited mechanisms for selective state updates\n\n2. **Attention Mechanism**:\n   - Current gated linear attention may not fully leverage the benefits of state space modeling\n   - Limited integration between attention and state space components\n   - Suboptimal parallel processing capabilities\n\n3. **Memory Efficiency**:\n   - High memory requirements for long sequences\n   - Inefficient state caching mechanisms\n   - Limited ability to selectively retain important information\n\n### Opportunities for Improvement\n\n1. **Dual State Architecture**:\n   - Leverage state space duality theory for enhanced modeling\n   - Implement parallel state processing paths\n   - Enable adaptive state dimension adjustment\n\n2. **Enhanced Gating Mechanisms**:\n   - Introduce sophisticated gating for state updates\n   - Implement selective information flow control\n   - Better integration of attention and state components\n\n3. **Efficient Implementation**:\n   - Optimize parallel processing capabilities\n   - Improve memory management\n   - Enhance hardware utilization\n\n## Core Idea and Philosophy\n\nThe DualStateFastTTTLinear introduces three key innovations:\n\n1. **Dual State Processing**:\n   - Maintain two parallel state spaces for different types of dependencies\n   - Enable adaptive state dimension adjustment\n   - Implement selective state update mechanisms\n\n2. **Enhanced Gating Integration**:\n   - Sophisticated gating mechanisms for state updates\n   - Improved attention-state integration\n   - Adaptive information flow control\n\n3. **Efficient Implementation**:\n   - Parallel processing optimizations\n   - Memory-efficient state management\n   - Hardware-aware design\n\n## Design Plan\n\n### Mathematical Formulation\n\n1. **Dual State Update**:\n   ```\n   s1_t = f_update(s1_{t-1}, x_t, g1_t)\n   s2_t = f_update(s2_{t-1}, x_t, g2_t)\n   ```\n   where:\n   - s1_t, s2_t are the two state vectors\n   - g1_t, g2_t are gating vectors\n   - f_update is the state update function\n\n2. **Gating Mechanism**:\n   ```\n   g1_t = \u03c3(W_g1 \u00b7 [x_t; s1_{t-1}] + b_g1)\n   g2_t = \u03c3(W_g2 \u00b7 [x_t; s2_{t-1}] + b_g2)\n   ```\n   where \u03c3 is the sigmoid activation function\n\n3. **Output Computation**:\n   ```\n   y_t = W_o \u00b7 [s1_t; s2_t] + b_o\n   ```\n\n### Implementation Details\n\n```python\nclass DualStateFastTTTLinear(nn.Module):\n    def __init__(self, embed_dim, num_heads, state_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.state_dim = state_dim\n        \n        # State processing components\n        self.state1_proj = nn.Linear(state_dim, embed_dim)\n        self.state2_proj = nn.Linear(state_dim, embed_dim)\n        \n        # Gating mechanisms\n        self.gate1 = nn.Linear(embed_dim + state_dim, state_dim)\n        self.gate2 = nn.Linear(embed_dim + state_dim, state_dim)\n        \n        # Output projection\n        self.out_proj = nn.Linear(2 * state_dim, embed_dim)\n        \n        # Attention components\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, x, state1=None, state2=None):\n        batch_size, seq_len, _ = x.shape\n        \n        # Initialize states if needed\n        if state1 is None:\n            state1 = torch.zeros(batch_size, self.state_dim)\n        if state2 is None:\n            state2 = torch.zeros(batch_size, self.state_dim)\n            \n        # Process states\n        for t in range(seq_len):\n            # Compute gates\n            g1 = torch.sigmoid(self.gate1(torch.cat([x[:, t], state1], dim=-1)))\n            g2 = torch.sigmoid(self.gate2(torch.cat([x[:, t], state2], dim=-1)))\n            \n            # Update states\n            state1 = g1 * state1 + (1 - g1) * self.state1_proj(x[:, t])\n            state2 = g2 * state2 + (1 - g2) * self.state2_proj(x[:, t])\n            \n        # Compute attention\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n        \n        # Combine attention and states\n        attn_output = self._compute_attention(q, k, v)\n        state_output = self.out_proj(torch.cat([state1, state2], dim=-1))\n        \n        return attn_output + state_output, (state1, state2)\n        \n    def _compute_attention(self, q, k, v):\n        # Implement efficient linear attention here\n        pass\n```\n\n### Optimization Strategies\n\n1. **Parallel Processing**:\n   - Implement batch processing for state updates\n   - Optimize attention computation for parallel execution\n   - Utilize hardware-specific optimizations\n\n2. **Memory Management**:\n   - Implement efficient state caching\n   - Use gradient checkpointing when needed\n   - Optimize memory access patterns\n\n3. **Hardware Utilization**:\n   - Leverage tensor cores for matrix operations\n   - Implement efficient memory access patterns\n   - Optimize for specific hardware architectures\n\n## Theoretical Analysis\n\n1. **Complexity Analysis**:\n   - Time Complexity: O(n) for sequence length n\n   - Space Complexity: O(d) for state dimension d\n   - Memory Access: O(1) per token during inference\n\n2. **Convergence Properties**:\n   - Stable gradient flow through dual states\n   - Controlled information propagation via gating\n   - Bounded state updates\n\n3. **Advantages**:\n   - Better long-range dependency modeling\n   - Adaptive processing capabilities\n   - Efficient parallel implementation\n   - Improved memory utilization\n\n## Expected Benefits\n\n1. **Enhanced Performance**:\n   - Improved perplexity on long sequences\n   - Better handling of varying input distributions\n   - Enhanced gradient flow\n\n2. **Computational Efficiency**:\n   - Efficient parallel processing\n   - Reduced memory requirements\n   - Better hardware utilization\n\n3. **Scalability**:\n   - Linear scaling with sequence length\n   - Efficient handling of large models\n   - Adaptable to different hardware platforms\n\n## References\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. International Conference on Machine Learning.\n\n2. He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n3. Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n4. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n5. Smith, J., et al. (2022). Simplified State Space Layers for Sequence Modeling. International Conference on Learning Representations.\n",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "gpt4o_0806",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "An enhancement to FastTTTLinear that integrates dual state space modeling with gated linear attention, leveraging state space duality theory to improve long-range dependency modeling while maintaining computational efficiency. The design introduces adaptive state management and parallel processing optimizations to better handle varying sequence lengths and input distributions.",
    "ideation": null,
    "modelname": "dualstategpt_1",
    "suggestions": "1. **Numerical Stability**: Implement stabilization techniques such as reparameterization, layer-wise nonlinearity, and appropriate normalization (e.g., LayerNorm, RMSNorm) to ensure numerical stability.\n2. **Optimization Strategies**: Focus on optimizing the implementation to minimize computational overhead, possibly leveraging hardware-aware algorithms and efficient parallelization.\n3. **Empirical Validation**: Plan for rigorous empirical evaluation against benchmarks to validate the proposed benefits and identify any unforeseen issues.\n4. **Detailed Implementation Plan**: Provide more granular implementation details, including parameter initialization strategies and training protocols that have proven effective in prior work.",
    "user_input": ""
}