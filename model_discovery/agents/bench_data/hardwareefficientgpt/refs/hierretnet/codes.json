{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = HierarchicalRetNetMLP(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass HierarchicalRetNetMLP(GAUBase):\n    \"\"\"\n    Hierarchical RetNet MLP with multi-timescale processing and adaptive compression.\n    \n    This implementation extends the original RetNetMLP with:\n    1. Multi-timescale processing paths\n    2. Adaptive state compression\n    3. Hierarchical feed-forward networks\n    4. Selective state updates\n    \n    Architecture diagram:\n    \n    .. code-block:: text\n\n                Input X\n                   |\n        +----------+----------+\n        |          |          |\n      Fast      Medium      Slow\n      Path       Path       Path\n        |          |          |\n     Process    Process    Process\n        |          |          |\n      Adapt      Adapt      Adapt\n      Compr      Compr      Compr\n        |          |          |\n        +----------+----------+\n                   |\n              Combine & Gate\n                   |\n                Output Y\n\n    Args:\n        embed_dim: Input embedding dimension\n        block_loc: Location of block in network (layer_idx, n_block)\n        kwarg_all: Dictionary of additional arguments\n        device: Device to place tensors on\n        dtype: Data type for tensors\n        hidden_size: Size of hidden layers (default: embed_dim)\n        num_timescales: Number of timescale paths (default: 3)\n        compress_factor: Factor for adaptive compression (default: 4)\n        update_freq: Update frequencies for each timescale (default: [1,4,16])\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_timescales: int=3,\n        compress_factor: int=4, update_freq: list=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_timescales = num_timescales\n        self.compress_factor = compress_factor\n        self.update_freq = update_freq if update_freq is not None else [1, \n            4, 16]\n        self.timescale_sizes = []\n        for i in range(num_timescales):\n            size = hidden_size // compress_factor ** i\n            size = max(size, hidden_size // compress_factor ** (\n                num_timescales - 1))\n            self.timescale_sizes.append(size)\n        self.up_projs = nn.ModuleList([nn.Linear(hidden_size, size * 2,\n            bias=False, **self.factory_kwargs) for size in self.\n            timescale_sizes])\n        self.down_projs = nn.ModuleList([nn.Linear(size, hidden_size, bias=\n            False, **self.factory_kwargs) for size in self.timescale_sizes])\n        self.compress_scores = nn.ModuleList([nn.Linear(hidden_size, 1,\n            bias=False, **self.factory_kwargs) for _ in range(num_timescales)])\n        self.register_buffer('step_counter', torch.zeros((), device=device))\n        self.act_fn = ACT2FN['swish']\n        self.out_proj = nn.Linear(hidden_size * num_timescales, hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _should_update(self, timescale_idx):\n        \"\"\"Determine if the given timescale should update at current step\"\"\"\n        return self.step_counter % self.update_freq[timescale_idx] == 0\n\n    def _process_timescale(self, X, timescale_idx, prev_state):\n        \"\"\"Process input through one timescale path\"\"\"\n        up_proj = self.up_projs[timescale_idx]\n        down_proj = self.down_projs[timescale_idx]\n        hidden = up_proj(X)\n        gate, value = hidden.chunk(2, dim=-1)\n        hidden = self.act_fn(gate) * value\n        importance = torch.sigmoid(self.compress_scores[timescale_idx](X))\n        if prev_state is not None and not self._should_update(timescale_idx):\n            hidden = prev_state\n        else:\n            hidden = hidden * importance\n        output = down_proj(hidden)\n        return output, hidden\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of hierarchical MLP.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, hidden_size)\n            Z: Dictionary containing previous states if any\n            \n        Returns:\n            Y: Output tensor of shape (batch_size, seq_len, hidden_size)\n            Z_: Dictionary containing updated states\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        self.step_counter += 1\n        prev_states = [Z.get(f'state_{i}', None) for i in range(self.\n            num_timescales)]\n        outputs = []\n        new_states = {}\n        for i in range(self.num_timescales):\n            output, state = self._process_timescale(X, i, prev_states[i])\n            outputs.append(output)\n            new_states[f'state_{i}'] = state\n        combined = torch.cat(outputs, dim=-1)\n        Y = self.out_proj(combined)\n        return Y, new_states\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'num_timescales': 3, 'compress_factor': 4, 'update_freq': None}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = HierarchicalRetNetMLP(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass HierarchicalRetNetMLP(GAUBase):\n    \"\"\"\n    Hierarchical RetNet MLP with multi-timescale processing and adaptive compression.\n    \n    This implementation extends the original RetNetMLP with:\n    1. Multi-timescale processing paths\n    2. Adaptive state compression\n    3. Hierarchical feed-forward networks\n    4. Selective state updates\n    \n    Architecture diagram:\n    \n    .. code-block:: text\n\n                Input X\n                   |\n        +----------+----------+\n        |          |          |\n      Fast      Medium      Slow\n      Path       Path       Path\n        |          |          |\n     Process    Process    Process\n        |          |          |\n      Adapt      Adapt      Adapt\n      Compr      Compr      Compr\n        |          |          |\n        +----------+----------+\n                   |\n              Combine & Gate\n                   |\n                Output Y\n\n    Args:\n        embed_dim: Input embedding dimension\n        block_loc: Location of block in network (layer_idx, n_block)\n        kwarg_all: Dictionary of additional arguments\n        device: Device to place tensors on\n        dtype: Data type for tensors\n        hidden_size: Size of hidden layers (default: embed_dim)\n        num_timescales: Number of timescale paths (default: 3)\n        compress_factor: Factor for adaptive compression (default: 4)\n        update_freq: Update frequencies for each timescale (default: [1,4,16])\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_timescales: int=3,\n        compress_factor: int=4, update_freq: list=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_timescales = num_timescales\n        self.compress_factor = compress_factor\n        self.update_freq = update_freq if update_freq is not None else [1, \n            4, 16]\n        self.timescale_sizes = []\n        for i in range(num_timescales):\n            size = hidden_size // compress_factor ** i\n            size = max(size, hidden_size // compress_factor ** (\n                num_timescales - 1))\n            self.timescale_sizes.append(size)\n        self.up_projs = nn.ModuleList([nn.Linear(hidden_size, size * 2,\n            bias=False, **self.factory_kwargs) for size in self.\n            timescale_sizes])\n        self.down_projs = nn.ModuleList([nn.Linear(size, hidden_size, bias=\n            False, **self.factory_kwargs) for size in self.timescale_sizes])\n        self.compress_scores = nn.ModuleList([nn.Linear(hidden_size, 1,\n            bias=False, **self.factory_kwargs) for _ in range(num_timescales)])\n        self.register_buffer('step_counter', torch.zeros((), device=device))\n        self.act_fn = ACT2FN['swish']\n        self.out_proj = nn.Linear(hidden_size * num_timescales, hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _should_update(self, timescale_idx):\n        \"\"\"Determine if the given timescale should update at current step\"\"\"\n        return self.step_counter % self.update_freq[timescale_idx] == 0\n\n    def _process_timescale(self, X, timescale_idx, prev_state):\n        \"\"\"Process input through one timescale path\"\"\"\n        up_proj = self.up_projs[timescale_idx]\n        down_proj = self.down_projs[timescale_idx]\n        hidden = up_proj(X)\n        gate, value = hidden.chunk(2, dim=-1)\n        hidden = self.act_fn(gate) * value\n        importance = torch.sigmoid(self.compress_scores[timescale_idx](X))\n        if prev_state is not None and not self._should_update(timescale_idx):\n            hidden = prev_state\n        else:\n            hidden = hidden * importance\n        output = down_proj(hidden)\n        return output, hidden\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of hierarchical MLP.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, hidden_size)\n            Z: Dictionary containing previous states if any\n            \n        Returns:\n            Y: Output tensor of shape (batch_size, seq_len, hidden_size)\n            Z_: Dictionary containing updated states\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        self.step_counter += 1\n        prev_states = [Z.get(f'state_{i}', None) for i in range(self.\n            num_timescales)]\n        outputs = []\n        new_states = {}\n        for i in range(self.num_timescales):\n            output, state = self._process_timescale(X, i, prev_states[i])\n            outputs.append(output)\n            new_states[f'state_{i}'] = state\n        combined = torch.cat(outputs, dim=-1)\n        Y = self.out_proj(combined)\n        return Y, new_states\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'num_timescales': 3, 'compress_factor': 4, 'update_freq': None}\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 64\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = HierarchicalRetNetMLP(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass HierarchicalRetNetMLP(GAUBase):\n    \"\"\"\n    Hierarchical RetNet MLP with multi-timescale processing and adaptive compression.\n    \n    This implementation extends the original RetNetMLP with:\n    1. Multi-timescale processing paths\n    2. Adaptive state compression\n    3. Hierarchical feed-forward networks\n    4. Selective state updates\n    \n    Architecture diagram:\n    \n    .. code-block:: text\n\n                Input X\n                   |\n        +----------+----------+\n        |          |          |\n      Fast      Medium      Slow\n      Path       Path       Path\n        |          |          |\n     Process    Process    Process\n        |          |          |\n      Adapt      Adapt      Adapt\n      Compr      Compr      Compr\n        |          |          |\n        +----------+----------+\n                   |\n              Combine & Gate\n                   |\n                Output Y\n\n    Args:\n        embed_dim: Input embedding dimension\n        block_loc: Location of block in network (layer_idx, n_block)\n        kwarg_all: Dictionary of additional arguments\n        device: Device to place tensors on\n        dtype: Data type for tensors\n        hidden_size: Size of hidden layers (default: embed_dim)\n        num_timescales: Number of timescale paths (default: 3)\n        compress_factor: Factor for adaptive compression (default: 4)\n        update_freq: Update frequencies for each timescale (default: [1,4,16])\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_timescales: int=3,\n        compress_factor: int=4, update_freq: list=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_timescales = num_timescales\n        self.compress_factor = compress_factor\n        self.update_freq = update_freq if update_freq is not None else [1, \n            4, 16]\n        self.timescale_sizes = []\n        for i in range(num_timescales):\n            size = hidden_size // compress_factor ** i\n            size = max(size, hidden_size // compress_factor ** (\n                num_timescales - 1))\n            self.timescale_sizes.append(size)\n        self.up_projs = nn.ModuleList([nn.Linear(hidden_size, size * 2,\n            bias=False, **self.factory_kwargs) for size in self.\n            timescale_sizes])\n        self.down_projs = nn.ModuleList([nn.Linear(size, hidden_size, bias=\n            False, **self.factory_kwargs) for size in self.timescale_sizes])\n        self.compress_scores = nn.ModuleList([nn.Linear(hidden_size, 1,\n            bias=False, **self.factory_kwargs) for _ in range(num_timescales)])\n        self.register_buffer('step_counter', torch.zeros((), device=device))\n        self.act_fn = ACT2FN['swish']\n        self.out_proj = nn.Linear(hidden_size * num_timescales, hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _should_update(self, timescale_idx):\n        \"\"\"Determine if the given timescale should update at current step\"\"\"\n        return self.step_counter % self.update_freq[timescale_idx] == 0\n\n    def _process_timescale(self, X, timescale_idx, prev_state):\n        \"\"\"Process input through one timescale path\"\"\"\n        up_proj = self.up_projs[timescale_idx]\n        down_proj = self.down_projs[timescale_idx]\n        hidden = up_proj(X)\n        gate, value = hidden.chunk(2, dim=-1)\n        hidden = self.act_fn(gate) * value\n        importance = torch.sigmoid(self.compress_scores[timescale_idx](X))\n        if prev_state is not None and not self._should_update(timescale_idx):\n            hidden = prev_state\n        else:\n            hidden = hidden * importance\n        output = down_proj(hidden)\n        return output, hidden\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of hierarchical MLP.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, hidden_size)\n            Z: Dictionary containing previous states if any\n            \n        Returns:\n            Y: Output tensor of shape (batch_size, seq_len, hidden_size)\n            Z_: Dictionary containing updated states\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        self.step_counter += 1\n        prev_states = [Z.get(f'state_{i}', None) for i in range(self.\n            num_timescales)]\n        outputs = []\n        new_states = {}\n        for i in range(self.num_timescales):\n            output, state = self._process_timescale(X, i, prev_states[i])\n            outputs.append(output)\n            new_states[f'state_{i}'] = state\n        combined = torch.cat(outputs, dim=-1)\n        Y = self.out_proj(combined)\n        return Y, new_states\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'num_timescales': 3, 'compress_factor': 4, 'update_freq': None}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = HierarchicalRetNetMLP(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass HierarchicalRetNetMLP(GAUBase):\n    \"\"\"\n    Hierarchical RetNet MLP with multi-timescale processing and adaptive compression.\n    \n    This implementation extends the original RetNetMLP with:\n    1. Multi-timescale processing paths\n    2. Adaptive state compression\n    3. Hierarchical feed-forward networks\n    4. Selective state updates\n    \n    Architecture diagram:\n    \n    .. code-block:: text\n\n                Input X\n                   |\n        +----------+----------+\n        |          |          |\n      Fast      Medium      Slow\n      Path       Path       Path\n        |          |          |\n     Process    Process    Process\n        |          |          |\n      Adapt      Adapt      Adapt\n      Compr      Compr      Compr\n        |          |          |\n        +----------+----------+\n                   |\n              Combine & Gate\n                   |\n                Output Y\n\n    Args:\n        embed_dim: Input embedding dimension\n        block_loc: Location of block in network (layer_idx, n_block)\n        kwarg_all: Dictionary of additional arguments\n        device: Device to place tensors on\n        dtype: Data type for tensors\n        hidden_size: Size of hidden layers (default: embed_dim)\n        num_timescales: Number of timescale paths (default: 3)\n        compress_factor: Factor for adaptive compression (default: 4)\n        update_freq: Update frequencies for each timescale (default: [1,4,16])\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_timescales: int=3,\n        compress_factor: int=4, update_freq: list=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_timescales = num_timescales\n        self.compress_factor = compress_factor\n        self.update_freq = update_freq if update_freq is not None else [1, \n            4, 16]\n        self.timescale_sizes = []\n        for i in range(num_timescales):\n            size = hidden_size // compress_factor ** i\n            size = max(size, hidden_size // compress_factor ** (\n                num_timescales - 1))\n            self.timescale_sizes.append(size)\n        self.up_projs = nn.ModuleList([nn.Linear(hidden_size, size * 2,\n            bias=False, **self.factory_kwargs) for size in self.\n            timescale_sizes])\n        self.down_projs = nn.ModuleList([nn.Linear(size, hidden_size, bias=\n            False, **self.factory_kwargs) for size in self.timescale_sizes])\n        self.compress_scores = nn.ModuleList([nn.Linear(hidden_size, 1,\n            bias=False, **self.factory_kwargs) for _ in range(num_timescales)])\n        self.register_buffer('step_counter', torch.zeros((), device=device))\n        self.act_fn = ACT2FN['swish']\n        self.out_proj = nn.Linear(hidden_size * num_timescales, hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _should_update(self, timescale_idx):\n        \"\"\"Determine if the given timescale should update at current step\"\"\"\n        return self.step_counter % self.update_freq[timescale_idx] == 0\n\n    def _process_timescale(self, X, timescale_idx, prev_state):\n        \"\"\"Process input through one timescale path\"\"\"\n        up_proj = self.up_projs[timescale_idx]\n        down_proj = self.down_projs[timescale_idx]\n        hidden = up_proj(X)\n        gate, value = hidden.chunk(2, dim=-1)\n        hidden = self.act_fn(gate) * value\n        importance = torch.sigmoid(self.compress_scores[timescale_idx](X))\n        if prev_state is not None and not self._should_update(timescale_idx):\n            hidden = prev_state\n        else:\n            hidden = hidden * importance\n        output = down_proj(hidden)\n        return output, hidden\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of hierarchical MLP.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, hidden_size)\n            Z: Dictionary containing previous states if any\n            \n        Returns:\n            Y: Output tensor of shape (batch_size, seq_len, hidden_size)\n            Z_: Dictionary containing updated states\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        self.step_counter += 1\n        prev_states = [Z.get(f'state_{i}', None) for i in range(self.\n            num_timescales)]\n        outputs = []\n        new_states = {}\n        for i in range(self.num_timescales):\n            output, state = self._process_timescale(X, i, prev_states[i])\n            outputs.append(output)\n            new_states[f'state_{i}'] = state\n        combined = torch.cat(outputs, dim=-1)\n        Y = self.out_proj(combined)\n        return Y, new_states\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'num_timescales': 3, 'compress_factor': 4, 'update_freq': None}\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 63\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = HierarchicalRetNetMLP(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass HierarchicalRetNetMLP(GAUBase):\n    \"\"\"\n    Hierarchical RetNet MLP with multi-timescale processing and adaptive compression.\n    \n    This implementation extends the original RetNetMLP with:\n    1. Multi-timescale processing paths\n    2. Adaptive state compression\n    3. Hierarchical feed-forward networks\n    4. Selective state updates\n    \n    Architecture diagram:\n    \n    .. code-block:: text\n\n                Input X\n                   |\n        +----------+----------+\n        |          |          |\n      Fast      Medium      Slow\n      Path       Path       Path\n        |          |          |\n     Process    Process    Process\n        |          |          |\n      Adapt      Adapt      Adapt\n      Compr      Compr      Compr\n        |          |          |\n        +----------+----------+\n                   |\n              Combine & Gate\n                   |\n                Output Y\n\n    Args:\n        embed_dim: Input embedding dimension\n        block_loc: Location of block in network (layer_idx, n_block)\n        kwarg_all: Dictionary of additional arguments\n        device: Device to place tensors on\n        dtype: Data type for tensors\n        hidden_size: Size of hidden layers (default: embed_dim)\n        num_timescales: Number of timescale paths (default: 3)\n        compress_factor: Factor for adaptive compression (default: 4)\n        update_freq: Update frequencies for each timescale (default: [1,4,16])\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_timescales: int=3,\n        compress_factor: int=4, update_freq: list=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_timescales = num_timescales\n        self.compress_factor = compress_factor\n        self.update_freq = update_freq if update_freq is not None else [1, \n            4, 16]\n        self.timescale_sizes = []\n        for i in range(num_timescales):\n            size = hidden_size // compress_factor ** i\n            size = max(size, hidden_size // compress_factor ** (\n                num_timescales - 1))\n            self.timescale_sizes.append(size)\n        self.up_projs = nn.ModuleList([nn.Linear(hidden_size, size * 2,\n            bias=False, **self.factory_kwargs) for size in self.\n            timescale_sizes])\n        self.down_projs = nn.ModuleList([nn.Linear(size, hidden_size, bias=\n            False, **self.factory_kwargs) for size in self.timescale_sizes])\n        self.compress_scores = nn.ModuleList([nn.Linear(hidden_size, 1,\n            bias=False, **self.factory_kwargs) for _ in range(num_timescales)])\n        self.register_buffer('step_counter', torch.zeros((), device=device))\n        self.act_fn = ACT2FN['swish']\n        self.out_proj = nn.Linear(hidden_size * num_timescales, hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _should_update(self, timescale_idx):\n        \"\"\"Determine if the given timescale should update at current step\"\"\"\n        return self.step_counter % self.update_freq[timescale_idx] == 0\n\n    def _process_timescale(self, X, timescale_idx, prev_state):\n        \"\"\"Process input through one timescale path\"\"\"\n        up_proj = self.up_projs[timescale_idx]\n        down_proj = self.down_projs[timescale_idx]\n        hidden = up_proj(X)\n        gate, value = hidden.chunk(2, dim=-1)\n        hidden = self.act_fn(gate) * value\n        importance = torch.sigmoid(self.compress_scores[timescale_idx](X))\n        if prev_state is not None and not self._should_update(timescale_idx):\n            hidden = prev_state\n        else:\n            hidden = hidden * importance\n        output = down_proj(hidden)\n        return output, hidden\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of hierarchical MLP.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, hidden_size)\n            Z: Dictionary containing previous states if any\n            \n        Returns:\n            Y: Output tensor of shape (batch_size, seq_len, hidden_size)\n            Z_: Dictionary containing updated states\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        self.step_counter += 1\n        prev_states = [Z.get(f'state_{i}', None) for i in range(self.\n            num_timescales)]\n        outputs = []\n        new_states = {}\n        for i in range(self.num_timescales):\n            output, state = self._process_timescale(X, i, prev_states[i])\n            outputs.append(output)\n            new_states[f'state_{i}'] = state\n        combined = torch.cat(outputs, dim=-1)\n        Y = self.out_proj(combined)\n        return Y, new_states\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'num_timescales': 3, 'compress_factor': 4, 'update_freq': None}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = HierarchicalRetNetMLP(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass HierarchicalRetNetMLP(GAUBase):\n    \"\"\"\n    Hierarchical RetNet MLP with multi-timescale processing and adaptive compression.\n    \n    This implementation extends the original RetNetMLP with:\n    1. Multi-timescale processing paths\n    2. Adaptive state compression\n    3. Hierarchical feed-forward networks\n    4. Selective state updates\n    \n    Architecture diagram:\n    \n    .. code-block:: text\n\n                Input X\n                   |\n        +----------+----------+\n        |          |          |\n      Fast      Medium      Slow\n      Path       Path       Path\n        |          |          |\n     Process    Process    Process\n        |          |          |\n      Adapt      Adapt      Adapt\n      Compr      Compr      Compr\n        |          |          |\n        +----------+----------+\n                   |\n              Combine & Gate\n                   |\n                Output Y\n\n    Args:\n        embed_dim: Input embedding dimension\n        block_loc: Location of block in network (layer_idx, n_block)\n        kwarg_all: Dictionary of additional arguments\n        device: Device to place tensors on\n        dtype: Data type for tensors\n        hidden_size: Size of hidden layers (default: embed_dim)\n        num_timescales: Number of timescale paths (default: 3)\n        compress_factor: Factor for adaptive compression (default: 4)\n        update_freq: Update frequencies for each timescale (default: [1,4,16])\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_timescales: int=3,\n        compress_factor: int=4, update_freq: list=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_timescales = num_timescales\n        self.compress_factor = compress_factor\n        self.update_freq = update_freq if update_freq is not None else [1, \n            4, 16]\n        self.timescale_sizes = []\n        for i in range(num_timescales):\n            size = hidden_size // compress_factor ** i\n            size = max(size, hidden_size // compress_factor ** (\n                num_timescales - 1))\n            self.timescale_sizes.append(size)\n        self.up_projs = nn.ModuleList([nn.Linear(hidden_size, size * 2,\n            bias=False, **self.factory_kwargs) for size in self.\n            timescale_sizes])\n        self.down_projs = nn.ModuleList([nn.Linear(size, hidden_size, bias=\n            False, **self.factory_kwargs) for size in self.timescale_sizes])\n        self.compress_scores = nn.ModuleList([nn.Linear(hidden_size, 1,\n            bias=False, **self.factory_kwargs) for _ in range(num_timescales)])\n        self.register_buffer('step_counter', torch.zeros((), device=device))\n        self.act_fn = ACT2FN['swish']\n        self.out_proj = nn.Linear(hidden_size * num_timescales, hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _should_update(self, timescale_idx):\n        \"\"\"Determine if the given timescale should update at current step\"\"\"\n        return self.step_counter % self.update_freq[timescale_idx] == 0\n\n    def _process_timescale(self, X, timescale_idx, prev_state):\n        \"\"\"Process input through one timescale path\"\"\"\n        up_proj = self.up_projs[timescale_idx]\n        down_proj = self.down_projs[timescale_idx]\n        hidden = up_proj(X)\n        gate, value = hidden.chunk(2, dim=-1)\n        hidden = self.act_fn(gate) * value\n        importance = torch.sigmoid(self.compress_scores[timescale_idx](X))\n        if prev_state is not None and not self._should_update(timescale_idx):\n            hidden = prev_state\n        else:\n            hidden = hidden * importance\n        output = down_proj(hidden)\n        return output, hidden\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of hierarchical MLP.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, hidden_size)\n            Z: Dictionary containing previous states if any\n            \n        Returns:\n            Y: Output tensor of shape (batch_size, seq_len, hidden_size)\n            Z_: Dictionary containing updated states\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        self.step_counter += 1\n        prev_states = [Z.get(f'state_{i}', None) for i in range(self.\n            num_timescales)]\n        outputs = []\n        new_states = {}\n        for i in range(self.num_timescales):\n            output, state = self._process_timescale(X, i, prev_states[i])\n            outputs.append(output)\n            new_states[f'state_{i}'] = state\n        combined = torch.cat(outputs, dim=-1)\n        Y = self.out_proj(combined)\n        return Y, new_states\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'num_timescales': 3, 'compress_factor': 4, 'update_freq': None}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = HierarchicalRetNetMLP(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass HierarchicalRetNetMLP(GAUBase):\n    \"\"\"\n    Hierarchical RetNet MLP with multi-timescale processing and adaptive compression.\n    \n    This implementation extends the original RetNetMLP with:\n    1. Multi-timescale processing paths\n    2. Adaptive state compression\n    3. Hierarchical feed-forward networks\n    4. Selective state updates\n    \n    Architecture diagram:\n    \n    .. code-block:: text\n\n                Input X\n                   |\n        +----------+----------+\n        |          |          |\n      Fast      Medium      Slow\n      Path       Path       Path\n        |          |          |\n     Process    Process    Process\n        |          |          |\n      Adapt      Adapt      Adapt\n      Compr      Compr      Compr\n        |          |          |\n        +----------+----------+\n                   |\n              Combine & Gate\n                   |\n                Output Y\n\n    Args:\n        embed_dim: Input embedding dimension\n        block_loc: Location of block in network (layer_idx, n_block)\n        kwarg_all: Dictionary of additional arguments\n        device: Device to place tensors on\n        dtype: Data type for tensors\n        hidden_size: Size of hidden layers (default: embed_dim)\n        num_timescales: Number of timescale paths (default: 3)\n        compress_factor: Factor for adaptive compression (default: 4)\n        update_freq: Update frequencies for each timescale (default: [1,4,16])\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_timescales: int=3,\n        compress_factor: int=4, update_freq: list=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_timescales = num_timescales\n        self.compress_factor = compress_factor\n        self.update_freq = update_freq if update_freq is not None else [1, \n            4, 16]\n        self.timescale_sizes = []\n        for i in range(num_timescales):\n            size = hidden_size // compress_factor ** i\n            size = max(size, hidden_size // compress_factor ** (\n                num_timescales - 1))\n            self.timescale_sizes.append(size)\n        self.up_projs = nn.ModuleList([nn.Linear(hidden_size, size * 2,\n            bias=False, **self.factory_kwargs) for size in self.\n            timescale_sizes])\n        self.down_projs = nn.ModuleList([nn.Linear(size, hidden_size, bias=\n            False, **self.factory_kwargs) for size in self.timescale_sizes])\n        self.compress_scores = nn.ModuleList([nn.Linear(hidden_size, 1,\n            bias=False, **self.factory_kwargs) for _ in range(num_timescales)])\n        self.register_buffer('step_counter', torch.zeros((), device=device))\n        self.act_fn = ACT2FN['swish']\n        self.out_proj = nn.Linear(hidden_size * num_timescales, hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n\n    def _should_update(self, timescale_idx):\n        \"\"\"Determine if the given timescale should update at current step\"\"\"\n        return self.step_counter % self.update_freq[timescale_idx] == 0\n\n    def _process_timescale(self, X, timescale_idx, prev_state):\n        \"\"\"Process input through one timescale path\"\"\"\n        up_proj = self.up_projs[timescale_idx]\n        down_proj = self.down_projs[timescale_idx]\n        hidden = up_proj(X)\n        gate, value = hidden.chunk(2, dim=-1)\n        hidden = self.act_fn(gate) * value\n        importance = torch.sigmoid(self.compress_scores[timescale_idx](X))\n        if prev_state is not None and not self._should_update(timescale_idx):\n            hidden = prev_state\n        else:\n            hidden = hidden * importance\n        output = down_proj(hidden)\n        return output, hidden\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of hierarchical MLP.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, hidden_size)\n            Z: Dictionary containing previous states if any\n            \n        Returns:\n            Y: Output tensor of shape (batch_size, seq_len, hidden_size)\n            Z_: Dictionary containing updated states\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        self.step_counter += 1\n        prev_states = [Z.get(f'state_{i}', None) for i in range(self.\n            num_timescales)]\n        outputs = []\n        new_states = {}\n        for i in range(self.num_timescales):\n            output, state = self._process_timescale(X, i, prev_states[i])\n            outputs.append(output)\n            new_states[f'state_{i}'] = state\n        combined = torch.cat(outputs, dim=-1)\n        Y = self.out_proj(combined)\n        return Y, new_states\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'num_timescales': 3, 'compress_factor': 4, 'update_freq': None}\n\n\n\nautoconfig = {\n    'd_model': 512,\n    'n_block': 65\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}