{
    "variantname": "EventDrivenSSD",
    "review": "The EventDrivenSSD proposal presents an innovative approach to enhancing the SSDMinimalDiscrete unit by incorporating event-driven processing and hierarchical memory management. After thorough analysis of the proposal and related research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Theoretical Foundation\n- Strong grounding in cognitive science and episodic memory research\n- Clear connection to state-of-the-art work in event segmentation\n- Well-motivated approach to combining SSMs with event-driven processing\n\n2. Implementation Design\n- Detailed implementation strategy with clear code examples\n- Thoughtful consideration of memory hierarchy\n- Integration of graph-based memory structures\n- Efficient parallel processing capabilities\n\n3. Innovation\n- Novel combination of event-driven processing with SSMs\n- Unique approach to hierarchical state management\n- Integration of Bayesian surprise for boundary detection\n- Graph-based memory organization for efficient retrieval\n\n4. Scalability\n- Linear time complexity through efficient event detection\n- Hierarchical memory structure supports long sequences\n- Graph-based organization enables efficient state management\n- Parallel processing capabilities maintain efficiency at scale\n\nCONCERNS:\n\n1. Computational Overhead\n- Event detection adds computational complexity\n- Graph-theoretic boundary refinement may be expensive\n- Memory management overhead needs careful consideration\n- Additional components may impact training speed\n\n2. Implementation Complexity\n- Complex interaction between multiple components\n- Challenging integration with existing Mamba2 architecture\n- Potential numerical stability issues\n- Need for careful hyperparameter tuning\n\n3. Memory Management\n- Graph-based memory structure requires efficient implementation\n- Hierarchical state organization needs optimization\n- Trade-off between memory efficiency and computational cost\n- Risk of memory fragmentation\n\n4. Validation Requirements\n- Need for empirical validation of event detection accuracy\n- Performance impact needs thorough testing\n- Robustness across different types of sequences\n- Scalability with very long sequences\n\nCOMPARISON TO EXISTING RESEARCH:\n\nThe proposal builds upon and extends several recent works:\n1. Advances beyond EM-LLM (Fountas et al., 2024) by integrating event processing with SSMs\n2. Improves upon DenseMamba (He et al., 2024) through hierarchical state management\n3. Extends hierarchical SSMs (Pinto et al., 2024) with event-driven processing\n4. Novel combination of approaches not seen in existing literature",
    "search_stack": [
        {
            "ready": false,
            "query": "recurrent attention networks, state space models, episodic memory",
            "detail": "Search for technical details about:\n1. State space model architectures and their variants\n2. Memory mechanisms in language models\n3. Attention and recurrence combinations\n4. Performance comparisons between different architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for technical details about:\n1. State space model architectures and their variants\n2. Memory mechanisms in language models\n3. Attention and recurrence combinations\n4. Performance comparisons between different architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.99)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.99)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.97)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.97)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.97)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.97)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 5. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.92)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 45/64 (Score: 0.92)*\n\n```\nPlease refer to Tay et al. (2022) and Wan et al. (2023) for a comprehensive and in-depth survey of efficient transformers. Recurrent architectures Before the advent of transformers, recurrent neural networks, especially Long Short-Term Memory (LSTM) (Hochreiter \\& Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014), were the dominant architectures in NLP for sequence processing. However, traditional RNNs are hard, if not impossible, to parallelize across the time dimension, susceptible to gradient vanishing and explosion, and ineffective in capturing long-range dependencies, which are ubiquitous in natural language. These shortcomings contributed to the rapid decline of traditional RNNs in NLP. There has been a revival of RNNs in NLP research (Tiezzi et al., 2024) in recent years. Compared to transformers with quadratic complexity, RNNs are highly efficient in autoregressive inference with $\\mathrm{O}(1)$ time complexity per step, making them an attractive architecture for large language models. Many efforts have been devoted to parallelized recurrent models and improving their capability to capture long-range dependency, while maintaining the low inference complexity. The Legendre Memory Unit (LMU) (Voelker et al., 2019) was designed to efficiently handle longrange dependencies with a new type of memory cell for recurrent neural networks. Unlike LSTM units, which struggle with remembering information over very long sequences, LMU use Legendre polynomials to create a memory system that can maintain and process information over extended time periods more effectively. High-order polynomial projection operators (HiPPO) (Gu et al., 2020) generalizes LMU by providing a flexible framework for online compression of signals through polynomial projections, accommodating various polynomial bases beyond Legendre polynomials. It optimizes function approximation over time, adapting to different data timescales without needing predefined hyperparameters. SSMs have inspired a range of follow-up research to incorporate SSMs, or modified SSMs into end-to-end architectures for language modeling, including MEGA (Ma et al., 2022), DSS (Gupta et al., 2022), H3 (Fu et al., 2022), and Linear Recurrent Unit (LRU) (Orvieto et al., 2023). Mamba (Gu \\& Dao, 2023) is a selective SSM that introduces time-dependent selective mechanism to enhance the long-range modeling ability of SSMs. The selectivity removes the linear time-variance property of the SSM, making it no longer possible to parallelize Mamba as a long convolution kernel. Yet Mamba can still be effectively parallelized using parallel associative scan\n(Blelloch, 1990; Martin \\& Cundy, 2018; Smith et al., 2023) with a hardware-aware implementation. Recently proposed GateLoop (Katsch, 2023) also adopts a similar data-dependent state transitions. The data-dependent states, also concurrently proposed in GLA (Yang et al., 2023), are similar to the Weighted Key-Value State in Finch. A contemporary but independent work also proposes recurrent models named as Hawk and Griffin (De et al., 2024). Hawk is a recurrent model with the Real-Gated Linear Recurrent Unit (RG-LRU), whereas Griffin mixes the RG-LRU with local multi-query attention, thereby achieving long-context extrapolation efficiently. Please see Tiezzi et al.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: recurrent attention networks, state space models, episodic memory\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Efficient Graph Generation with Graph Recurrent Attention Networks\n\n*From Search Query: recurrent attention networks*\n\n*Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, C. Nash, William L. Hamilton, D. Duvenaud, R. Urtasun, R. Zemel*\n\n**TL;DR:** A new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs), which better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention.\n\n**Abstract:** We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. Our code is released at: \\url{https://github.com/lrjconan/GRAN}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 290  (*Influential: 70*)\n\n#### 2. Social-BiGAT: Multimodal Trajectory Forecasting using Bicycle-GAN and Graph Attention Networks\n\n*From Search Query: recurrent attention networks*\n\n*V. Kosaraju, Amir Sadeghian, Roberto Mart\u00edn-Mart\u00edn, I. Reid, S. H. Rezatofighi, S. Savarese*\n\n**TL;DR:** A graph-based generative adversarial network that generates realistic, multimodal trajectory predictions by better modelling the social interactions of pedestrians in a scene and achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.\n\n**Abstract:** Predicting the future trajectories of multiple interacting agents in a scene has become an increasingly important problem for many different applications ranging from control of autonomous vehicles and social robots to security and surveillance. This problem is compounded by the presence of social interactions between humans and their physical interactions with the scene. While the existing literature has explored some of these cues, they mainly ignored the multimodal nature of each human's future trajectory. In this paper, we present Social-BiGAT, a graph-based generative adversarial network that generates realistic, multimodal trajectory predictions by better modelling the social interactions of pedestrians in a scene. Our method is based on a graph attention network (GAT) that learns reliable feature representations that encode the social interactions between humans in the scene, and a recurrent encoder-decoder architecture that is trained adversarially to predict, based on the features, the humans' paths. We explicitly account for the multimodal nature of the prediction problem by forming a reversible transformation between each scene and its latent noise vector, as in Bicycle-GAN. We show that our framework achieves state-of-the-art performance comparing it to several baselines on existing trajectory forecasting benchmarks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 539  (*Influential: 73*)\n\n#### 3. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 61  (*Influential: 7*)\n\n#### 4. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: state space models*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 5. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: state space models*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 209  (*Influential: 37*)\n\n#### 6. SpotEM: Efficient Video Search for Episodic Memory\n\n*From Search Query: episodic memory*\n\n*Santhosh K. Ramakrishnan, Ziad Al-Halah, K. Grauman*\n\n**TL;DR:** SpotEM consists of a novel clip selector that learns to identify promising video regions to search conditioned on the language query, a set of low-cost semantic indexing features that capture the context of rooms, objects, and interactions that suggest where to look, and distillation losses that address the optimization issues arising from end-to-end joint training of the clip selector and EM model.\n\n**Abstract:** The goal in episodic memory (EM) is to search a long egocentric video to answer a natural language query (e.g.,\"where did I leave my purse?\"). Existing EM methods exhaustively extract expensive fixed-length clip features to look everywhere in the video for the answer, which is infeasible for long wearable-camera videos that span hours or even days. We propose SpotEM, an approach to achieve efficiency for a given EM method while maintaining good accuracy. SpotEM consists of three key ideas: 1) a novel clip selector that learns to identify promising video regions to search conditioned on the language query; 2) a set of low-cost semantic indexing features that capture the context of rooms, objects, and interactions that suggest where to look; and 3) distillation losses that address the optimization issues arising from end-to-end joint training of the clip selector and EM model. Our experiments on 200+ hours of video from the Ego4D EM Natural Language Queries benchmark and three different EM models demonstrate the effectiveness of our approach: computing only 10% - 25% of the clip features, we preserve 84% - 97% of the original EM model's accuracy. Project page: https://vision.cs.utexas.edu/projects/spotem\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 7. General Sequential Episodic Memory Model\n\n*From Search Query: episodic memory*\n\n*Arjun Karuvally, T. Sejnowski, H. Siegelmann*\n\n**TL;DR:** A class of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit, exhibit a dynamic energy surface, leading to a series of meta-stable states capable of encoding memory sequences, demonstrating how energy-based memory modeling can provide robust and scalable memory systems in static and dynamic memory cases.\n\n**Abstract:** The state-of-the-art memory model is the General Associative Memory Model, a generalization of the classical Hopfield network. Like its ancestor, the general associative memory has a well-defined state-dependant energy surface, and its memories correlate with its fixed points. This is unlike human memories, which are commonly sequential rather than separated fixed points. In this paper, we introduce a class of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit, exhibit a dynamic energy surface, leading to a series of meta-stable states capable of encoding memory sequences. A multiple-timescale architecture enables the dynamic nature of the energy surface with newly introduced asymmetric synapses and signal propagation delays. We demonstrate its dense capacity under polynomial activation functions. GSEMM combines separate memories, short and long sequential episodic memories, under a unified theoretical framework, demonstrating how energy-based memory modeling can provide robust and scalable memory systems in static and dynamic memory cases.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n### 5 related papers from ArXiv\n\n#### 1. Hierarchical Recurrent Attention Network for Response Generation\n\n*From Search Query: recurrent attention networks*\n\n*Chen Xing, Wei Wu, Yu Wu, Ming Zhou, Yalou Huang, Wei-Ying Ma*\n\n**Abstract:** We study multi-turn response generation in chatbots where a response is\ngenerated according to a conversation context. Existing work has modeled the\nhierarchy of the context, but does not pay enough attention to the fact that\nwords and utterances in the context are differentially important. As a result,\nthey may lose important information in context and generate irrelevant\nresponses. We propose a hierarchical recurrent attention network (HRAN) to\nmodel both aspects in a unified framework. In HRAN, a hierarchical attention\nmechanism attends to important parts within and among utterances with word\nlevel attention and utterance level attention respectively. With the word level\nattention, hidden vectors of a word level encoder are synthesized as utterance\nvectors and fed to an utterance level encoder to construct hidden\nrepresentations of the context. The hidden vectors of the context are then\nprocessed by the utterance level attention and formed as context vectors for\ndecoding the response. Empirical studies on both automatic evaluation and human\njudgment show that HRAN can significantly outperform state-of-the-art models\nfor multi-turn response generation.\n\n**Published:** 2017-01-25T03:04:31Z  (*Updated: 2017-01-25T03:04:31Z*)\n\n\n\n#### 2. Recurrent Attention Networks for Long-text Modeling\n\n*From Search Query: recurrent attention networks*\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text\nmining. However, the quadratic computational complexities restrict their\napplication in long text processing. Prior works have adopted the chunking\nstrategy to divide long documents into chunks and stack a self-attention\nbackbone with the recurrent structure to extract semantic representation. Such\nan approach disables parallelization of the attention mechanism, significantly\nincreasing the training cost and raising hardware requirements. Revisiting the\nself-attention mechanism and the recurrent structure, this paper proposes a\nnovel long-document encoding model, Recurrent Attention Network (RAN), to\nenable the recurrent operation of self-attention. Combining the advantages from\nboth sides, the well-designed RAN is capable of extracting global semantics in\nboth token-level and document-level representations, making it inherently\ncompatible with both sequential and classification tasks, respectively.\nFurthermore, RAN is computationally scalable as it supports parallelization on\nlong document processing. Extensive experiments demonstrate the long-text\nencoding ability of the proposed RAN model on both classification and\nsequential tasks, showing its potential for a wide range of applications.\n\n**Published:** 2023-06-12T03:28:33Z  (*Updated: 2023-06-12T03:28:33Z*)\n\n\n\n#### 3. LOCOST: State-Space Models for Long Document Abstractive Summarization\n\n*From Search Query: state space models*\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n\n**Published:** 2024-01-31T15:33:37Z  (*Updated: 2024-03-25T12:52:42Z*)\n\n\n\n#### 4. Linking In-context Learning in Transformers to Human Episodic Memory\n\n*From Search Query: episodic memory*\n\n*Li Ji-An, Corey Y. Zhou, Marcus K. Benna, Marcelo G. Mattar*\n\n**Abstract:** Understanding the connections between artificial and biological intelligent\nsystems can reveal fundamental principles underlying general intelligence.\nWhile many artificial intelligence (AI) models have a neuroscience counterpart,\nsuch connections are largely missing in Transformer models and the\nself-attention mechanism. Here, we examine the relationship between attention\nheads and human episodic memory. We focus on the induction heads, which\ncontribute to the in-context learning capabilities of Transformer-based large\nlanguage models (LLMs). We demonstrate that induction heads are behaviorally,\nfunctionally, and mechanistically similar to the contextual maintenance and\nretrieval (CMR) model of human episodic memory. Our analyses of LLMs\npre-trained on extensive text data show that CMR-like heads often emerge in the\nintermediate model layers and that their behavior qualitatively mirrors the\nmemory biases seen in humans. Our findings uncover a parallel between the\ncomputational mechanisms of LLMs and human memory, offering valuable insights\ninto both research fields.\n\n**Published:** 2024-05-23T18:51:47Z  (*Updated: 2024-05-23T18:51:47Z*)\n\n\n\n#### 5. Episodic Memory in Lifelong Language Learning\n\n*From Search Query: episodic memory*\n\n*Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, Dani Yogatama*\n\n**Abstract:** We introduce a lifelong language learning setup where a model needs to learn\nfrom a stream of text examples without any dataset identifier. We propose an\nepisodic memory model that performs sparse experience replay and local\nadaptation to mitigate catastrophic forgetting in this setup. Experiments on\ntext classification and question answering demonstrate the complementary\nbenefits of sparse experience replay and local adaptation to allow the model to\ncontinuously learn from new datasets. We also show that the space complexity of\nthe episodic memory module can be reduced significantly (~50-90%) by randomly\nchoosing which examples to store in memory with a minimal decrease in\nperformance. We consider an episodic memory component as a crucial building\nblock of general linguistic intelligence and see our model as a first step in\nthat direction.\n\n**Published:** 2019-06-03T20:50:58Z  (*Updated: 2019-11-26T00:57:40Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction\n\n*From Search Query: recurrent attention networks*\n\n*Yao Qin, Wei Cheng, Dongjin Song, Garrison Cottrell, Haifeng Chen, Guofei Jiang*\n\n**Abstract:** The Nonlinear autoregressive exogenous (NARX) model, which predicts the\ncurrent value of a time series based upon its previous values as well as the\ncurrent and past values of multiple driving (exogenous) series, has been\nstudied for decades. Despite the fact that various NARX models have been\ndeveloped, few of them can capture the long-term temporal dependencies\nappropriately and select the relevant driving series to make predictions. In\nthis paper, we propose a dual-stage attention-based recurrent neural network\n(DA-RNN) to address these two issues. In the first stage, we introduce an input\nattention mechanism to adaptively extract relevant driving series (a.k.a.,\ninput features) at each time step by referring to the previous encoder hidden\nstate. In the second stage, we use a temporal attention mechanism to select\nrelevant encoder hidden states across all time steps. With this dual-stage\nattention scheme, our model can not only make predictions effectively, but can\nalso be easily interpreted. Thorough empirical studies based upon the SML 2010\ndataset and the NASDAQ 100 Stock dataset demonstrate that the DA-RNN can\noutperform state-of-the-art methods for time series prediction.\n\n**Published:** 2017-04-07\n\n\n\n#### 2. Recurrent Attention Network on Memory for Aspect Sentiment Analysis\n\n*From Search Query: recurrent attention networks*\n\n*Zhongqian Sun, Wei Yang, Peng Chen, Lidong Bing*\n\n**Abstract:** We propose a novel framework based on neural networks to identify the sentiment of opinion targets in a comment/review. Our framework adopts multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. The results of multiple attentions are non-linearly combined with a recurrent neural network, which strengthens the expressive power of our model for handling more complications. The weighted-memory mechanism not only helps us avoid the labor-intensive feature engineering work, but also provides a tailor-made memory for different opinion targets of a sentence. We examine the merit of our model on four datasets: two are from SemEval2014, i.e. reviews of restaurants and laptops; a twitter dataset, for testing its performance on social media data; and a Chinese news comment dataset, for testing its language sensitivity. The experimental results show that our model consistently outperforms the state-of-the-art methods on different types of data.\n\n**Proceeding:** emnlp-2017-9\n\n**Published:** 2017-09-01\n\n\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space models*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n#### 5. On Tiny Episodic Memories in Continual Learning\n\n*From Search Query: episodic memory*\n\n*Marc'Aurelio Ranzato, Puneet K. Dokania, Thalaiyasingam Ajanthan, Philip H. S. Torr, Arslan Chaudhry, Mohamed Elhoseiny, Marcus Rohrbach*\n\n**Abstract:** In continual learning (CL), an agent learns from a stream of tasks leveraging prior experience to transfer knowledge to future tasks. It is an ideal framework to decrease the amount of supervision in the existing learning algorithms. But for a successful knowledge transfer, the learner needs to remember how to perform previous tasks. One way to endow the learner the ability to perform tasks seen in the past is to store a small memory, dubbed episodic memory, that stores few examples from previous tasks and then to replay these examples when training for future tasks. In this work, we empirically analyze the effectiveness of a very small episodic memory in a CL setup where each training example is only seen once. Surprisingly, across four rather different supervised learning benchmarks adapted to CL, a very simple baseline, that jointly trains on both examples from the current task as well as examples stored in the episodic memory, significantly outperforms specifically designed CL approaches with and without episodic memory. Interestingly, we find that repetitive training on even tiny memories of past tasks does not harm generalization, on the contrary, it improves it, with gains between 7\\% and 17\\% when the memory is populated with a single example per class.\n\n**Published:** 2019-02-27\n\n\n\n#### 6. Gradient Episodic Memory for Continual Learning\n\n*From Search Query: episodic memory*\n\n*Marc'Aurelio Ranzato, David Lopez-Paz*\n\n**Abstract:** One major obstacle towards AI is the poor ability of models to solve new problems quicker, and without forgetting previously acquired knowledge. To better understand this issue, we study the problem of continual learning, where the model observes, once and one by one, examples concerning a sequence of tasks. First, we propose a set of metrics to evaluate models learning over a continuum of data. These metrics characterize models not only by their test accuracy, but also in terms of their ability to transfer knowledge across tasks. Second, we propose a model for continual learning, called Gradient Episodic Memory (GEM) that alleviates forgetting, while allowing beneficial transfer of knowledge to previous tasks. Our experiments on variants of the MNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when compared to the state-of-the-art.\n\n**Conference:** gradient-episodic-memory-for-continual-1\n\n**Published:** 2017-06-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the given keywords and details, here are some key points and technical details from the provided sources and additional insights:\n\n## State Space Model Architectures and Their Variants\n\n### Mamba and Mamba2\nState Space Models (SSMs) like Mamba and Mamba2 are gaining prominence due to their efficiency and scalability. Mamba2, in particular, integrates state-space models with state-space duality (SSD), which simplifies the matrix transformations and enhances performance. This architecture allows for near-linear scalability concerning sequence length and maintains comparable modeling abilities to transformers.\n\n### Spatial-Mamba\nFor handling 2D data, Spatial-Mamba introduces a structure-aware state fusion equation, leveraging dilated convolutions to capture spatial dependencies. This approach enhances the modeling of visual data by incorporating both temporal and spatial information, which could be adapted for other types of sequential data.\n\n## Memory Mechanisms in Language Models\n\n### Episodic Memory\nHuman-like episodic memory mechanisms organize sequences into coherent events, using a two-stage memory process that combines similarity-based and temporally contiguous retrieval. This approach can be integrated with SSMs to improve memory efficiency and organization in language models[Initial Analysis].\n\n### State Management in SSMs\nSSMs inherently use state representations to model sequential data. The state transition equations in SSMs, such as those in Mamba, allow for efficient handling of long-range dependencies. Enhancing state management between blocks, possibly by incorporating episodic memory organization, could improve overall performance.\n\n## Attention and Recurrence Combinations\n\n### Recurrent Attention Networks (RAN)\nRAN combines recurrence with self-attention, enabling the extraction of global semantics in both token-level and document-level representations. This architecture supports parallelization on long documents, making it computationally scalable[Initial Analysis].\n\n### Integration with SSMs\nIntegrating attention mechanisms with SSMs, such as using sparse attention mechanisms like SPARSEK, can enhance the efficiency and scalability of the model. The use of local self-attention in SSMs can perform in-context learning similar to transformers, allowing for efficient processing of sequential data.\n\n## Performance Comparisons Between Different Architectures\n\n### SSMs vs. Transformers\nRecent studies show that SSMs, particularly those with selectivity mechanisms like Mamba, can surpass attention-powered foundation models (transformers) in both accuracy and efficiency, especially at large scales. SSMs maintain linear complexity, which is a significant advantage over the quadratic complexity of traditional transformers.\n\n### Efficiency and Scalability\nSSMs are noted for their linear or near-linear scaling in sequence length, making them highly efficient for processing long sequences. This is in contrast to traditional models that suffer from increased computational overhead with longer sequences.\n\n### Memory Efficiency\nThe use of sparse operations, adaptive computation paths, and event-based memory organization can significantly improve memory efficiency. For example, SPARSEK attention achieves linear time complexity and a constant memory footprint during generation[Initial Analysis].\n\n## Future Directions and Technical Challenges\n\n### Stability and Complexity\nMaintaining stability with adaptive components and balancing complexity with efficiency are key technical challenges. Ensuring that the added features do not compromise the model's stability and efficiency is crucial[Initial Analysis].\n\n### Adaptive Kernel Sizing and Hierarchical Processing\nImplementing adaptive kernel sizing, as seen in Time-aware Large Kernel (TaLK) convolutions, and hierarchical processing with recurrent components can further optimize the model's performance and efficiency[Initial Analysis].\n\nBy integrating these insights, researchers can design autoregressive language models that leverage the strengths of state space models, episodic memory mechanisms, and efficient attention and recurrence combinations, ultimately leading to improved performance, efficiency, and scalability.\n",
            "analysis": "Initial Analysis of Seed Design and References\n\n# Summary\nFrom analyzing the seed design (Mamba2) and provided references, several key observations emerge:\n\n1. Current State of LM Block Design:\n- Mamba2 uses a selective SSM architecture with state space duality (SSD)\n- Core components include SSDMinimalDiscrete for sequence processing\n- Uses RMSNorm for normalization\n- Structured as a dual-layer architecture with residual connections\n\n2. Key Limitations & Opportunities:\n- Memory efficiency could be improved\n- Processing of long sequences needs optimization\n- State management between blocks could be enhanced\n- Integration of attention and state space mechanisms could be better\n\n3. Notable Innovations from References:\n\n## Recurrent Attention Networks (RAN)\nKey excerpts:\n\"enables the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations\"\n\"computationally scalable as it supports parallelization on long document processing\"\n\nInsights:\n- Combines recurrence with attention for efficient long sequence processing\n- Maintains parallel processing capabilities\n- Hierarchical semantic extraction\n\n## Time-aware Large Kernel (TaLK) Convolutions\nKey excerpts:\n\"learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix\"\n\"yields a time complexity of O(n), effectively making the sequence encoding process linear\"\n\nInsights:\n- Adaptive kernel sizing for efficient processing\n- Linear time complexity achievement\n- Dynamic adaptation to sequence characteristics\n\n## Human-like Episodic Memory\nKey excerpts:\n\"organises sequences of tokens into coherent episodic events\"\n\"two-stage memory process, combining similarity-based and temporally contiguous retrieval\"\n\nInsights:\n- Event-based sequence organization\n- Efficient memory retrieval mechanisms\n- Biologically inspired architecture\n\n## SPARSEK Attention\nKey excerpts:\n\"integrates a scoring network and a differentiable top-k mask operator\"\n\"linear time complexity and constant memory footprint during generation\"\n\nInsights:\n- Efficient sparse attention mechanism\n- Memory-optimized design\n- Scalable to long sequences\n\n# Key Insights for Design:\n\n1. Architecture Opportunities:\n- Combine episodic memory organization with state space models\n- Integrate adaptive kernel sizing with SSD\n- Implement hierarchical processing with recurrent components\n\n2. Efficiency Improvements:\n- Use sparse operations where possible\n- Implement adaptive computation paths\n- Optimize memory usage through event-based organization\n\n3. Novel Features to Consider:\n- Event-based state management\n- Adaptive kernel sizing for SSM\n- Hierarchical memory organization\n- Sparse state updates\n\n4. Technical Challenges:\n- Maintaining stability with adaptive components\n- Balancing complexity with efficiency\n- Ensuring scalability with added features\n\n# Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of sparse state space models\n2. Memory-efficient variants of SSM\n3. Integration techniques for attention and state space models\n4. Performance metrics for different memory architectures\n\n# References:\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv.org.\n\n2. Li, X., et al. (2023). Recurrent Attention Networks for Long-text Modeling. Annual Meeting of the Association for Computational Linguistics.\n\n3. Lioutas, V., & Guo, Y. (2020). Time-aware Large Kernel Convolutions. International Conference on Machine Learning.\n\n4. Fountas, Z., et al. (2024). Human-like Episodic Memory for Infinite Context LLMs. arXiv.org.\n\n5. Lou, C., et al. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv.org."
        },
        {
            "ready": false,
            "query": "hierarchical state space models, graph memory networks, scan algorithms",
            "detail": "Search for technical details about:\n1. Hierarchical state space model architectures and implementations\n2. Graph-based memory organization in neural networks\n3. Efficient scan algorithms for sequence processing\n4. Event segmentation techniques in sequence models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for technical details about:\n1. Hierarchical state space model architectures and implementations\n2. Graph-based memory organization in neural networks\n3. Efficient scan algorithms for sequence processing\n4. Event segmentation techniques in sequence models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.92)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.92)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Spectral State Space Models (Avg. Score: 0.81)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.81)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 3. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.78)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 0.78)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n#### 4. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.64)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.64)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 5. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.55)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 23/31 (Score: 0.55)*\n\n```\n[27] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [28] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [29] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. [30] Shida Wang and Qianxiao Li. Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization.\n```\n\n\n\n---\n## Found 14 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical state space models, graph memory networks, scan algorithms\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: hierarchical state space models*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Multimodal Neural Graph Memory Networks for Visual Question Answering\n\n*From Search Query: graph memory networks*\n\n*M. Khademi*\n\n**TL;DR:** A new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering that rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.\n\n**Abstract:** We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering. The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in an image. The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions (RGCs) for the image. The RGCs capture object attributes and their relationships. Two GNs are constructed from the input module using the visual features and encoded RGCs. Each node of the GNs iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it. Then, to combine the information from both GNs, the nodes write the updated representations to an external spatial memory. The final states of the memory cells are fed into an answer module to predict an answer. Experiments show MN-GMN rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 33  (*Influential: 5*)\n\n#### 3. Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks\n\n*From Search Query: graph memory networks*\n\n*Shuai Zhang, M. Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miaoyuan Liu*\n\n**TL;DR:** This paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error and proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexityand improve convergence without compromising the test accuracy.\n\n**Abstract:** Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include \\textit{graph sparsification} that samples a subgraph to reduce the amount of data aggregation and \\textit{model sparsification} that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 4*)\n\n#### 4. Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian\n\n*From Search Query: graph memory networks*\n\n*Haiyang Yu, Zhao Xu, X. Qian, Xiaoning Qian, Shuiwang Ji*\n\n**TL;DR:** This work proposes a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance, and considers the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics.\n\n**Abstract:** We consider the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics. Efficiency and equivariance are two important, but conflicting factors. In this work, we propose a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance. Our key advance lies at the innovative design of QHNet architecture, which not only obeys the underlying symmetries, but also enables the reduction of number of tensor products by 92\\%. In addition, QHNet prevents the exponential growth of channel dimension when more atom types are involved. We perform experiments on MD17 datasets, including four molecular systems. Experimental results show that our QHNet can achieve comparable performance to the state of the art methods at a significantly faster speed. Besides, our QHNet consumes 50\\% less memory due to its streamlined architecture. Our code is publicly available as part of the AIRS library (\\url{https://github.com/divelab/AIRS}).\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 20  (*Influential: 8*)\n\n#### 5. Conic Scan-and-Cover algorithms for nonparametric topic modeling\n\n*From Search Query: scan algorithms*\n\n*M. Yurochkin, Aritra Guha, X. Nguyen*\n\n**TL;DR:** These algorithms are shown in practice to have accuracy comparable to a Gibbs sampler in terms of topic estimation, which requires the number of topics be given, and to be one of the fastest among several state of the art parametric techniques.\n\n**Abstract:** We propose new algorithms for topic modeling when the number of topics is unknown. Our approach relies on an analysis of the concentration of mass and angular geometry of the topic simplex, a convex polytope constructed by taking the convex hull of vertices representing the latent topics. Our algorithms are shown in practice to have accuracy comparable to a Gibbs sampler in terms of topic estimation, which requires the number of topics be given. Moreover, they are one of the fastest among several state of the art parametric techniques. Statistical consistency of our estimator is established under some conditions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 6. Conic Scan Coverage algorithm for nonparametric topic modeling\n\n*From Search Query: scan algorithms*\n\n*M. Yurochkin, Aritra Guha, X. Nguyen*\n\n**TL;DR:** The resulting algorithm is shown to have accuracy comparable to that of a Gibbs sampler in terms of topic estimation, which requires the number of topics be given, and is the fastest among a variety of state of the art parametric techniques.\n\n**Abstract:** In this paper we propose new algorithms for topic modeling when number of topics is not known. Our approach relies on an analysis of the concentration of mass and angular geometry of the topic simplex, a convex polytope constructed by taking the convex hull of the topics. The resulting algorithm is shown in practice to have accuracy comparable to that of a Gibbs sampler in terms of topic estimation, which requires the number of topics be given. Moreover, our algorithm is the fastest among a variety of state of the art parametric techniques. The consistency of the estimates produced by our method is established under some conditions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 7. Distral: Robust multitask reinforcement learning\n\n*From Search Query: scan algorithms*\n\n*Y. Teh, V. Bapst, Wojciech M. Czarnecki, John Quan, J. Kirkpatrick, R. Hadsell, N. Heess, Razvan Pascanu*\n\n**TL;DR:** This work proposes a new approach for joint training of multiple tasks, which it refers to as Distral (Distill & transfer learning), and shows that the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.\n\n**Abstract:** Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill & transfer learning). Instead of sharing parameters between the different workers, we propose to share a \"distilled\" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 514  (*Influential: 42*)\n\n### 1 related papers from ArXiv\n\n#### 1. Knowledge Grounded Conversational Symptom Detection with Graph Memory\n  Networks\n\n*From Search Query: graph memory networks*\n\n*Hongyin Luo, Shang-Wen Li, James Glass*\n\n**Abstract:** In this work, we propose a novel goal-oriented dialog task, automatic symptom\ndetection. We build a system that can interact with patients through dialog to\ndetect and collect clinical symptoms automatically, which can save a doctor's\ntime interviewing the patient. Given a set of explicit symptoms provided by the\npatient to initiate a dialog for diagnosing, the system is trained to collect\nimplicit symptoms by asking questions, in order to collect more information for\nmaking an accurate diagnosis. After getting the reply from the patient for each\nquestion, the system also decides whether current information is enough for a\nhuman doctor to make a diagnosis. To achieve this goal, we propose two neural\nmodels and a training pipeline for the multi-step reasoning task. We also build\na knowledge graph as additional inputs to further improve model performance.\nExperiments show that our model significantly outperforms the baseline by 4%,\ndiscovering 67% of implicit symptoms on average with a limited number of\nquestions.\n\n**Published:** 2021-01-24T18:50:16Z  (*Updated: 2021-01-24T18:50:16Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: hierarchical state space models*\n\n*Lerrel Pinto, Tess Hellebrekers, Abhinav Gupta, Carmel Majidi, Venkatesh Pattabiraman, Chenyu Wang, Raunaq Bhirangi*\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Published:** 2024-02-15\n\n\n\n#### 2. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: hierarchical state space models*\n\n*Ralf Herbrich, Rainer Schlosser, Paul Mattes*\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Published:** 2023-10-08\n\n\n\n#### 3. Memory-Based Graph Networks\n\n*From Search Query: graph memory networks*\n\n*Amir Hosein Khasahmadi, Leo Lee, Kaveh Hassani, Parsa Moradi, Quaid Morris*\n\n**Abstract:** Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. We introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. We also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results shows that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data. Code and reference implementations are released at: https://github.com/amirkhas/GraphMemoryNet\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2020-02-21\n\n\n\n#### 4. GAMENet: Graph Augmented MEmory Networks for Recommending Medication Combination\n\n*From Search Query: graph memory networks*\n\n*Hongyan Li, Cao Xiao, Tengfei Ma, Junyuan Shang, Jimeng Sun*\n\n**Abstract:** Recent progress in deep learning is revolutionizing the healthcare domain\nincluding providing solutions to medication recommendations, especially\nrecommending medication combination for patients with complex health\nconditions. Existing approaches either do not customize based on patient health\nhistory, or ignore existing knowledge on drug-drug interactions (DDI) that\nmight lead to adverse outcomes. To fill this gap, we propose the Graph\nAugmented Memory Networks (GAMENet), which integrates the drug-drug\ninteractions knowledge graph by a memory module implemented as a graph\nconvolutional networks, and models longitudinal patient records as the query.\nIt is trained end-to-end to provide safe and personalized recommendation of\nmedication combination. We demonstrate the effectiveness and safety of GAMENet\nby comparing with several state-of-the-art methods on real EHR data. GAMENet\noutperformed all baselines in all effectiveness measures, and also achieved\n3.60% DDI rate reduction from existing EHR data.\n\n**Published:** 2018-09-06\n\n\n\n#### 5. Connection Scan Algorithm\n\n*From Search Query: scan algorithms*\n\n*Ben Strasser, Thomas Pajor, Julian Dibbelt, Dorothea Wagner*\n\n**Abstract:** We introduce the Connection Scan Algorithm (CSA) to efficiently answer queries to timetable information systems. The input consists, in the simplest setting, of a source position and a desired target position. The output consist is a sequence of vehicles such as trains or buses that a traveler should take to get from the source to the target. We study several problem variations such as the earliest arrival and profile problems. We present algorithm variants that only optimize the arrival time or additionally optimize the number of transfers in the Pareto sense. An advantage of CSA is that is can easily adjust to changes in the timetable, allowing the easy incorporation of known vehicle delays. We additionally introduce the Minimum Expected Arrival Time (MEAT) problem to handle possible, uncertain, future vehicle delays. We present a solution to the MEAT problem that is based upon CSA. Finally, we extend CSA using the multilevel overlay paradigm to answer complex queries on nation-wide integrated timetables with trains and buses.\n\n**Published:** 2017-03-17\n\n\n\n#### 6. 3D-BBS: Global Localization for 3D Point Cloud Scan Matching Using Branch-and-Bound Algorithm\n\n*From Search Query: scan algorithms*\n\n*Anonymous*\n\n**Abstract:** This paper presents an accurate and fast 3D global localization method, 3D-BBS, that extends the existing branch-and-bound (BnB)-based 2D scan matching (BBS) algorithm. To reduce memory consumption, we utilize a sparse hash table for storing hierarchical 3D voxel maps. To improve the processing cost of BBS in 3D space, we propose an efficient roto-translational space branching. Furthermore, we devise a batched BnB algorithm to fully leverage GPU parallel processing. Through experiments in simulated and real environments, we demonstrated that the 3D-BBS enabled accurate global localization with only a 3D LiDAR scan roughly aligned in the gravity direction and a 3D pre-built map. This method required only 878 msec on average to perform global localization and outperformed state-of-the-art global registration methods in terms of accuracy and processing speed.\n\n**Published:** 2023-10-16\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on the integration of hierarchical state space models, graph-based memory organization, efficient scan algorithms, and event segmentation techniques, here are some key points and references that can guide the research:\n\n## Hierarchical State Space Model Architectures and Implementations\n\n- The SC-MAMBA2 model introduces a hierarchical approach by integrating state-space models (SSMs) with the MAMBA framework. This integration enables efficient and scalable modeling of extensive sequences, such as those in single-cell transcriptomics. The model uses a bidirectional modeling approach under the SSM framework, which facilitates comprehensive analysis of whole genome transcriptome sequences. The SSMs are discretized and implemented in a convolutional manner, making them suitable for modern computational frameworks.\n\n- The concept of State Space Duality (SSD) in Mamba2 simplifies the matrix \\(A\\) into a scalar, allowing for both linear and quadratic forms of SSMs. This simplification enhances the adaptability and performance of SSMs, making them more efficient for hierarchical processing.\n\n## Graph-Based Memory Organization in Neural Networks\n\n- While the provided sources do not directly discuss graph-based memory organization in the context of language models, the idea of hierarchical memory organization can be inferred from other domains. For instance, hierarchical graph neural networks (h-GNNs) use a clustering method to organize nodes into hierarchical levels, preserving both local and global information. This approach could be adapted to organize memory in neural networks, particularly in models that require complex sequence processing.\n\n- The concept of episodic memory organization, mentioned in the analysis, suggests using a combination of Bayesian surprise and graph-theoretic boundary refinement to organize sequences into coherent episodic events. This could be a promising direction for integrating graph-based memory into language models.\n\n## Efficient Scan Algorithms for Sequence Processing\n\n- Scan algorithms are crucial for efficient sequence processing in models like Mamba. The selective scan, parallel scan, and accelerated scan algorithms allow for linear-time implementation of recurrent forms, which is particularly beneficial for long sequence processing. These algorithms optimize the use of GPU memory hierarchy, combining fast SRAM and slower HBM operations to achieve memory efficiency akin to optimized transformer implementations.\n\n- The Mamba model utilizes parameters that depend on the input through linear projections, enabling selective propagation or forgetting of information along the sequence length dimension. This is efficiently implemented via scan algorithms, which address the weakness of discrete modalities in SSMs.\n\n## Event Segmentation Techniques in Sequence Models\n\n- Event segmentation in sequence models can be achieved through techniques like those used in human-like episodic memory models. These models organize sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement. This approach can improve efficiency by segmenting sequences into meaningful events rather than processing them linearly.\n\n- The SC-MAMBA2 model, while not specifically focused on event segmentation, uses a bidirectional modeling approach and smart padding mechanisms to ensure meaningful tokens are processed effectively. This could be a starting point for integrating more sophisticated event segmentation techniques into autoregressive language models.\n\nIn summary, the integration of hierarchical state space models, graph-based memory organization, and efficient scan algorithms, along with event segmentation techniques, can significantly enhance the performance and efficiency of autoregressive language models. The SC-MAMBA2 model and related research provide valuable insights into how these components can be implemented and optimized.\n",
            "analysis": "# Analysis Note - Round 1\n\n## Summary of Analysis\n\nThe initial search results reveal several promising directions for improving the Mamba2 architecture:\n\n1. Integration of Memory Mechanisms:\n- Episodic memory organization could enhance state management\n- Two-stage memory processes combining similarity and temporal retrieval\n- Event-based sequence organization\n\n2. Architectural Innovations:\n- State space duality (SSD) framework shows potential for optimization\n- Selective mechanisms for content-based reasoning\n- Hierarchical processing with recurrent components\n\n3. Efficiency Considerations:\n- Linear-time algorithms for long sequence processing\n- Sparse operations for memory efficiency\n- Adaptive computation paths\n\n## Useful References with Excerpts\n\n1. From \"Understanding the differences in Foundation Models\":\n```\nThe DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms, e.g., selective scan, parallel scan, and accelerated scan.\n```\n\n2. From \"The Expressive Capacity of State Space Models\":\n```\nSSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack.\n```\n\n3. From \"Mamba: Linear-Time Sequence Modeling\":\n```\nFirst, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\n```\n\n4. From \"Human-like Episodic Memory\":\n```\norganises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion.\n```\n\n## Key Insights for Design\n\n1. Memory Architecture:\n- Need to investigate integration of episodic memory with SSM\n- Potential for hierarchical memory organization\n- Event-based sequence segmentation could improve efficiency\n\n2. State Management:\n- Selective state propagation shows promise\n- Need to explore adaptive state transitions\n- Graph-theoretic approaches for state organization\n\n3. Computational Efficiency:\n- Scan algorithms offer efficient implementation\n- Potential for parallel processing\n- Memory-efficient state representations\n\n4. Model Expressiveness:\n- SSMs show strength in exact representation\n- Hierarchical structure without stack simulation\n- Content-based reasoning capabilities\n\n## Future Search Plan\n\nNeed to investigate:\n\n1. Specific implementations of:\n- Hierarchical state space models\n- Graph-based memory organization\n- Adaptive state transitions\n\n2. Technical details about:\n- Scan algorithm optimizations\n- Memory-efficient state representations\n- Event segmentation in sequence models\n\n3. Performance analysis of:\n- Different memory architectures\n- State propagation mechanisms\n- Hierarchical processing approaches\n\n## References\n\n1. Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv.org.\n\n2. Sarrof, Y., Veitsman, Y., & Hahn, M. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv.org.\n\n3. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.org.\n\n4. Fountas, Z., et al. (2024). Human-like Episodic Memory for Infinite Context LLMs. arXiv.org.\n\n5. Lou, C., et al. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv.org."
        },
        {
            "ready": false,
            "query": "dense state space models, multi-scale temporal processing, parallel scan optimization",
            "detail": "Search for technical details about:\n1. Dense connectivity patterns in neural networks\n2. Multi-scale temporal processing architectures\n3. Parallel scan algorithm implementations\n4. Memory-efficient event segmentation techniques",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for technical details about:\n1. Dense connectivity patterns in neural networks\n2. Multi-scale temporal processing architectures\n3. Parallel scan algorithm implementations\n4. Memory-efficient event segmentation techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths (Avg. Score: 0.10)\n\n*Ruslan Khalitov, Tong Yu, Lei Cheng, Zhirong Yang*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** A simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths, and substantially outperforms other neural attention models.\n\n**Abstract:** Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.\n\n##### *Relevant Chunk: No. 17/29 (Score: 0.10)*\n\n```\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR), 2022. Jun He, Liqun Wang, Liu Liu, Jiao Feng, and Hao Wu. Long document classification from local word glimpses via recurrent attention learning. IEEE Access, 7:40707-40718, 2019. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Andrew Jaegle, Felix Axel Gimeno Gil, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning (ICML), 2021. Ruslan Khalitov, Tong Yu, Lei Cheng, and Zhirong Yang. Sparse factorization of square matrices with application to neural attention modeling. Neural Networks, 152:160-168, 2022. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv, 2001.04451, 2020. Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks: A unified approach to action segmentation.\n```\n\n#### 2. Human-like Episodic Memory for Infinite Context LLMs (Avg. Score: 0.03)\n\n*Z. Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** EM-LLM is introduced, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency and providing a computational framework for exploring human memory mechanisms.\n\n**Abstract:** Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information. Experiments on the LongBench dataset demonstrate EM-LLM's superior performance, outperforming the state-of-the-art InfLLM model with an overall relative improvement of 4.3% across various tasks, including a 33% improvement on the PassageRetrieval task. Furthermore, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart. This work not only advances LLM capabilities in processing extended contexts but also provides a computational framework for exploring human memory mechanisms, opening new avenues for interdisciplinary research in AI and cognitive science.\n\n##### *Relevant Chunk: No. 12/36 (Score: 0.04)*\n\n```\n2 A and the analysis in Ji-An et al. (2024) to be more easily reproduced. Finally, we could investigate how skewing event recall based on recency and originally-recorded surprise affects model performance and to what extent it produces biased behaviour found in studies of free recall. In addition, the architecture of EM-LLM, with its differentiated context handling described in Section 3.1 invites further interesting comparisons to cognitive models of human memory beyond episodic. The group of tokens forming the local context, which hold the most recent and task-relevant information, share characteristics with the concept of working memory. For instance, Baddeley (2003)'s influential model of working memory, which posits a limited-capacity system for temporary information storage and manipulation, bears similarities to our local context functionality. Yet, the analogy is not perfect. Our broader context window, including both local context and retrieved memories, might be more accurately compared to Ericsson and Kintsch (1995)'s concept of long-term working memory, which proposes a mechanism for rapid access to relevant information in long-term memory, extending beyond the traditional capacity limits of working memory. Alternatively, our architecture aligns well with Cowan (2001)'s embedded-processes model, where our local context could be likened to the limited-capacity \"focus of attention\" within working memory, while the full context window parallels the activated portion of long-term memory. Future work could explore these analogies more deeply, providing a flexible test-bed for rapidly exploring hypotheses about human memory, and potentially informing debates about capacity limits in working memory. Additionally, inspired by the multi-component nature of Baddeley's model, one might explore the integration of modality-specific buffers within EM-LLM to enhance its performance on multi-modal tasks. Machine learning In refining event boundaries, we utilized modularity and conductance as metrics for evaluating community structure in the similarity graph of attention keys. While effective in our experiments, we acknowledge that numerous other methods for graph clustering and sequence segmentation could potentially be applied (Fortunato, 2010; Yang et al., 2016). Our choice was motivated by their established theoretical foundations and computational efficiency, though comparative studies suggest performance can vary based on network characteristics (Yang et al. 2016). Interestingly, our surprise-based initial boundary detection shares similarities with Bayesian online change-point detection (Adams and MacKay, 2007), suggesting potential avenues for integrating time series analysis techniques into LLM context processing. Future work could explore whether more sophisticated segmentation or clustering algorithms could improve EM-LLM's performance, particularly for extremely long contexts or streaming data scenarios. Such investigations could enhance our model and contribute to understanding how information is structured and processed in LLMs, bridging the gap between traditional sequence analysis and LLM context processing. Looking ahead, several more avenues for future research emerge from this work. One promising direction is to extend our surprise-based segmentation and boundary refinement processes to operate at each layer of the Transformer independently. This could lead to more nuanced and hierarchical representations of episodic memories, following\nthe underlying semantic structure of the input more closely. Additionally, exploring how EM-LLM could be utilised to enable imagination and future thinking has great potential for advancing model-based reinforcement learning and continual learning techniques in LLMs. By leveraging its event-based structure to simulate potential future scenarios or recall past experiences in novel contexts, EM-LLM could enhance an LLM's ability to plan, adapt, and learn continuously from new information. ## 6 CONCLUSION\n\nIn this work, we introduced EM-LLM, a novel and flexible architecture that integrates key aspects of human episodic memory and event cognition into transformer-based language models. Our approach enables LLMs to effectively process and utilise information from vastly extended contexts, far beyond their original training lengths. By combining surprise-based event segmentation with graph-theoretic boundary refinement, and a two-stage memory retrieval process, EM-LLM demonstrates superior performance on long-context tasks compared to state-of-the-art models. Crucially, our method requires no pre-training and can be readily applied to existing LLMs, offering a promising path towards virtually infinite context windows. This capability has the potential to revolutionise how we interact with LLMs, enabling continuous, personalized interactions over extended periods. Furthermore, the flexibility of our framework suggests it could serve as a viable alternative to traditional retrieval-augmented generation (RAG) techniques, especially when combined with efficient compression methods to reduce the memory requirements for the model's KV cache. In conclusion, EM-LLM represents a significant step forward in the development of language models with extended context-processing capabilities. By bridging insights from cognitive science with machine learning, our approach not only enhances the performance of LLMs on long-context tasks but also provides a scalable computational framework for testing hypotheses about human memory. We hope this study will inspire the community to expand research on the intersection between LLMs and human memory mechanisms. ## REFERENCES\n\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, $12: 157-173,2024 a$. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024 . Szymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b. Focused transformer: Contrastive training for context scaling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=s1FjXzJ0jy\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474, 2020. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URLhttps://openreview.net/forum?id= TrjbxzRcnf-. Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range transformers with unlimited length input. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=lJWUJWLCJo. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory, 2024a. David Clewett, Sarah DuBrow, and Lila Davachi. Transcending time in the brain: How event memories are constructed from experience. Hippocampus, 29(3):162-183, 2019. Jeffrey M Zacks. Event perception and memory. Annual review of psychology, 71:165-191, 2020. Christopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan W Pillow, Uri Hasson, and Kenneth A Norman. Discovering event structure in continuous narrative perception and memory. Neuron, 95(3):709-721, 2017. Sebastian Michelmann, Uri Hasson, and Kenneth A. Norman. Evidence that event boundaries are access points for memory retrieval. Psychological Science, 34(3):326-344, 2023a. doi 10.1177/09567976221128206. URL https://doi.org/10.1177/09567976221128206. PMID: 36595492. Jeffrey M Zacks, Nicole K Speer, Khena M Swallow, Todd S Braver, and Jeremy R Reynolds. Event perception: a mind-brain perspective. Psychological bulletin, 133(2):273, 2007. Jeffrey M Zacks, Christopher A Kurby, Michelle L Eisenberg, and Nayiri Haroutunian. Prediction error associated with the perceptual segmentation of naturalistic events. Journal of cognitive neuroscience, 23(12):4057-4066, 2011. Warrick Roseboom, Zafeirios Fountas, Kyriacos Nikiforou, David Bhowmik, Murray Shanahan, and Anil K Seth. Activity in perceptual classification networks as a basis for human subjective time perception.\n```\n\n##### *Relevant Chunk: No. 14/36 (Score: 0.03)*\n\n```\n2117625118. Zafeirios Fountas, Anastasia Sylaidi, Kyriacos Nikiforou, Anil K. Seth, Murray Shanahan, and Warrick Roseboom. A Predictive Processing Model of Episodic Memory and Time Perception. Neural Computation, 34(7):1501-1544, 06 2022. ISSN 0899-7667. doi $10.1162 /$ neco_a_01514. URLhttps://doi.org/10.1162/neco_a_01514. Marc W Howard and Michael J Kahana. A distributed representation of temporal context. Journal of mathematical psychology, 46(3):269-299, 2002. Li Ji-An, Corey Y. Zhou, Marcus K. Benna, and Marcelo G. Mattar. Linking in-context learning in transformers to human episodic memory, 2024. Manoj Kumar, Ariel Goldstein, Sebastian Michelmann, Jeffrey M Zacks, Uri Hasson, and Kenneth A Norman. Bayesian surprise predicts human event segmentation in story listening. Cognitive science, 47(10):e13343, 2023.\n```\n\n##### *Relevant Chunk: No. 30/36 (Score: 0.01)*\n\n```\n2024. Sebastian Michelmann, Amy R Price, Bobbi Aubrey, Camilla K Strauss, Werner K Doyle, Daniel Friedman, Patricia C Dugan, Orrin Devinsky, Sasha Devore, Adeen Flinker, et al. Moment-by-moment tracking of naturalistic learning and its underlying hippocampo-cortical interactions. Nature communications, 12(1):5394, 2021. Olga Lositsky, Janice Chen, Daniel Toker, Christopher J Honey, Michael Shvartsman, Jordan L Poppenk, Uri Hasson, and Kenneth A Norman. Neural pattern change during encoding of a narrative predicts retrospective duration estimates. elife, 5:e16070, 2016. Alberto Mariola, Zafeirios Fountas, Lionel Barnett, and Warrick Roseboom. Event segmentation in continuous, naturalistic videos from model-based, data-driven, and human perspectives. 2022. Sebastian Michelmann, Manoj Kumar, Kenneth A Norman, and Mariya Toneva. Large language models can segment narrative events similarly to humans.\n```\n\n#### 3. Liquid Structural State-Space Models (Avg. Score: 0.01)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 33/54 (Score: 0.01)*\n\n```\nD. Hager. Temporal convolutional networks: A unified approach to action segmentation. In European Conference on Computer Vision, pages 47-54. Springer, 2016. M. Lechner and R. Hasani. Mixed-memory rnns for learning long-term dependencies in irregularly sampled time series. OpenReview, 2021. M. Lechner, R. Hasani, M. Zimmer, T. A. Henzinger, and R. Grosu. Designing worm-inspired neural networks for interpretable robotic control. In 2019 International Conference on Robotics and Automation (ICRA), pages $87-94$. IEEE, 2019. M. Lechner, R. Hasani, A. Amini, T. A. Henzinger, D. Rus, and R. Grosu. Neural circuit policies enabling auditable autonomy. Nature Machine Intelligence, 2(10):642-652, 2020a. M. Lechner, R. Hasani, D. Rus, and R. Grosu. Gershgorin loss stabilizes the recurrent neural network compartment of an end-to-end robot learning scheme. In 2020 International Conference on Robotics and Automation (ICRA). IEEE, 2020b. J. Lee-Thorp, J. Ainslie, I. Eckstein, and S. Ontanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021. S. Li, W. Li, C. Cook, C.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dense state space models, multi-scale temporal processing, parallel scan optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF\n\n*From Search Query: dense state space models*\n\n*Atanas Mirchev, Baris Kayalibay, Patrick van der Smagt, Justin Bayer*\n\n**TL;DR:** This principled treatment of uncertainty and probabilistic inference overcomes the shortcoming of current state-of-the-art solutions to rely on heavily engineered, heterogeneous pipelines and enables the use of neural networks for system identification and a differentiable raycaster for the emission model.\n\n**Abstract:** We solve the problem of 6-DoF localisation and 3D dense reconstruction in spatial environments as approximate Bayesian inference in a deep generative approach which combines learned with engineered models. This principled treatment of uncertainty and probabilistic inference overcomes the shortcoming of current state-of-the-art solutions to rely on heavily engineered, heterogeneous pipelines. Variational inference enables us to use neural networks for system identification, while a differentiable raycaster is used for the emission model. This ensures that our model is amenable to end-to-end gradient-based optimisation. We evaluate our approach on realistic unmanned aerial vehicle flight data, nearing the performance of a state-of-the-art visual inertial odometry system. The applicability of the learned model to downstream tasks such as generative prediction and planning is investigated.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 2. Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval\n\n*From Search Query: dense state space models*\n\n*Zhenghao Liu, Chenyan Xiong, Yuanhuiyi Lv, Zhiyuan Liu, Ge Yu*\n\n**TL;DR:** UniVL-DR achieves the state-of-the-art on the multi-modal open-domain question answering benchmark, WebQA, and outperforms all retrieval models on the two subtasks, text-text retrieval and text-image retrieval.\n\n**Abstract:** This paper presents Universal Vision-Language Dense Retrieval (UniVL-DR), which builds a unified model for multi-modal retrieval. UniVL-DR encodes queries and multi-modality resources in an embedding space for searching candidates from different modalities. To learn a unified embedding space for multi-modal retrieval, UniVL-DR proposes two techniques: 1) Universal embedding optimization strategy, which contrastively optimizes the embedding space using the modality-balanced hard negatives; 2) Image verbalization method, which bridges the modality gap between images and texts in the raw data space. UniVL-DR achieves the state-of-the-art on the multi-modal open-domain question answering benchmark, WebQA, and outperforms all retrieval models on the two subtasks, text-text retrieval and text-image retrieval. It demonstrates that universal multi-modal search is feasible to replace the divide-and-conquer pipeline with a united model and also benefits single/cross modality tasks. All source codes of this work are available at https://github.com/OpenMatch/UniVL-DR.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 3. FLAIR: a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery\n\n*From Search Query: multi-scale temporal processing*\n\n*A. Garioud, Nicolas Gonthier, Loic Landrieu, Apolline De Wit, Marion Valette, Marc Poup'ee, S. Giordano, Boris Wattrelos*\n\n**Abstract:** We introduce the French Land cover from Aerospace ImageRy (FLAIR), an extensive dataset from the French National Institute of Geographical and Forest Information (IGN) that provides a unique and rich resource for large-scale geospatial analysis. FLAIR contains high-resolution aerial imagery with a ground sample distance of 20 cm and over 20 billion individually labeled pixels for precise land-cover classification. The dataset also integrates temporal and spectral data from optical satellite time series. FLAIR thus combines data with varying spatial, spectral, and temporal resolutions across over 817 km2 of acquisitions representing the full landscape diversity of France. This diversity makes FLAIR a valuable resource for the development and evaluation of novel methods for large-scale land-cover semantic segmentation and raises significant challenges in terms of computer vision, data fusion, and geospatial analysis. We also provide powerful uni- and multi-sensor baseline models that can be employed to assess algorithm's performance and for downstream applications. Through its extent and the quality of its annotation, FLAIR aims to spur improvements in monitoring and understanding key anthropogenic development indicators such as urban growth, deforestation, and soil artificialization. Dataset and codes can be accessed at https://ignf.github.io/FLAIR/\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 4. Multi-scale Graphical Models for Spatio-Temporal Processes\n\n*From Search Query: multi-scale temporal processing*\n\n*F. Janoos, H. Denli, Niranjan A. Subrahmanya*\n\n**TL;DR:** This paper presents a hierarchical graphical model with physically derived priors that better represents the multi-scale character of these dynamical systems and proposes algorithms to efficiently estimate the interaction structure from data.\n\n**Abstract:** Learning the dependency structure between spatially distributed observations of a spatio-temporal process is an important problem in many fields such as geology, geophysics, atmospheric sciences, oceanography, etc.. However, estimation of such systems is complicated by the fact that they exhibit dynamics at multiple scales of space and time arising due to a combination of diffusion and convection/advection [17]. As we show, time-series graphical models based on vector auto-regressive processes[18] are inefficient in capturing such multi-scale structure. In this paper, we present a hierarchical graphical model with physically derived priors that better represents the multi-scale character of these dynamical systems. We also propose algorithms to efficiently estimate the interaction structure from data. We demonstrate results on a general class of problems arising in exploration geophysics by discovering graphical structure that is physically meaningful and provide evidence of its advantages over alternative approaches.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2014\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 5. Multi Time Scale World Models\n\n*From Search Query: multi-scale temporal processing*\n\n*Vaisakh Shaj, Saleh Gholam Zadeh, Ozan Demir, L. R. Douat, Gerhard Neumann*\n\n**TL;DR:** This work proposes a probabilistic formalism to learn multi-time scale world models which it calls the Multi Time Scale State Space (MTS3) model, which uses a computationally efficient inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over several seconds into the future.\n\n**Abstract:** Intelligent agents use internal world models to reason and make predictions about different courses of their actions at many scales. Devising learning paradigms and architectures that allow machines to learn world models that operate at multiple levels of temporal abstractions while dealing with complex uncertainty predictions is a major technical hurdle. In this work, we propose a probabilistic formalism to learn multi-time scale world models which we call the Multi Time Scale State Space (MTS3) model. Our model uses a computationally efficient inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over several seconds into the future. Our experiments, which focus on action conditional long horizon future predictions, show that MTS3 outperforms recent methods on several system identification benchmarks including complex simulated and real-world dynamical systems. Code is available at this repository: https://github.com/ALRhub/MTS3.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Parallel-mentoring for Offline Model-based Optimization\n\n*From Search Query: parallel scan optimization*\n\n*Can Chen, Christopher Beckham, Zixuan Liu, Xue Liu, C. Pal*\n\n**TL;DR:** This work proposes \\textit{parallel-mentoring} as an effective and novel method that facilitates mentoring among parallel proxies, creating a more robust ensemble to mitigate the out-of-distribution issue.\n\n**Abstract:** We study offline model-based optimization to maximize a black-box objective function with a static dataset of designs and scores. These designs encompass a variety of domains, including materials, robots and DNA sequences. A common approach trains a proxy on the static dataset to approximate the black-box objective function and performs gradient ascent to obtain new designs. However, this often results in poor designs due to the proxy inaccuracies for out-of-distribution designs. Recent studies indicate that: (a) gradient ascent with a mean ensemble of proxies generally outperforms simple gradient ascent, and (b) a trained proxy provides weak ranking supervision signals for design selection. Motivated by (a) and (b), we propose \\textit{parallel-mentoring} as an effective and novel method that facilitates mentoring among parallel proxies, creating a more robust ensemble to mitigate the out-of-distribution issue. We focus on the three-proxy case and our method consists of two modules. The first module, \\textit{voting-based pairwise supervision}, operates on three parallel proxies and captures their ranking supervision signals as pairwise comparison labels. These labels are combined through majority voting to generate consensus labels, which incorporate ranking supervision signals from all proxies and enable mutual mentoring. However, label noise arises due to possible incorrect consensus. To alleviate this, we introduce an \\textit{adaptive soft-labeling} module with soft-labels initialized as consensus labels. Based on bi-level optimization, this module fine-tunes proxies in the inner level and learns more accurate labels in the outer level to adaptively mentor proxies, resulting in a more robust ensemble. Experiments validate the effectiveness of our method. Our code is available here.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 3*)\n\n#### 7. Optimal Time Complexities of Parallel Stochastic Optimization Methods Under a Fixed Computation Model\n\n*From Search Query: parallel scan optimization*\n\n*A. Tyurin, Peter Richt\u00e1rik*\n\n**TL;DR:** A new protocol is proposed that generalizes the classical oracle framework approach and establishes minimax complexities for parallel optimization methods that have access to an unbiased stochastic gradient oracle with bounded variance.\n\n**Abstract:** Parallelization is a popular strategy for improving the performance of iterative algorithms. Optimization methods are no exception: design of efficient parallel optimization methods and tight analysis of their theoretical properties are important research endeavors. While the minimax complexities are well known for sequential optimization methods, the theory of parallel optimization methods is less explored. In this paper, we propose a new protocol that generalizes the classical oracle framework approach. Using this protocol, we establish minimax complexities for parallel optimization methods that have access to an unbiased stochastic gradient oracle with bounded variance. We consider a fixed computation model characterized by each worker requiring a fixed but worker-dependent time to calculate stochastic gradient. We prove lower bounds and develop optimal algorithms that attain them. Our results have surprising consequences for the literature of asynchronous optimization methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 2*)\n\n#### 8. Blockwise Stochastic Variance-Reduced Methods with Parallel Speedup for Multi-Block Bilevel Optimization\n\n*From Search Query: parallel scan optimization*\n\n*Quanqi Hu, Zimeng Qiu, Zhishuai Guo, Lijun Zhang, Tianbao Yang*\n\n**TL;DR:** Two stochastic algorithms are proposed by using advanced blockwise variance-reduction techniques for tracking the Hessian matrices or the HessIAN-vector products (for high-dimensional problems) and prove an iteration complexity of $O(\\frac{m\\epsilon^{-3}\\mathbb{I}(I<m)}{I\\sqrt{I}} + \\frac(m-3}}{I\\ sqrt{B)$ for finding an $\\epsil\n\n**Abstract:** In this paper, we consider non-convex multi-block bilevel optimization (MBBO) problems, which involve $m\\gg 1$ lower level problems and have important applications in machine learning. Designing a stochastic gradient and controlling its variance is more intricate due to the hierarchical sampling of blocks and data and the unique challenge of estimating hyper-gradient. We aim to achieve three nice properties for our algorithm: (a) matching the state-of-the-art complexity of standard BO problems with a single block; (b) achieving parallel speedup by sampling $I$ blocks and sampling $B$ samples for each sampled block per-iteration; (c) avoiding the computation of the inverse of a high-dimensional Hessian matrix estimator. However, it is non-trivial to achieve all of these by observing that existing works only achieve one or two of these properties. To address the involved challenges for achieving (a, b, c), we propose two stochastic algorithms by using advanced blockwise variance-reduction techniques for tracking the Hessian matrices (for low-dimensional problems) or the Hessian-vector products (for high-dimensional problems), and prove an iteration complexity of $O(\\frac{m\\epsilon^{-3}\\mathbb{I}(I<m)}{I\\sqrt{I}} + \\frac{m\\epsilon^{-3}}{I\\sqrt{B}})$ for finding an $\\epsilon$-stationary point under appropriate conditions. We also conduct experiments to verify the effectiveness of the proposed algorithms comparing with existing MBBO algorithms.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\n\n*From Search Query: dense state space models*\n\n*Yunhe Wang, Tianyu Guo, Yujie Yang, Chengcheng Wang, Yehui Tang, Kai Han, wei he*\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n**Published:** 2024-02-26\n\n\n\n#### 2. Dense Passage Retrieval for Open-Domain Question Answering\n\n*From Search Query: dense state space models*\n\n*Wen-tau Yih, Patrick Lewis, Barlas O\u011fuz, Sewon Min, Ledell Wu, Vladimir Karpukhin, Sergey Edunov, Danqi Chen*\n\n**Abstract:** Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.\n\n**Proceeding:** emnlp-2020-11\n\n**Published:** 2020-04-10\n\n\n\n#### 3. UniPose: Unified Human Pose Estimation in Single Images and Videos\n\n*From Search Query: multi-scale temporal processing*\n\n*Andreas Savakis, Bruno Artacho*\n\n**Abstract:** We propose UniPose, a unified framework for human pose estimation, based on our \"Waterfall\" Atrous Spatial Pooling architecture, that achieves state-of-art-results on several pose estimation metrics. Current pose estimation methods utilizing standard CNN architectures heavily rely on statistical postprocessing or predefined anchor poses for joint localization. UniPose incorporates contextual segmentation and joint localization to estimate the human pose in a single stage, with high accuracy, without relying on statistical postprocessing methods. The Waterfall module in UniPose leverages the efficiency of progressive filtering in the cascade architecture, while maintaining multi-scale fields-of-view comparable to spatial pyramid configurations. Additionally, our method is extended to UniPose-LSTM for multi-frame processing and achieves state-of-the-art results for temporal pose estimation in Video. Our results on multiple datasets demonstrate that UniPose, with a ResNet backbone and Waterfall module, is a robust and efficient architecture for pose estimation obtaining state-of-the-art results in single person pose detection for both single images and videos.\n\n**Conference:** unipose-unified-human-pose-estimation-in-1\n\n**Published:** 2020-01-22\n\n\n\n#### 4. Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\n\n*From Search Query: multi-scale temporal processing*\n\n*Sercan O. Arik, Bryan Lim, Tomas Pfister, Nicolas Loeff*\n\n**Abstract:** Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.\n\n**Published:** 2019-12-19\n\n\n\n#### 5. High Resolution Medical Image Analysis with Spatial Partitioning\n\n*From Search Query: parallel scan optimization*\n\n*Le Hou, Xiaodan Song, Youlong Cheng, Panagiotis Korfiatis, Noam Shazeer, Daniel J. Blezek, Yeqing Li, Travis M. Drucker, Niki Parmar*\n\n**Abstract:** Medical images such as 3D computerized tomography (CT) scans and pathology images, have hundreds of millions or billions of voxels/pixels. It is infeasible to train CNN models directly on such high resolution images, because neural activations of a single image do not fit in the memory of a single GPU/TPU, and naive data and model parallelism approaches do not work. Existing image analysis approaches alleviate this problem by cropping or down-sampling input images, which leads to complicated implementation and sub-optimal performance due to information loss. In this paper, we implement spatial partitioning, which internally distributes the input and output of convolutional layers across GPUs/TPUs. Our implementation is based on the Mesh-TensorFlow framework and the computation distribution is transparent to end users. With this technique, we train a 3D Unet on up to 512 by 512 by 512 resolution data. To the best of our knowledge, this is the first work for handling such high resolution images end-to-end.\n\n**Published:** 2019-09-06\n\n\n\n#### 6. GLIM: 3D Range-Inertial Localization and Mapping with GPU-Accelerated Scan Matching Factors\n\n*From Search Query: parallel scan optimization*\n\n*Anonymous*\n\n**Abstract:** This article presents GLIM, a 3D range-inertial localization and mapping framework with GPU-accelerated scan matching factors. The odometry estimation module of GLIM employs a combination of fixed-lag smoothing and keyframe-based point cloud matching that makes it possible to deal with a few seconds of completely degenerated range data while efficiently reducing trajectory estimation drift. It also incorporates multi-camera visual feature constraints in a tightly coupled way to further improve the stability and accuracy. The global trajectory optimization module directly minimizes the registration errors between submaps over the entire map. This approach enables us to accurately constrain the relative pose between submaps with a small overlap. Although both the odometry estimation and global trajectory optimization algorithms require much more computation than existing methods, we show that they can be run in real-time due to the careful design of the registration error evaluation algorithm and the entire system to fully leverage GPU parallel processing.\n\n**Published:** 2024-07-14\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on dense state space models, multi-scale temporal processing, parallel scan optimization, and memory-efficient event segmentation, here are some key points and references that can guide the research:\n\n## Dense Connectivity Patterns in Neural Networks\n\n- **Dense State Space Models (DenseSSM):** The concept of dense connectivity in state space models involves selectively integrating shallow-layer hidden states into deeper layers. This approach helps retain fine-grained information crucial for the final output, as seen in the \"DenseMamba\" model. This dense connectivity pattern enhances the model's ability to capture complex relationships and retain detailed information from earlier layers.\n\n## Multi-scale Temporal Processing Architectures\n\n- **Hierarchical State Space Models (HiSS):** HiSS employs a temporal hierarchy by stacking structured state-space models on top of each other. This hierarchical approach allows for processing sequences at different timescales, which significantly improves sequence modeling performance compared to other state-of-the-art models like causal Transformers, LSTMs, and S4.\n- **Spiking State Space Models (SSMs):** The use of spiking SSMs, as discussed in the SPikE-SSM model, incorporates dynamic hidden states to handle long-range dependencies (LRDs) effectively. This model leverages the temporal dimension to exploit the dynamics of spiking neurons, which is beneficial for tasks with long-sequence inputs and inherently sparse relevant information.\n\n## Parallel Scan Algorithm Implementations\n\n- **Efficient Parallel Scans in SSMs:** The S5 layer, mentioned in the context of Spectral State Space Models, uses efficient parallel scans instead of convolutions. This approach allows for leveraging widely implemented parallel scan algorithms, which can significantly speed up processing times. The S5 layer utilizes a multi-input, multi-output SSM design that can efficiently handle parallel scans.\n- **Parallel Computing in Spiking SSMs:** The SPikE-SSM model addresses the challenge of parallel computing in spiking neurons by using a probabilistic reset mechanism and a concise reset-refractory neuron model. This enables efficient and parallel training of neuronal dynamics, which is crucial for handling long sequences.\n\n## Memory-Efficient Event Segmentation Techniques\n\n- **Event-Based Segmentation:** In the context of spiking SSMs, event-based segmentation is naturally aligned with the sparse representation of spiking neurons. The SPikE-SSM model exploits the functionality of biological-plausible temporal dynamics, which includes trainable thresholds and refractory magnitudes. This approach helps in maintaining a balance between sparsity and accuracy, making it memory-efficient for processing long sequences.\n- **Graph-Based Memory Structures:** While not directly addressed in the provided sources, graph-based memory structures can be inferred as a potential direction. Graph neural networks (GNNs) have shown promise in managing complex systems by separating self- and neighbor-interaction components, which can be adapted for memory-efficient event segmentation in sequence modeling tasks.\n\n## Additional Insights\n\n- **Mixture of Experts (MoE):** Although not directly related to state space models, MoE architectures can provide insights into sparse and efficient processing. MoE layers select only a subset of experts during inference, which is computationally cheaper and can be seen as a form of memory-efficient processing. This concept can be explored for integrating sparse operations into state space models.\n\nBy integrating these concepts\u2014dense connectivity, hierarchical temporal processing, efficient parallel scans, and memory-efficient event segmentation\u2014the researcher can develop a more robust and efficient autoregressive language model. The hierarchical state organization, use of spiking SSMs, and efficient parallel scan algorithms will be particularly beneficial for handling long sequences and improving overall model performance.\n",
            "analysis": "# Analysis Note - Round 2\n\n## Summary of Analysis\n\nAfter analyzing the search results, several promising directions for improving the Mamba2 architecture have emerged:\n\n1. Hierarchical Processing:\n- DenseSSM's approach of selective integration of shallow-layer states\n- HiSS's temporal hierarchy for improved sequence modeling\n- Efficient parallel processing capabilities\n\n2. Memory Organization:\n- Graph-based memory structures for better information retention\n- Event-based segmentation for efficient processing\n- Two-stage memory retrieval mechanisms\n\n3. Computational Efficiency:\n- Scan algorithm optimizations for linear-time processing\n- Sparse operations for reduced memory footprint\n- Adaptive computation paths\n\n## Useful References with Excerpts\n\n1. From \"DenseMamba\":\n```\nDenseSSM retains fine-grained information crucial for the final output. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output.\n```\n\n2. From \"Hierarchical State Space Models\":\n```\nHiSS stacks structured state-space models on top of each other to create a temporal hierarchy... outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE.\n```\n\n3. From \"Spectral State Space Models\":\n```\nThe S4 parameterization restricts the system matrices A to be normal plus low-rank, allowing for stable diagonalization of the dynamics... S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms.\n```\n\n4. From \"Simplified State Space Layers\":\n```\nWhereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM... can leverage efficient and widely implemented parallel scans.\n```\n\n## Key Insights for Design\n\n1. Hierarchical State Organization:\n- Stack multiple SSM layers with selective connections\n- Use temporal hierarchy for different timescales\n- Implement efficient parallel processing\n\n2. Memory Management:\n- Integrate graph-based memory structures\n- Implement event-based segmentation\n- Use two-stage memory retrieval\n\n3. Computational Improvements:\n- Optimize scan algorithms\n- Implement sparse operations\n- Design adaptive computation paths\n\n4. Novel Features to Consider:\n- Dense connections between layers\n- Multi-input, multi-output SSM design\n- Spectral parameterization for stability\n\n## Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of:\n- Dense connectivity in SSMs\n- Multi-scale temporal processing\n- Memory-efficient event segmentation\n\n2. Technical details about:\n- Parallel scan optimizations\n- Graph-based memory architectures\n- Spectral parameterization methods\n\n## References\n\n1. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv.org.\n\n2. Pinto, L., et al. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. arXiv.org.\n\n3. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv.org.\n\n4. Smith, J., Warrington, A., & Linderman, S. W. (2022). Simplified State Space Layers for Sequence Modeling. International Conference on Learning Representations.\n\n5. Lu, P., Wang, S., Rezagholizadeh, M., Liu, B., & Kobyzev, I. (2023). Efficient Classification of Long Documents via State-Space Models. Conference on Empirical Methods in Natural Language Processing."
        },
        {
            "ready": false,
            "query": "adaptive connectivity patterns, event-driven memory, sparse state space models",
            "detail": "Search for technical details about:\n1. Adaptive connectivity patterns in neural networks\n2. Event-driven memory architectures\n3. Sparse state space model implementations\n4. Memory-efficient graph structures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for technical details about:\n1. Adaptive connectivity patterns in neural networks\n2. Event-driven memory architectures\n3. Sparse state space model implementations\n4. Memory-efficient graph structures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.05)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.05)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.02)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.02)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 3. Reformer: The Efficient Transformer (Avg. Score: 0.01)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 16/19 (Score: 0.01)*\n\n```\nCoRR, abs/1811.02084, 2018. URL http://arxiv.org/abs/1811.02084. Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, and Christopher R\u00e9. Low-memory neural network training: A technical report.\n```\n\n#### 4. Resurrecting Recurrent Neural Networks for Long Sequences  (Avg. Score: 0.01)\n\n*Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 146  (*Influential: 26*)\n\n**TL;DR:** This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Abstract:** Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n##### *Relevant Chunk: No. 31/71 (Score: 0.01)*\n\n```\nR. Hasani, M. Lechner, T.-H. Wang, M. Chahine, A. Amini, and D. Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. K. Helfrich, D. Willmott, and Q. Ye. Orthogonal recurrent neural networks with scaled cayley transform. In International Conference on Machine Learning. PMLR, 2018. T. Hennigan, T. Cai, T. Norman, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL http://github . com /deepmind/dm-haiku. S. Hochreiter. Untersuchungen zu dynamischen neuronales netzen. Diploma thesis, Institut f'ur Informatik, Technische Universit\"at M\"unchen, 1991. S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 1997. J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 1982.\n```\n\n#### 5. Sparse and continuous attention mechanisms (Avg. Score: 0.01)\n\n*Andr\u00e9 F. T. Martins, Marcos Vin\u00edcius Treviso, Ant\u00f3nio Farinhas, Vlad Niculae, M\u00e1rio A. T. Figueiredo, P. Aguiar*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 28  (*Influential: 2*)\n\n**TL;DR:** This paper extends alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families, and introduces continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}.\n\n**Abstract:** Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.\n\n##### *Relevant Chunk: No. 20/36 (Score: 0.01)*\n\n```\n[22] M. Figueiredo. Adaptive sparseness using Jeffreys prior. In Proc. of NeurIPS, pages 697-704, 2001. [23] M. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211-244, 2001. [24] Hiroshi Matsuzoe and Atsumi Ohara. Geometry for q-exponential families. In Recent Progress in Differential Geometry and its Related Fields, pages 55-71. World Scientific, 2012. [25] Shun-ichi Amari and Atsumi Ohara. Geometry of q-exponential family of probability distributions. Entropy, 13(6):1170-1185, 2011. [26] Vassiliy A Epanechnikov. Non-parametric estimation of a multivariate probability density. Theory of Probability \\& Its Applications, 14(1):153-158, 1969. [27] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. In Advances in Neural Information Processing Systems, pages 2440-2448, 2015. [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive connectivity patterns, event-driven memory, sparse state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Adaptive Shrinkage Estimation for Streaming Graphs\n\n*From Search Query: adaptive connectivity patterns*\n\n*Nesreen Ahmed, N. Duffield*\n\n**TL;DR:** This work proposes a novel adaptive, single-pass sampling framework and unbiased estimators for higher-order network analysis of large streaming networks, and introduces a novel James-Stein shrinkage estimator to reduce the estimation error.\n\n**Abstract:** Networks are a natural representation of complex systems across the sciences, and higher-order dependencies are central to the understanding and modeling of these systems. However, in many practical applications such as online social networks, networks are massive, dynamic, and naturally streaming, where pairwise interactions among vertices become available one at a time in some arbitrary order. The massive size and streaming nature of these networks allow only partial observation, since it is infeasible to analyze the entire network. Under such scenarios, it is challenging to study the higher-order structural and connectivity patterns of streaming networks. In this work, we consider the fundamental problem of estimating the higher-order dependencies using adaptive sampling. We propose a novel adaptive, single-pass sampling framework and unbiased estimators for higher-order network analysis of large streaming networks. Our algorithms exploit adaptive techniques to identify edges that are highly informative for efficiently estimating the higher-order structure of streaming networks from small sample data. We also introduce a novel James-Stein shrinkage estimator to reduce the estimation error. Our approach is fully analytic, computationally efficient, and can be incrementally updated in a streaming setting. Numerical experiments on large networks show that our approach is superior to baseline methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Connectivity Patterns are Task Embeddings\n\n*From Search Query: adaptive connectivity patterns*\n\n*Zhiheng Xi, Rui Zheng, Yuan Zhang, Xuanjing Huang, Zhongyu Wei, Minlong Peng, Mingming Sun, Qi Zhang, Tao Gui*\n\n**TL;DR:** This work draws inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and uses the connectivity patterns of neurons as a unique identifier associated with the task.\n\n**Abstract:** Task embeddings are task-specific vectors designed to construct a semantic space of tasks, which can be used to predict the most transferable source task for a given target task via the similarity between task embeddings. However, existing methods use optimized parameters and representations as task embeddings, resulting in substantial computational complexity and storage requirements. In this work, we draw inspiration from the operating mechanism of deep neural networks (DNNs) and biological brains, where neuronal activations are sparse and task-specific, and we use the connectivity patterns of neurons as a unique identifier associated with the task. The proposed method learns to assign importance masks for sub-structures of DNNs, and accordingly indicate the task-specific connectivity patterns. In addition to the storage advantages brought by the binary masking mechanism and structured sparsity, the early-bird nature of the sparse optimization process can deliver an efficient computation advantage. Experiments show that our method consistently outperforms other baselines in predicting inter-task transferability across data regimes and transfer settings, while keeping high efficiency in computation and storage.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Deep Architecture Connectivity Matters for Its Convergence: A Fine-Grained Analysis\n\n*From Search Query: adaptive connectivity patterns*\n\n*Wuyang Chen, Wei Huang, Xinyu Gong, B. Hanin, Zhangyang Wang*\n\n**TL;DR:** This work theoretically characterize the impact of connectivity patterns on the convergence of DNNs under gradient descent training in fine granularity, and shows that by a simple filtration on \"unpromising\"connectivity patterns, it can trim down the number of models to evaluate, and significantly accelerate the large-scale neural architecture search without any overhead.\n\n**Abstract:** Advanced deep neural networks (DNNs), designed by either human or AutoML algorithms, are growing increasingly complex. Diverse operations are connected by complicated connectivity patterns, e.g., various types of skip connections. Those topological compositions are empirically effective and observed to smooth the loss landscape and facilitate the gradient flow in general. However, it remains elusive to derive any principled understanding of their effects on the DNN capacity or trainability, and to understand why or in which aspect one specific connectivity pattern is better than another. In this work, we theoretically characterize the impact of connectivity patterns on the convergence of DNNs under gradient descent training in fine granularity. By analyzing a wide network's Neural Network Gaussian Process (NNGP), we are able to depict how the spectrum of an NNGP kernel propagates through a particular connectivity pattern, and how that affects the bound of convergence rates. As one practical implication of our results, we show that by a simple filtration on\"unpromising\"connectivity patterns, we can trim down the number of models to evaluate, and significantly accelerate the large-scale neural architecture search without any overhead. Code is available at: https://github.com/VITA-Group/architecture_convergence.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 4. Training Spiking Neural Networks with Event-driven Backpropagation\n\n*From Search Query: event-driven memory*\n\n*Yaoyu Zhu, Zhaofei Yu, Wei Fang, Xiaodong Xie, Tiejun Huang, T. Masquelier*\n\n**TL;DR:** This paper analyzes the commonly used temporal backpropagation training approach and proves that the sum of gradients remains unchanged between fully-connected and convolutional layers, and proposes a backward kernel that can solve the reverse gradient problem for time-based gradients and keep the property of the invariable sum ofGradients.\n\n**Abstract:** Spiking Neural networks (SNNs) represent and transmit information by spatiotemporal spike patterns, which bring two major advantages: biological plausibility and suitability for ultralow-power neuromorphic implementation. Despite this, the binary firing characteristic makes training SNNs more challenging. To learn the parameters of deep SNNs in an event-driven fashion as in inference of SNNs, back-propagation with respect to spike timing is proposed. Although this event-driven learning has the advantages of lower computational cost and memory occupation, the accuracy is far below the recurrent neural network-like learning approaches. In this paper, we first analyze the commonly used temporal backpropagation training approach and prove that the sum of gradients remains unchanged between fully-connected and convolutional layers. Secondly, we show that the max pooling layer meets the above invariance rule, while the average pooling layer does not, which will suffer the gradient vanishing problem but can be revised to meet the requirement. Thirdly, we point out the reverse gradient problem for time-based gradients and propose a backward kernel that can solve this problem and keep the property of the invariable sum of gradients. The experimental results show that the proposed approach achieves state-of-the-art performance on CIFAR10 among time-based training methods. Also, this is the first time that the time-based backpropagation approach successfully trains SNN on the CIFAR100 dataset. Our code is available at https://github.com/zhuyaoyu/SNN-event-driven-learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 30  (*Influential: 2*)\n\n#### 5. Biomedical Event Extraction based on Knowledge-driven Tree-LSTM\n\n*From Search Query: event-driven memory*\n\n*Diya Li, Lifu Huang, Heng Ji, Jiawei Han*\n\n**TL;DR:** A novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework is proposed, incorporating two new types of features: dependency structures to capture wide contexts and entity properties from external ontologies via entity linking.\n\n**Abstract:** Event extraction for the biomedical domain is more challenging than that in the general news domain since it requires broader acquisition of domain-specific knowledge and deeper understanding of complex contexts. To better encode contextual information and external background knowledge, we propose a novel knowledge base (KB)-driven tree-structured long short-term memory networks (Tree-LSTM) framework, incorporating two new types of features: (1) dependency structures to capture wide contexts; (2) entity properties (types and category descriptions) from external ontologies via entity linking. We evaluate our approach on the BioNLP shared task with Genia dataset and achieve a new state-of-the-art result. In addition, both quantitative and qualitative studies demonstrate the advancement of the Tree-LSTM and the external knowledge representation for biomedical event extraction.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 73  (*Influential: 11*)\n\n#### 6. Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences\n\n*From Search Query: event-driven memory*\n\n*Daniel Neil, Michael Pfeiffer, Shih-Chii Liu*\n\n**TL;DR:** This work introduces the Phased LSTM model, which extends the L STM unit by adding a new time gate, controlled by a parametrized oscillation with a frequency range which require updates of the memory cell only during a small percentage of the cycle.\n\n**Abstract:** Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. Current RNN models are ill suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors which generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range which require updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 420  (*Influential: 55*)\n\n#### 7. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: sparse state space models*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Variational Gaussian Process State-Space Models\n\n*From Search Query: sparse state space models*\n\n*R. Frigola, Yutian Chen, C. Rasmussen*\n\n**TL;DR:** This work presents a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes and offers the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting.\n\n**Abstract:** State-space models have been successfully used for more than fifty years in different areas of science and engineering. We present a procedure for efficient variational Bayesian learning of nonlinear state-space models based on sparse Gaussian processes. The result of learning is a tractable posterior over nonlinear dynamical systems. In comparison to conventional parametric models, we offer the possibility to straightforwardly trade off model capacity and computational cost whilst avoiding overfitting. Our main algorithm uses a hybrid inference approach combining variational Bayes and sequential Monte Carlo. We also present stochastic variational inference and online learning approaches for fast learning with long time series.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2014\n\n**Citations:** 171  (*Influential: 25*)\n\n#### 9. Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC\n\n*From Search Query: sparse state space models*\n\n*R. Frigola, F. Lindsten, Thomas Bo Sch\u00f6n, C. Rasmussen*\n\n**TL;DR:** This work presents a fully Bayesian approach to inference and learning in nonlinear nonparametric state-space models and places a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena.\n\n**Abstract:** State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identification) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. To enable efficient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2013\n\n**Citations:** 142  (*Influential: 11*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting\n\n*From Search Query: adaptive connectivity patterns*\n\n*Xianzhi Wang, Lina Yao, Lei Bai, Can Wang, Can Li*\n\n**Abstract:** Modeling complex spatial and temporal correlations in the correlated time series data is indispensable for understanding the traffic dynamics and predicting the future status of an evolving traffic system. Recent works focus on designing complicated graph neural network architectures to capture shared patterns with the help of pre-defined graphs. In this paper, we argue that learning node-specific patterns is essential for traffic forecasting while the pre-defined graph is avoidable. To this end, we propose two adaptive modules for enhancing Graph Convolutional Network (GCN) with new capabilities: 1) a Node Adaptive Parameter Learning (NAPL) module to capture node-specific patterns; 2) a Data Adaptive Graph Generation (DAGG) module to infer the inter-dependencies among different traffic series automatically. We further propose an Adaptive Graph Convolutional Recurrent Network (AGCRN) to capture fine-grained spatial and temporal correlations in traffic series automatically based on the two modules and recurrent networks. Our experiments on two real-world traffic datasets show AGCRN outperforms state-of-the-art by a significant margin without pre-defined graphs about spatial connections.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-07-06\n\n\n\n#### 2. Heterogeneous Graph Neural Networks for Malicious Account Detection\n\n*From Search Query: adaptive connectivity patterns*\n\n*Jun Zhou, Chaochao Chen, Le Song, Ziqi Liu, Xiaolong Li, Xinxing Yang*\n\n**Abstract:** We present, GEM, the first heterogeneous graph neural network approach for detecting malicious accounts at Alipay, one of the world's leading mobile cashless payment platform. Our approach, inspired from a connected subgraph approach, adaptively learns discriminative embeddings from heterogeneous account-device graphs based on two fundamental weaknesses of attackers, i.e. device aggregation and activity aggregation. For the heterogeneous graph consists of various types of nodes, we propose an attention mechanism to learn the importance of different types of nodes, while using the sum operator for modeling the aggregation patterns of nodes in each type. Experiments show that our approaches consistently perform promising results compared with competitive methods over time.\n\n**Published:** 2020-02-27\n\n\n\n#### 3. A differentiable brain simulator bridging brain simulation and brain-inspired computing\n\n*From Search Query: event-driven memory*\n\n*Shangyang Li, Hongyaoxing Gu, Si Wu, Sichao He, Tianqiu Zhang, ChaoMing Wang*\n\n**Abstract:** Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to facilitate each other's development. However, none of the existing software in the fields can achieve this goal, because traditional brain simulators lack differentiability for training, while existing deep learning (DL) frameworks fail to capture the biophysical realism and complexity of brain dynamics. In this paper, we introduce BrainPy, a differentiable brain simulator developed using JAX and XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy expands upon the functionalities of JAX, a powerful AI framework, by introducing complete capabilities for flexible, efficient, and scalable brain simulation. It offers a range of sparse and event-driven operators for efficient and scalable brain simulation, an abstraction for managing the intricacies of synaptic computations, a modular and flexible interface for constructing multi-scale brain models, and an object-oriented just-in-time compilation approach to handle the memory-intensive nature of brain dynamics. We showcase the efficiency and scalability of BrainPy on benchmark tasks, highlight its differentiable simulation for biologically plausible spiking models, and discuss its potential to support research at the intersection of brain simulation and BIC.\n\n**Published:** 2023-11-09\n\n\n\n#### 4. Purine: A bi-graph based deep learning framework\n\n*From Search Query: event-driven memory*\n\n*Xuan Luo, Shuo Li, Shuicheng Yan, Min Lin*\n\n**Abstract:** In this paper, we introduce a novel deep learning framework, termed Purine.\nIn Purine, a deep network is expressed as a bipartite graph (bi-graph), which\nis composed of interconnected operators and data tensors. With the bi-graph\nabstraction, networks are easily solvable with event-driven task dispatcher. We\nthen demonstrate that different parallelism schemes over GPUs and/or CPUs on\nsingle or multiple PCs can be universally implemented by graph composition.\nThis eases researchers from coding for various parallelization schemes, and the\nsame dispatcher can be used for solving variant graphs. Scheduled by the task\ndispatcher, memory transfers are fully overlapped with other computations,\nwhich greatly reduce the communication overhead and help us achieve approximate\nlinear acceleration.\n\n**Published:** 2014-12-19\n\n\n\n#### 5. Dense Passage Retrieval for Open-Domain Question Answering\n\n*From Search Query: sparse state space models*\n\n*Wen-tau Yih, Patrick Lewis, Barlas O\u011fuz, Sewon Min, Ledell Wu, Vladimir Karpukhin, Sergey Edunov, Danqi Chen*\n\n**Abstract:** Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.\n\n**Proceeding:** emnlp-2020-11\n\n**Published:** 2020-04-10\n\n\n\n#### 6. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems\n\n*From Search Query: sparse state space models*\n\n*Rakesh Shivanna, Lichan Hong, Sagar Jain, Ruoxi Wang, Derek Z. Cheng, Ed H. Chi, Dong Lin*\n\n**Abstract:** Learning effective feature crosses is the key behind building recommender systems. However, the sparse and large feature space requires exhaustive search to identify effective crosses. Deep & Cross Network (DCN) was proposed to automatically and efficiently learn bounded-degree predictive feature interactions. Unfortunately, in models that serve web-scale traffic with billions of training examples, DCN showed limited expressiveness in its cross network at learning more predictive feature interactions. Despite significant research progress made, many deep learning models in production still rely on traditional feed-forward neural networks to learn feature crosses inefficiently. In light of the pros/cons of DCN and existing feature interaction learning approaches, we propose an improved framework DCN-V2 to make DCN more practical in large-scale industrial settings. In a comprehensive experimental study with extensive hyper-parameter search and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building blocks, and has delivered significant offline accuracy and online business metrics gains across many web-scale learning to rank systems at Google.\n\n**Published:** 2020-08-19\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on adaptive connectivity patterns, event-driven memory architectures, sparse state space models, and memory-efficient graph structures, here are some key points and references that can be useful:\n\n## Adaptive Connectivity Patterns in Neural Networks\n\n- The concept of adaptive connectivity is inspired by the plasticity of neural connections in the brain. Studies on temporal and adaptive neural networks show that adaptivity in the coupling weights can lead to complex synchronization patterns such as chimera and bump states. This adaptivity can be crucial for dynamic information processing and learning.\n- In the context of neural networks, adaptive control using conceptors can enhance the functionality and robustness of recurrent neural networks (RNNs) by continuously adjusting their internal representations to adapt to changing conditions. This adaptivity can be applied to improve the connectivity patterns within state space models (SSMs).\n\n## Event-Driven Memory Architectures\n\n- Event-driven memory updates can be achieved through mechanisms that organize sequences into coherent episodic events. For example, the EM-LLM model uses a combination of Bayesian surprise and graph-theoretic boundary refinement to segment sequences into episodic events, which can be applied to event-driven memory architectures in SSMs - Analysis Note.\n- Dynamic patterns of functional connectivity in the brain, such as those observed during memory encoding and retrieval, can provide insights into how event-driven memory can be implemented. The reinstatement of dynamic coupling patterns during successful memory retrieval suggests a mechanism for updating memory based on significant events.\n\n## Sparse State Space Model Implementations\n\n- Sparse operations optimization is crucial for efficient SSMs. Techniques such as selective integration between layers, as seen in DenseSSM, can retain fine-grained information while maintaining computational efficiency. This approach can be combined with sparse operations to optimize the state space models - Analysis Note.\n- The use of graph-based memory structures can also facilitate sparse operations. For instance, organizing sequences into coherent events using graph-theoretic methods can help in sparse state space model implementations - Analysis Note.\n\n## Memory-Efficient Graph Structures\n\n- Graph-based memory structures are highlighted as a key component for efficient memory management in SSMs. These structures can help in organizing and updating memory in an event-driven manner. For example, the EM-LLM model's use of graph-theoretic boundary refinement for segmenting sequences into episodic events can be adapted for memory-efficient graph structures - Analysis Note.\n- Efficient parallel scan algorithms and optimized memory access patterns are also essential for memory-efficient implementations. These can be achieved through modular neural network designs and optimized state transition modules, such as those in Liquid Structural State-Space Models - Analysis Note.\n\n## Additional Considerations\n\n- **Hierarchical Organization and Multi-scale Processing**: Implementing hierarchical information flow and multi-scale processing can enhance the adaptivity and efficiency of SSMs. This can be achieved through stacked SSMs and adaptive temporal processing, as seen in Multi Time Scale World Models - Analysis Note.\n- **Parallel Processing and Optimization**: Efficient parallel scan implementations and optimized memory access patterns are critical for scaling the model. Techniques such as those used in Liquid Structural State-Space Models can help in achieving this - Analysis Note.\n\nBy integrating these concepts, the researchers can develop an autoregressive language model that leverages adaptive connectivity patterns, event-driven memory architectures, sparse state space models, and memory-efficient graph structures to achieve better performance, efficiency, and scalability.\n",
            "analysis": "# Analysis Note - Round 3\n\n## Summary of Analysis\n\nAfter three rounds of searches, several promising directions have emerged for improving the Mamba2 architecture:\n\n1. Dense Connectivity Integration:\n- DenseSSM's approach of selective integration between layers\n- Hierarchical information flow optimization\n- Fine-grained feature retention\n\n2. Multi-scale Processing:\n- Temporal hierarchy through stacked SSMs\n- Event-based sequence organization\n- Adaptive temporal processing\n\n3. Memory Efficiency:\n- Graph-based memory structures\n- Event segmentation techniques\n- Sparse operations optimization\n\n4. Parallel Processing:\n- Efficient parallel scan implementations\n- Multi-input, multi-output SSM designs\n- Optimized memory access patterns\n\n## Useful References with Excerpts\n\n1. From \"DenseMamba\":\n```\nBy selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\n```\n\n2. From \"Multi Time Scale World Models\":\n```\nOur model uses a computationally efficient inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over several seconds into the future.\n```\n\n3. From \"Human-like Episodic Memory\":\n```\nEM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion.\n```\n\n4. From \"Liquid Structural State-Space Models\":\n```\nLTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference.\n```\n\n## Key Insights for Design\n\n1. Dense State Integration:\n- Selective connectivity between layers can improve information flow\n- Need to balance connectivity density with computational efficiency\n- Potential for adaptive connectivity patterns\n\n2. Multi-scale Processing:\n- Hierarchical organization of temporal information\n- Event-based sequence segmentation\n- Adaptive temporal resolution\n\n3. Memory Management:\n- Graph-based memory structures for efficient organization\n- Event-driven memory updates\n- Sparse operation optimization\n\n4. Implementation Considerations:\n- Parallel scan algorithms for efficient processing\n- Memory access pattern optimization\n- Balance between connectivity and computational cost\n\n## Future Search Plan\n\nNeed to investigate:\n\n1. Specific implementations of:\n- Adaptive connectivity patterns in SSMs\n- Event-driven memory update mechanisms\n- Efficient parallel scan algorithms\n\n2. Technical details about:\n- Memory-efficient graph structures\n- Adaptive temporal processing\n- Sparse operation optimization\n\n## References\n\n1. Wang, Y., Guo, T., Yang, Y., Wang, C., Tang, Y., Han, K., & He, W. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv.org.\n\n2. Shaj, V., Zadeh, S. G., Demir, O., Douat, L. R., & Neumann, G. (2023). Multi Time Scale World Models. Neural Information Processing Systems.\n\n3. Fountas, Z., et al. (2024). Human-like Episodic Memory for Infinite Context LLMs. arXiv.org.\n\n4. Hasani, R. M., Lechner, M., Wang, T. H., Chahine, M., Amini, A., & Rus, D. (2022). Liquid Structural State-Space Models. International Conference on Learning Representations.\n\n5. Mirchev, A., Kayalibay, B., van der Smagt, P., & Bayer, J. (2020). Variational State-Space Models for Localisation and Dense 3D Mapping in 6 DoF. International Conference on Learning Representations.\n\n6. Hou, L., et al. (2019). High Resolution Medical Image Analysis with Spatial Partitioning. arXiv.org."
        },
        {
            "ready": false,
            "query": "parallel event detection neural networks, efficient graph memory operations, hierarchical state transitions",
            "detail": "Find papers discussing:\n1. Parallel algorithms for event detection and boundary identification in neural networks\n2. Memory-efficient implementations of graph operations in deep learning\n3. Hierarchical state transition mechanisms in sequence models\n4. Optimization techniques for state space models\n5. Integration of parallel processing with state space architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Parallel algorithms for event detection and boundary identification in neural networks\n2. Memory-efficient implementations of graph operations in deep learning\n3. Hierarchical state transition mechanisms in sequence models\n4. Optimization techniques for state space models\n5. Integration of parallel processing with state space architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.96)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 14/53 (Score: 0.96)*\n\n```\nAdvances in Neural Information Processing Systems, 33: $1474-1487,2020 \\mathrm{a}$. Albert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800-3809. PMLR, 2020b. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021b. Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers, 2020. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): $1735-1780,1997$. Arieh Iserles. A first course in the numerical analysis of differential equations. 44. Cambridge university press, 2009. Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pp. 87-104, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Shiva Kaul. Linear dynamical systems as a core computational primitive. Advances in Neural Information Processing Systems, 33:16808-16820, 2020. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Alex Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009. Richard Ladner and Michael Fischer. Parallel prefix computation. Journal of the ACM (JACM), 27 (4):831-838, 1980. Sivaramakrishnan Lakshmivarahan and Sudarshan Dhall. Parallel computing using the prefix problem.\n```\n\n#### 2. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.88)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 9/20 (Score: 0.88)*\n\n```\nIn International Conference on Learning Representations, January 2021. [7] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models. Advances in Neural Information Processing Systems, 35:35971-35983, December 2022. [8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [9] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections. In International Conference on Learning Representations, February 2023. [10] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [11] Eric Martin and Chris Cundy. Parallelizing Linear Recurrent Neural Nets Over Sequence Length. In International Conference on Learning Representations, February 2018. [12] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards Larger Convolutional Language Models. In International Conference on Machine Learning, June 2023. [13] Joshua Hanson, Maxim Raginsky, and Eduardo Sontag. Learning Recurrent Neural Net Models of Nonlinear Systems. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, pages 425-435. PMLR, May 2021. [14] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks. Journal of Machine Learning Research, 23(42):1-85, 2022. ISSN 1533-7928. [15] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, March 1994.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.64)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.64)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.60)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 53/86 (Score: 0.60)*\n\n```\nIn: arXiv preprint arXiv:1606.08415 (2016). [48] W Daniel Hillis and Guy L Steele Jr. \"Data Parallel Algorithms\". In: Communications of the ACM 29.12 (1986), pp. $1170-1183$. [49] Sepp Hochreiter and J\u00fcrgen Schmidhuber. \"Long Short-Term Memory\". In: Neural Computation 9.8 (1997), pp. 17351780 . [50] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. \"An Empirical Analysis of Compute-\n\nOptimal Large Language Model Training\". In: Advances in Neural Information Processing Systems (NeurIPS) 35 (2022), pp. 30016-30030. [51] Samy Jelassi, David Brandfonbrener, Sham M Kakade, and Eran Malach. \"Repeat After Me: Transformers Are Better Than State Space Models at Copying\".\n```\n\n#### 5. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.49)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 17/24 (Score: 0.49)*\n\n```\narXiv preprint arXiv:2209.10655, 2022. [36] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. arXiv preprint arXiv:1709.04057, 2017. [37] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. [38] Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf, and Seong-Whan Lee. Relative attributing propagation: Interpreting the comparative contributions of individual units in deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $2501-2508,2020$. [39] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [40] Badri Narayana Patro and Vijay Srinivas Agneeswaran. Mamba-360: Survey of state space models as transformer alternative for long sequence modelling: Methods, applications, and challenges. arXiv preprint arXiv:2404.16112, 2024. [41] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [42] Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys\u0142aw Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: parallel event detection neural networks, efficient graph memory operations, hierarchical state transitions\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks\n\n*From Search Query: parallel event detection neural networks*\n\n*Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen*\n\n**TL;DR:** This study proposes a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate and introduces novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.\n\n**Abstract:** Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 89  (*Influential: 9*)\n\n#### 2. Event Detection and Domain Adaptation with Convolutional Neural Networks\n\n*From Search Query: parallel event detection neural networks*\n\n*Thien Huu Nguyen, R. Grishman*\n\n**TL;DR:** Convolutional neural networks that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features are studied.\n\n**Abstract:** We study the event detection problem using convolutional neural networks (CNNs) that overcome the two fundamental limitations of the traditional feature-based approaches to this task: complicated feature engineering for rich feature sets and error propagation from the preceding stages which generate these features. The experimental results show that the CNNs outperform the best reported feature-based systems in the general setting as well as the domain adaptation setting without resorting to extensive external resources.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2015\n\n**Citations:** 392  (*Influential: 26*)\n\n#### 3. Neural Cross-Lingual Event Detection with Minimal Parallel Resources\n\n*From Search Query: parallel event detection neural networks*\n\n*Jian Liu, Yubo Chen, Kang Liu, Jun Zhao*\n\n**TL;DR:** To construct a lexical mapping between different languages, a context-dependent translation method is devised; to treat the word order difference problem, a shared syntactic order event detector for multilingual co-training is proposed.\n\n**Abstract:** The scarcity in annotated data poses a great challenge for event detection (ED). Cross-lingual ED aims to tackle this challenge by transferring knowledge between different languages to boost performance. However, previous cross-lingual methods for ED demonstrated a heavy dependency on parallel resources, which might limit their applicability. In this paper, we propose a new method for cross-lingual ED, demonstrating a minimal dependency on parallel resources. Specifically, to construct a lexical mapping between different languages, we devise a context-dependent translation method; to treat the word order difference problem, we propose a shared syntactic order event detector for multilingual co-training. The efficiency of our method is studied through extensive experiments on two standard datasets. Empirical results indicate that our method is effective in 1) performing cross-lingual transfer concerning different directions and 2) tackling the extremely annotation-poor scenario.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 47  (*Influential: 6*)\n\n#### 4. Penguin: Parallel-Packed Homomorphic Encryption for Fast Graph Convolutional Network Inference\n\n*From Search Query: efficient graph memory operations*\n\n*Ran Ran, Nuo Xu, Tao Liu, Wei Wang, Gang Quan, Wujie Wen*\n\n**TL;DR:** Penguin is the first work that can ensure the protection of both graph structure and features when accelerating HE-GCN inference on encrypted data, and is the first work that can ensure the protection of both graph structure and features when accelerating HE-GCN inference on encrypted data.\n\n**Abstract:** The marriage of Graph Convolutional Network (GCN) and Homomorphic Encryption (HE) enables the inference of graph data on the cloud with significantly enhanced client data privacy. However, the tremendous computation and memory overhead associated with HE operations challenges the practicality of HE-based GCN inference. GCN inference involves a sequence of expensive matrix-matrix multiplications, and we observe that directly applying the state-of-the-art HE-based secure matrix-matrix multiplication solutions to accelerate HE-GCN inference is far less efficient as it does not exploit the unique aggregation mechanism of two-dimension graph node-features in GCN layer computation. As a result, in this paper, we propose a novel HE-based ciphertext packing technique, i.e., Penguin , that can take advantage of the unique computation pattern during the HE-GCN inference to significantly reduce the computation and memory overhead associated with HE operations. Specifically, Penguin employs ( i ) an effective two-dimension parallel packing technique for feature ciphertext with optimal graph node partitioning and graph feature interleaving, and ( ii ) an interleaved assembly technique that can effectively make use of blank slots to merge ciphertexts after feature reduction and thus significantly reduce costly rotation operations. We perform detailed theoretical analysis to support our arguments. In the meantime, our experimental results also show that Penguin can achieve up to \u223c 10 \u00d7 speedup and around \u223c 79% reduction in computational memory overhead, significantly out-performing state-of-the-art solutions. To the best of our knowledge, this is the first work that can ensure the protection of both graph structure and features when accelerating HE-GCN inference on encrypted data. Our code is publicly available at https://github.com/ranran0523/Penguin .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. Mesh-TensorFlow: Deep Learning for Supercomputers\n\n*From Search Query: efficient graph memory operations*\n\n*Noam M. Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, C. Young, Ryan Sepassi, Blake A. Hechtman*\n\n**TL;DR:** Mesh-TensorFlow is introduced, a language for specifying a general class of distributed tensor computations and used to implement an efficient data-parallel, model-Parallel version of the Transformer sequence-to-sequence model, surpassing state of the art results on WMT'14 English- to-French translation task and the one-billion-word language modeling benchmark.\n\n**Abstract:** Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 354  (*Influential: 30*)\n\n#### 6. Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks\n\n*From Search Query: efficient graph memory operations*\n\n*Shuai Zhang, M. Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miaoyuan Liu*\n\n**TL;DR:** This paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error and proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexityand improve convergence without compromising the test accuracy.\n\n**Abstract:** Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include \\textit{graph sparsification} that samples a subgraph to reduce the amount of data aggregation and \\textit{model sparsification} that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 4*)\n\n#### 7. Neural Hybrid Automata: Learning Dynamics with Multiple Modes and Stochastic Transitions\n\n*From Search Query: hierarchical state transitions*\n\n*Michael Poli, Stefano Massaroli, Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, A. Yamashita, H. Asama, Jinkyoo Park, Animesh Garg*\n\n**TL;DR:** This work introduces Neural Hybrid Automata (NHAs), a recipe for learning SHS dynamics without a priori knowledge on the number of modes and inter-modal transition dynamics, and provides a systematic inference method based on normalizing flows, neural differential equations and self-supervision.\n\n**Abstract:** Effective control and prediction of dynamical systems often require appropriate handling of continuous-time and discrete, event-triggered processes. Stochastic hybrid systems (SHSs), common across engineering domains, provide a formalism for dynamical systems subject to discrete, possibly stochastic, state jumps and multi-modal continuous-time flows. Despite the versatility and importance of SHSs across applications, a general procedure for the explicit learning of both discrete events and multi-mode continuous dynamics remains an open problem. This work introduces Neural Hybrid Automata (NHAs), a recipe for learning SHS dynamics without a priori knowledge on the number of modes and inter-modal transition dynamics. NHAs provide a systematic inference method based on normalizing flows, neural differential equations and self-supervision. We showcase NHAs on several tasks, including mode recovery and flow learning in systems with stochastic transitions, and end-to-end learning of hierarchical robot controllers.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 8. Relational State-Space Model for Stochastic Multi-Object Systems\n\n*From Search Query: hierarchical state transitions*\n\n*Fan Yang, Ling Chen, Fan Zhou, Yusong Gao, Wei Cao*\n\n**TL;DR:** The relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects, is introduced.\n\n**Abstract:** Real-world dynamical systems often consist of multiple stochastic subsystems that interact with each other. Modeling and forecasting the behavior of such dynamics are generally not easy, due to the inherent hardness in understanding the complicated interactions and evolutions of their constituents. This paper introduces the relational state-space model (R-SSM), a sequential hierarchical latent variable model that makes use of graph neural networks (GNNs) to simulate the joint state transitions of multiple correlated objects. By letting GNNs cooperate with SSM, R-SSM provides a flexible way to incorporate relational information into the modeling of multi-object dynamics. We further suggest augmenting the model with normalizing flows instantiated for vertex-indexed random variables and propose two auxiliary contrastive objectives to facilitate the learning. The utility of R-SSM is empirically evaluated on synthetic and real time series datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 9. An Infinite Hidden Markov Model With Similarity-Biased Transitions\n\n*From Search Query: hierarchical state transitions*\n\n*C. Dawson, Chaofan Huang, Clayton T. Morrison*\n\n**TL;DR:** A generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between \"nearby\" states is described.\n\n**Abstract:** We describe a generalization of the Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM) which is able to encode prior information that state transitions are more likely between \"nearby\" states. This is accomplished by defining a similarity function on the state space and scaling transition probabilities by pair-wise similarities, thereby inducing correlations among the transition distributions. We present an augmented data representation of the model as a Markov Jump Process in which: (1) some jump attempts fail, and (2) the probability of success is proportional to the similarity between the source and destination states. This augmentation restores conditional conjugacy and admits a simple Gibbs sampler. We evaluate the model and inference method on a speaker diarization task and a \"harmonic parsing\" task using four-part chorale data, as well as on several synthetic datasets, achieving favorable comparisons to existing models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 0  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Sound Event Localization and Detection of Overlapping Sources Using Convolutional Recurrent Neural Networks\n\n*From Search Query: parallel event detection neural networks*\n\n*Archontis Politis, Sharath Adavanne, Tuomas Virtanen, Joonas Nikunen*\n\n**Abstract:** In this paper, we propose a convolutional recurrent neural network for joint\nsound event localization and detection (SELD) of multiple overlapping sound\nevents in three-dimensional (3D) space. The proposed network takes a sequence\nof consecutive spectrogram time-frames as input and maps it to two outputs in\nparallel. As the first output, the sound event detection (SED) is performed as\na multi-label classification task on each time-frame producing temporal\nactivity for all the sound event classes. As the second output, localization is\nperformed by estimating the 3D Cartesian coordinates of the\ndirection-of-arrival (DOA) for each sound event class using multi-output\nregression. The proposed method is able to associate multiple DOAs with\nrespective sound event labels and further track this association with respect\nto time. The proposed method uses separately the phase and magnitude component\nof the spectrogram calculated on each audio channel as the feature, thereby\navoiding any method- and array-specific feature extraction. The method is\nevaluated on five Ambisonic and two circular array format datasets with\ndifferent overlapping sound events in anechoic, reverberant and real-life\nscenarios. The proposed method is compared with two SED, three DOA estimation,\nand one SELD baselines. The results show that the proposed method is generic\nand applicable to any array structures, robust to unseen DOA values,\nreverberation, and low SNR scenarios. The proposed method achieved a\nconsistently higher recall of the estimated number of DOAs across datasets in\ncomparison to the best baseline. Additionally, this recall was observed to be\nsignificantly better than the best baseline method for a higher number of\noverlapping sound events.\n\n**Published:** 2018-06-30\n\n\n\n#### 2. Sound Event Detection with Depthwise Separable and Dilated Convolutions\n\n*From Search Query: parallel event detection neural networks*\n\n*Shayan Gharib, Yanxiong Li, Tuomas Virtanen, Konstantinos Drossos, Stylianos I. Mimilakis*\n\n**Abstract:** State-of-the-art sound event detection (SED) methods usually employ a series of convolutional neural networks (CNNs) to extract useful features from the input audio signal, and then recurrent neural networks (RNNs) to model longer temporal context in the extracted features. The number of the channels of the CNNs and size of the weight matrices of the RNNs have a direct effect on the total amount of parameters of the SED method, which is to a couple of millions. Additionally, the usually long sequences that are used as an input to an SED method along with the employment of an RNN, introduce implications like increased training time, difficulty at gradient flow, and impeding the parallelization of the SED method. To tackle all these problems, we propose the replacement of the CNNs with depthwise separable convolutions and the replacement of the RNNs with dilated convolutions. We compare the proposed method to a baseline convolutional neural network on a SED task, and achieve a reduction of the amount of parameters by 85% and average training time per epoch by 78%, and an increase the average frame-wise F1 score and reduction of the average error rate by 4.6% and 3.8%, respectively.\n\n**Published:** 2020-02-02\n\n\n\n#### 3. Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels\n\n*From Search Query: efficient graph memory operations*\n\n*Yuting Ye, Zimo Li, Jason Saragih, Chen Cao, Chenglei Wu, Yi Zhou, Yaser Sheikh, Hao Li*\n\n**Abstract:** Learning latent representations of registered meshes is useful for many 3D tasks. Techniques have recently shifted to neural mesh autoencoders. Although they demonstrate higher precision than traditional methods, they remain unable to capture fine-grained deformations. Furthermore, these methods can only be applied to a template-specific surface mesh, and is not applicable to more general meshes, like tetrahedrons and non-manifold meshes. While more general graph convolution methods can be employed, they lack performance in reconstruction precision and require higher memory usage. In this paper, we propose a non-template-specific fully convolutional mesh autoencoder for arbitrary registered mesh data. It is enabled by our novel convolution and (un)pooling operators learned with globally shared weights and locally varying coefficients which can efficiently capture the spatially varying contents presented by irregular mesh connections. Our model outperforms state-of-the-art methods on reconstruction accuracy. In addition, the latent codes of our network are fully localized thanks to the fully convolutional structure, and thus have much higher interpolation capability than many traditional 3D mesh generation models.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-06-08\n\n\n\n#### 4. Temporal Graph Networks for Deep Learning on Dynamic Graphs\n\n*From Search Query: efficient graph memory operations*\n\n*Emanuele Rossi, Federico Monti, Fabrizio Frasca, Ben Chamberlain, Michael Bronstein, Davide Eynard*\n\n**Abstract:** Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.\n\n**Published:** 2020-06-18\n\n\n\n#### 5. Koopa: Learning Non-stationary Time Series Dynamics with Koopman Predictors\n\n*From Search Query: hierarchical state transitions*\n\n*Mingsheng Long, Jianmin Wang, Chenyu Li, Yong liu*\n\n**Abstract:** Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory of portraying complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by Fourier Filter and design Koopman Predictor to advance respective dynamics forward. Technically, we propose Koopa as a novel Koopman forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal neighborhood and is able to utilize incoming ground truth to scale up forecast horizon. Besides, by integrating Koopman Predictors into deep residual structure, we ravel out the binding reconstruction loss in previous Koopman forecasters and achieve end-to-end forecasting objective optimization. Compared with the state-of-the-art model, Koopa achieves competitive performance while saving 77.3% training time and 76.0% memory.\n\n**Conference:** koopa-learning-non-stationary-time-series\n\n**Published:** 2023-05-30\n\n\n\n#### 6. Learning Multi-Level Hierarchies with Hindsight\n\n*From Search Query: hierarchical state transitions*\n\n*Kate Saenko, Robert Platt, George Konidaris, Andrew Levy*\n\n**Abstract:** Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it is inherently unstable: changes in a policy at one level of the hierarchy may cause changes in the transition and reward functions at higher levels in the hierarchy, making it difficult to jointly learn multiple levels of policies. In this paper, we introduce a new Hierarchical Reinforcement Learning (HRL) framework, Hierarchical Actor-Critic (HAC), that can overcome the instability issues that arise when agents try to jointly learn multiple levels of policies. The main idea behind HAC is to train each level of the hierarchy independently of the lower levels by training each level as if the lower level policies are already optimal. We demonstrate experimentally in both grid world and simulated robotics domains that our approach can significantly accelerate learning relative to other non-hierarchical and hierarchical methods. Indeed, our framework is the first to successfully learn 3-level hierarchies in parallel in tasks with continuous state and action spaces.\n\n**Published:** 2017-12-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on parallel event detection, efficient graph memory operations, hierarchical state transitions, and optimization techniques, here are some key insights and relevant information from the provided sources and additional context:\n\n## Parallel Event Detection Algorithms\n- For parallel event detection, the concept of **pipeline parallelism** and **tensor parallelism** from the Amazon SageMaker documentation is relevant. These techniques allow for the distribution of model components across multiple devices, enabling parallel processing of different parts of the model. This can be applied to event detection by parallelizing the computation of different event detection modules or layers within the model.\n\n## Memory-Efficient Implementations of Graph Operations\n- The Amazon SageMaker documentation discusses several memory-saving techniques such as **sharded data parallelism**, **optimizer state sharding**, **activation checkpointing**, and **activation offloading**. These techniques can be crucial for memory-efficient implementations of graph operations in deep learning models. For example, sharded data parallelism splits the model state across GPUs, reducing memory usage and improving training efficiency.\n\n## Hierarchical State Transition Mechanisms\n- **Hierarchical State Space Models** as described in the analysis note, involve stacking structured state-space models to create a temporal hierarchy. This approach can be adapted to autoregressive language models by implementing hierarchical state transitions, allowing the model to process information at multiple scales. This can enhance the model's ability to capture long-range dependencies and improve overall performance.\n\n## Optimization Techniques for State Space Models\n- The **Spectral State Space Models** and **DenseMamba** architectures highlight the importance of efficient parallel processing and selective state mechanisms. These models demonstrate how to balance efficiency and effectiveness by selectively integrating shallow-layer hidden states into deeper layers and maintaining training parallelizability. Such techniques can be optimized for state transitions in autoregressive models to improve both efficiency and accuracy.\n\n## Integration of Parallel Processing with State Space Architectures\n- The integration of parallel processing with state space architectures is crucial for scalability. The **pipeline parallelism** and **tensor parallelism** techniques mentioned earlier can be combined with hierarchical state space models to ensure that the model can be trained efficiently on large-scale datasets. This integration ensures that the model can leverage the benefits of parallel processing while maintaining the hierarchical structure necessary for complex sequence modeling.\n\nIn summary, to improve autoregressive language model design:\n\n- **Parallel Event Detection**: Utilize pipeline and tensor parallelism to parallelize event detection modules.\n- **Memory Efficiency**: Implement sharded data parallelism, optimizer state sharding, activation checkpointing, and activation offloading to optimize memory usage.\n- **Hierarchical State Transitions**: Adopt hierarchical state space models to process information at multiple scales.\n- **Optimization Techniques**: Use selective state mechanisms and dense connections to enhance information flow while maintaining parallelizability.\n- **Integration**: Combine parallel processing techniques with hierarchical state space architectures to ensure scalability and efficiency.\n\nThese strategies can help in designing more efficient, scalable, and accurate autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about state space models, memory efficiency, and event-driven architectures. The findings suggest both opportunities and challenges for the EventDrivenSSD proposal.\n\n2. Key References and Excerpts:\n\na) Mamba Architecture (Gu & Dao, 2023):\n- \"SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures\"\n- \"A fundamental problem of sequence modeling is compressing context into a smaller state\"\n- Key insight: Selective state mechanisms can balance efficiency and effectiveness\n\nb) DenseMamba (He et al., 2024):\n- \"Dense connections enhanced DenseSSM still maintains the training parallelizability\"\n- \"Selectively integrating shallow-layer hidden states into deeper layers\"\n- Shows how to enhance information flow while maintaining efficiency\n\nc) Spectral State Space Models (Agarwal et al., 2023):\n- \"Provable robustness properties as their performance depends on neither the spectrum nor dimensionality\"\n- Demonstrates efficient parallel processing techniques\n\nd) Hierarchical State Space Models (Pinto et al., 2024):\n- \"Stacks structured state-space models to create a temporal hierarchy\"\n- \"Outperforms state-of-the-art sequence models by at least 23% on MSE\"\n- Provides framework for multi-scale temporal processing\n\n3. Key Insights:\n\na) Event Detection:\n- Need to carefully balance computational overhead of event detection\n- Graph-theoretic boundary refinement should be optimized for efficiency\n- Consider using hierarchical event detection at multiple scales\n\nb) Memory Management:\n- Dense connections can improve information flow\n- Hierarchical state organization shows promise for efficiency\n- Need to optimize graph-based memory structures\n\nc) Integration Considerations:\n- SSM architectures are flexible for modifications\n- Parallel processing capabilities must be preserved\n- Memory efficiency is crucial for scalability\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of parallel event detection\n2. Memory-efficient graph operations\n3. Integration of hierarchical processing with SSMs\n4. Optimization techniques for state transitions\n\nNext search queries should focus on:\n- Parallel event detection algorithms\n- Efficient graph memory implementations\n- Hierarchical state transition mechanisms\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv preprint.\n\n[2] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\" arXiv preprint.\n\n[3] Agarwal, N., et al. (2023). \"Spectral State Space Models.\" arXiv preprint.\n\n[4] Pinto, L., et al. (2024). \"Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling.\" arXiv preprint.\n\n[5] Smith, J., et al. (2022). \"Simplified State Space Layers for Sequence Modeling.\" ICLR.\n\nFurther investigation is needed to fully understand implementation details and optimization strategies."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "episodic memory language models, event segmentation neural networks, graph memory networks",
            "detail": "Find papers discussing:\n1. Integration of episodic memory or event segmentation in neural networks\n2. Hierarchical state space models with graph-based memory\n3. Efficient implementations of memory management in language models\n4. Online event detection and boundary refinement techniques\n5. Applications of Bayesian surprise in sequence modeling",
            "search_ret": "\n---\n## Found 1 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of episodic memory or event segmentation in neural networks\n2. Hierarchical state space models with graph-based memory\n3. Efficient implementations of memory management in language models\n4. Online event detection and boundary refinement techniques\n5. Applications of Bayesian surprise in sequence modeling\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 1 papers in Internal Library\n\n#### 1. Human-like Episodic Memory for Infinite Context LLMs (Avg. Score: 0.99)\n\n*Z. Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** EM-LLM is introduced, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency and providing a computational framework for exploring human memory mechanisms.\n\n**Abstract:** Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information. Experiments on the LongBench dataset demonstrate EM-LLM's superior performance, outperforming the state-of-the-art InfLLM model with an overall relative improvement of 4.3% across various tasks, including a 33% improvement on the PassageRetrieval task. Furthermore, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart. This work not only advances LLM capabilities in processing extended contexts but also provides a computational framework for exploring human memory mechanisms, opening new avenues for interdisciplinary research in AI and cognitive science.\n\n##### *Relevant Chunk: No. 12/36 (Score: 1.00)*\n\n```\n2 A and the analysis in Ji-An et al. (2024) to be more easily reproduced. Finally, we could investigate how skewing event recall based on recency and originally-recorded surprise affects model performance and to what extent it produces biased behaviour found in studies of free recall. In addition, the architecture of EM-LLM, with its differentiated context handling described in Section 3.1 invites further interesting comparisons to cognitive models of human memory beyond episodic. The group of tokens forming the local context, which hold the most recent and task-relevant information, share characteristics with the concept of working memory. For instance, Baddeley (2003)'s influential model of working memory, which posits a limited-capacity system for temporary information storage and manipulation, bears similarities to our local context functionality. Yet, the analogy is not perfect. Our broader context window, including both local context and retrieved memories, might be more accurately compared to Ericsson and Kintsch (1995)'s concept of long-term working memory, which proposes a mechanism for rapid access to relevant information in long-term memory, extending beyond the traditional capacity limits of working memory. Alternatively, our architecture aligns well with Cowan (2001)'s embedded-processes model, where our local context could be likened to the limited-capacity \"focus of attention\" within working memory, while the full context window parallels the activated portion of long-term memory. Future work could explore these analogies more deeply, providing a flexible test-bed for rapidly exploring hypotheses about human memory, and potentially informing debates about capacity limits in working memory. Additionally, inspired by the multi-component nature of Baddeley's model, one might explore the integration of modality-specific buffers within EM-LLM to enhance its performance on multi-modal tasks. Machine learning In refining event boundaries, we utilized modularity and conductance as metrics for evaluating community structure in the similarity graph of attention keys. While effective in our experiments, we acknowledge that numerous other methods for graph clustering and sequence segmentation could potentially be applied (Fortunato, 2010; Yang et al., 2016). Our choice was motivated by their established theoretical foundations and computational efficiency, though comparative studies suggest performance can vary based on network characteristics (Yang et al. 2016). Interestingly, our surprise-based initial boundary detection shares similarities with Bayesian online change-point detection (Adams and MacKay, 2007), suggesting potential avenues for integrating time series analysis techniques into LLM context processing. Future work could explore whether more sophisticated segmentation or clustering algorithms could improve EM-LLM's performance, particularly for extremely long contexts or streaming data scenarios. Such investigations could enhance our model and contribute to understanding how information is structured and processed in LLMs, bridging the gap between traditional sequence analysis and LLM context processing. Looking ahead, several more avenues for future research emerge from this work. One promising direction is to extend our surprise-based segmentation and boundary refinement processes to operate at each layer of the Transformer independently. This could lead to more nuanced and hierarchical representations of episodic memories, following\nthe underlying semantic structure of the input more closely. Additionally, exploring how EM-LLM could be utilised to enable imagination and future thinking has great potential for advancing model-based reinforcement learning and continual learning techniques in LLMs. By leveraging its event-based structure to simulate potential future scenarios or recall past experiences in novel contexts, EM-LLM could enhance an LLM's ability to plan, adapt, and learn continuously from new information. ## 6 CONCLUSION\n\nIn this work, we introduced EM-LLM, a novel and flexible architecture that integrates key aspects of human episodic memory and event cognition into transformer-based language models. Our approach enables LLMs to effectively process and utilise information from vastly extended contexts, far beyond their original training lengths. By combining surprise-based event segmentation with graph-theoretic boundary refinement, and a two-stage memory retrieval process, EM-LLM demonstrates superior performance on long-context tasks compared to state-of-the-art models. Crucially, our method requires no pre-training and can be readily applied to existing LLMs, offering a promising path towards virtually infinite context windows. This capability has the potential to revolutionise how we interact with LLMs, enabling continuous, personalized interactions over extended periods. Furthermore, the flexibility of our framework suggests it could serve as a viable alternative to traditional retrieval-augmented generation (RAG) techniques, especially when combined with efficient compression methods to reduce the memory requirements for the model's KV cache. In conclusion, EM-LLM represents a significant step forward in the development of language models with extended context-processing capabilities. By bridging insights from cognitive science with machine learning, our approach not only enhances the performance of LLMs on long-context tasks but also provides a scalable computational framework for testing hypotheses about human memory. We hope this study will inspire the community to expand research on the intersection between LLMs and human memory mechanisms. ## REFERENCES\n\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, $12: 157-173,2024 a$. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36, 2024 . Szymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b. Focused transformer: Contrastive training for context scaling. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URLhttps://openreview.net/forum?id=s1FjXzJ0jy\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474, 2020. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URLhttps://openreview.net/forum?id= TrjbxzRcnf-. Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range transformers with unlimited length input. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=lJWUJWLCJo. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory, 2024a. David Clewett, Sarah DuBrow, and Lila Davachi. Transcending time in the brain: How event memories are constructed from experience. Hippocampus, 29(3):162-183, 2019. Jeffrey M Zacks. Event perception and memory. Annual review of psychology, 71:165-191, 2020. Christopher Baldassano, Janice Chen, Asieh Zadbood, Jonathan W Pillow, Uri Hasson, and Kenneth A Norman. Discovering event structure in continuous narrative perception and memory. Neuron, 95(3):709-721, 2017. Sebastian Michelmann, Uri Hasson, and Kenneth A. Norman. Evidence that event boundaries are access points for memory retrieval. Psychological Science, 34(3):326-344, 2023a. doi 10.1177/09567976221128206. URL https://doi.org/10.1177/09567976221128206. PMID: 36595492. Jeffrey M Zacks, Nicole K Speer, Khena M Swallow, Todd S Braver, and Jeremy R Reynolds. Event perception: a mind-brain perspective. Psychological bulletin, 133(2):273, 2007. Jeffrey M Zacks, Christopher A Kurby, Michelle L Eisenberg, and Nayiri Haroutunian. Prediction error associated with the perceptual segmentation of naturalistic events. Journal of cognitive neuroscience, 23(12):4057-4066, 2011. Warrick Roseboom, Zafeirios Fountas, Kyriacos Nikiforou, David Bhowmik, Murray Shanahan, and Anil K Seth. Activity in perceptual classification networks as a basis for human subjective time perception.\n```\n\n##### *Relevant Chunk: No. 1/36 (Score: 1.00)*\n\n```\n# HUMAN-LIKE EPISODIC MEMORY FOR INFINITE CONTEXT LLMS \n\nZafeirios Fountas ${ }^{1}$, Martin A Benfeghoul ${ }^{1,{ }^{* *}}$, Adnan Oomerjee ${ }^{1, *}$, Fenia Christopoulou ${ }^{1}$,<br>Gerasimos Lampouras ${ }^{1}$, Haitham Bou-Ammar ${ }^{1,2}$ and Jun Wang ${ }^{2}$<br>${ }^{1}$ Huawei Noah's Ark Lab, London, UK<br>${ }^{2}$ University College London, UK<br>\\{zafeirios.fountas, adnan.ebrahim.oomerjee, efstathia.christopoulou\\}@huawei.com<br>\\{gerasimos.lampouras, haitham. ammar\\}@huawei.com<br>martin.antoine.benfeghoul@h-partners.com<br>jun.wang@ucl.ac.uk\n\n\n#### Abstract\n\nLarge language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information. Experiments on the LongBench dataset demonstrate EM-LLM's superior performance, outperforming the state-of-the-art InfLLM model with an overall relative improvement of $4.3 \\%$ across various tasks, including a $33 \\%$ improvement on the PassageRetrieval task. Furthermore, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart. This work not only advances LLM capabilities in processing extended contexts but also provides a computational framework for exploring human memory mechanisms, opening new avenues for interdisciplinary research in AI and cognitive science. ## 1 INTRODUCTION\n\nFor contemporary pre-trained large language models (LLMs), the context window serves as the primary mechanism to incorporate domain-specific, private, or common up-to-date information. However, despite their remarkable and ever-expanding capabilities, LLMs still exhibit significant limitations when tasked with processing extensive contexts (Liu et al., 2024a). These limitations stem from inherent challenges in Transformer-based architectures. Recent studies have shown that Transformers struggle with extrapolating to contexts longer than their training window size (Kazemnejad et al., 2024). On top of this, employing softmax attention over extended token sequences requires substantial computational resources for each token generation, and the resulting attention embeddings risk becoming excessively noisy and losing their distinctiveness (Tworkowski et al., 2023). To mitigate those challenges, recent works have focused on retrieval-based methods, either in the form of in-context augmentation (e.g., RAG-based techniques (Lewis et al., 2020, Gao et al, 2024)) or via retrieval of previously-inferred key-value pairs (KV) within individual attention heads (Wu et al., 2022; Tworkowski et al., 2023; Bertsch et al., 2023). Notably, state-of-the-art performance is achieved when KV pairs are initially organised into non-overlapping segments and then retrieved together as one block of sequential tokens (Xiao et al. 2024a). While such techniques present interesting avenues of research, results still indicate a significant gap between the performance of LLMs in short- vs long-context tasks, even when existing long-context architectures are employed (Liu et al., 2024a). This work tackles the above challenges and attempts to bridge this performance gap by taking inspiration from the algorithmic interpretation of episodic memory in the human brain - the memory system responsible for encoding, storing, and retrieving personal experiences and events. The brain makes sense of its continuous experience in the real world by segmenting it into discrete episodic events (Clewett et al., 2019, Zacks, 2020), which are organised in a\n\n[^0]hierarchical and nested-timescale structure (Baldassano et al. 2017) and stored in long-term memory. Notably, the boundaries between such events are the access points when it comes to memory retrieval (Michelmann et al., 2023a) and are widely believed to correspond to points in time with high prediction errors between the brain's generative model and its raw sensory input (a.k.a., surprise). In this context, surprise refers to moments when the brain's predictions about incoming sensory information are significantly violated, leading to a mismatch between what is expected and what is actually perceived. These instances of high surprise are thought to signal important changes in the environment or narrative, prompting the brain to segment the ongoing experience into distinct events (Zacks et al., 2007; 2011; Roseboom et al., 2019; Sinclair et al., 2021; Fountas et al., 2022). Once segmented and stored, the brain can recall episodic memories based on their similarity to its current experience, recency, original temporal order, and their proximity to other recalled memories (temporal asymmetry and contiguity, Howard and Kahana, 2002). Following these insights, we propose a novel architecture, $E M-L L M$, that integrates crucial aspects of event cognition and episodic memory into Transformer-based LLMs. For memory formation, we segment the sequence of the tokens presented to the underlying LLM into individual memory units representing episodic events. The boundaries, and thus the size of those events, are initially determined dynamically, based on the level of surprise of the model during inference, and then refined to maximise cohesion within memory units and separation of memory content across them (see Section 3.2. This refinement process leverages graph-theoretic metrics, treating the similarity between attention keys (the learned representations used in Transformer self-attention mechanisms) as a weighted adjacency matrix, and aims to enhance the model's ability to efficiently recall relevant information when addressing complex tasks with extended contexts. Importantly, this memory formation process incurs minimal additional computational cost, with the surprise-based segmentation requiring no extra computation and the refinement step having a complexity of $\\mathcal{O}(k n)$, where $k$ is typically very small compared to the number of tokens $n$. With this efficient memory formation process, by grouping similar information in single units, we minimise the number of units needed to recall details around specific events. For memory recall, our approach integrates similarity-based retrieval with mechanisms that facilitate temporal contiguity and asymmetry effects. By retrieving and buffering salient memory units, our model leverages and enhances the recently discovered propensity of LLMs to exhibit human-like patterns in sequential information retrieval (Ji-An et al., 2024). This method not only ensures efficient access to pertinent information but also mimics the temporal dynamics found in human free recall studies (such as Howard and Kahana, 2002), further enhancing the model's ability to handle complex tasks that require nuanced temporal reasoning. To prove our hypotheses, we first employ a series of human-annotated podcast scripts, where we show that information in LLM attention heads can be semantically grouped in a way that correlates with the event structure perceived by humans. Therefore, LLM-perceived surprise can indeed serve as a proxy for the cognitive signals that drive human event segmentation, as confirmed by previous works (Kumar et al.\n```\n\n##### *Relevant Chunk: No. 8/36 (Score: 0.99)*\n\n```\n(2020); Xiao et al. (2024a). This architecture enables EM-LLM to effectively process and utilise information from positions outside its pre-trained local context window, while maintaining the underlying LLM's performance characteristics. ### 3.2 MEMORY FORMATION VIA SURPRISE\n\nIn the context of LLMs, we define episodic memory as the organised, event-based collection of past key-value pairs, analogous to the latent representations of personal experiences in human memory. Just as unexpected or novel information plays a crucial role in human memory formation, we posit that analogous indicators of novelty in LLMs can serve as an effective proxy for identifying significant \"events\" within the model's experience. In Bayesian terms, surprise is quantified by the negative log-likelihood of observing the current, ground-truth token given the previous tokens in an auto-regressive model, with high values indicating the unpredictability or novelty of each new token within the context according to the model, i.e., it is \"surprised\" by the next token. Following work on cognitive\nmodelling (Roseboom et al., 2019, Fountas et al., 2022), we employ a thresholding mechanism to perform an initial identification of event boundaries (used for the first time in LLMs). Formally, a token $x_{t}$ is considered a potential boundary if its surprise value exceeds a threshold $T$ :\n\n$$\n-\\log P\\left(x_{t} \\mid x_{1}, \\ldots, x_{t-1} ; \\theta\\right)>T \\quad \\text { with } \\quad T=\\mu_{t-\\tau}+\\gamma \\sigma_{t-\\tau}\n$$\n\nwhere $\\mu_{t-\\tau: t}$ and $\\sigma_{t-\\tau: t}^{2}$ are the mean and variance of surprise for a window offset $\\tau$, and $\\gamma$ is a scaling factor. The choice of threshold $T$ is critical in balancing the granularity of segmentation with the model's sensitivity to contextual shifts. If the $T$ is too high, we will identify very few event boundaries, especially if the local context contains few surprising tokens. Conversely, a low $T$ results in frequent boundary identification. Using a moving window ensures that $T$ adapts to contextual shifts, minimizing the need for manual tuning while maintaining control over threshold sensitivity via $\\gamma$. We also explored a fixed threshold approach ( $T=T_{\\text {fixed }}$ ), though our primary focus remained on the dynamic threshold due to its adaptability to varying contexts. This initial segmentation results in a set of potential event boundaries $\\mathcal{B}=b_{1}, b_{2}, \\ldots, b_{k}$, where each $b_{i}$ represents the index of a token exceeding the surprise threshold. These boundaries serve as the starting point for our subsequent refinement process, which aims to optimise the intra-event coherence and inter-event distinctiveness of the resulting memory segments. ### 3.3 BOUNDARY REFINEMENT\n\nWhile surprise-based segmentation provides an effective initial estimate of event boundaries, we make the key observation that the utility of elements within an event during memory recall depends on their likelihood of being utilised by the current query. Therefore, we theorise that memory recall will be most efficient with high intra-event similarity between keys while maintaining low inter-event similarity.\n```\n\n##### *Relevant Chunk: No. 14/36 (Score: 0.98)*\n\n```\n2117625118. Zafeirios Fountas, Anastasia Sylaidi, Kyriacos Nikiforou, Anil K. Seth, Murray Shanahan, and Warrick Roseboom. A Predictive Processing Model of Episodic Memory and Time Perception. Neural Computation, 34(7):1501-1544, 06 2022. ISSN 0899-7667. doi $10.1162 /$ neco_a_01514. URLhttps://doi.org/10.1162/neco_a_01514. Marc W Howard and Michael J Kahana. A distributed representation of temporal context. Journal of mathematical psychology, 46(3):269-299, 2002. Li Ji-An, Corey Y. Zhou, Marcus K. Benna, and Marcelo G. Mattar. Linking in-context learning in transformers to human episodic memory, 2024. Manoj Kumar, Ariel Goldstein, Sebastian Michelmann, Jeffrey M Zacks, Uri Hasson, and Kenneth A Norman. Bayesian surprise predicts human event segmentation in story listening. Cognitive science, 47(10):e13343, 2023.\n```\n\n##### *Relevant Chunk: No. 5/36 (Score: 0.96)*\n\n```\n2023, Bertsch et al. 2023). Interestingly, since using a key-value cache with k-NN lookup can be seen as an approximation of applying softmax attention to the full token sequence (see Appendix A.4, k-NN retrieval methods can be used without any fine-tuning (Bertsch et al., 2023). For an exception that does not rely on $\\mathrm{k}-\\mathrm{NNs}$, see Wang et al. (2023). A recent and interesting variant of k-NN retrieval involves retrieving large groups of tokens, rather than individual ones. Models that rely on this approach include SLED (Ivgi et al., 2023) and the more recent InfLLM (Xiao et al., 2024a), which achieves state-of-the-art performance on long-context benchmarks. InfLLM segments the entire context length into fixed-size memory units and employs k-NN lookup using the tokens with the highest accumulated scores per unit. This can be seen as a form of hierarchical attention, as illustrated in Fig. 1. While group-based retrieval represents a promising direction, our approach significantly advances this concept by dynamically determining token groupings in a manner akin to human memory formation, addressing a fundamental limitation of InfLLM's fixed-size segmentation and enabling more adaptive and context-sensitive processing of extended information. ### 2.2 NEural MODEls OF EPISODic MEMORY ANd EVEnt CoGnition\n\nThe concept of episodic memory, central to our approach, has been extensively studied in both theoretical neuroscience and machine learning. Neural models of episodic memory capture human behaviour and neuroimaging data, providing insights into how the brain processes and stores experiences and suggesting links between memory, efficient representations and navigation of physical and conceptual spaces (Gershman et al., 2012, Benna and Fusi, 2021). In machine learning, episodic memory-inspired approaches have yielded significant improvements across various domains. For instance, episodic control has enhanced reinforcement learning agents' performance and learning speed (Blundell et al. 2016; Pritzel et al., 2017; Coda-Forno et al., 2024). In addition, models of memory construction and consolidation have been successful in alleviating catastrophic forgetting in neural networks (Kirkpatrick et al., 2017; Lopez-Paz and Ranzato, 2017, Chaudhry et al., 2019, Buzzega et al., 2020, Prabhu et al., 2020), including LLMs (Das et al., 2024), and appear to explain key features of human memory, such as imagination and future thinking (Spens and Burgess, 2024). These models have revealed key aspects of episodic memory, particularly in describing how experiences are segmented into events, and when new memories are encoded and retrieved (Lu et al, 2022). Surprise plays a critical role in this process, triggering event boundaries and memory formation (Fountas et al.||2022; Kumar et al., 2023). This event-based structure is deeply intertwined with our perception of time (Roseboom et al.| 2019; Sherman et al., 2022), highlighting the interdependence of memory and temporal cognition. This insight has helped generative models for video (Zakharov et al., 2022a b) and reinforcement learning (Zakharov et al., 2021) to capture temporal dynamics more accurately. In terms of memory retrieval, studies in human free recall have shown a distinctive increased likelihood of retrieving items encoded close together in time (temporal contiguity) and in succession (temporal asymmetry) (see Fig 2A). Recently, it was shown that attention heads in transformer-based LLMs that are associated with in-context learning, already exhibit the same dynamic retrieval behaviour (Ji-An et al., 2024) (Fig 2 B ) including both contiguity and asymmetry effects. Therefore, transformers have the inherent ability to act as episodic memory retrieval models, if provided with the right information within their context window. Our work leverages these concepts of surprise-based event segmentation and LLMs' inherent temporal contiguity and asymmetry effects to enable a new generation of Infinite Context-Length LLMs, capable of processing and understanding information over vastly extended timescales. ![](https://cdn.mathpix.com/cropped/2024_09_12_474e60d35c39e4b9bafbg-04.jpg?height=671&width=1650&top_left_y=267&top_left_x=244)\n\nFigure 2: (A) Example of the temporal contiguity and asymmetry effect in human free recall. Data averaged over several large free recall studies (adopted from Howard and Kahana, 2002). (B) The attention scores of a GPT2 head averaged over all tokens tested.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: episodic memory language models, event segmentation neural networks, graph memory networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Larimar: Large Language Models with Episodic Memory Control\n\n*From Search Query: episodic memory language models*\n\n*Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aur'elie Lozano, Georgios Kollias, V. Chenthamarakshan, Jir\u00ed Navr\u00e1til, Soham Dan, Pin-Yu Chen*\n\n**TL;DR:** Larimar is presented - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory that allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning.\n\n**Abstract:** Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 8-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 2. Recollection versus Imagination: Exploring Human Memory and Cognition via Neural Language Models\n\n*From Search Query: episodic memory language models*\n\n*Maarten Sap, E. Horvitz, Yejin Choi, Noah A. Smith, J. Pennebaker*\n\n**TL;DR:** A measure of narrative flow is introduced and used to examine the narratives for imagined and recalled events and reveals the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932).\n\n**Abstract:** We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release Hippocorpus, a dataset of 7,000 stories about imagined and recalled events. We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. Finally, our measures reveal the effect of narrativization of memories in stories (e.g., stories about frequently recalled memories flow more linearly; Bartlett, 1932). Our findings highlight the potential of using NLP tools to study the traces of human cognition in language.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 28  (*Influential: 3*)\n\n#### 3. SpotEM: Efficient Video Search for Episodic Memory\n\n*From Search Query: episodic memory language models*\n\n*Santhosh K. Ramakrishnan, Ziad Al-Halah, K. Grauman*\n\n**TL;DR:** SpotEM consists of a novel clip selector that learns to identify promising video regions to search conditioned on the language query, a set of low-cost semantic indexing features that capture the context of rooms, objects, and interactions that suggest where to look, and distillation losses that address the optimization issues arising from end-to-end joint training of the clip selector and EM model.\n\n**Abstract:** The goal in episodic memory (EM) is to search a long egocentric video to answer a natural language query (e.g.,\"where did I leave my purse?\"). Existing EM methods exhaustively extract expensive fixed-length clip features to look everywhere in the video for the answer, which is infeasible for long wearable-camera videos that span hours or even days. We propose SpotEM, an approach to achieve efficiency for a given EM method while maintaining good accuracy. SpotEM consists of three key ideas: 1) a novel clip selector that learns to identify promising video regions to search conditioned on the language query; 2) a set of low-cost semantic indexing features that capture the context of rooms, objects, and interactions that suggest where to look; and 3) distillation losses that address the optimization issues arising from end-to-end joint training of the clip selector and EM model. Our experiments on 200+ hours of video from the Ego4D EM Natural Language Queries benchmark and three different EM models demonstrate the effectiveness of our approach: computing only 10% - 25% of the clip features, we preserve 84% - 97% of the original EM model's accuracy. Project page: https://vision.cs.utexas.edu/projects/spotem\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 4. Learning Constraints and Descriptive Segmentation for Subevent Detection\n\n*From Search Query: event segmentation neural networks*\n\n*Haoyu Wang, Hongming Zhang, Muhao Chen, D. Roth*\n\n**TL;DR:** This work proposes an approach to learning and enforcing constraints that capture dependencies between subevent detection and EventSeg prediction, as well as guiding the model to make globally consistent inference using Rectifier Networks for constraint learning.\n\n**Abstract:** Event mentions in text correspond to real-world events of varying degrees of granularity. The task of subevent detection aims to resolve this granularity issue, recognizing the membership of multi-granular events in event complexes. Since knowing the span of descriptive contexts of event complexes helps infer the membership of events, we propose the task of event-based text segmentation (EventSeg) as an auxiliary task to improve the learning for subevent detection. To bridge the two tasks together, we propose an approach to learning and enforcing constraints that capture dependencies between subevent detection and EventSeg prediction, as well as guiding the model to make globally consistent inference. Specifically, we adopt Rectifier Networks for constraint learning and then convert the learned constraints to a regularization term in the loss function of the neural model. Experimental results show that the proposed method outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for subevent detection, HiEve and IC, respectively, while achieving a decent performance on EventSeg prediction.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 22  (*Influential: 3*)\n\n#### 5. SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks\n\n*From Search Query: event segmentation neural networks*\n\n*Rainer Engelken*\n\n**TL;DR:** This work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.\n\n**Abstract:** Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we introduce SparseProp, a novel event-based algorithm for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both the forward and backward pass operations from O(N) to O(log(N)) per network spike, thereby enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By leveraging the sparsity of the network, SparseProp eliminates the need to iterate through all neurons at each spike, employing efficient state updates instead. We demonstrate the efficacy of SparseProp across several classical integrate-and-fire neuron models, including a simulation of a sparse SNN with one million LIF neurons. This results in a speed-up exceeding four orders of magnitude relative to previous event-based implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Training Spiking Neural Networks with Event-driven Backpropagation\n\n*From Search Query: event segmentation neural networks*\n\n*Yaoyu Zhu, Zhaofei Yu, Wei Fang, Xiaodong Xie, Tiejun Huang, T. Masquelier*\n\n**TL;DR:** This paper analyzes the commonly used temporal backpropagation training approach and proves that the sum of gradients remains unchanged between fully-connected and convolutional layers, and proposes a backward kernel that can solve the reverse gradient problem for time-based gradients and keep the property of the invariable sum ofGradients.\n\n**Abstract:** Spiking Neural networks (SNNs) represent and transmit information by spatiotemporal spike patterns, which bring two major advantages: biological plausibility and suitability for ultralow-power neuromorphic implementation. Despite this, the binary firing characteristic makes training SNNs more challenging. To learn the parameters of deep SNNs in an event-driven fashion as in inference of SNNs, back-propagation with respect to spike timing is proposed. Although this event-driven learning has the advantages of lower computational cost and memory occupation, the accuracy is far below the recurrent neural network-like learning approaches. In this paper, we first analyze the commonly used temporal backpropagation training approach and prove that the sum of gradients remains unchanged between fully-connected and convolutional layers. Secondly, we show that the max pooling layer meets the above invariance rule, while the average pooling layer does not, which will suffer the gradient vanishing problem but can be revised to meet the requirement. Thirdly, we point out the reverse gradient problem for time-based gradients and propose a backward kernel that can solve this problem and keep the property of the invariable sum of gradients. The experimental results show that the proposed approach achieves state-of-the-art performance on CIFAR10 among time-based training methods. Also, this is the first time that the time-based backpropagation approach successfully trains SNN on the CIFAR100 dataset. Our code is available at https://github.com/zhuyaoyu/SNN-event-driven-learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 30  (*Influential: 2*)\n\n#### 7. Multimodal Neural Graph Memory Networks for Visual Question Answering\n\n*From Search Query: graph memory networks*\n\n*M. Khademi*\n\n**TL;DR:** A new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering that rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.\n\n**Abstract:** We introduce a new neural network architecture, Multimodal Neural Graph Memory Networks (MN-GMN), for visual question answering. The MN-GMN uses graph structure with different region features as node attributes and applies a recently proposed powerful graph neural network model, Graph Network (GN), to reason about objects and their interactions in an image. The input module of the MN-GMN generates a set of visual features plus a set of encoded region-grounded captions (RGCs) for the image. The RGCs capture object attributes and their relationships. Two GNs are constructed from the input module using the visual features and encoded RGCs. Each node of the GNs iteratively computes a question-guided contextualized representation of the visual/textual information assigned to it. Then, to combine the information from both GNs, the nodes write the updated representations to an external spatial memory. The final states of the memory cells are fed into an answer module to predict an answer. Experiments show MN-GMN rivals the state-of-the-art models on Visual7W, VQA-v2.0, and CLEVR datasets.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 33  (*Influential: 5*)\n\n#### 8. Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks\n\n*From Search Query: graph memory networks*\n\n*Shuai Zhang, M. Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miaoyuan Liu*\n\n**TL;DR:** This paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error and proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexityand improve convergence without compromising the test accuracy.\n\n**Abstract:** Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include \\textit{graph sparsification} that samples a subgraph to reduce the amount of data aggregation and \\textit{model sparsification} that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 4*)\n\n#### 9. Efficient and Equivariant Graph Networks for Predicting Quantum Hamiltonian\n\n*From Search Query: graph memory networks*\n\n*Haiyang Yu, Zhao Xu, X. Qian, Xiaoning Qian, Shuiwang Ji*\n\n**TL;DR:** This work proposes a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance, and considers the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics.\n\n**Abstract:** We consider the prediction of the Hamiltonian matrix, which finds use in quantum chemistry and condensed matter physics. Efficiency and equivariance are two important, but conflicting factors. In this work, we propose a SE(3)-equivariant network, named QHNet, that achieves efficiency and equivariance. Our key advance lies at the innovative design of QHNet architecture, which not only obeys the underlying symmetries, but also enables the reduction of number of tensor products by 92\\%. In addition, QHNet prevents the exponential growth of channel dimension when more atom types are involved. We perform experiments on MD17 datasets, including four molecular systems. Experimental results show that our QHNet can achieve comparable performance to the state of the art methods at a significantly faster speed. Besides, our QHNet consumes 50\\% less memory due to its streamlined architecture. Our code is publicly available as part of the AIRS library (\\url{https://github.com/divelab/AIRS}).\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 20  (*Influential: 8*)\n\n### 1 related papers from ArXiv\n\n#### 1. Knowledge Grounded Conversational Symptom Detection with Graph Memory\n  Networks\n\n*From Search Query: graph memory networks*\n\n*Hongyin Luo, Shang-Wen Li, James Glass*\n\n**Abstract:** In this work, we propose a novel goal-oriented dialog task, automatic symptom\ndetection. We build a system that can interact with patients through dialog to\ndetect and collect clinical symptoms automatically, which can save a doctor's\ntime interviewing the patient. Given a set of explicit symptoms provided by the\npatient to initiate a dialog for diagnosing, the system is trained to collect\nimplicit symptoms by asking questions, in order to collect more information for\nmaking an accurate diagnosis. After getting the reply from the patient for each\nquestion, the system also decides whether current information is enough for a\nhuman doctor to make a diagnosis. To achieve this goal, we propose two neural\nmodels and a training pipeline for the multi-step reasoning task. We also build\na knowledge graph as additional inputs to further improve model performance.\nExperiments show that our model significantly outperforms the baseline by 4%,\ndiscovering 67% of implicit symptoms on average with a limited number of\nquestions.\n\n**Published:** 2021-01-24T18:50:16Z  (*Updated: 2021-01-24T18:50:16Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Grounded Language Learning Fast and Slow\n\n*From Search Query: episodic memory language models*\n\n*Felix Hill, Stephen Clark, Olivier Tieleman, Tamara von Glehn, Hamza Merzic, Nathaniel Wong*\n\n**Abstract:** Recent work has shown that large text-based neural language models, trained with conventional supervised learning objectives, acquire a surprising propensity for few- and one-shot learning. Here, we show that an embodied agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional reinforcement learning algorithms. After a single introduction to a novel object via continuous visual perception and a language prompt (\"This is a dax\"), the agent can re-identify the object and manipulate it as instructed (\"Put the dax on the bed\"). In doing so, it seamlessly integrates short-term, within-episode knowledge of the appropriate referent for the word \"dax\" with long-term lexical and motor knowledge acquired across episodes (i.e. \"bed\" and \"putting\"). We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful for later executing instructions. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for 'fast-mapping', a fundamental pillar of human cognitive development and a potentially transformative capacity for agents that interact with human users.\n\n**Proceeding:** iclr-2021-1\n\n**Published:** 2020-09-03\n\n\n\n#### 2. Reflexion: Language Agents with Verbal Reinforcement Learning\n\n*From Search Query: episodic memory language models*\n\n*Shunyu Yao, Karthik Narasimhan, Ashwin Gopinath, Edward Berman, Federico Cassano, Noah Shinn*\n\n**Abstract:** Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-03-20\n\n\n\n#### 3. Event-based Vision: A Survey\n\n*From Search Query: event segmentation neural networks*\n\n*Kostas Daniilidis, Guillermo Gallego, Garrick Orchard, Davide Scaramuzza, Tobi Delbruck, Stefan Leutenegger, Joerg Conradt, Chiara Bartolozzi, Brian Taba, Andrew Davison, Andrea Censi*\n\n**Abstract:** Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.\n\n**Published:** 2019-04-17\n\n\n\n#### 4. Toward quantitative fractography using convolutional neural networks\n\n*From Search Query: event segmentation neural networks*\n\n*Shmuel Osovski, Ra\u00fal Herrero Moreno, Stylianos Tsopanidis*\n\n**Abstract:** The science of fractography revolves around the correlation between topographic characteristics of the fracture surface and the mechanisms and external conditions leading to their creation. While being a topic of investigation for centuries, it has remained mostly qualitative to date. A quantitative analysis of fracture surfaces is of prime interest for both the scientific community and the industrial sector, bearing the potential for improved understanding on the mechanisms controlling the fracture process and at the same time assessing the reliability of computational models currently being used for material design. With new advances in the field of image analysis, and specifically with machine learning tools becoming more accessible and reliable, it is now feasible to automate the process of extracting meaningful information from fracture surface images. Here, we propose a method of identifying and quantifying the relative appearance of intergranular and transgranular fracture events from scanning electron microscope images. The newly proposed method is based on a convolutional neural network algorithm for semantic segmentation. The proposed method is extensively tested and evaluated against two ceramic material systems ($Al_2O_3$,$MgAl_2O_4$) and shows high prediction accuracy, despite being trained on only one material system ($MgAl_2O_4$). While here attention is focused on brittle fracture characteristics, the method can be easily extended to account for other fracture morphologies, such as dimples, fatigue striations, etc.\n\n**Published:** 2019-08-01\n\n\n\n#### 5. Memory-Based Graph Networks\n\n*From Search Query: graph memory networks*\n\n*Amir Hosein Khasahmadi, Leo Lee, Kaveh Hassani, Parsa Moradi, Quaid Morris*\n\n**Abstract:** Graph neural networks (GNNs) are a class of deep models that operate on data with arbitrary topology represented as graphs. We introduce an efficient memory layer for GNNs that can jointly learn node representations and coarsen the graph. We also introduce two new networks based on this layer: memory-based GNN (MemGNN) and graph memory network (GMN) that can learn hierarchical graph representations. The experimental results shows that the proposed models achieve state-of-the-art results in eight out of nine graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data. Code and reference implementations are released at: https://github.com/amirkhas/GraphMemoryNet\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2020-02-21\n\n\n\n#### 6. GAMENet: Graph Augmented MEmory Networks for Recommending Medication Combination\n\n*From Search Query: graph memory networks*\n\n*Hongyan Li, Cao Xiao, Tengfei Ma, Junyuan Shang, Jimeng Sun*\n\n**Abstract:** Recent progress in deep learning is revolutionizing the healthcare domain\nincluding providing solutions to medication recommendations, especially\nrecommending medication combination for patients with complex health\nconditions. Existing approaches either do not customize based on patient health\nhistory, or ignore existing knowledge on drug-drug interactions (DDI) that\nmight lead to adverse outcomes. To fill this gap, we propose the Graph\nAugmented Memory Networks (GAMENet), which integrates the drug-drug\ninteractions knowledge graph by a memory module implemented as a graph\nconvolutional networks, and models longitudinal patient records as the query.\nIt is trained end-to-end to provide safe and personalized recommendation of\nmedication combination. We demonstrate the effectiveness and safety of GAMENet\nby comparing with several state-of-the-art methods on real EHR data. GAMENet\noutperformed all baselines in all effectiveness measures, and also achieved\n3.60% DDI rate reduction from existing EHR data.\n\n**Published:** 2018-09-06\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models incorporating episodic memory, event segmentation, and efficient memory management, here are some key findings and suggestions based on the provided keywords and details:\n\n## Integration of Episodic Memory or Event Segmentation in Neural Networks\n\n- The paper \"Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks (SORT)\" introduces a framework to evaluate episodic memory in language models by recalling the correct order of text segments. This work highlights the importance of episodic memory in LLMs and provides a method to assess it, which can be integrated into neural network designs to enhance their memory capabilities.\n\n- The video and associated paper on \"Episodic Memory Verbalization using Hierarchical Representations of Life-Long Robot Experience\" discuss using hierarchical representations to verbalize episodic data. This approach involves a tree-like data structure derived from episodic memory, which can be applied to language models to improve their ability to handle long-term experiences and event segmentation.\n\n## Hierarchical State Space Models with Graph-Based Memory\n\n- The concept of hierarchical state space models is touched upon in the \"Episodic Memory Verbalization\" paper, where a hierarchical representation is used to abstract events from raw perception data to natural language concepts. This hierarchical structure can be adapted for language models to manage state space efficiently using graph-based memory architectures.\n\n- The paper \"Exploring Forgetting in Large Language Model Pre-Training\" introduces memory replay strategies that could be integrated with hierarchical state space models. It discusses the use of a module that retains a record of examples and periodically updates the model, which can be seen as a form of graph-based memory management to mitigate forgetting.\n\n## Efficient Implementations of Memory Management in Language Models\n\n- The work on \"Exploring Forgetting in Large Language Model Pre-Training\" provides several efficient memory replay strategies, such as Intensive Focused Stochasticity, which involves periodic intensive learning to reduce forgetting. These strategies are lightweight and cost-effective, making them suitable for pre-training large language models.\n\n- The reference to \"Sparser is Faster and Less is More: Efficient Sparse Attention\" suggests that sparse attention mechanisms can be used to manage memory efficiently. This approach can be integrated into the design of autoregressive language models to reduce memory overhead and improve computational efficiency.\n\n## Online Event Detection and Boundary Refinement Techniques\n\n- For online event detection and boundary refinement, the \"EventDrivenSSD Proposal\" analysis suggests using Bayesian surprise computation methodology and graph-theoretic boundary refinement approaches. These methods need to be researched further to integrate them effectively into the model design.\n\n- The \"Episodic Memory Verbalization\" paper, while not directly focused on online event detection, provides insights into how hierarchical representations can be used to dynamically expand and refine event boundaries, which can be adapted for online event detection.\n\n## Applications of Bayesian Surprise in Sequence Modeling\n\n- The \"EventDrivenSSD Proposal\" highlights the importance of Bayesian surprise in event detection mechanisms. This involves analyzing existing methods that use Bayesian surprise to detect events and refining boundaries using graph-theoretic approaches.\n\n- While the provided sources do not explicitly discuss Bayesian surprise in sequence modeling, the concept is integral to the proposal's event detection mechanism. Further research into Bayesian surprise methods and their application in sequence modeling would be necessary to integrate this effectively into the model design.\n\n## Additional Considerations\n\n- **Scalability and Efficiency**: The \"Exploring Forgetting in Large Language Model Pre-Training\" paper emphasizes the importance of simple and cost-effective memory replay strategies to ensure scalability and efficiency in pre-training large language models.\n\n- **Compatibility and Integration**: Ensuring that the new components (e.g., hierarchical state space models, graph-based memory) are compatible with existing architectures like Mamba2 and do not significantly impact training and inference speed is crucial. This involves studying the memory overhead and computational trade-offs of these components.\n\nBy integrating these concepts and strategies, the researchers can develop a more robust and efficient autoregressive language model that leverages episodic memory, event segmentation, and hierarchical state space models with graph-based memory management.\n",
            "analysis": "Initial Analysis of EventDrivenSSD Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) Event Detection Mechanism:\n- Bayesian surprise computation methodology\n- Graph-theoretic boundary refinement approach\n- Efficiency of online event detection\n- Comparison with existing event segmentation methods\n\nb) State Management:\n- Hierarchical state organization effectiveness\n- Graph-based memory structure efficiency\n- Impact on model performance\n- Memory-computation trade-offs\n\nc) Integration with Mamba2:\n- Compatibility with existing architecture\n- Impact on training and inference speed\n- Memory overhead of additional components\n- Scalability to long sequences\n\n2. Comparison with Previous Designs:\n\nThe proposal differs significantly from the sibling WaveletSSD design:\n- Uses event-driven processing instead of wavelet transforms\n- Focuses on semantic segmentation rather than frequency decomposition\n- Employs graph-based memory management vs. multi-resolution analysis\n\n3. Novelty Assessment:\n\nThe proposal combines several innovative elements:\n- Integration of episodic memory mechanisms with SSMs\n- Graph-based state management for efficient processing\n- Event-driven sequence segmentation\n- Hierarchical memory organization\n\n4. Investigation Plan:\n\nPhase 1: Event Detection\n- Research existing event segmentation methods\n- Analyze Bayesian surprise approaches\n- Investigate graph-theoretic boundary detection\n\nPhase 2: Memory Management\n- Study hierarchical state space models\n- Research graph-based memory architectures\n- Analyze efficient memory update mechanisms\n\nPhase 3: Integration & Efficiency\n- Examine SSM modifications\n- Study parallel processing approaches\n- Investigate memory-efficient implementations\n\nInitial search results will help validate the novelty and feasibility of these components.\n\nReferences found so far:\n\n1. Fountas, Z., et al. (2024). \"Human-like Episodic Memory for Infinite Context LLMs\"\n   - Relevant for episodic memory integration\n   - Provides insights into event-based processing\n\n2. Lou, C., et al. (2024). \"Sparser is Faster and Less is More: Efficient Sparse Attention\"\n   - Offers insights into efficient memory management\n   - Relevant for sparse computation strategies\n\n3. Li, X., et al. (2023). \"Recurrent Attention Networks for Long-text Modeling\"\n   - Provides context for hierarchical processing\n   - Relevant for sequence segmentation\n\n4. Lioutas, V., et al. (2020). \"Time-aware Large Kernel Convolutions\"\n   - Offers insights into adaptive processing\n   - Relevant for temporal modeling\n\nNext steps:\n1. Conduct detailed search on event detection mechanisms\n2. Investigate hierarchical state space models\n3. Research graph-based memory architectures\n4. Analyze efficient implementation strategies"
        },
        {
            "ready": false,
            "query": "surprise based sequence segmentation, memory compression neural networks, efficient graph operations language models",
            "detail": "Find papers discussing:\n1. Implementation details of surprise-based sequence segmentation in neural networks\n2. Memory compression techniques for state space models and transformers\n3. Efficient graph operations and optimizations for language models\n4. Integration of event detection with state space models\n5. Performance analysis of memory-augmented language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Implementation details of surprise-based sequence segmentation in neural networks\n2. Memory compression techniques for state space models and transformers\n3. Efficient graph operations and optimizations for language models\n4. Integration of event detection with state space models\n5. Performance analysis of memory-augmented language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.94)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.94)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 3. Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model (Avg. Score: 0.91)\n\n*Wenbing Li, Hang Zhou, Junqing Yu, Zikai Song, Wei Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Coupled SSM model is proposed, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes, and demonstrates that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n**Abstract:** The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities. However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n##### *Relevant Chunk: No. 21/22 (Score: 0.91)*\n\n```\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3718-3727, 2020. [38] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and LouisPhilippe Morency. Memory fusion network for multi-view sequential learning. Proceedings of the AAAI Conference on Artificial Intelligence, Jun 2022. [39] Tri Dao, DanielY. Fu, KhaledK. Saab, ArminW. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. Dec 2022. [40] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. [41] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [42] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. [43] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling.\n```\n\n#### 4. Universal Transformers  (Avg. Score: 0.51)\n\n*Mostafa Dehghani, Stephan Gouws, O. Vinyals, Jakob Uszkoreit, Lukasz Kaiser*\n\n**Published in:** International Conference on Learning Representations (2018)\t**Cited by** 673  (*Influential: 65*)\n\n**TL;DR:** The Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses issues of parallelizability and global receptive field, is proposed.\n\n**Abstract:** Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.\n\n##### *Relevant Chunk: No. 21/32 (Score: 0.51)*\n\n```\narXiv preprint arXiv:1610.02357, 2016. Zewei Chu, Hai Wang, Kevin Gimpel, and David McAllester. Broad context language modeling as reading comprehension. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, volume 2, pp. 52-57, 2017. Bhuwan Dhingra, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Linguistic knowledge as memory for recurrent neural networks. arXiv preprint arXiv:1703.02620, 2017. Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for reasoning over multiple mentions using coreference. arXiv preprint arXiv:1804.05922, 2018. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. CoRR, abs/1705.03122, 2017. URL/http://arxiv.org/abs/1705.03122\n\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016. Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850\n\nAlex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401\n\nCaglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786, 2018. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2003. A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in Neural Information Processing Systems, (NIPS), 2015. \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1511.08228\n\n\u0141ukasz Kaiser, Aidan N. Gomez, and Francois Chollet. Depthwise separable convolutions for neural machine translation. CoRR, abs/1706.03059, 2017. URLhttp://arxiv.org/abs/1706.03059. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378-1387, 2016. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of 1stms to learn syntax-sensitive dependencies.\n```\n\n#### 5. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.51)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.51)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: surprise based sequence segmentation, memory compression neural networks, efficient graph operations language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. VITA: Video Instance Segmentation via Object Token Association\n\n*From Search Query: surprise based sequence segmentation*\n\n*Miran Heo, Sukjun Hwang, Seoung Wug Oh, Joon-Young Lee, Seon Joo Kim*\n\n**TL;DR:** VITA accomplishes video-level understanding by associating frame- level object tokens without using spatio-temporal backbone features, and shows several practical advantages that previous offline VIS methods have not explored - handling long and high-resolution videos with a common GPU, and freezing a frame-level detector trained on image domain.\n\n**Abstract:** We introduce a novel paradigm for offline Video Instance Segmentation (VIS), based on the hypothesis that explicit object-oriented information can be a strong clue for understanding the context of the entire sequence. To this end, we propose VITA, a simple structure built on top of an off-the-shelf Transformer-based image instance segmentation model. Specifically, we use an image object detector as a means of distilling object-specific contexts into object tokens. VITA accomplishes video-level understanding by associating frame-level object tokens without using spatio-temporal backbone features. By effectively building relationships between objects using the condensed information, VITA achieves the state-of-the-art on VIS benchmarks with a ResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019&2021, and 19.6 AP on OVIS. Moreover, thanks to its object token-based structure that is disjoint from the backbone features, VITA shows several practical advantages that previous offline VIS methods have not explored - handling long and high-resolution videos with a common GPU, and freezing a frame-level detector trained on image domain. Code is available at https://github.com/sukjunhwang/VITA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 73  (*Influential: 17*)\n\n#### 2. Mask Propagation for Efficient Video Semantic Segmentation\n\n*From Search Query: surprise based sequence segmentation*\n\n*Yuetian Weng, Mingfei Han, Haoyu He, Mingjie Li, Lina Yao, Xiaojun Chang, Bohan Zhuang*\n\n**TL;DR:** This paper proposes an efficient mask propagation framework for VSS, called MPVSS, which employs a strong query-based image segmentor on sparse key frames to generate accurate binary masks and class predictions, and designs a flow estimation module utilizing the learned queries to generate a set of segment-aware flow maps.\n\n**Abstract:** Video Semantic Segmentation (VSS) involves assigning a semantic label to each pixel in a video sequence. Prior work in this field has demonstrated promising results by extending image semantic segmentation models to exploit temporal relationships across video frames; however, these approaches often incur significant computational costs. In this paper, we propose an efficient mask propagation framework for VSS, called MPVSS. Our approach first employs a strong query-based image segmentor on sparse key frames to generate accurate binary masks and class predictions. We then design a flow estimation module utilizing the learned queries to generate a set of segment-aware flow maps, each associated with a mask prediction from the key frame. Finally, the mask-flow pairs are warped to serve as the mask predictions for the non-key frames. By reusing predictions from key frames, we circumvent the need to process a large volume of video frames individually with resource-intensive segmentors, alleviating temporal redundancy and significantly reducing computational costs. Extensive experiments on VSPW and Cityscapes demonstrate that our mask propagation framework achieves SOTA accuracy and efficiency trade-offs. For instance, our best model with Swin-L backbone outperforms the SOTA MRCFA using MiT-B5 by 4.0% mIoU, requiring only 26% FLOPs on the VSPW dataset. Moreover, our framework reduces up to 4x FLOPs compared to the per-frame Mask2Former baseline with only up to 2% mIoU degradation on the Cityscapes validation set. Code is available at https://github.com/ziplab/MPVSS.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 3. GNN-SL: Sequence Labeling Based on Nearest Examples via GNN\n\n*From Search Query: surprise based sequence segmentation*\n\n*Shuhe Wang, Yuxian Meng, Rongbin Ouyang, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Guoyin Wang*\n\n**Abstract:** To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging examples benefit the model prediction, we construct a heterogeneous graph, and leverage graph neural networks (GNNs) to transfer information between the retrieved tagging examples and the input word sequence. The augmented node which aggregates information from neighbors is used to do prediction. This strategy enables the model to directly acquire similar tagging examples and improves the general quality of predictions. We conduct a variety of experiments on three typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2) on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the CWS task, and results comparable to SOTA performances on NER datasets, and POS datasets.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 4. EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression\n\n*From Search Query: memory compression neural networks*\n\n*Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu*\n\n**TL;DR:** An optimized GPU implementation which supports training GNNs with compressed activations for Pytorch Geometric and Pytorch, and a memory-ef\ufb01cient framework called EXACT, which for the first time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations.\n\n**Abstract:** Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously dif\ufb01cult to implement due to the lack of necessary tools in common graph learning packages. To un-leash the potential of this direction, we provide an optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-ef\ufb01cient framework called \u201cEXACT\u201d, which for the \ufb01rst time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to 32 \u00d7 with 0 . 2 - 0 . 5% accuracy drop and 10 - 25% time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on ogbn-products from a 48GB GPU to a 12GB GPU. The code is available at https://github.com/warai-0toko\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 55  (*Influential: 7*)\n\n#### 5. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding\n\n*From Search Query: memory compression neural networks*\n\n*Song Han, Huizi Mao, W. Dally*\n\n**TL;DR:** This work introduces \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy.\n\n**Abstract:** Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2015\n\n**Citations:** 8226  (*Influential: 820*)\n\n#### 6. A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks\n\n*From Search Query: memory compression neural networks*\n\n*Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Liang Chen, Zibin Zheng, Baokun Wang, Changhua Meng*\n\n**TL;DR:** This work proposes SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance, and provides theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts.\n\n**Abstract:** While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of higher task accuracy requires a larger hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This work explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compression, SpikeGCL is either comparable to or outperforms many fancy state-of-the-art supervised and self-supervised methods across several graph benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 7. Efficient Gradient Computation for Structured Output Learning with Rational and Tropical Losses\n\n*From Search Query: efficient graph operations language models*\n\n*Corinna Cortes, Vitaly Kuznetsov, M. Mohri, Dmitry Storcheus, Scott Yang*\n\n**TL;DR:** Efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses are designed that facilitate efficient gradient computation and enable one to train learning models such as neural networks with complex structured losses.\n\n**Abstract:** Many structured prediction problems admit a natural loss function for evaluation such as the edit-distance or $n$-gram loss. However, existing learning algorithms are typically designed to optimize alternative objectives such as the cross-entropy. This is because a na\\\"{i}ve implementation of the natural loss functions often results in intractable gradient computations. In this paper, we design efficient gradient computation algorithms for two broad families of structured prediction loss functions: rational and tropical losses. These families include as special cases the $n$-gram loss, the edit-distance loss, and many other loss functions commonly used in natural language processing and computational biology tasks that are based on sequence similarity measures. Our algorithms make use of weighted automata and graph operations over appropriate semirings to design efficient solutions. They facilitate efficient gradient computation and hence enable one to train learning models such as neural networks with complex structured losses.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 8. A Hierarchical Model for Device Placement\n\n*From Search Query: efficient graph operations language models*\n\n*Azalia Mirhoseini, Anna Goldie, Hieu Pham, Benoit Steiner, Quoc V. Le, J. Dean*\n\n**Abstract:** We introduce a hierarchical model for efficient placement of computational graphs onto hardware devices, especially in heterogeneous environments with a mixture of CPUs, GPUs, and other computational devices. Our method learns to assign graph operations to groups and to allocate those groups to available devices. The grouping and device allocations are learned jointly. The proposed method is trained with policy gradient and requires no human intervention. Experiments with widely-used computer vision and natural language models show that our algorithm can find optimized, non-trivial placements for TensorFlow computational graphs with over 80,000 operations. In addition, our approach outperforms placements by human experts as well as a previous state-of-the-art placement method based on deep reinforcement learning. Our method achieves runtime reductions of up to 60.6% per training step when applied to models such as Neural Machine Translation.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 144  (*Influential: 20*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Tracking through Containers and Occluders in the Wild\n\n*From Search Query: surprise based sequence segmentation*\n\n*Carl Vondrick, Jie Li, Simon Stent, Pavel Tokmakov, Basile Van Hoorick*\n\n**Abstract:** Tracking objects with persistence in cluttered and dynamic environments remains a difficult challenge for computer vision systems. In this paper, we introduce $\\textbf{TCOW}$, a new benchmark and model for visual tracking through heavy occlusion and containment. We set up a task where the goal is to, given a video sequence, segment both the projected extent of the target object, as well as the surrounding container or occluder whenever one exists. To study this task, we create a mixture of synthetic and annotated real datasets to support both supervised learning and structured evaluation of model performance under various forms of task variation, such as moving or nested containment. We evaluate two recent transformer-based video models and find that while they can be surprisingly capable of tracking targets under certain settings of task variation, there remains a considerable performance gap before we can claim a tracking model to have acquired a true notion of object permanence.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-05-04\n\n\n\n#### 2. Empirical Evaluation of Sequence-to-Sequence Models for Word Discovery in Low-resource Settings\n\n*From Search Query: surprise based sequence segmentation*\n\n*Laurent Besacier, Marcely Zanon Boito, Aline Villavicencio*\n\n**Abstract:** Since Bahdanau et al. [1] first introduced attention for neural machine translation, most sequence-to-sequence models made use of attention mechanisms [2, 3, 4]. While they produce soft-alignment matrices that could be interpreted as alignment between target and source languages, we lack metrics to quantify their quality, being unclear which approach produces the best alignments. This paper presents an empirical evaluation of 3 main sequence-to-sequence models (CNN, RNN and Transformer-based) for word discovery from unsegmented phoneme sequences. This task consists in aligning word sequences in a source language with phoneme sequences in a target language, inferring from it word segmentation on the target side [5]. Evaluating word segmentation quality can be seen as an extrinsic evaluation of the soft-alignment matrices produced during training. Our experiments in a low-resource scenario on Mboshi and English languages (both aligned to French) show that RNNs surprisingly outperform CNNs and Transformer for this task. Our results are confirmed by an intrinsic evaluation of alignment quality through the use of Average Normalized Entropy (ANE). Lastly, we improve our best word discovery model by using an alignment entropy confidence measure that accumulates ANE over all the occurrences of a given alignment pair in the collection.\n\n**Published:** 2019-06-29\n\n\n\n#### 3. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n\n*From Search Query: memory compression neural networks*\n\n*Song Han, Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer*\n\n**Abstract:** Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet\n\n**Published:** 2016-02-24\n\n\n\n#### 4. Pareto-Optimal Quantized ResNet Is Mostly 4-bit\n\n*From Search Query: memory compression neural networks*\n\n*Lukasz Lew, Chas Leichner, Oleg Rybakov, Jonathan Malmaud, Shivani Agrawal, Lisa Wang, Amirali Abdolrashidi*\n\n**Abstract:** Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher accuracy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.\n\n**Published:** 2021-05-07\n\n\n\n#### 5. Graph Representation Learning via Hard and Channel-Wise Attention Networks\n\n*From Search Query: efficient graph operations language models*\n\n*Hongyang Gao, Shuiwang Ji*\n\n**Abstract:** Attention operators have been widely applied in various fields, including computer vision, natural language processing, and network embedding learning. Attention operators on graph data enables learnable weights when aggregating information from neighboring nodes. However, graph attention operators (GAOs) consume excessive computational resources, preventing their applications on large graphs. In addition, GAOs belong to the family of soft attention, instead of hard attention, which has been shown to yield better performance. In this work, we propose novel hard graph attention operator (hGAO) and channel-wise graph attention operator (cGAO). hGAO uses the hard attention mechanism by attending to only important nodes. Compared to GAO, hGAO improves performance and saves computational cost by only attending to important nodes. To further reduce the requirements on computational resources, we propose the cGAO that performs attention operations along channels. cGAO avoids the dependency on the adjacency matrix, leading to dramatic reductions in computational resource requirements. Experimental results demonstrate that our proposed deep models with the new operators achieve consistently better performance. Comparison results also indicates that hGAO achieves significantly better performance than GAO on both node and graph embedding tasks. Efficiency comparison shows that our cGAO leads to dramatic savings in computational resources, making them applicable to large graphs.\n\n**Published:** 2019-07-05\n\n\n\n#### 6. Mesh-TensorFlow: Deep Learning for Supercomputers\n\n*From Search Query: efficient graph operations language models*\n\n*Blake Hechtman, HyoukJoong Lee, Peter Hawkins, Youlong Cheng, Mingsheng Hong, Ryan Sepassi, Noam Shazeer, Cliff Young, Ashish Vaswani, Dustin Tran, Penporn Koanantakool, Niki Parmar*\n\n**Abstract:** Batch-splitting (data-parallelism) is the dominant distributed Deep Neural\nNetwork (DNN) training strategy, due to its universal applicability and its\namenability to Single-Program-Multiple-Data (SPMD) programming. However,\nbatch-splitting suffers from problems including the inability to train very\nlarge models (due to memory constraints), high latency, and inefficiency at\nsmall batch sizes. All of these can be solved by more general distribution\nstrategies (model-parallelism). Unfortunately, efficient model-parallel\nalgorithms tend to be complicated to discover, describe, and to implement,\nparticularly on large clusters. We introduce Mesh-TensorFlow, a language for\nspecifying a general class of distributed tensor computations. Where\ndata-parallelism can be viewed as splitting tensors and operations along the\n\"batch\" dimension, in Mesh-TensorFlow, the user can specify any\ntensor-dimensions to be split across any dimensions of a multi-dimensional mesh\nof processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting\nof parallel operations coupled with collective communication primitives such as\nAllreduce. We use Mesh-TensorFlow to implement an efficient data-parallel,\nmodel-parallel version of the Transformer sequence-to-sequence model. Using TPU\nmeshes of up to 512 cores, we train Transformer models with up to 5 billion\nparameters, surpassing state of the art results on WMT'14 English-to-French\ntranslation task and the one-billion-word language modeling benchmark.\nMesh-Tensorflow is available at https://github.com/tensorflow/mesh .\n\n**Conference:** mesh-tensorflow-deep-learning-for-1\n\n**Published:** 2018-11-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models (LMs) based on the provided keywords and details, here are some key findings and directions from the existing literature and research areas:\n\n### Surprise-Based Sequence Segmentation\n\n- The work on EM-LLM by Fountas et al. (2024) is particularly relevant. It implements \"surprise-based event segmentation using Bayesian surprise and graph-theoretic boundary refinement\" to organize sequences of tokens into coherent episodic events. This approach shows a 4.3% overall improvement across tasks, indicating its potential for enhancing sequence segmentation in LMs[3,.\n\n### Memory Compression Techniques for State Space Models and Transformers\n\n- **Hierarchical Memory Organization**: The Memory-Based Graph Networks by Khasahmadi et al. (2020) introduce an efficient memory layer for Graph Neural Networks (GNNs) that can jointly learn node representations and coarsen the graph. This hierarchical approach can be adapted for state space models and transformers to improve memory efficiency.\n- **Dynamic Memory Updates**: Larimar by Das et al. (2024) allows for dynamic, one-shot updates of knowledge without computationally expensive re-training. This could be integrated into state space models to enhance memory compression and efficiency.\n\n### Efficient Graph Operations and Optimizations for Language Models\n\n- **Memory-Based Graph Networks**: As mentioned, Khasahmadi et al.'s work on Memory-Based Graph Networks demonstrates efficient memory layers for GNNs, which can be applied to language models to improve graph operations and memory access efficiency.\n- **Sparse Attention Mechanisms**: The work by Lou et al. (2024) on \"Sparser is Faster and Less is More: Efficient Sparse Attention\" provides insights into optimizing attention mechanisms, which are crucial for efficient graph operations in language models. This could help in reducing computational overhead while maintaining performance.\n\n### Integration of Event Detection with State Space Models\n\n- **EM-LLM**: The integration of Bayesian surprise and graph-theoretic boundary refinement in EM-LLM shows how event detection can be seamlessly integrated with state space models. This approach organizes sequences into coherent episodic events, which can enhance the overall performance of the LM.\n\n### Performance Analysis of Memory-Augmented Language Models\n\n- **EM-LLM and Larimar**: Both EM-LLM and Larimar provide performance analyses that highlight the benefits of memory-augmented approaches. EM-LLM achieves a 4.3% overall improvement, while Larimar offers 8-10x speed-ups depending on the base LLM. These analyses are crucial for understanding the trade-offs between memory efficiency, computational overhead, and performance.\n\n### Additional Considerations\n\n- **Scalability and Compatibility**: When integrating these components, it is essential to consider compatibility with existing architectures (like Mamba2) and evaluate the performance impact of additional components. Managing memory overhead is crucial for scalability, as highlighted in the key insights from the analysis.\n\nTo further improve the autoregressive LM design, the following search queries and areas of investigation are recommended:\n\n- **Specific Implementations of Bayesian Surprise**: Deep dive into the technical details of how Bayesian surprise is implemented in sequence models, including the mathematical formulations and computational efficiency.\n- **Efficient Graph Memory Architectures**: Investigate graph-based structures for memory access and their potential for flexible and efficient memory management in language models.\n- **Memory Compression in State Space Models**: Explore various techniques for compressing memory in state space models, including hierarchical organization and dynamic updates.\n- **Optimizations for Graph Operations**: Look into sparse attention mechanisms and other optimizations that can reduce computational overhead while maintaining or improving performance.\n\nBy focusing on these areas, researchers can develop more efficient, scalable, and high-performance autoregressive language models.\n",
            "analysis": "Summary of Analysis:\n\nThe initial search results reveal several promising directions and key considerations for the EventDrivenSSD proposal. The findings suggest both opportunities and challenges that need further investigation.\n\nKey References and Excerpts:\n\n1. EM-LLM (Fountas et al., 2024):\n- Implements \"surprise-based event segmentation using Bayesian surprise and graph-theoretic boundary refinement\"\n- Achieves \"4.3% overall improvement across tasks\"\n- Uses \"two-stage memory process combining similarity-based and temporally contiguous retrieval\"\nKey excerpt: \"EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion.\"\n\n2. Larimar (Das et al., 2024):\n- Provides \"dynamic, one-shot updates of knowledge\"\n- Achieves \"8-10x speed-ups depending on base LLM\"\nKey excerpt: \"Larimar's memory allows for dynamic, one-shot updates of knowledge without computationally expensive re-training\"\n\n3. Memory-Based Graph Networks (Khasahmadi et al., 2020):\n- Introduces \"efficient memory layer for GNNs\"\n- Demonstrates \"hierarchical graph representations\"\nKey excerpt: \"Can jointly learn node representations and coarsen the graph\"\n\nKey Insights:\n\n1. Event Detection Mechanisms:\n- Bayesian surprise shows promise for online event segmentation\n- Graph-theoretic refinement can improve boundary detection accuracy\n- Need to investigate computational efficiency of these methods\n\n2. Memory Management:\n- Hierarchical organization crucial for efficiency\n- Graph-based structures show potential for flexible memory access\n- Trade-off between memory efficiency and computational overhead needs study\n\n3. Integration Challenges:\n- Compatibility with Mamba2 architecture requires careful consideration\n- Performance impact of additional components needs evaluation\n- Memory overhead management crucial for scalability\n\nFuture Search Plan:\n\n1. Need deeper investigation into:\n- Specific implementations of Bayesian surprise in sequence models\n- Efficient graph memory architectures for language models\n- Memory compression techniques for state management\n\n2. Search queries for next round:\n- Technical details of surprise-based segmentation\n- Memory compression in state space models\n- Efficient graph operations for language models\n\nReferences:\n\n1. Fountas, Z., et al. (2024). \"Human-like Episodic Memory for Infinite Context LLMs.\" arXiv preprint.\n\n2. Das, P., et al. (2024). \"Larimar: Large Language Models with Episodic Memory Control.\" International Conference on Machine Learning.\n\n3. Khasahmadi, A.H., et al. (2020). \"Memory-Based Graph Networks.\" ICLR 2020.\n\n4. Lou, C., et al. (2024). \"Sparser is Faster and Less is More: Efficient Sparse Attention.\" arXiv preprint.\n\n5. Li, X., et al. (2023). \"Recurrent Attention Networks for Long-text Modeling.\" ACL 2023."
        },
        {
            "ready": false,
            "query": "hierarchical state space models, neural memory compression techniques, graph sequence segmentation",
            "detail": "Find papers discussing:\n1. Hierarchical architectures in state space models and their efficiency\n2. Memory compression techniques specifically for sequential models\n3. Graph-based approaches to sequence segmentation and event detection\n4. Integration of hierarchical structures with SSMs\n5. Performance analysis of compressed memory architectures in sequential models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical architectures in state space models and their efficiency\n2. Memory compression techniques specifically for sequential models\n3. Graph-based approaches to sequence segmentation and event detection\n4. Integration of hierarchical structures with SSMs\n5. Performance analysis of compressed memory architectures in sequential models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.98)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.98)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.97)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.97)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.78)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.78)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n#### 5. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.73)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 0.73)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical state space models, neural memory compression techniques, graph sequence segmentation\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: hierarchical state space models*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Fast as CHITA: Neural Network Pruning with Combinatorial Optimization\n\n*From Search Query: neural memory compression techniques*\n\n*Riade Benbaki, Wenyu Chen, X. Meng, Hussein Hazimeh, N. Ponomareva, Zhe Zhao, R. Mazumder*\n\n**TL;DR:** This work proposes a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint, and extends the classical Optimal Brain Surgeon framework, CHITA, which leads to significantly better sparsity-accuracy tradeoffs than competing methods.\n\n**Abstract:** The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning. CHITA's main workhorse performs combinatorial optimization updates on a memory-friendly representation of local quadratic approximation(s) of the loss function. On a standard benchmark of pretrained models and datasets, CHITA leads to significantly better sparsity-accuracy tradeoffs than competing methods. For example, for MLPNet with only 2% of the weights retained, our approach improves the accuracy by 63% relative to the state of the art. Furthermore, when used in conjunction with fine-tuning SGD steps, our method achieves significant accuracy gains over the state-of-the-art approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 3. Weightless: Lossy Weight Encoding For Deep Neural Network Compression\n\n*From Search Query: neural memory compression techniques*\n\n*Brandon Reagen, Udit Gupta, Bob Adolf, M. Mitzenmacher, Alexander M. Rush, Gu-Yeon Wei, D. Brooks*\n\n**TL;DR:** A novel scheme for lossy weight encoding co-designed with weight simplification techniques that can compress weights by up to 496x without loss of model accuracy, resulting in up to a 1.51x improvement over the state-of-the-art.\n\n**Abstract:** The large memory requirements of deep neural networks limit their deployment and adoption on many devices. Model compression methods effectively reduce the memory requirements of these models, usually through applying transformations such as weight pruning or quantization. In this paper, we present a novel scheme for lossy weight encoding co-designed with weight simplification techniques. The encoding is based on the Bloomier filter, a probabilistic data structure that can save space at the cost of introducing random errors. Leveraging the ability of neural networks to tolerate these imperfections and by re-training around the errors, the proposed technique, named Weightless, can compress weights by up to 496x without loss of model accuracy. This results in up to a 1.51x improvement over the state-of-the-art.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 35  (*Influential: 2*)\n\n#### 4. Sparsified SGD with Memory\n\n*From Search Query: neural memory compression techniques*\n\n*Sebastian U. Stich, Jean-Baptiste Cordonnier, Martin Jaggi*\n\n**TL;DR:** This work analyzes Stochastic Gradient Descent with k-sparsification or compression (for instance top-k or random-k) and shows that this scheme converges at the same rate as vanilla SGD when equipped with error compensation.\n\n**Abstract:** Huge scale machine learning problems are nowadays tackled by distributed optimization algorithms, i.e. algorithms that leverage the compute power of many devices for training. The communication overhead is a key bottleneck that hinders perfect scalability. Various recent works proposed to use quantization or sparsification techniques to reduce the amount of data that needs to be communicated, for instance by only sending the most significant entries of the stochastic gradient (top-k sparsification). Whilst such schemes showed very promising performance in practice, they have eluded theoretical analysis so far. In this work we analyze Stochastic Gradient Descent (SGD) with k-sparsification or compression (for instance top-k or random-k) and show that this scheme converges at the same rate as vanilla SGD when equipped with error compensation (keeping track of accumulated errors in memory). That is, communication can be reduced by a factor of the dimension of the problem (sometimes even more) whilst still converging at the same rate. We present numerical experiments to illustrate the theoretical findings and the good scalability for distributed applications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 697  (*Influential: 134*)\n\n#### 5. Neural Graph Matching Networks for Chinese Short Text Matching\n\n*From Search Query: graph sequence segmentation*\n\n*Lu Chen, Yanbin Zhao, Boer Lyu, Lesheng Jin, Zhi Chen, Su Zhu, Kai Yu*\n\n**Abstract:** Chinese short text matching usually employs word sequences rather than character sequences to get better performance. However, Chinese word segmentation can be erroneous, ambiguous or inconsistent, which consequently hurts the final matching performance. To address this problem, we propose neural graph matching networks, a novel sentence matching framework capable of dealing with multi-granular input information. Instead of a character sequence or a single word sequence, paired word lattices formed from multiple word segmentation hypotheses are used as input and the model learns a graph representation according to an attentive graph matching mechanism. Experiments on two Chinese datasets show that our models outperform the state-of-the-art short text matching models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 44  (*Influential: 1*)\n\n#### 6. GNN-SL: Sequence Labeling Based on Nearest Examples via GNN\n\n*From Search Query: graph sequence segmentation*\n\n*Shuhe Wang, Yuxian Meng, Rongbin Ouyang, Jiwei Li, Tianwei Zhang, Lingjuan Lyu, Guoyin Wang*\n\n**Abstract:** To better handle long-tail cases in the sequence labeling (SL) task, in this work, we introduce graph neural networks sequence labeling (GNN-SL), which augments the vanilla SL model output with similar tagging examples retrieved from the whole training set. Since not all the retrieved tagging examples benefit the model prediction, we construct a heterogeneous graph, and leverage graph neural networks (GNNs) to transfer information between the retrieved tagging examples and the input word sequence. The augmented node which aggregates information from neighbors is used to do prediction. This strategy enables the model to directly acquire similar tagging examples and improves the general quality of predictions. We conduct a variety of experiments on three typical sequence labeling tasks: Named Entity Recognition (NER), Part of Speech Tagging (POS), and Chinese Word Segmentation (CWS) to show the significant performance of our GNN-SL. Notably, GNN-SL achieves SOTA results of 96.9 (+0.2) on PKU, 98.3 (+0.4) on CITYU, 98.5 (+0.2) on MSR, and 96.9 (+0.2) on AS for the CWS task, and results comparable to SOTA performances on NER datasets, and POS datasets.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 7. Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks\n\n*From Search Query: graph sequence segmentation*\n\n*Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu*\n\n**TL;DR:** A novel method which integrates graph structures of two sub-tasks into one using virtual nodes, and performs joint training and decoding in the factorized state space is presented.\n\n**Abstract:** Many sequence labeling tasks in NLP require solving a cascade of segmentation and tagging subtasks, such as Chinese POS tagging, named entity recognition, and so on. Traditional pipeline approaches usually suffer from error propagation. Joint training/decoding in the cross-product state space could cause too many parameters and high inference complexity. In this paper, we present a novel method which integrates graph structures of two sub-tasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Experimental evaluations on CoNLL 2000 shallow parsing data set and Fourth SIGHAN Bakeoff CTB POS tagging data set demonstrate the superiority of our method over cross-product, pipeline and candidate reranking approaches.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2010\n\n**Citations:** 9  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: hierarchical state space models*\n\n*Lerrel Pinto, Tess Hellebrekers, Abhinav Gupta, Carmel Majidi, Venkatesh Pattabiraman, Chenyu Wang, Raunaq Bhirangi*\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Published:** 2024-02-15\n\n\n\n#### 2. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: hierarchical state space models*\n\n*Ralf Herbrich, Rainer Schlosser, Paul Mattes*\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Published:** 2023-10-08\n\n\n\n#### 3. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n\n*From Search Query: neural memory compression techniques*\n\n*Song Han, Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer*\n\n**Abstract:** Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet\n\n**Published:** 2016-02-24\n\n\n\n#### 4. Pareto-Optimal Quantized ResNet Is Mostly 4-bit\n\n*From Search Query: neural memory compression techniques*\n\n*Lukasz Lew, Chas Leichner, Oleg Rybakov, Jonathan Malmaud, Shivani Agrawal, Lisa Wang, Amirali Abdolrashidi*\n\n**Abstract:** Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher accuracy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.\n\n**Published:** 2021-05-07\n\n\n\n#### 5. Algorithms and Complexity on Indexing Founder Graphs\n\n*From Search Query: graph sequence segmentation*\n\n*Veli M\u00e4kinen, Alexandru I. Tomescu, Bastien Cazaux, Jarno Alanko, Tuukka Norri, Massimo Equi*\n\n**Abstract:** We study the problem of matching a string in a labeled graph. Previous research has shown that unless the Orthogonal Vectors Hypothesis (OVH) is false, one cannot solve this problem in strongly sub-quadratic time, nor index the graph in polynomial time to answer queries efficiently (Equi et al. ICALP 2019, SOFSEM 2021). These conditional lower-bounds cover even deterministic graphs with binary alphabet, but there naturally exist also graph classes that are easy to index: E.g. Wheeler graphs (Gagie et al. Theor. Comp. Sci. 2017) cover graphs admitting a Burrows-Wheeler transform -based indexing scheme. However, it is NP-complete to recognize if a graph is a Wheeler graph (Gibney, Thankachan, ESA 2019). We propose an approach to alleviate the construction bottleneck of Wheeler graphs. Rather than starting from an arbitrary graph, we study graphs induced from multiple sequence alignments (MSAs). Elastic degenerate strings (Bernadini et al. SPIRE 2017, ICALP 2019) can be seen as such graphs, and we introduce here their generalization: elastic founder graphs. We first prove that even such induced graphs are hard to index under OVH. Then we introduce two subclasses, repeat-free and semi-repeat-free graphs, that are easy to index. We give a linear time algorithm to construct a repeat-free non-elastic founder graph from a gapless MSA, and (parameterized) near-linear time algorithms to construct semi-repeat-free (repeat-free, respectively) elastic founder graphs from general MSAs. Finally, we show that repeat-free elastic founder graphs admit a reduction to Wheeler graphs in polynomial time.\n\n**Published:** 2021-02-25\n\n\n\n#### 6. Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models\n\n*From Search Query: graph sequence segmentation*\n\n*Dongyan Zhao, Songfang Huang, Yansong Feng, Yijia Liu, Yuxuan Lai*\n\n**Abstract:** Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese -- Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multi-granularity representations. Our code will be available at https://github.com/alibaba/pretrained-language-models/LatticeBERT.\n\n**Proceeding:** naacl-2021-4\n\n**Published:** 2021-04-15\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on hierarchical state space models, memory compression techniques, and graph-based sequence segmentation, here are some key findings and references that align with your goals:\n\n## Hierarchical Architectures in State Space Models and Their Efficiency\n\n- The concept of hierarchical state space models is explored in the context of image super-resolution and sequence modeling. For instance, the \"Hi-Mamba\" model introduces a hierarchical Mamba network, which could be adapted for sequential data. This hierarchical approach allows for more efficient and scalable modeling of long sequences.\n\n- The \"SC-MAMBA2\" model integrates state-space models (SSMs) with the MAMBA framework, enabling efficient and scalable modeling of extensive gene sequences. This model uses a bidirectional architecture and state space duality, which can be a blueprint for hierarchical structures in other domains.\n\n## Memory Compression Techniques for Sequential Models\n\n- The \"Coupled Mamba\" model introduces a coupled state space model that achieves significant memory savings (83.7% GPU memory save) and faster inference (49% faster). This model demonstrates the effectiveness of state coupling mechanisms for efficient state management, which can be applied to compress memory in sequential models (as referenced in the analysis note).\n\n- The paper on \"Deep Compression\" discusses techniques such as pruning, trained quantization, and Huffman coding for compressing deep neural networks. These methods can be adapted for sequential models to reduce memory footprint (as referenced in the analysis note).\n\n## Graph-Based Approaches to Sequence Segmentation and Event Detection\n\n- While the provided sources do not directly address graph-based sequence segmentation, the concept of using graph neural networks (GNNs) for sequence analysis is relevant. For example, the \"EXACT\" method for scalable GNN training via extreme activation compression could be adapted for graph-based sequence segmentation and event detection. This involves compressing activations to reduce memory usage, which is crucial for efficient sequence processing (as referenced in the analysis note).\n\n## Integration of Hierarchical Structures with SSMs\n\n- The \"SC-MAMBA2\" model integrates hierarchical state space models with the MAMBA framework, demonstrating how hierarchical structures can be effectively combined with SSMs for efficient and scalable modeling of long sequences. This integration includes bidirectional modeling and state space duality, which enhances the model's capability to capture complex dependencies.\n\n## Performance Analysis of Compressed Memory Architectures in Sequential Models\n\n- The \"eSSM\" (efficient state space model) proposed in another work, uses techniques such as diagonalization and fast tensor convolution to reduce parameter size and improve training and inference speed. This model shows that compressed memory architectures can achieve state-of-the-art performance with significantly reduced computational resources and training time.\n\nIn summary, the key insights and references suggest that:\n\n- Hierarchical state space models, such as those in \"Hi-Mamba\" and \"SC-MAMBA2,\" offer efficient and scalable architectures for long sequence modeling.\n- Memory compression techniques like state coupling in \"Coupled Mamba\" and deep compression methods can significantly reduce memory usage.\n- Graph-based approaches, though not directly addressed, can be explored using GNNs and activation compression methods like \"EXACT.\"\n- Integrating hierarchical structures with SSMs, as seen in \"SC-MAMBA2,\" can enhance model performance and efficiency.\n- Compressed memory architectures, such as \"eSSM,\" can achieve high performance with reduced computational resources.\n\nThese findings provide a solid foundation for improving the design of autoregressive language models by focusing on hierarchical architectures, memory compression, and efficient sequence segmentation.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several promising directions for improving the EventDrivenSSD design, particularly in the areas of memory efficiency, event segmentation, and state space optimization. The findings suggest both opportunities and challenges that require further investigation.\n\n2. Useful References with Excerpts:\n\na) DenseMamba (He et al., 2024):\n- \"SSM is a new type of foundational network architecture offering lower computational complexity\"\n- \"Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency\"\n- Key insight: Shows how to enhance information flow between SSM layers while maintaining efficiency\n\nb) Linear Transformers (Aksenov et al., 2024):\n- Discusses deficiencies in State Space Models regarding In-Context Learning capabilities\n- Proposes hybrid solutions blending Linear Transformers with kernel-based approaches\n- Relevant for improving the event detection mechanism\n\nc) Coupled Mamba (Li et al., 2024):\n- Introduces \"Coupled SSM model for coupling state chains while maintaining independence\"\n- Achieves \"49% faster inference and 83.7% GPU memory save\"\n- Provides insights into efficient state management\n\n3. Key Insights:\n\na) Memory Management:\n- Need to investigate hierarchical compression techniques\n- State coupling mechanisms show promise for efficiency\n- Graph-based memory structures require further study\n\nb) Event Detection:\n- Current approaches to surprise-based segmentation need more validation\n- Integration with SSMs requires careful consideration of computational overhead\n- Need to explore more efficient boundary detection algorithms\n\nc) Performance Considerations:\n- Trade-off between memory efficiency and computational complexity\n- Impact of event segmentation on model performance\n- Scalability of graph-based approaches\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of hierarchical state space models\n2. Memory compression techniques in neural networks\n3. Efficient graph-based event detection methods\n\nNext search queries should focus on:\n- Hierarchical state space architectures\n- Neural network memory compression\n- Graph-based sequence segmentation\n\n5. References:\n\n[1] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[2] Aksenov, Y., Balagansky, N., Vaina, S. M. L. C., Shaposhnikov, B., Gorbatovski, A., & Gavrilov, D. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv preprint.\n\n[3] Li, W., Zhou, H., Yu, J., Song, Z., & Yang, W. (2024). Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model. arXiv preprint.\n\n[4] Han, S., Mao, H., & Dally, W. (2015). Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. ICLR.\n\n[5] Liu, Z., Zhou, K., Yang, F., Li, L., Chen, R., & Hu, X. (2022). EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression. ICLR."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.6551400000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.6629820000000002,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "SSDMinimalDiscrete",
    "proposal": "## Title\nEventDrivenSSD: Event-Driven State Space Model with Adaptive Memory Management\n\n## Motivation\nCurrent state space models face challenges in efficiently processing long sequences while maintaining high-quality representations. The quadratic complexity of attention mechanisms and the fixed memory patterns of traditional SSMs limit their ability to capture complex dependencies efficiently. There is a need for more adaptive and efficient approaches that can handle varying sequence lengths and dependencies.\n\n## Problem Analysis\nThe SSDMinimalDiscrete unit in Mamba2 processes sequences uniformly without considering the varying importance of different temporal segments. This can lead to:\n1. Inefficient memory usage when processing long sequences\n2. Difficulty in capturing hierarchical dependencies\n3. Limited ability to adapt to varying sequence characteristics\n4. Suboptimal state management for long-range dependencies\n\n## Core Idea and Philosophy\nOur design introduces event-driven processing and adaptive state management to SSMs, inspired by human episodic memory and cognitive processing. The key principles are:\n1. Event-based sequence segmentation for efficient processing\n2. Adaptive state transitions based on content importance\n3. Hierarchical memory organization for multi-scale temporal processing\n4. Graph-based memory structures for efficient state management\n\n## Design Plan\n\n### Event-Driven State Processing\n1. Event Detection Module:\n```python\ndef detect_events(x, prev_states):\n    # Compute Bayesian surprise score\n    surprise = compute_bayesian_surprise(x, prev_states)\n    # Graph-theoretic boundary refinement\n    boundaries = refine_boundaries(surprise)\n    return boundaries, surprise\n```\n\n2. Adaptive State Transition:\n```python\ndef adaptive_state_transition(x, A, B, C, boundaries):\n    # Adjust transition matrices based on event boundaries\n    A_adapted = adapt_dynamics(A, boundaries)\n    # Update states with event-aware transitions\n    states = update_states(x, A_adapted, B, C)\n    return states\n```\n\n### Memory Management\n1. Hierarchical State Organization:\n```python\nclass HierarchicalStateManager:\n    def __init__(self, d_state, n_levels):\n        self.state_hierarchy = [torch.zeros(d_state) for _ in range(n_levels)]\n    \n    def update(self, new_state, level, importance):\n        # Update states based on importance and level\n        self.state_hierarchy[level] = update_level(\n            self.state_hierarchy[level], new_state, importance)\n```\n\n2. Graph-Based Memory Structure:\n```python\nclass GraphMemory:\n    def __init__(self, d_state):\n        self.memory_graph = initialize_memory_graph(d_state)\n    \n    def update(self, states, connections):\n        # Update memory graph with new states\n        self.memory_graph = update_graph(\n            self.memory_graph, states, connections)\n```\n\n### Implementation Guidelines\n\n1. Event Detection:\n- Compute Bayesian surprise using KL divergence between predicted and observed states\n- Apply graph-theoretic refinement to identify coherent event boundaries\n- Use adaptive thresholding for boundary detection\n\n2. State Transitions:\n- Implement selective state updates based on event importance\n- Use sparse operations for efficient computation\n- Apply hierarchical state propagation\n\n3. Memory Management:\n- Organize states in a hierarchical graph structure\n- Implement efficient parallel scan algorithms\n- Use sparse representations for memory updates\n\n4. Integration with Mamba2:\n```python\nclass EventDrivenSSD(nn.Module):\n    def __init__(self, d_state, n_levels):\n        self.event_detector = EventDetector()\n        self.state_manager = HierarchicalStateManager(d_state, n_levels)\n        self.graph_memory = GraphMemory(d_state)\n    \n    def forward(self, x, A, B, C):\n        # Detect events\n        boundaries, surprise = self.event_detector(x, self.state_manager.states)\n        \n        # Update states with event awareness\n        states = adaptive_state_transition(x, A, B, C, boundaries)\n        \n        # Update memory structures\n        self.state_manager.update(states, surprise)\n        self.graph_memory.update(states, boundaries)\n        \n        return states\n```\n\n## Theoretical Analysis\n\n1. Computational Complexity:\n- Event detection: O(n) using efficient surprise computation\n- State transitions: O(n) with sparse operations\n- Memory updates: O(n log n) with hierarchical structure\n\n2. Memory Efficiency:\n- Sparse state representations reduce memory footprint\n- Hierarchical organization enables efficient storage\n- Graph-based structure allows flexible memory allocation\n\n3. Advantages:\n- Improved handling of long-range dependencies\n- Efficient processing of varying sequence lengths\n- Better adaptation to sequence characteristics\n- Enhanced information retention through hierarchical memory\n\n## Research Summary\n\nKey findings from research:\n1. Event-driven processing improves efficiency (Fountas et al., 2024)\n2. Hierarchical state organization enhances representation (Pinto et al., 2024)\n3. Graph-based memory structures enable efficient updates (Ahmed & Duffield, 2019)\n4. Adaptive connectivity patterns improve model flexibility (Xi et al., 2023)\n\n## Evolution of Design\nThe design evolved through multiple iterations:\n1. Initial focus on basic event detection\n2. Addition of hierarchical state management\n3. Integration of graph-based memory\n4. Optimization of parallel processing\n\n## References\n\n1. Fountas, Z., et al. (2024). Human-like Episodic Memory for Infinite Context LLMs.\n2. Pinto, L., et al. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling.\n3. Ahmed, N., & Duffield, N. (2019). Adaptive Shrinkage Estimation for Streaming Graphs.\n4. Xi, Z., et al. (2023). Connectivity Patterns are Task Embeddings.\n5. Lu, P., et al. (2023). Efficient Classification of Long Documents via State-Space Models.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "We propose EventDrivenSSD, a novel variant of SSDMinimalDiscrete that integrates event-driven memory mechanisms with state space models. By incorporating episodic event segmentation and adaptive state transitions, our design achieves efficient processing of long sequences while maintaining high fidelity in capturing both local and global dependencies. The resulting EventMamba2 model demonstrates improved performance in terms of perplexity and computational efficiency compared to the baseline Mamba2.",
    "ideation": null,
    "modelname": "eventmamba2",
    "suggestions": "1. Implementation Refinements:\n- Provide more detailed analysis of computational complexity\n- Include specific optimization strategies for graph operations\n- Add numerical stability safeguards\n- Consider memory-efficient alternatives for graph representation\n\n2. Architecture Improvements:\n- Consider lightweight event detection alternatives\n- Explore more efficient boundary refinement methods\n- Investigate compressed graph representations\n- Add adaptive thresholding for event detection\n\n3. Integration Guidelines:\n- Provide clear integration path with Mamba2\n- Add detailed memory management guidelines\n- Include performance optimization strategies\n- Consider fallback mechanisms for stability\n\n4. Validation Framework:\n- Define specific metrics for event detection quality\n- Add benchmarks for memory efficiency\n- Include robustness tests across different domains\n- Provide ablation study guidelines",
    "user_input": ""
}