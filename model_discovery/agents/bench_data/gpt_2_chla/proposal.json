{
    "variantname": "CompressedHierarchicalLinearAttention",
    "review": "### Comprehensive Review of CompressedHierarchicalLinearAttention Proposal\n\n#### 1. Clarity\nThe proposal for CompressedHierarchicalLinearAttention (CHLA) is clearly articulated, with well-defined objectives and a coherent design plan. The motivation for addressing the limitations of current linear attention mechanisms is well-explained, and the proposal outlines specific modifications, such as dynamic memory compression and hierarchical processing. The inclusion of pseudo-code and mathematical formulations enhances the clarity of the proposed design.\n\n#### 2. Innovation\nThe proposal introduces innovative concepts by integrating dynamic memory compression with hierarchical attention structures into LinearAttention. This approach aims to improve efficiency and scalability for long sequences while maintaining model expressiveness. Compared to existing research, such as LoMA and CHELA, the proposal offers a novel combination of hierarchical processing and adaptive computation, potentially providing a unique contribution to the field.\n\n#### 3. Feasibility\nThe proposed design appears feasible within the given framework. The use of dynamic memory compression and hierarchical processing aligns with existing techniques, and the proposal provides detailed implementation guidelines. However, the complexity of integrating these components may pose challenges, particularly in ensuring seamless cooperation and stability during training.\n\n#### 4. Scalability\nThe design is poised to scale efficiently with larger models and more data. Hierarchical processing and memory management are expected to reduce computational overhead and improve memory utilization, enabling the model to handle longer sequences effectively. The proposal's emphasis on hardware-aware strategies further supports its scalability potential.\n\n#### 5. Accuracy and Robustness\nThe proposed changes are likely to enhance model performance by improving the ability to capture hierarchical patterns and long-range dependencies. The adaptive computation aspect may also contribute to robustness by dynamically adjusting processing granularity based on input characteristics. However, careful tuning and validation are necessary to ensure that these benefits are realized without compromising accuracy.\n\n#### 6. Efficiency\nThe design offers significant potential improvements in computational efficiency and memory usage. By reducing memory footprint through dynamic compression and optimizing data movement with hardware-aware strategies, the proposal addresses key efficiency challenges associated with long-sequence processing.\n\n### Strengths and Concerns\n\n**Strengths:**\n- **Innovative Integration:** The proposal combines dynamic memory compression with hierarchical processing, offering a novel approach to improving linear attention mechanisms.\n- **Efficiency Gains:** The design addresses critical challenges in computational efficiency and memory usage, particularly for long sequences.\n- **Scalability:** The hierarchical structure and adaptive computation support efficient scaling with larger models and datasets.\n\n**Concerns:**\n- **Implementation Complexity:** The integration of dynamic memory compression and hierarchical processing may introduce complexity, potentially affecting training stability and implementation feasibility.\n- **Validation Needs:** Empirical validation is crucial to substantiate the theoretical benefits and ensure that the proposed modifications lead to tangible improvements in performance.\n- **Hardware Optimization:** The practical efficiency gains depend on effective hardware-aware implementations, which require careful optimization and testing.\n\n### Suggestions for Improvement\n- **Empirical Validation:** Conduct comprehensive experiments to validate the proposed enhancements, including benchmarks comparing performance, memory usage, and efficiency against baseline models.\n- **Implementation Details:** Provide more detailed guidelines for integrating dynamic memory compression and hierarchical processing, including potential challenges and solutions.\n- **Hardware Optimization Strategies:** Outline specific strategies for optimizing the design for different hardware architectures, leveraging parallel processing capabilities and efficient tensor operations.\n- **Training Stability:** Investigate potential training stability issues and incorporate techniques such as gradient clipping or adaptive learning rates to ensure robust training dynamics.\n\n### Final Rating\nBased on the comprehensive analysis, the proposal is well-conceived, addresses significant challenges in linear attention mechanisms, and introduces novel contributions through dynamic memory compression and hierarchical strategies. The potential benefits in efficiency, scalability, and expressiveness are substantial, aligning with current research trends. However, empirical validation and careful implementation are crucial to assess the practical impact and address potential concerns.\n\n**Rating: 4.3 out of 5**",
    "search_stack": [
        {
            "ready": false,
            "query": "compressive memory transformer, hierarchical sparse attention, efficient memory access patterns",
            "detail": "Search for specific implementations and techniques related to:\n1. Compressive memory mechanisms in transformers, particularly focusing on memory-efficient designs and compression strategies\n2. Hierarchical attention structures that combine local and global patterns\n3. Hardware-efficient implementations of sparse attention mechanisms\n4. Memory access optimization techniques for transformer architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific implementations and techniques related to:\n1. Compressive memory mechanisms in transformers, particularly focusing on memory-efficient designs and compression strategies\n2. Hierarchical attention structures that combine local and global patterns\n3. Hardware-efficient implementations of sparse attention mechanisms\n4. Memory access optimization techniques for transformer architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Recurrent Memory Transformer (Avg. Score: 0.93)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 0.93)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n#### 2. Reformer: The Efficient Transformer (Avg. Score: 0.36)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.36)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n#### 3. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.27)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.27)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 4. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.26)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.26)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n#### 5. Memorizing Transformers (Avg. Score: 0.19)\n\n*Yuhuai Wu, M. Rabe, DeLesley S. Hutchins, Christian Szegedy*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 138  (*Influential: 15*)\n\n**TL;DR:** It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).\n\n**Abstract:** Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.\n\n##### *Relevant Chunk: No. 7/26 (Score: 0.19)*\n\n```\nIn $A C L, 2019$. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. Angela Fan, Claire Gardent, Chlo\u00e9 Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computational Linguistics, 9:82-99, 2021. Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In ICLR, 2017. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In ICML, 2020. Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. URL/https://arxiv.org/ $\\mathrm{abs} / 2106.06899$. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In ICML, 2020. Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks.\n```\n\n\n\n---\n## Found 14 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: compressive memory transformer, hierarchical sparse attention, efficient memory access patterns\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Birth of a Transformer: A Memory Viewpoint\n\n*From Search Query: compressive memory transformer*\n\n*A. Bietti, Vivien A. Cabannes, Diane Bouchacourt, H. J\u00e9gou, L. Bottou*\n\n**TL;DR:** This work illustrates the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams, and highlights the role of weight matrices as associative memories, as well as providing theoretical insights on how gradients enable their learning during training, and study the roles of data-distributional properties.\n\n**Abstract:** Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 61  (*Influential: 5*)\n\n#### 2. Transformer Memory as a Differentiable Search Index\n\n*From Search Query: compressive memory transformer*\n\n*Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler*\n\n**TL;DR:** The Differentiable Search Index is introduced, a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process.\n\n**Abstract:** In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 207  (*Influential: 62*)\n\n#### 3. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention\n\n*From Search Query: hierarchical sparse attention*\n\n*Yuandong Tian, Yiping Wang, Zhenyu (Allen) Zhang, Beidi Chen, Simon S. Du*\n\n**TL;DR:** JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time.\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 29  (*Influential: 1*)\n\n#### 4. Green Hierarchical Vision Transformer for Masked Image Modeling\n\n*From Search Query: hierarchical sparse attention*\n\n*Lang Huang, Shan You, Mingkai Zheng, Fei Wang, Chen Qian, T. Yamasaki*\n\n**Abstract:** We present an efficient approach for Masked Image Modeling (MIM) with hierarchical Vision Transformers (ViTs), allowing the hierarchical ViTs to discard masked patches and operate only on the visible ones. Our approach consists of three key designs. First, for window attention, we propose a Group Window Attention scheme following the Divide-and-Conquer strategy. To mitigate the quadratic complexity of the self-attention w.r.t. the number of patches, group attention encourages a uniform partition that visible patches within each local window of arbitrary size can be grouped with equal size, where masked self-attention is then performed within each group. Second, we further improve the grouping strategy via the Dynamic Programming algorithm to minimize the overall computation cost of the attention on the grouped patches. Third, as for the convolution layers, we convert them to the Sparse Convolution that works seamlessly with the sparse data, i.e., the visible patches in MIM. As a result, MIM can now work on most, if not all, hierarchical ViTs in a green and efficient way. For example, we can train the hierarchical ViTs, e.g., Swin Transformer and Twins Transformer, about 2.7$\\times$ faster and reduce the GPU memory usage by 70%, while still enjoying competitive performance on ImageNet classification and the superiority on downstream COCO object detection benchmarks. Code and pre-trained models have been made publicly available at https://github.com/LayneH/GreenMIM.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 57  (*Influential: 2*)\n\n#### 5. HEDNet: A Hierarchical Encoder-Decoder Network for 3D Object Detection in Point Clouds\n\n*From Search Query: hierarchical sparse attention*\n\n*Gang Zhang, Junnan Chen, Guohuan Gao, Jianmin Li, Xiaolin Hu*\n\n**TL;DR:** HEDNet is proposed, a hierarchical encoder-decoder network for 3D object detection, which leverages encoder and decoder blocks to capture long-range dependencies among features in the spatial space, particularly for large and distant objects.\n\n**Abstract:** 3D object detection in point clouds is important for autonomous driving systems. A primary challenge in 3D object detection stems from the sparse distribution of points within the 3D scene. Existing high-performance methods typically employ 3D sparse convolutional neural networks with small kernels to extract features. To reduce computational costs, these methods resort to submanifold sparse convolutions, which prevent the information exchange among spatially disconnected features. Some recent approaches have attempted to address this problem by introducing large-kernel convolutions or self-attention mechanisms, but they either achieve limited accuracy improvements or incur excessive computational costs. We propose HEDNet, a hierarchical encoder-decoder network for 3D object detection, which leverages encoder-decoder blocks to capture long-range dependencies among features in the spatial space, particularly for large and distant objects. We conducted extensive experiments on the Waymo Open and nuScenes datasets. HEDNet achieved superior detection accuracy on both datasets than previous state-of-the-art methods with competitive efficiency. The code is available at https://github.com/zhanggang001/HEDNet.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 2*)\n\n#### 6. Learning Memory Access Patterns\n\n*From Search Query: efficient memory access patterns*\n\n*Milad Hashemi, Kevin Swersky, Jamie A. Smith, Grant Ayers, Heiner Litz, Jichuan Chang, Christos Kozyrakis, Parthasarathy Ranganathan*\n\n**TL;DR:** The potential of deep learning to address the von Neumann bottleneck of memory performance is demonstrated, and contemporary prefetching strategies to n-gram models in natural language processing are related, and recurrent neural networks can serve as a drop-in replacement.\n\n**Abstract:** The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 185  (*Influential: 23*)\n\n#### 7. Graph Reordering for Cache-Efficient Near Neighbor Search\n\n*From Search Query: efficient memory access patterns*\n\n*Benjamin Coleman, Santiago Segarra, Anshumali Shrivastava, A. Smola*\n\n**TL;DR:** This paper presents exhaustive experiments applying several reordering algorithms to a leading graph-based near neighbor method based on the HNSW index, and finds that reordering improves the query time by up to 40%, and demonstrates that the time needed to reorder the graph is negligible compared to the time required to construct the index.\n\n**Abstract:** Graph search is one of the most successful algorithmic trends in near neighbor search. Several of the most popular and empirically successful algorithms are, at their core, a simple walk along a pruned near neighbor graph. Such algorithms consistently perform at the top of industrial speed benchmarks for applications such as embedding search. However, graph traversal applications often suffer from poor memory access patterns, and near neighbor search is no exception to this rule. Our measurements show that popular search indices such as the hierarchical navigable small-world graph (HNSW) can have poor cache miss performance. To address this problem, we apply graph reordering algorithms to near neighbor graphs. Graph reordering is a memory layout optimization that groups commonly-accessed nodes together in memory. We present exhaustive experiments applying several reordering algorithms to a leading graph-based near neighbor method based on the HNSW index. We find that reordering improves the query time by up to 40%, and we demonstrate that the time needed to reorder the graph is negligible compared to the time required to construct the index.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 9  (*Influential: 0*)\n\n### 1 related papers from ArXiv\n\n#### 1. An Exploration of Hierarchical Attention Transformers for Efficient Long\n  Document Classification\n\n*From Search Query: hierarchical sparse attention*\n\n*Ilias Chalkidis, Xiang Dai, Manos Fergadiotis, Prodromos Malakasiotis, Desmond Elliott*\n\n**Abstract:** Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.\n\n**Published:** 2022-10-11T15:17:56Z  (*Updated: 2022-10-11T15:17:56Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: compressive memory transformer*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 2. Memory AMP\n\n*From Search Query: compressive memory transformer*\n\n*Brian M. Kurkoski, Shunqi Huang, Lei Liu*\n\n**Abstract:** Approximate message passing (AMP) is a low-cost iterative parameter-estimation technique for certain high-dimensional linear systems with non-Gaussian distributions. AMP only applies to independent identically distributed (IID) transform matrices, but may become unreliable (e.g., perform poorly or even diverge) for other matrix ensembles, especially for ill-conditioned ones. To solve this issue, orthogonal/vector AMP (OAMP/VAMP) was proposed for general right-unitarily-invariant matrices. However, the Bayes-optimal OAMP/VAMP (BO-OAMP/VAMP) requires a high-complexity linear minimum mean square error (MMSE) estimator. This prevents OAMP/VAMP from being used in large-scale systems. To address the drawbacks of AMP and BO-OAMP/VAMP, this paper offers a memory AMP (MAMP) framework based on the orthogonality principle, which ensures that estimation errors in MAMP are asymptotically IID Gaussian. To realize the required orthogonality for MAMP, we provide an orthogonalization procedure for the local memory estimators. In addition, we propose a Bayes-optimal MAMP (BO-MAMP), in which a long-memory matched filter is used for interference suppression. The complexity of BO-MAMP is comparable to AMP. To asymptotically characterize the performance of BO-MAMP, a state evolution is derived. The relaxation parameters and damping vector in BO-MAMP are optimized based on state evolution. Most crucially, the state evolution of the optimized BO-MAMP converges to the same fixed point as that of the high-complexity BO-OAMP/VAMP for all right-unitarily-invariant matrices, and achieves the Bayes optimal MSE predicted by the replica method if its state evolution has a unique fixed point. Finally, simulations are provided to verify the theoretical results' validity and accuracy.\n\n**Published:** 2020-12-20\n\n\n\n#### 3. Dilated Neighborhood Attention Transformer\n\n*From Search Query: hierarchical sparse attention*\n\n*Humphrey Shi, Ali Hassani*\n\n**Abstract:** Transformers are quickly becoming one of the most heavily applied deep learning architectures across modalities, domains, and tasks. In vision, on top of ongoing efforts into plain transformers, hierarchical transformers have also gained significant attention, thanks to their performance and easy integration into existing frameworks. These models typically employ localized attention mechanisms, such as the sliding-window Neighborhood Attention (NA) or Swin Transformer's Shifted Window Self Attention. While effective at reducing self attention's quadratic complexity, local attention weakens two of the most desirable properties of self attention: long range inter-dependency modeling, and global receptive field. In this paper, we introduce Dilated Neighborhood Attention (DiNA), a natural, flexible and efficient extension to NA that can capture more global context and expand receptive fields exponentially at no additional cost. NA's local attention and DiNA's sparse global attention complement each other, and therefore we introduce Dilated Neighborhood Attention Transformer (DiNAT), a new hierarchical vision transformer built upon both. DiNAT variants enjoy significant improvements over strong baselines such as NAT, Swin, and ConvNeXt. Our large model is faster and ahead of its Swin counterpart by 1.6% box AP in COCO object detection, 1.4% mask AP in COCO instance segmentation, and 1.4% mIoU in ADE20K semantic segmentation. Paired with new frameworks, our large variant is the new state of the art panoptic segmentation model on COCO (58.5 PQ) and ADE20K (49.4 PQ), and instance segmentation model on Cityscapes (45.1 AP) and ADE20K (35.4 AP) (no extra data). It also matches the state of the art specialized semantic segmentation models on ADE20K (58.1 mIoU), and ranks second on Cityscapes (84.5 mIoU) (no extra data).\n\n**Published:** 2022-09-29\n\n\n\n#### 4. Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing\n\n*From Search Query: hierarchical sparse attention*\n\n*Zhenghua Li, Lijie Wang, Kun Wu, Haifeng Wang, Min Zhang, Hua Wu, Xinyan Xiao, Ao Zhang*\n\n**Abstract:** Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.\n\n**Proceeding:** emnlp-2021-11\n\n**Published:** 2021-03-03\n\n\n\n#### 5. FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU\n\n*From Search Query: efficient memory access patterns*\n\n*Ce Zhang, Ion Stoica, Christopher R\u00e9, Percy Liang, Joseph E. Gonzalez, Clark Barrett, Beidi Chen, Zhiqiang Xie, Daniel Y. Fu, Max Ryabinin, Zhuohan Li, Binhang Yuan, Lianmin Zheng, Ying Sheng*\n\n**Abstract:** The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen\n\n**Published:** 2023-03-13\n\n\n\n#### 6. Large Memory Layers with Product Keys\n\n*From Search Query: efficient memory access patterns*\n\n*Herv\u00e9 J\u00e9gou, Marc'Aurelio Ranzato, Alexandre Sablayrolles, Ludovic Denoyer, Guillaume Lample*\n\n**Abstract:** This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.\n\n**Conference:** large-memory-layers-with-product-keys-1\n\n**Published:** 2019-07-10\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, focusing on the specified areas of compressive memory mechanisms, hierarchical attention structures, hardware-efficient implementations of sparse attention, and memory access optimization, here are some relevant findings and techniques:\n\n## Compressive Memory Mechanisms in Transformers\n\n- The concept of **Neural Attention Memory Models (NAMMs)** introduced in the paper \"An Evolved Universal Transformer Memory\" is particularly relevant. NAMMs optimize the latent memory KV cache of transformers, enabling efficient management of long-context information. This approach reduces the number of retained tokens in the KV cache, leading to both performance and efficiency gains. It also allows for zero-shot transfer of NAMMs to other transformer models, including those with different architectures and input modalities.\n\n## Hierarchical Attention Structures\n\n- **Hierarchical sparse attention mechanisms** can be inspired by the \"BP-Transformer\" approach, which uses binary partitioning to model long-range context. This method balances computation complexity and model capacity by controlling the density of attention connections, resulting in \\(O(k \\cdot n \\log(n/k))\\) connections. This hierarchical structure can be combined with other attention mechanisms to improve efficiency and capacity.\n\n- The **Infini-attention** technique mentioned in the analysis incorporates both masked local attention and long-term linear attention mechanisms within a single Transformer block. This approach can be seen as a hierarchical structure that combines local and global attention patterns, which is crucial for efficient and effective attention mechanisms.\n\n## Hardware-Efficient Implementations of Sparse Attention\n\n- **Tensor Parallelism** and **Model Parallelism** are advanced techniques that can be applied to optimize the training and inference of large transformer models. By distributing the model components across multiple GPUs, these methods reduce memory requirements and improve computational speed. This is particularly beneficial for sparse attention mechanisms, as it allows for the efficient utilization of memory resources and faster processing times.\n\n- **Memristor-based accelerators** for transformer self-attention, as described in the Nature article, offer a hardware-efficient implementation. This approach uses memristor crossbar arrays to perform matrix operations in an analog manner, significantly accelerating transformer self-attention calculations and reducing power consumption. This can be particularly useful for sparse attention mechanisms by optimizing the matrix multiplication operations involved.\n\n## Memory Access Optimization Techniques\n\n- **Memory-augmented Transformers (Memformers)** can implement linear first-order optimization methods by retaining intermediate attention values across layers through memory registers. This approach enables more complex computations and optimization methods, which can be optimized for memory access patterns. By storing suitable intermediate attention values, Memformers can efficiently learn advanced optimization algorithms, which can be beneficial for optimizing memory access in transformer architectures.\n\n- **NAMMs** also optimize memory access by managing the KV cache efficiently. By conditioning the memory on features constructed from the attention matrix, NAMMs ensure that each latent level and attention head focuses on the most relevant information, reducing the need for full-context models and thus optimizing memory access.\n\n## Summary\n\n- **Compressive Memory Mechanisms**: NAMMs and Infini-attention provide efficient ways to manage long-context information and reduce memory requirements.\n- **Hierarchical Attention Structures**: BP-Transformer and Infini-attention offer hierarchical structures that balance efficiency and capacity.\n- **Hardware-Efficient Implementations**: Tensor parallelism, model parallelism, and memristor-based accelerators optimize the computational and memory efficiency of sparse attention mechanisms.\n- **Memory Access Optimization**: Memformers and NAMMs optimize memory access patterns by retaining and managing intermediate attention values efficiently.\n\nThese techniques and implementations can significantly improve the design of autoregressive language models by enhancing memory efficiency, computational speed, and adaptability to different input patterns.\n",
            "analysis": "### Summary of Initial Analysis\n\nThe seed design \"gpt_2_ala\" introduces AdaptiveLinearAttention (ALA) which combines linear attention with meta-learning and context-adaptive encoding. While innovative, there are several areas that could be further improved or explored differently:\n\n1. Memory Management:\n- The current design lacks explicit mechanisms for efficient memory management\n- No clear strategy for handling very long sequences\n- Memory bottlenecks could limit scalability\n\n2. Computational Efficiency:\n- Meta-learning components may introduce significant overhead\n- Linear attention implementation could be further optimized\n- Room for improvement in hardware utilization\n\n3. Adaptability:\n- Current context adaptation is limited to positional encoding\n- No explicit mechanism for handling different types of input patterns\n- Could benefit from more sophisticated adaptation strategies\n\n### Key References and Excerpts\n\n1. From \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\":\n```\nA key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\n```\nThis suggests potential for combining different types of attention mechanisms within a single block.\n\n2. From \"BP-Transformer: Modelling Long-Range Context via Binary Partitioning\":\n```\nBPT yields O(k\u00b7n log(n/k)) connections where k is a hyperparameter to control the density of attention. BPT has a good balance between computation complexity and model capacity.\n```\nThis indicates the possibility of using hierarchical structures to balance efficiency and capacity.\n\n3. From \"Exploring Length Generalization in Large Language Models\":\n```\nWe first establish that naively finetuning transformers on length generalization tasks shows significant generalization deficiencies independent of model scale.\n```\nThis highlights the importance of architectural innovations for length generalization.\n\n4. From \"RRWKV: Capturing Long-range Dependencies in RWKV\":\n```\nThe paper devises the Retrospected Receptance Weighted Key Value architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\n```\nThis suggests potential benefits of incorporating retrospection mechanisms.\n\n### Key Insights and Analysis\n\n1. Memory Architecture:\n- Compressive memory mechanisms (from Infini-attention) could be integrated with linear attention\n- Hierarchical structures (from BP-Transformer) could improve efficiency\n- Retrospection mechanisms (from RRWKV) could enhance information retention\n\n2. Attention Mechanisms:\n- Combining local and global attention patterns\n- Using hierarchical attention structures\n- Incorporating efficient memory-based attention\n\n3. Length Generalization:\n- Need for explicit architectural support\n- Importance of efficient memory management\n- Role of position encoding and context handling\n\n4. Hardware Efficiency:\n- Potential for structured sparsity\n- Importance of memory access patterns\n- Need for parallelizable computations\n\n### Future Search Plan\n\n1. Need to investigate:\n- Specific implementations of compressive memory mechanisms\n- Efficient combinations of different attention patterns\n- Hardware-efficient implementations of hierarchical structures\n\n2. Search Keywords for Next Round:\n- \"compressive memory transformer\"\n- \"hierarchical sparse attention\"\n- \"efficient memory access patterns\"\n\n### References\n\n[1] Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv:2401.04677.\n\n[2] Ye, Z., Guo, Q., Gan, Q., Qiu, X., & Zhang, Z. (2019). BP-Transformer: Modelling Long-Range Context via Binary Partitioning. arXiv:1911.04070.\n\n[3] Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., ... & Neyshabur, B. (2022). Exploring Length Generalization in Large Language Models. Neural Information Processing Systems.\n\n[4] Wang, L. (2023). RRWKV: Capturing Long-range Dependencies in RWKV. arXiv:2305.13048."
        },
        {
            "ready": false,
            "query": "transformer memory compression techniques, efficient hierarchical attention implementation, memory-efficient transformer variants",
            "detail": "Search for specific techniques and implementations related to:\n1. Memory compression methods in transformers, including both lossy and lossless approaches\n2. Efficient implementations of hierarchical attention mechanisms, focusing on computational and memory efficiency\n3. Memory-efficient transformer architectures and their key design principles\n4. Hardware-aware optimization techniques for transformer memory management",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific techniques and implementations related to:\n1. Memory compression methods in transformers, including both lossy and lossless approaches\n2. Efficient implementations of hierarchical attention mechanisms, focusing on computational and memory efficiency\n3. Memory-efficient transformer architectures and their key design principles\n4. Hardware-aware optimization techniques for transformer memory management\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Recurrent Memory Transformer (Avg. Score: 0.92)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 0.92)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n#### 2. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.86)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.98)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n##### *Relevant Chunk: No. 8/16 (Score: 0.74)*\n\n```\nSee Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.\n```\n\n#### 3. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.64)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.64)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.46)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 23/41 (Score: 0.46)*\n\n```\nhutter1. net, 2012. Kao, S.-C., Subramanian, S., Agrawal, G., Yazdanbakhsh, A., and Krishna, T. FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks. In ASPLOS, 2023. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In ICML, 2020. Kim, S., Mangalam, K., Malik, J., Mahoney, M. W., Gholami, A., and Keutzer, K. Big Little Transformer Decoder. arXiv preprint arXiv:2302.07863, 2023. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient Memory Management for Large Language Model Serving with PagedAttention. In SOSP, 2023.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: transformer memory compression techniques, efficient hierarchical attention implementation, memory-efficient transformer variants\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Kronecker Decomposition for GPT Compression\n\n*From Search Query: transformer memory compression techniques*\n\n*A. Edalati, Marzieh S. Tahaei, Ahmad Rashid, V. Nia, J. Clark, Mehdi Rezagholizadeh*\n\n**TL;DR:** This work uses Kronecker decomposition to compress the linear mappings of the GPT-2 model and demonstrates that with more efficient pre-training and similar number of parameters, the KnGPT2 outperforms the existing DistilG PT2 model significantly.\n\n**Abstract:** GPT is an auto-regressive Transformer-based pre-trained language model which has attracted a lot of attention in the natural language processing (NLP) domain. The success of GPT is mostly attributed to its pre-training on huge amount of data and its large number of parameters. Despite the superior performance of GPT, this overparameterized nature of GPT can be very prohibitive for deploying this model on devices with limited computational power or memory. This problem can be mitigated using model compression techniques; however, compressing GPT models has not been investigated much in the literature. In this work, we use Kronecker decomposition to compress the linear mappings of the GPT-2 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on the Kronecker decomposed version of the GPT-2 model and then is undergone a very light pre- training on only a small portion of the training data with intermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is fine-tuned on downstream tasks using ILKD as well. We evaluate our model on both language modeling and General Language Understanding Evaluation benchmark tasks and show that with more efficient pre-training and similar number of parameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 28  (*Influential: 4*)\n\n#### 2. A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models\n\n*From Search Query: transformer memory compression techniques*\n\n*Krithika Ramesh, Arnav Chavan, Shrey Pandit, Sunayana Sitaram*\n\n**TL;DR:** This analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification and indicates that compression strategies can have an adverse effect on fairness measures.\n\n**Abstract:** Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 0*)\n\n#### 3. Unified Visual Transformer Compression\n\n*From Search Query: transformer memory compression techniques*\n\n*Shixing Yu, Tianlong Chen, Jiayi Shen, Huan Yuan, Jianchao Tan, Sen Yang, Ji Liu, Zhangyang Wang*\n\n**TL;DR:** This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation, and formulate a budget-constrained, end-to-end optimization framework.\n\n**Abstract:** Vision transformers (ViTs) have gained popularity recently. Even without customized image operators such as convolutions, ViTs can yield competitive performance when properly trained on massive data. However, the computational overhead of ViTs remains prohibitive, due to stacking multi-head self-attention modules and else. Compared to the vast literature and prevailing success in compressing convolutional neural networks, the study of Vision Transformer compression has also just emerged, and existing works focused on one or two aspects of compression. This paper proposes a unified ViT compression framework that seamlessly assembles three effective techniques: pruning, layer skipping, and knowledge distillation. We formulate a budget-constrained, end-to-end optimization framework, targeting jointly learning model weights, layer-wise pruning ratios/masks, and skip configurations, under a distillation loss. The optimization problem is then solved using the primal-dual algorithm. Experiments are conducted with several ViT variants, e.g. DeiT and T2T-ViT backbones on the ImageNet dataset, and our approach consistently outperforms recent competitors. For example, DeiT-Tiny can be trimmed down to 50\\% of the original FLOPs almost without losing accuracy. Codes are available online:~\\url{https://github.com/VITA-Group/UVC}.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 80  (*Influential: 10*)\n\n#### 4. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: efficient hierarchical attention implementation*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: efficient hierarchical attention implementation*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 6. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences\n\n*From Search Query: efficient hierarchical attention implementation*\n\n*Zhenhai Zhu, Radu Soricut*\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 35  (*Influential: 7*)\n\n#### 7. When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants\n\n*From Search Query: memory-efficient transformer variants*\n\n*Anuj Diwan, Eunsol Choi, David Harwath*\n\n**TL;DR:** This work identifies input length thresholds at which efficient Transformer variants become more efficient than vanilla models, using a variety of efficiency metrics (latency, throughput, and memory) and introduces L-HuBERT, a novel local-attention variant of a self-supervised speech model.\n\n**Abstract:** We present the first unified study of the efficiency of self-attention-based Transformer variants spanning text, speech and vision. We identify input length thresholds (tipping points) at which efficient Transformer variants become more efficient than vanilla models, using a variety of efficiency metrics (latency, throughput, and memory). To conduct this analysis for speech, we introduce L-HuBERT, a novel local-attention variant of a self-supervised speech model. We observe that these thresholds are (a) much higher than typical dataset sequence lengths and (b) dependent on the metric and modality, showing that choosing the right model depends on modality, task type (long-form vs. typical context) and resource constraints (time vs. memory). By visualising the breakdown of the computational costs for transformer components, we also show that non-self-attention components exhibit significant computational costs. We release our profiling toolkit at https://github.com/ajd12342/profiling-transformers .\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 8. Linearizing Transformer with Key-Value Memory\n\n*From Search Query: memory-efficient transformer variants*\n\n*Yizhe Zhang, Deng Cai*\n\n**TL;DR:** It is demonstrated that Memsizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling.\n\n**Abstract:** Efficient transformer variants with linear time complexity have been developed to mitigate the quadratic computational overhead of the vanilla transformer. Among them are low-rank projection methods such as Linformer and kernel-based Transformers. Despite their unique merits, they usually suffer from a performance drop comparing with the vanilla transformer on many sequence generation tasks, and often fail to obtain computation gain when the generation is short. We propose Memsizer, an approach towards closing the performance gap while improving the efficiency even with short generation. It projects the source sequences into lower dimension representations like Linformer, while enjoying efficient recurrent-style incremental computation similar to kernel-based transformers. This yields linear computation time and constant memory complexity at inference time. Memsizer also employs a lightweight multi-head mechanism which renders the computation as light as a single-head model. We demonstrate that Memsizer provides an improved balance between efficiency and accuracy over the vanilla transformer and other efficient transformer variants in three typical sequence generation tasks, including machine translation, abstractive text summarization, and language modeling.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 9. Trainable Transformer in Transformer\n\n*From Search Query: memory-efficient transformer variants*\n\n*A. Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora*\n\n**TL;DR:** This work proposes an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models), and introduces innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulateand fine-Tune a 125 million parameter transformer model within a single forward pass.\n\n**Abstract:** Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n\n*From Search Query: transformer memory compression techniques*\n\n*Siddharth Gopal, Manaal Faruqui, Tsendsuren Munkhdalai*\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n**Published:** 2024-04-10\n\n\n\n#### 2. Extreme Compression of Large Language Models via Additive Quantization\n\n*From Search Query: transformer memory compression techniques*\n\n*Dan Alistarh, Artem Babenko, Elias Frantar, Denis Kuznedelev, Andrei Panferov, Vage Egiazarian*\n\n**Abstract:** The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of \"extreme\" LLM compression-defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter-from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.\n\n**Published:** 2024-01-11\n\n\n\n#### 3. RACER: Rapid Collaborative Exploration with a Decentralized Multi-UAV System\n\n*From Search Query: efficient hierarchical attention implementation*\n\n*Anonymous*\n\n**Abstract:** Although the use of multiple Unmanned Aerial Vehicles (UAVs) has great potential for fast autonomous exploration, it has received far too little attention. In this paper, we present RACER, a RApid Collaborative ExploRation approach using a fleet of decentralized UAVs. To effectively dispatch the UAVs, a pairwise interaction based on an online hgrid space decomposition is used. It ensures that all UAVs simultaneously explore distinct regions, using only asynchronous and limited communication. Further, we optimize the coverage paths of unknown space and balance the workloads partitioned to each UAV with a Capacitated Vehicle Routing Problem(CVRP) formulation. Given the task allocation, each UAV constantly updates the coverage path and incrementally extracts crucial information to support the exploration planning. A hierarchical planner finds exploration paths, refines local viewpoints and generates minimum-time trajectories in sequence to explore the unknown space agilely and safely. The proposed approach is evaluated extensively, showing high exploration efficiency, scalability and robustness to limited communication. Furthermore, for the first time, we achieve fully decentralized collaborative exploration with multiple UAVs in real world. We will release our implementation as an open-source package.\n\n**Published:** 2022-09-18\n\n\n\n#### 4. Focal Modulation Networks\n\n*From Search Query: efficient hierarchical attention implementation*\n\n*Jianfeng Gao, Lu Yuan, Xiyang Dai, Chunyuan Li, Jianwei Yang*\n\n**Abstract:** We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3. Code and checkpoints are available at https://github.com/microsoft/FocalNet.\n\n**Published:** 2022-03-22\n\n\n\n#### 5. An Attention Free Transformer\n\n*From Search Query: memory-efficient transformer variants*\n\n*Josh Susskind, Ruixiang Zhang, Hanlin Goh, Chen Huang, Nitish Srivastava, Walter Talbott, Shuangfei Zhai*\n\n**Abstract:** We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.\n\n**Conference:** an-attention-free-transformer\n\n**Published:** 2021-05-28\n\n\n\n#### 6. Three things everyone should know about Vision Transformers\n\n*From Search Query: memory-efficient transformer variants*\n\n*Herv\u00e9 J\u00e9gou, Jakob Verbeek, Alaaeldin El-Nouby, Matthieu Cord, Hugo Touvron*\n\n**Abstract:** After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing state-of-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.\n\n**Published:** 2022-03-18\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on memory efficiency, hierarchical attention mechanisms, and hardware-aware optimizations, here are some key findings and techniques from the provided sources and additional insights:\n\n### Memory Compression Techniques in Transformers\n\n1. **KV-Compress**:\n   - This technique involves compressing the key-value (KV) caches in attention-based models by applying variable compression rates per attention head. It exploits the heterogeneity of attention heads, applying higher compression to less important heads and lower compression to more important ones. Additionally, it uses paging to divide the KV cache into smaller chunks, reducing the memory footprint significantly without compromising model performance.\n\n2. **CompAct**:\n   - This method reduces peak memory utilization during the training of large language models (LLMs) by storing low-rank, compressed activations instead of full activation tensors. This approach compresses the compute graph and reduces the size of optimizer states, leading to significant memory savings of up to 25-30% for pretraining and 50% for fine-tuning.\n\n### Efficient Hierarchical Attention Implementation\n\n1. **Hierarchical Attention Mechanisms**:\n   - Implementing hierarchical attention can help in capturing long-range dependencies efficiently. This can be achieved by combining local and global attention patterns. For example, using hierarchical sparse attention mechanisms can focus on salient tokens first and then on less salient ones, improving computational and memory efficiency.\n\n2. **Memory-Augmented Attention**:\n   - This approach enhances the capabilities of attention mechanisms by incorporating memory-augmented structures. This can involve using compressed past memories for long-range sequence learning, as seen in the Compressive Transformer, which achieves efficient long-range dependencies while reducing memory usage.\n\n### Memory-Efficient Transformer Architectures\n\n1. **Compressive Transformers**:\n   - The Compressive Transformer is an example of a memory-efficient architecture that compresses past memories for long-range sequence learning. This model achieves significant reductions in memory usage while maintaining performance, such as achieving 17.1 perplexity per word (ppl) and 0.97 bits per character (bpc).\n\n2. **Dynamic Memory Management**:\n   - Dynamic memory management based on token importance can be implemented. Techniques like KV-Compress and CompAct show how memory can be managed efficiently by compressing less important information and optimizing memory access patterns.\n\n### Hardware-Aware Optimization Techniques\n\n1. **Memristor-Based Accelerators**:\n   - Leveraging memristor-based in-memory computing can significantly accelerate transformer self-attention operations. This hardware approach reduces latency and power consumption, demonstrating a 10\u00d7 acceleration compared to digital counterparts. It also optimizes memory access by storing key and value matrices in memristor crossbars.\n\n2. **Cache-Efficient Designs**:\n   - Optimizing memory access patterns and using cache-friendly memory layouts can enhance hardware efficiency. Techniques like paging the KV cache and storing compressed activations can reduce the memory footprint and improve computational efficiency.\n\n### Key Design Principles and Integration Opportunities\n\n1. **Combining Compressive Memory and Linear Attention**:\n   - Integrating compressive memory techniques with linear attention mechanisms can enhance both memory efficiency and attention capabilities. This can involve using hierarchical structures for both memory and attention, ensuring that important information is preserved while reducing overall memory usage.\n\n2. **Structured Sparsity and Optimized Memory Access**:\n   - Implementing structured sparsity in the model and optimizing memory access patterns can further enhance hardware efficiency. This includes using random projection matrices for compression and ensuring that memory access is cache-friendly.\n\nBy incorporating these techniques, researchers can develop more efficient, scalable, and robust autoregressive language models that optimize memory usage, computational efficiency, and hardware utilization.\n",
            "analysis": "### Summary of Analysis\n\nThe search results reveal several promising directions for improving the seed design's AdaptiveLinearAttention (ALA) unit:\n\n1. Memory Management:\n- Compressive memory mechanisms can reduce memory footprint while preserving important information\n- Hierarchical structures enable efficient long-range dependencies\n- Memory access patterns can be optimized through careful design\n\n2. Attention Mechanisms:\n- Combining local and global attention patterns\n- Using hierarchical sparse attention for better efficiency\n- Implementing memory-augmented attention for enhanced capabilities\n\n3. Hardware Efficiency:\n- Memory access optimization is crucial for performance\n- Cache-efficient designs can significantly improve speed\n- Structured sparsity can enhance hardware utilization\n\n### Useful References with Excerpts\n\n1. From \"Birth of a Transformer\":\n```\nWe highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.\n```\nThis suggests potential improvements in how memory is structured and learned.\n\n2. From \"Compressive Transformers\":\n```\nWe present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning... achieving 17.1 ppl and 0.97 bpc respectively.\n```\nShows the effectiveness of compressed memory representations.\n\n3. From \"JoMA\":\n```\nJoMA predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations.\n```\nProvides insights into attention dynamics that could inform design.\n\n4. From \"Learning Memory Access Patterns\":\n```\nWe demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance... focusing on learning memory access patterns.\n```\nSuggests ways to optimize memory access.\n\n### Key Insights and Analysis\n\n1. Memory Architecture:\n- Compressive memory can be implemented using learned compression mechanisms\n- Hierarchical organization can improve efficiency\n- Dynamic memory management based on token importance\n\n2. Attention Design:\n- Combining sparse and dense attention phases\n- Using hierarchical structures for different ranges\n- Implementing efficient memory access patterns\n\n3. Hardware Considerations:\n- Cache-friendly memory layouts\n- Structured sparsity for hardware efficiency\n- Optimized memory access patterns\n\n4. Integration Opportunities:\n- Combining compressive memory with linear attention\n- Using hierarchical structures for both memory and attention\n- Implementing efficient memory access patterns\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific compression techniques for transformer memory\n2. Implementation details of hierarchical attention mechanisms\n3. Memory-efficient attention variants\n\nNext search queries:\n1. \"transformer memory compression techniques\"\n2. \"efficient hierarchical attention implementation\"\n3. \"memory-efficient transformer variants\"\n\n### References\n\n[1] Bietti, A., Cabannes, V. A., Bouchacourt, D., J\u00e9gou, H., & Bottou, L. (2023). Birth of a Transformer: A Memory Viewpoint. Neural Information Processing Systems.\n\n[2] Jayakumar, S. M., Potapenko, A., Rae, J. W., & Lillicrap, T. P. (2019). Compressive Transformers for Long-Range Sequence Modelling. ICLR 2020.\n\n[3] Tian, Y., Wang, Y., Zhang, Z., Chen, B., & Du, S. S. (2023). JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention. International Conference on Learning Representations.\n\n[4] Hashemi, M., Swersky, K., Smith, J. A., Ayers, G., Litz, H., Chang, J., ... & Ranganathan, P. (2018). Learning Memory Access Patterns. International Conference on Machine Learning.\n\n[5] Coleman, B., Segarra, S., Shrivastava, A., & Smola, A. (2021). Graph Reordering for Cache-Efficient Near Neighbor Search. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "data-dependent gating transformer, hardware-efficient hierarchical attention, memory compression attention integration",
            "detail": "Search for specific techniques and implementations related to:\n1. Data-dependent gating mechanisms in transformers, focusing on adaptive computation and efficiency\n2. Hardware-efficient implementations of hierarchical attention, particularly for memory access patterns\n3. Integration methods for combining memory compression with attention mechanisms\n4. Performance analysis and optimization techniques for memory-efficient transformers",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific techniques and implementations related to:\n1. Data-dependent gating mechanisms in transformers, focusing on adaptive computation and efficiency\n2. Hardware-efficient implementations of hierarchical attention, particularly for memory access patterns\n3. Integration methods for combining memory compression with attention mechanisms\n4. Performance analysis and optimization techniques for memory-efficient transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.90)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 25/29 (Score: 0.90)*\n\n```\nArXiv, abs/2405.05254, 2024b. URL https://api. semanticscholar org/CorpusID:269626143. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention in transformer models, 2021a. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview net/forum?id=qVyeW-grC2k\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers \\& distillation through attention. In International Conference on Machine Learning, volume 139, pp. 10347-10357, July 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Jos van der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. CoRR, abs/1804.04849, 2018. Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M. Rush. Pretraining without attention. CoRR, abs/2212.10544, 2022. Songlin Yang and Yu Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, January 2024. URL https://github.com/ sustcsonglin/flash-linear-attention\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. CoRR, abs/2312.06635, 2023. doi: 10.48550/ARXIV.2312.06635. URL https://doi.org/10.48550/arXiv.2312.06635. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length.\n```\n\n#### 2. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.78)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.78)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 3. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.71)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 14/40 (Score: 0.71)*\n\n```\non pp. 1-4, 12, 16, 19, 29, 30). [13] Songlin Yang et al. \"Gated Linear Attention Transformers with Hardware-Efficient Training\". In: arXiv preprint arXiv:2312.06635 (2023) (cit.\n```\n\n#### 4. Memorizing Transformers (Avg. Score: 0.66)\n\n*Yuhuai Wu, M. Rabe, DeLesley S. Hutchins, Christian Szegedy*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 138  (*Influential: 15*)\n\n**TL;DR:** It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).\n\n**Abstract:** Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.\n\n##### *Relevant Chunk: No. 7/26 (Score: 0.66)*\n\n```\nIn $A C L, 2019$. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. Angela Fan, Claire Gardent, Chlo\u00e9 Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computational Linguistics, 9:82-99, 2021. Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In ICLR, 2017. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In ICML, 2020. Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. URL/https://arxiv.org/ $\\mathrm{abs} / 2106.06899$. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In ICML, 2020. Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks.\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.36)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.36)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: data-dependent gating transformer, hardware-efficient hierarchical attention, memory compression attention integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Learning Latent Structures in Network Games via Data-Dependent Gated-Prior Graph Variational Autoencoders\n\n*From Search Query: data-dependent gating transformer*\n\n*Xue Yu, Muchen Li, Yan Leng, Renjie Liao*\n\n**Abstract:** None\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. GNOT: A General Neural Operator Transformer for Operator Learning\n\n*From Search Query: data-dependent gating transformer*\n\n*Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, Jian Song*\n\n**TL;DR:** This work proposes a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators that is highly flexible to handle multiple input functions and irregular meshes and introduces a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems.\n\n**Abstract:** Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improvement compared with alternative methods. Our code and data are publicly available at \\url{https://github.com/thu-ml/GNOT}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 97  (*Influential: 15*)\n\n#### 3. Taming Sparsely Activated Transformer with Stochastic Experts\n\n*From Search Query: data-dependent gating transformer*\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, T. Zhao, Jianfeng Gao*\n\n**TL;DR:** This paper proposes a new expert-based model, THOR (Transformer witH StOchastic ExpeRts), which is trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions.\n\n**Abstract:** Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can easily scale to have outrageously large amounts of parameters without significant increase in computational cost. However, SAMs are reported to be parameter inefficient such that larger models do not always lead to better performance. While most on-going research focuses on improving SAMs models by exploring methods of routing inputs to experts, our analysis reveals that such research might not lead to the solution we expect, i.e., the commonly-used routing methods based on gating mechanisms do not work better than randomly routing inputs to experts. In this paper, we propose a new expert-based model, THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models, such as the Switch Transformer, experts in THOR are randomly activated for each input during training and inference. THOR models are trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions. We validate the effectiveness of THOR on machine translation tasks. Results show that THOR models are more parameter efficient in that they significantly outperform the Transformer and MoE models across various settings. For example, in multilingual translation, THOR outperforms the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as that of a state-of-the-art MoE model that is 18 times larger. Our code is publicly available at: https://github.com/microsoft/Stochastic-Mixture-of-Experts.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 90  (*Influential: 13*)\n\n#### 4. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hardware-efficient hierarchical attention*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n#### 5. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware-efficient hierarchical attention*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 6. LoCoCo: Dropping In Convolutions for Long Context Compression\n\n*From Search Query: memory compression attention integration*\n\n*Ruisi Cai, Yuandong Tian, Zhangyang Wang, Beidi Chen*\n\n**Abstract:** This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward\"drop-in\"integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 7. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: memory compression attention integration*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 8. Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time\n\n*From Search Query: memory compression attention integration*\n\n*Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali Shrivastava*\n\n**TL;DR:** This work hypothesizes the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations, and proposes Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model.\n\n**Abstract:** Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5X without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20X compression.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 109  (*Influential: 11*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: data-dependent gating transformer*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: data-dependent gating transformer*\n\n*Yoon Kim, Rameswar Panda, Yikang Shen, Bailin Wang, Songlin Yang*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Published:** 2023-12-11\n\n\n\n#### 3. HGRN2: Gated Linear RNNs with State Expansion\n\n*From Search Query: hardware-efficient hierarchical attention*\n\n*Yiran Zhong, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Songlin Yang, Zhen Qin*\n\n**Abstract:** Hierarchically gated linear RNN (HGRN, \\citealt{HGRN}) has demonstrated competitive training speed and performance in language modeling while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, limiting its expressiveness. To address this issue, we introduce a simple outer product-based state expansion mechanism, which significantly enlarges the recurrent state size without introducing any additional parameters. This enhancement also provides a linear attention interpretation for HGRN2, enabling hardware-efficient training. Our extensive experiments verify the advantage of HGRN2 over HGRN consistently across different settings and competitive with other recurrent models.\n\n**Published:** 2024-04-11\n\n\n\n#### 4. Bayesian Integration of Information Using Top-Down Modulated WTA Networks\n\n*From Search Query: hardware-efficient hierarchical attention*\n\n*Johan Kwisthout, Leila Bagheriye, Otto van der Himst*\n\n**Abstract:** Winner Take All (WTA) circuits a type of Spiking Neural Networks (SNN) have been suggested as facilitating the brain's ability to process information in a Bayesian manner. Research has shown that WTA circuits are capable of approximating hierarchical Bayesian models via Expectation Maximization (EM). So far, research in this direction has focused on bottom up processes. This is contrary to neuroscientific evidence that shows that, besides bottom up processes, top down processes too play a key role in information processing by the human brain. Several functions ascribed to top down processes include direction of attention, adjusting for expectations, facilitation of encoding and recall of learned information, and imagery. This paper explores whether WTA circuits are suitable for further integrating information represented in separate WTA networks. Furthermore, it explores whether, and under what circumstances, top down processes can improve WTA network performance with respect to inference and learning. The results show that WTA circuits are capable of integrating the probabilistic information represented by other WTA networks, and that top down processes can improve a WTA network's inference and learning performance. Notably, it is able to do this according to key neuromorphic principles, making it ideal for low-latency and energy efficient implementation on neuromorphic hardware.\n\n**Published:** 2023-08-29\n\n\n\n#### 5. Extremely Low Footprint End-to-End ASR System for Smart Device\n\n*From Search Query: memory compression attention integration*\n\n*Anonymous*\n\n**Abstract:** Recently, end-to-end (E2E) speech recognition has become popular, since it can integrate the acoustic, pronunciation and language models into a single neural network, which outperforms conventional models. Among E2E approaches, attention-based models, e.g. Transformer, have emerged as being superior. Such models have opened the door to deployment of ASR on smart devices, however they still suffer from requiring a large number of model parameters. We propose an extremely low footprint E2E ASR system for smart devices, to achieve the goal of satisfying resource constraints without sacrificing recognition accuracy. We design cross-layer weight sharing to improve parameter efficiency and further exploit model compression methods including sparsification and quantization, to reduce memory storage and boost decoding efficiency. We evaluate our approaches on the public AISHELL-1 and AISHELL-2 benchmarks. On the AISHELL-2 task, the proposed method achieves more than 10x compression (model size reduces from 248 to 24MB), at the cost of only minor performance loss (CER reduces from 6.49% to 6.92%).\n\n**Published:** 2021-04-06\n\n\n\n#### 6. KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head\n\n*From Search Query: memory compression attention integration*\n\n*Isaac Rehg*\n\n**Abstract:** Context lengths of Large Language Models (LLMs) have exploded in recent years, with 128k-token context becoming a standard and million-token context becoming a reality. Efficiently supporting long-context inference remains challenging as the memory that must be allocated in key-value (KV) cache for a generation scales with its context length, limiting the number of long-context requests that can be served concurrently under a given memory budget. KV cache compression can mitigate this issue by removing under-utilized KVs from each attention head's cache and reducing its memory footprint. Higher theoretical compression rates can be achieved when the number of removed KVs varies across attention heads, but application of such a strategy within existing inference frameworks adds fragmentation and cannot realize the theoretical compression rates in physical memory. We introduce KV-Compress, a novel compression method that evicts contiguous KV blocks within a PagedAttention framework, reducing the memory footprint of the KV cache proportionally to this theoretical compression rate. Our method achieves state-of-the-art performance on LongBench for both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the total number of compressed KVs by 4x compared with prior methods. Evaluations on Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression rates up to 8x with negligible impact on performance, and up to 64x while retaining over 90% of full-cache performance for all but three of the suite's subsets. We benchmark an integration of our method with vLLM that increases total throughput by up to 5.18x by enabling larger decoding batches.\n\n**Published:** 2024-09-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design with a focus on data-dependent gating mechanisms, hardware-efficient hierarchical attention, and integration of memory compression with attention mechanisms, here are some key findings and suggestions based on the provided search results and analysis:\n\n### Data-Dependent Gating Mechanisms in Transformers\n\n- The concept of data-dependent gating is well-explored in the context of linear attention mechanisms. The \"Gated Linear Attention Transformers\" (GLA Transformers) introduce a variant of linear attention with data-dependent gates. This approach allows the model to perform competitively against other state-of-the-art models, especially in terms of length generalization and efficiency. The GLA Transformer is effective at generalizing to sequences longer than 20K without significant perplexity degradation, which is crucial for autoregressive models.\n\n### Hardware-Efficient Implementations of Hierarchical Attention\n\n- Hierarchical attention mechanisms can significantly reduce computational complexity. The \"linear adaptive mixer network (LAMNet)\" proposes a convolution-based Transformer framework that incorporates a dual-branch structure and an ultra-lightweight information exchange module. This design enhances the aggregation of information while maintaining computational efficiency, similar to convolutional neural networks. LAMNet achieves better performance than existing self-attention-based Transformer methods and reduces inference time by up to three times.\n\n- Another approach is the \"Hierarchical Attention (HAT)\" introduced in \"FasterViT: Fast Vision Transformers with Hierarchical Attention.\" This method decomposes global self-attention into a multi-level attention mechanism, reducing computational costs. Each window in HAT has access to dedicated carrier tokens that participate in both local and global representation learning, which can be adapted for language models[References].\n\n### Integration Methods for Combining Memory Compression with Attention Mechanisms\n\n- For integrating memory compression with attention mechanisms, the concept of \"Lossless Compressed Memory Attention (LoMA)\" is promising. LoMA enables lossless compression of the KV cache, reducing memory and computational demands during autoregressive generation. This method can be combined with hierarchical attention mechanisms to further optimize memory usage and computational efficiency[References].\n\n- The \"Gated Linear Attention Transformers\" also offer a framework where data-dependent gates can be integrated with memory management techniques. By using gated linear units, these models can efficiently manage the flow of information and reduce the need for extensive memory resources, making them more suitable for autoregressive models.\n\n### Performance Analysis and Optimization Techniques\n\n- **Efficient Memory Access Patterns**: The use of memristor-based in-memory computing, as described in the paper on \"Efficient memristor accelerator for transformer self-attention,\" can significantly improve energy efficiency and reduce latency. This hardware-efficient approach can be integrated into the design of autoregressive models to optimize memory access patterns and reduce power consumption.\n\n- **Cache-Friendly Memory Layouts**: Ensuring cache-friendly memory layouts is crucial for performance optimization. The \"linear adaptive mixer network (LAMNet)\" and other hierarchical attention mechanisms can be designed to optimize memory access patterns, reducing bottlenecks and improving overall performance.\n\n- **Structured Sparsity Patterns**: Implementing structured sparsity patterns, as suggested in the analysis, can enhance hardware utilization and reduce computational overhead. This can be particularly beneficial when combined with data-dependent gating mechanisms and hierarchical attention[Analysis].\n\n### Summary\n\nTo achieve the goal of improving autoregressive language model design:\n\n- **Data-Dependent Gating**: Implement data-dependent gating mechanisms using GLA Transformers to enhance expressiveness and efficiency.\n- **Hardware-Efficient Hierarchical Attention**: Use hierarchical attention mechanisms like HAT or LAMNet to reduce computational complexity and optimize memory access patterns.\n- **Memory Compression Integration**: Integrate lossless compression techniques like LoMA with hierarchical attention and data-dependent gating to optimize memory usage.\n- **Performance Optimization**: Utilize memristor-based in-memory computing, cache-friendly memory layouts, and structured sparsity patterns to enhance performance and efficiency.\n\nBy combining these techniques, the researcher can develop an autoregressive language model that is highly efficient, scalable, and performs well across various tasks.\n",
            "analysis": "### Summary of Analysis\n\nAfter analyzing the search results, several promising directions have emerged for improving the seed design's AdaptiveLinearAttention (ALA) unit:\n\n1. Memory Efficiency:\n- Lossless compression techniques for KV caches\n- Hierarchical memory structures\n- Dynamic memory management based on importance\n\n2. Attention Mechanisms:\n- Hierarchical attention with multi-level processing\n- Integration of local and global attention patterns\n- Hardware-efficient attention implementations\n\n3. Hardware Optimization:\n- Cache-friendly memory layouts\n- Structured sparsity patterns\n- Efficient memory access patterns\n\n### Key References and Excerpts\n\n1. From \"LoMA: Lossless Compressed Memory Attention\":\n```\nWe introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation... Our method compresses the KV cache after every tc generated tokens with a compression ratio of c and a target compressed length t.\n```\n\n2. From \"FasterViT: Fast Vision Transformers with Hierarchical Attention\":\n```\nOur newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs... Each window has access to dedicated carrier tokens that participate in local and global representation learning.\n```\n\n3. From \"Gated Linear Attention Transformers\":\n```\nWe then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates... GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\n```\n\n4. From \"H-Transformer-1D\":\n```\nThe proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix)... and has linear run time and memory complexity.\n```\n\n### Key Insights and Analysis\n\n1. Memory Management:\n- Lossless compression of KV caches can significantly reduce memory requirements without information loss\n- Hierarchical memory structures enable efficient handling of long sequences\n- Dynamic memory management based on token importance can optimize resource usage\n\n2. Attention Design:\n- Multi-level attention decomposition reduces computational complexity\n- Combining local and global attention patterns improves efficiency\n- Data-dependent gating mechanisms enhance expressiveness\n\n3. Hardware Considerations:\n- Cache-friendly memory layouts improve performance\n- Structured sparsity patterns enhance hardware utilization\n- Efficient memory access patterns reduce bottlenecks\n\n4. Integration Opportunities:\n- Combining lossless compression with hierarchical attention\n- Integrating data-dependent gating with memory management\n- Implementing hardware-aware optimizations\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of data-dependent gating mechanisms\n2. Hardware-efficient implementations of hierarchical structures\n3. Integration techniques for memory compression and attention\n\nNext search queries:\n1. \"data-dependent gating transformer\"\n2. \"hardware-efficient hierarchical attention\"\n3. \"memory compression attention integration\"\n\n### References\n\n[1] Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv:2401.08541.\n\n[2] Hatamizadeh, A., Heinrich, G., Yin, H., Tao, A., \u00c1lvarez, J., Kautz, J., & Molchanov, P. (2023). FasterViT: Fast Vision Transformers with Hierarchical Attention. International Conference on Learning Representations.\n\n[3] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n[4] Zhu, Z., & Soricut, R. (2021). H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences. Annual Meeting of the Association for Computational Linguistics.\n\n[5] Diwan, A., Choi, E., & Harwath, D. (2023). When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants. Annual Meeting of the Association for Computational Linguistics."
        },
        {
            "ready": false,
            "query": "dynamic compression attention implementation, convolution attention integration techniques, hardware efficient hierarchical structures",
            "detail": "Search for specific techniques and implementations related to:\n1. Dynamic compression methods in attention mechanisms, focusing on adaptive compression ratios and efficiency\n2. Integration techniques for combining convolution operations with attention mechanisms\n3. Hardware-efficient implementations of hierarchical structures, particularly for memory access patterns\n4. Performance optimization techniques for compressed attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific techniques and implementations related to:\n1. Dynamic compression methods in attention mechanisms, focusing on adaptive compression ratios and efficiency\n2. Integration techniques for combining convolution operations with attention mechanisms\n3. Hardware-efficient implementations of hierarchical structures, particularly for memory access patterns\n4. Performance optimization techniques for compressed attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.25)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 3/38 (Score: 0.25)*\n\n```\nThis ensures an accurate profiling of the attention influences to facilitate better compression results. Our contributions are summarized as follows. - Heterogeneous Elastic Rules. We propose heterogeneous elastic rules for masks of each attention head. We formulate MoA compression search space to include a diverse range of elastic rules that tailor the local attention span relative to the input length for each attention head. The heterogeneous elastic rules improve the fact retrieval accuracy of MoA from 25\\% to $98 \\%$ compared with masks with uniform span and scaling function for each head. - Calibration Dataset Construction We emphasize the importance of data engineering in LLM compression. Our findings demonstrate that using datasets with long-range dependencies and referencing the original LLM's responses are crucial for accurately profiling the influences of compression. - Automatic Optimization. We propose an automatic pipeline to find the optimal compression plan encompassing heterogeneous elastic rules for various attention heads. This pipeline can efficiently find the optimal plan within several hours, for example, two hours for compressing Vicuna-13B. Experiments show that MoA achieves $5.5 \\times$ to $6.7 \\times$ throughput improvements on 7 B and 13 B dense LLMs at a $50 \\%$ density (the average of KV-Cache length / input length), with only $1 \\%$ average relative degradation in retrieval accuracy. Additionally, MoA achieves over $90 \\%$ retrieval accuracy with just $25 \\%$ average density, far surpassing sparse attention baselines that need a density of $75 \\%$ to $100 \\%$ for similar performance. On long-context understanding benchmarks, MoA performs comparably to dense models, with a maximum relative performance drop of less than $5 \\%$, which is about one-sixth of that observed with the uniform sparse attention baseline. Our code is available at https://github.com/thu-nics/MoA\n\n## 2 Preliminary and Related work\n\n### 2.1 Attention mechanism\n\nThe Multi-Head Self Attention (MHA) mechanism [62] is crucial to the functionality of LLMs.\n```\n\n#### 2. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.12)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 35/41 (Score: 0.12)*\n\n```\nIn NAACL, 2018. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and Efficient Posttraining Quantization for Large Language Models. In ICML, 2023. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A Nystr\u00f6m-based Algorithm for Approximating Self-attention. In AAAI, 2021. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated Linear Attention Transformers with Hardware-efficient Training. arXiv preprint arXiv:2312.06635, 2023. You, H., Sun, Z., Shi, H., Yu, Z., Zhao, Y., Zhang, Y., Li, C., Li, B., and Lin, Y. ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 273-286. IEEE, 2023a. You, H., Xiong, Y., Dai, X., Wu, B., Zhang, P., Fan, H., Vajda, P., and Lin, Y. C. Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference. In CVPR, 2023b. You, H., Shi, H., Guo, Y., and Lin, Y. ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. Advances in Neural Information Processing Systems, 36, 2024. Zeng, Z., Xiong, Y., Ravi, S., Acharya, S., Fung, G. M., and Singh, V. You Only Sample (almost) Once: Linear Cost Self-attention via Bernoulli Sampling. In ICML, 2021. Zhang, X., Zhao, J., and LeCun, Y. Character-level Convolutional Networks for Text Classification.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.10)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.10)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.08)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.08)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 5. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.05)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.05)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic compression attention implementation, convolution attention integration techniques, hardware efficient hierarchical structures\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Dodo: Dynamic Contextual Compression for Decoder-only LMs\n\n*From Search Query: dynamic compression attention implementation*\n\n*Guanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van Durme*\n\n**TL;DR:** Dodo is proposed, a solution for context compression that represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space.\n\n**Abstract:** Transformer-based language models (LMs) are inefficient in long contexts. We propose Dodo, a solution for context compression. Instead of one vector per token in a standard transformer model, Dodo represents text with a dynamic number of hidden states at each layer, reducing the cost of self-attention to a fraction of typical time and space. Moreover, off-the-shelf models such as LLaMA can be adapted to Dodo by efficient parameter tuning methods such as LoRA. In use, Dodo can act as either an autoregressive LM or a context compressor for downstream tasks. We demonstrate through experiments in language modeling, question answering, and summarization that Dodo retains capabilities in these tasks, while drastically reducing the overhead during decoding. For example, in the autoencoding task, Dodo shrinks context at a 20x compression ratio with a BLEU score of 98% for reconstruction, achieving nearly lossless encoding.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\n\n*From Search Query: dynamic compression attention implementation*\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, F. Fleuret*\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention\u2014which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2 . 0 \u00d7 and 3 . 3 \u00d7 for sequences of respectively 8 k and 16 k tokens.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 3. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: dynamic compression attention implementation*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 4. MOAT: Alternating Mobile Convolution and Attention Brings Strong Vision Models\n\n*From Search Query: convolution attention integration techniques*\n\n*Chenglin Yang, Siyuan Qiao, Qihang Yu, Xiaoding Yuan, Yukun Zhu, A. Yuille, Hartwig Adam, Liang-Chieh Chen*\n\n**TL;DR:** The simple yet effective MOAT will inspire more seamless integration of convolution and self-attention, and the tiny-MOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet.\n\n**Abstract:** This paper presents MOAT, a family of neural networks that build on top of MObile convolution (i.e., inverted residual blocks) and ATtention. Unlike the current works that stack separate mobile convolution and transformer blocks, we effectively merge them into a MOAT block. Starting with a standard Transformer block, we replace its multi-layer perceptron with a mobile convolution block, and further reorder it before the self-attention operation. The mobile convolution block not only enhances the network representation capacity, but also produces better downsampled features. Our conceptually simple MOAT networks are surprisingly effective, achieving 89.1% / 81.5% top-1 accuracy on ImageNet-1K / ImageNet-1K-V2 with ImageNet22K pretraining. Additionally, MOAT can be seamlessly applied to downstream tasks that require large resolution inputs by simply converting the global attention to window attention. Thanks to the mobile convolution that effectively exchanges local information between pixels (and thus cross-windows), MOAT does not need the extra window-shifting mechanism. As a result, on COCO object detection, MOAT achieves 59.2% box AP with 227M model parameters (single-scale inference, and hard NMS), and on ADE20K semantic segmentation, MOAT attains 57.6% mIoU with 496M model parameters (single-scale inference). Finally, the tiny-MOAT family, obtained by simply reducing the channel sizes, also surprisingly outperforms several mobile-specific transformer-based models on ImageNet. The tiny-MOAT family is also benchmarked on downstream tasks, serving as a baseline for the community. We hope our simple yet effective MOAT will inspire more seamless integration of convolution and self-attention. Code is publicly available.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 50  (*Influential: 5*)\n\n#### 5. Demystifying Oversmoothing in Attention-Based Graph Neural Networks\n\n*From Search Query: convolution attention integration techniques*\n\n*Xinyi Wu, A. Ajorlou, Zihui Wu, A. Jadbabaie*\n\n**TL;DR:** It is established that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially.\n\n**Abstract:** Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 3*)\n\n#### 6. Revealing Vision-Language Integration in the Brain with Multimodal Networks\n\n*From Search Query: convolution attention integration techniques*\n\n*Vighnesh Subramaniam, C. Conwell, Christopher Wang, Gabriel Kreiman, B. Katz, Ignacio Cases, Andrei Barbu*\n\n**TL;DR:** DNNs are used to probe for sites of multimodal integration in the human brain by predicting stereoen-cephalography recordings taken while human subjects watched movies and it is found that among the variants of multimodal training techniques the authors assess, CLIP-style training is the best suited for downstream prediction of the neural activity in these sites.\n\n**Abstract:** We use (multi)modal deep neural networks (DNNs) to probe for sites of multimodal integration in the human brain by predicting stereoen-cephalography (SEEG) recordings taken while human subjects watched movies. We operationalize sites of multimodal integration as regions where a multimodal vision-language model predicts recordings better than unimodal language, unimodal vision, or linearly-integrated language-vision models. Our target DNN models span different architectures (e.g., convolutional networks and transformers) and multimodal training techniques (e.g., cross-attention and contrastive learning). As a key enabling step, we first demonstrate that trained vision and language models systematically outperform their randomly initialized counterparts in their ability to predict SEEG signals. We then compare unimodal and multimodal models against one another. Because our target DNN models often have different architectures, number of parameters, and training sets (possibly obscuring those differences attributable to integration), we carry out a controlled comparison of two models (SLIP and SimCLR), which keep all of these attributes the same aside from input modality. Using this approach, we identify a sizable number of neural sites (on average 141 out of 1090 total sites or 12.94%) and brain regions where multimodal integration seems to occur. Additionally, we find that among the variants of multimodal training techniques we assess, CLIP-style training is the best suited for downstream prediction of the neural activity in these sites.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design\n\n*From Search Query: hardware efficient hierarchical structures*\n\n*Cheng Tan, Yijie Zhang, Zhangyang Gao, Bozhen Hu, Siyuan Li, Zicheng Liu, Stan Z. Li*\n\n**TL;DR:** This study crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure and proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level to fully leverage the limited data.\n\n**Abstract:** While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Moreover, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural representations through contrastive learning at both cluster-level and sample-level to fully leverage the limited data. By constraining data representations within a limited hyperspherical space, the intrinsic relationships between data points could be explicitly imposed. Moreover, we incorporated extracted secondary structures with base pairs as prior knowledge to facilitate the RNA design process. Extensive experiments demonstrate the effectiveness of our proposed method, providing a reliable baseline for future RNA design tasks. The source code and benchmark dataset are available at https://github.com/A4Bio/RDesign.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Efficient Hierarchical Domain Adaptation for Pretrained Language Models\n\n*From Search Query: hardware efficient hierarchical structures*\n\n*Alexandra Chronopoulou, Matthew E. Peters, Jesse Dodge*\n\n**TL;DR:** This paper introduces a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach based on the observation that textual domains are partially overlapping, and represents domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights.\n\n**Abstract:** The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 38  (*Influential: 2*)\n\n#### 9. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware efficient hierarchical structures*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Powerpropagation: A sparsity inducing weight reparameterisation\n\n*From Search Query: dynamic compression attention implementation*\n\n*Yee Whye Teh, Peter E. Latham, Razvan Pascanu, Siddhant M. Jayakumar, Jonathan Schwarz*\n\n**Abstract:** The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a \"rich get richer\" dynamic, leaving low-magnitude parameters largely unaffected by learning. Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely. Powerpropagation is general, intuitive, cheap and straight-forward to implement and can readily be combined with various other techniques. To highlight its versatility, we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representations allow accommodating a large number of tasks at fixed model capacity. In all cases our reparameterisation considerably increases the efficacy of the off-the-shelf methods.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-10-01\n\n\n\n#### 2. The NiuTrans System for WNGT 2020 Efficiency Task\n\n*From Search Query: dynamic compression attention implementation*\n\n*Jingbo Zhu, Tong Xiao, Chenglong Wang, Yanyang Li, Yinqiao Li, Ye Lin, Bei Li, Chi Hu*\n\n**Abstract:** This paper describes the submissions of the NiuTrans Team to the WNGT 2020 Efficiency Shared Task. We focus on the efficient implementation of deep Transformer models \\cite{wang-etal-2019-learning, li-etal-2019-niutrans} using NiuTensor (https://github.com/NiuTrans/NiuTensor), a flexible toolkit for NLP tasks. We explored the combination of deep encoder and shallow decoder in Transformer models via model compression and knowledge distillation. The neural machine translation decoding also benefits from FP16 inference, attention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on \\textit{newstest2018}. The code, models, and docker images are available at NiuTrans.NMT (https://github.com/NiuTrans/NiuTrans.NMT).\n\n**Conference:** the-niutrans-system-for-wngt-2020-efficiency\n\n**Published:** 2021-09-16\n\n\n\n#### 3. TenSEAL: A Library for Encrypted Tensor Operations Using Homomorphic Encryption\n\n*From Search Query: convolution attention integration techniques*\n\n*Alaa Eddine Belfedhal, Bogdan Cebere, Bilal Retiat, Ayoub Benaissa*\n\n**Abstract:** Machine learning algorithms have achieved remarkable results and are widely applied in a variety of domains. These algorithms often rely on sensitive and private data such as medical and financial records. Therefore, it is vital to draw further attention regarding privacy threats and corresponding defensive techniques applied to machine learning models. In this paper, we present TenSEAL, an open-source library for Privacy-Preserving Machine Learning using Homomorphic Encryption that can be easily integrated within popular machine learning frameworks. We benchmark our implementation using MNIST and show that an encrypted convolutional neural network can be evaluated in less than a second, using less than half a megabyte of communication.\n\n**Published:** 2021-04-07\n\n\n\n#### 4. Face Attention Network: An Effective Face Detector for the Occluded Faces\n\n*From Search Query: convolution attention integration techniques*\n\n*Jianfeng Wang, Ye Yuan, Gang Yu*\n\n**Abstract:** The performance of face detection has been largely improved with the\ndevelopment of convolutional neural network. However, the occlusion issue due\nto mask and sunglasses, is still a challenging problem. The improvement on the\nrecall of these occluded cases usually brings the risk of high false positives.\nIn this paper, we present a novel face detector called Face Attention Network\n(FAN), which can significantly improve the recall of the face detection problem\nin the occluded case without compromising the speed. More specifically, we\npropose a new anchor-level attention, which will highlight the features from\nthe face region. Integrated with our anchor assign strategy and data\naugmentation techniques, we obtain state-of-art results on public face\ndetection benchmarks like WiderFace and MAFA. The code will be released for\nreproduction.\n\n**Published:** 2017-11-20\n\n\n\n#### 5. AdaLAM: Revisiting Handcrafted Outlier Detection\n\n*From Search Query: hardware efficient hierarchical structures*\n\n*Viktor Larsson, Marc Pollefeys, Torsten Sattler, Martin Ralf Oswald, Luca Cavalli*\n\n**Abstract:** Local feature matching is a critical component of many computer vision pipelines, including among others Structure-from-Motion, SLAM, and Visual Localization. However, due to limitations in the descriptors, raw matches are often contaminated by a majority of outliers. As a result, outlier detection is a fundamental problem in computer vision, and a wide range of approaches have been proposed over the last decades. In this paper we revisit handcrafted approaches to outlier filtering. Based on best practices, we propose a hierarchical pipeline for effective outlier detection as well as integrate novel ideas which in sum lead to AdaLAM, an efficient and competitive approach to outlier rejection. AdaLAM is designed to effectively exploit modern parallel hardware, resulting in a very fast, yet very accurate, outlier filter. We validate AdaLAM on multiple large and diverse datasets, and we submit to the Image Matching Challenge (CVPR2020), obtaining competitive results with simple baseline descriptors. We show that AdaLAM is more than competitive to current state of the art, both in terms of efficiency and effectiveness.\n\n**Published:** 2020-06-07\n\n\n\n#### 6. Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on Mobile Devices\n\n*From Search Query: hardware efficient hierarchical structures*\n\n*Tien-Ju Yang, Yu-Hsin Chen, Vivienne Sze, Joel Emer*\n\n**Abstract:** A recent trend in DNN development is to extend the reach of deep learning applications to platforms that are more resource and energy constrained, e.g., mobile devices. These endeavors aim to reduce the DNN model size and improve the hardware processing efficiency, and have resulted in DNNs that are much more compact in their structures and/or have high data sparsity. These compact or sparse models are different from the traditional large ones in that there is much more variation in their layer shapes and sizes, and often require specialized hardware to exploit sparsity for performance improvement. Thus, many DNN accelerators designed for large DNNs do not perform well on these models. In this work, we present Eyeriss v2, a DNN accelerator architecture designed for running compact and sparse DNNs. To deal with the widely varying layer shapes and sizes, it introduces a highly flexible on-chip network, called hierarchical mesh, that can adapt to the different amounts of data reuse and bandwidth requirements of different data types, which improves the utilization of the computation resources. Furthermore, Eyeriss v2 can process sparse data directly in the compressed domain for both weights and activations, and therefore is able to improve both processing speed and energy efficiency with sparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65nm CMOS process achieves a throughput of 1470.6 inferences/sec and 2560.3 inferences/J at a batch size of 1, which is 12.6x faster and 2.5x more energy efficient than the original Eyeriss running MobileNet. We also present an analysis methodology called Eyexam that provides a systematic way of understanding the performance limits for DNN processors as a function of specific characteristics of the DNN model and accelerator design; it applies these characteristics as sequential steps to increasingly tighten the bound on the performance limits.\n\n**Published:** 2018-07-10\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, here are some specific techniques and implementations related to the given keywords and details:\n\n### Dynamic Compression Methods in Attention Mechanisms\n\n- **Dynamic Memory Compression (DMC):** This method learns to apply different compression ratios in different heads and layers of the transformer model. DMC achieves significant throughput increases during auto-regressive inference by adaptively compressing the KV cache, ensuring minimal loss of contextual information.\n- **Adaptive KV Cache Compression:** This approach involves dynamically adjusting the compression of the KV cache based on the importance of the tokens. Techniques like LoMA (Lossless KV cache compression) and Scissorhands, which exploit the persistence of importance hypothesis, can be integrated to optimize memory usage and maintain performance.\n\n### Integration Techniques for Combining Convolution Operations with Attention Mechanisms\n\n- **Short-Long Convolutions with Linear Attention:** The CHELA approach combines short-range convolutions with long-range linear attention. This method implements linear attention in a divide-and-conquer manner, allowing for efficient processing of long sequences while maintaining global abstraction and data-dependent selection. This integration helps in focusing on both local and global patterns efficiently[Liu et al., 2024].\n- **Data-Driven Adaptive Fusion:** Techniques like LoCoCo involve blending previous KV pairs with incoming tokens using a data-driven adaptive fusion method. This approach minimizes the loss of contextual information and ensures accurate attention modeling, which can be combined with convolutional layers to enhance the model's expressiveness.\n\n### Hardware-Efficient Implementations of Hierarchical Structures\n\n- **Memristor-Based Acceleration:** Using memristor crossbar arrays for matrix multiplication in transformer networks can significantly improve hardware efficiency. This approach integrates computation into memory, reducing computational latency and enhancing the efficiency of self-attention mechanisms.\n- **Cache-Friendly Memory Layouts:** Implementing hierarchical memory structures with cache-friendly memory layouts can optimize memory access patterns. This is crucial for efficient handling of long sequences and reducing the computational overhead associated with attention mechanisms[Analysis].\n\n### Performance Optimization Techniques for Compressed Attention Mechanisms\n\n- **Attention with Dynamic Depths (MindSkip):** This method applies dynamic depth selectively to attention layers, skipping less important layers to preserve model accuracy while enhancing computational and memory efficiency. By focusing on attention layers, MindSkip alleviates the quadratic complexity and memory-intensive operations associated with KV caching.\n- **Structured Sparsity:** Implementing structured sparsity in the model can improve hardware utilization. This involves optimizing the model's architecture to leverage sparse matrix operations, which can be more efficient in hardware implementations[Analysis].\n\n### Summary of Key Insights\n\n- **Dynamic Compression and Hierarchical Structures:** Combining dynamic compression methods like DMC with hierarchical memory structures can significantly optimize memory usage and computational efficiency.\n- **Convolution-Attention Integration:** Techniques like CHELA and LoCoCo provide effective ways to integrate convolution operations with attention mechanisms, enhancing the model's ability to handle long sequences and maintain contextual information.\n- **Hardware Efficiency:** Using memristor-based acceleration, cache-friendly memory layouts, and structured sparsity can improve the hardware efficiency of the model, making it more scalable and efficient.\n\nBy integrating these techniques, researchers can develop autoregressive language models that are more efficient, scalable, and performant.\n",
            "analysis": "### Summary of Analysis\n\nAfter three rounds of searches, several promising directions have emerged for improving the seed design:\n\n1. **Memory Management and Compression**:\n- Lossless KV cache compression (LoMA) shows significant potential\n- Hierarchical memory structures enable efficient handling of long sequences\n- Dynamic memory management based on token importance\n\n2. **Attention Mechanisms**:\n- Gated Linear Attention (GLA) with data-dependent gates shows strong performance\n- Hierarchical attention structures reduce computational complexity\n- Short-long convolutions help with efficient processing\n\n3. **Hardware Efficiency**:\n- CHELA's divide-and-conquer approach for linear attention\n- Cache-friendly memory layouts and access patterns\n- Structured sparsity for better hardware utilization\n\n### Key References and Excerpts\n\n1. From \"Gated Linear Attention Transformers\":\n```\nGLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations... The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2.\n```\n\n2. From \"CHELA: Short-Long Convolutions with Hardware-Efficient Linear Attention\":\n```\nCHELA implements linear attention in a divide-and-conquer manner... enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n```\n\n3. From \"LoCoCo: Dropping In Convolutions for Long Context Compression\":\n```\nLoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling.\n```\n\n4. From \"Dynamic Memory Compression\":\n```\nDMC learns to apply different compression ratios in different heads and layers... achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU.\n```\n\n### Key Insights and Analysis\n\n1. **Memory Optimization**:\n- Combining lossless compression with hierarchical structures\n- Dynamic compression ratios per attention head\n- Adaptive fusion of KV pairs for context preservation\n\n2. **Attention Design**:\n- Data-dependent gating improves expressiveness\n- Divide-and-conquer approach for linear attention\n- Integration of local and global patterns\n\n3. **Hardware Considerations**:\n- Tiling strategies for efficient computation\n- Memory access pattern optimization\n- Structured sparsity for hardware efficiency\n\n4. **Integration Opportunities**:\n- Combining GLA with dynamic memory compression\n- Using short-long convolutions with linear attention\n- Implementing hierarchical memory management\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of dynamic compression in attention mechanisms\n2. Integration methods for convolution and attention\n3. Hardware-efficient implementations of hierarchical structures\n\nNext search queries:\n1. \"dynamic compression attention implementation\"\n2. \"convolution attention integration techniques\"\n3. \"hardware efficient hierarchical structures\"\n\n### References\n\n[1] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint arXiv:2312.06635.\n\n[2] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n[3] Cai, R., Tian, Y., Wang, Z., & Chen, B. (2024). LoCoCo: Dropping In Convolutions for Long Context Compression. International Conference on Machine Learning.\n\n[4] Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n[5] Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis, A., & Shrivastava, A. (2023). Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "dynamic memory compression, hierarchical attention, hardware-efficient linear attention",
            "detail": "Lossless Memory Compression in Attention Models, Hierarchical Attention Structures in Language Models, Hardware-Efficient Tiling Strategies",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nLossless Memory Compression in Attention Models, Hierarchical Attention Structures in Language Models, Hardware-Efficient Tiling Strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.96)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.99)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n##### *Relevant Chunk: No. 8/16 (Score: 0.93)*\n\n```\nSee Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.\n```\n\n#### 2. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.07)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.07)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.02)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.02)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.00)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.00)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic memory compression, hierarchical attention, hardware-efficient linear attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming\n\n*From Search Query: dynamic memory compression*\n\n*Jinuk Kim, Yeonwoo Jeong, Deokjae Lee, Hyun Oh Song*\n\n**TL;DR:** A subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent Convolution operations for efficient end-to-end inference latency is proposed.\n\n**Abstract:** Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evaluate our methods and baselines by TensorRT for a fair inference latency comparison. Our method outperforms the baseline method with higher accuracy and faster inference speed in MobileNetV2 on the ImageNet dataset. Specifically, we achieve $1.41\\times$ speed-up with $0.11$\\%p accuracy gain in MobileNetV2-1.0 on the ImageNet.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Clustering the Sketch: Dynamic Compression for Embedding Tables\n\n*From Search Query: dynamic memory compression*\n\n*Henry Ling-Hei Tsang, Thomas Dybdahl Ahle*\n\n**TL;DR:** Clustered Compositional Embeddings (CCE) is suggested which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and CompositionalEmbeddings to achieve the best of both worlds.\n\n**Abstract:** Embedding tables are used by machine learning systems to work with categorical features. In modern Recommendation Systems, these tables can be very large, necessitating the development of new methods for fitting them in memory, even during training. We suggest Clustered Compositional Embeddings (CCE) which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and Compositional Embeddings (Shi et al., 2020). Experimentally CCE achieves the best of both worlds: The high compression rate of codebook-based quantization, but *dynamically* like hashing-based methods, so it can be used during training. Theoretically, we prove that CCE is guaranteed to converge to the optimal codebook and give a tight bound for the number of iterations required.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4299  (*Influential: 554*)\n\n#### 4. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n#### 5. HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level\n\n*From Search Query: hierarchical attention*\n\n*Haoran Luo, E. Haihong, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin*\n\n**TL;DR:** A novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention, that addresses the issue of HKG multi-position prediction for the first time and achieves state-of-the-art performance in link prediction tasks on HKG standard datasets.\n\n**Abstract:** Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs\u2019 representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware-efficient linear attention*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\n\n*From Search Query: hardware-efficient linear attention*\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n### 3 related papers from ArXiv\n\n#### 1. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: dynamic memory compression*\n\n*Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti*\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Published:** 2024-03-14T17:59:26Z  (*Updated: 2024-07-23T17:55:30Z*)\n\n\n\n#### 2. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 3. Bidirectional Context-Aware Hierarchical Attention Network for Document\n  Understanding\n\n*From Search Query: hierarchical attention*\n\n*Jean-Baptiste Remy, Antoine Jean-Pierre Tixier, Michalis Vazirgiannis*\n\n**Abstract:** The Hierarchical Attention Network (HAN) has made great strides, but it\nsuffers a major limitation: at level 1, each sentence is encoded in complete\nisolation. In this work, we propose and compare several modifications of HAN in\nwhich the sentence encoder is able to make context-aware attentional decisions\n(CAHAN). Furthermore, we propose a bidirectional document encoder that\nprocesses the document forwards and backwards, using the preceding and\nfollowing sentences as context. Experiments on three large-scale sentiment and\ntopic classification datasets show that the bidirectional version of CAHAN\noutperforms HAN everywhere, with only a modest increase in computation time.\nWhile results are promising, we expect the superiority of CAHAN to be even more\nevident on tasks requiring a deeper understanding of the input documents, such\nas abstractive summarization. Code is publicly available.\n\n**Published:** 2019-08-16T15:20:04Z  (*Updated: 2019-08-16T15:20:04Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks\n\n*From Search Query: dynamic memory compression*\n\n*Lili Mou, Yanshuai Cao, Yongchang Hao*\n\n**Abstract:** The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.\n\n**Published:** 2024-10-28\n\n\n\n#### 2. Build a Deep Neural Network model using CPUs Builds a feed-forward multilayer artificial neural network on an H2OFrame\n\n*From Search Query: dynamic memory compression*\n\n*Viraj Parmar, Erin LeDell, Jessica Lanford, Arno Candel, Anisha Arora*\n\n**Abstract:** H2O is fast, scalable, open-source machine learning and deep learning for\r\nsmarter applications. With H2O, enterprises like PayPal, Nielsen Catalina,\r\nCisco, and others can use all their data without sampling to get accurate\r\npredictions faster. Advanced algorithms such as deep learning, boosting, and\r\nbagging ensembles are built-in to help application designers create smarter\r\napplications through elegant APIs. Some of our initial customers have built\r\npowerful domain-speci\fc predictive engines for recommendations, customer\r\nchurn, propensity to buy, dynamic pricing, and fraud detection for the insurance,\r\nhealthcare, telecommunications, ad tech, retail, and payment systems industries.\r\nUsing in-memory compression, H2O handles billions of data rows in-memory,\r\neven with a small cluster. To make it easier for non-engineers to create complete\r\nanalytic work\r\nows, H2O's platform includes interfaces for R, Python, Scala,\r\nJava, JSON, and Co\u000beeScript/JavaScript, as well as a built-in web interface,\r\nFlow. H2O is designed to run in standalone mode, on Hadoop, or within a\r\nSpark Cluster, and typically deploys within minutes.\r\nH2O includes many common machine learning algorithms, such as generalized\r\nlinear modeling (linear regression, logistic regression, etc.), Na\u007f\u0010ve Bayes, principal\r\ncomponents analysis, k-means clustering, and others. H2O also implements\r\nbest-in-class algorithms at scale, such as distributed random forest, gradient\r\nboosting and deep learning. Customers can build thousands of models and\r\ncompare the results to get the best predictions.\r\nH2O is nurturing a grassroots movement of physicists, mathematicians, and\r\ncomputer scientists to herald the new wave of discovery with data science by\r\ncollaborating closely with academic researchers and industrial data scientists.\r\nStanford university giants Stephen Boyd, Trevor Hastie, Rob Tibshirani advise\r\nthe H2O team on building scalable machine learning algorithms. With hundreds\r\nof meetups over the past three years, H2O has become a word-of-mouth\r\nphenomenon, growing amongst the data community by a hundred-fold, and\r\nis now used by 30,000+ users and is deployed using R, Python, Hadoop, and\r\nSpark in 2000+ corporations.\n\n**Proceeding:** h2o-ai-inc-2015-9\n\n**Published:** 2015-09-01\n\n\n\n#### 3. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 4. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware-efficient linear attention*\n\n*Yoon Kim, Rameswar Panda, Yikang Shen, Bailin Wang, Songlin Yang*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Published:** 2023-12-11\n\n\n\n#### 6. EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction\n\n*From Search Query: hardware-efficient linear attention*\n\n*Chuang Gan, Junyan Li, Muyan Hu, Song Han, Han Cai*\n\n**Abstract:** High-resolution dense prediction enables many appealing real-world applications, such as computational photography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art high-resolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of high-resolution vision models with novel multi-scale linear attention. Unlike prior high-resolution dense prediction models that rely on heavy softmax attention, hardware-inefficient large-kernel convolution, or complicated topology structure to obtain good performances, our multi-scale linear attention achieves the global receptive field and multi-scale learning (two desirable features for high-resolution dense prediction) with only lightweight and hardware-efficient operations. As such, EfficientViT delivers remarkable performance gains over previous state-of-the-art models with significant speedup on diverse hardware platforms, including mobile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 13.9$\\times$ and 6.2$\\times$ GPU latency reduction over SegFormer and SegNeXt, respectively. For super-resolution, EfficientViT delivers up to 6.4x speedup over Restormer while providing 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers 48.9x higher throughput on A100 GPU while achieving slightly better zero-shot instance segmentation performance on COCO.\n\n**Published:** 2022-05-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on dynamic memory compression, hierarchical attention structures, and hardware-efficient implementations is crucial. Here are some key insights and references that align with these objectives:\n\n### Dynamic Memory Compression\n\n- **PyramidKV** introduces a dynamic key-value (KV) cache compression technique based on pyramidal information funneling. This method adaptively allocates cache resources based on the importance of KV pairs, significantly reducing memory and latency. It retains only a fraction of the KV cache while matching the performance of models with a full KV cache.\n\n- **Dynamic Memory Compression (DMC)** for online key-value cache compression is another approach that achieves significant throughput increases during autoregressive inference. This method supports the proposal's focus on dynamic memory compression and has shown up to 7x throughput increase[Analysis].\n\n### Hierarchical Attention Structures\n\n- **Hierarchical Attention Mechanisms** can be enhanced by integrating short-long convolutions, as seen in the **CHELA** approach. This combines short-long convolutions with hardware-efficient linear attention, addressing the efficiency bottleneck of linear attention and capturing both local and global patterns effectively[Analysis].\n\n- **Selective State Space Models (S6)** also offer a hierarchical approach by integrating a selection mechanism computed via a scan. This allows the model to focus on the most relevant parts of the input sequence, enhancing performance and efficiency. The S6 models dynamically prioritize different parts of the input data, similar to human cognitive processes.\n\n### Hardware-Efficient Implementations\n\n- **Memristor-Based Acceleration** for transformer networks is a promising hardware-efficient approach. This method uses memristor crossbar arrays to perform matrix operations in an analog manner, significantly improving efficiency with low power consumption and latency. It demonstrates the potential for hardware-efficient linear attention mechanisms.\n\n- **Gated Linear Attention Transformers** with hardware-efficient training algorithms are designed to be especially effective at length generalization. These models enable efficient training and inference on long sequences, which is crucial for autoregressive language models[Analysis].\n\n### Additional Insights\n\n- **Context Compression and Selectivity**: Effective context management is pivotal in sequence models. Techniques like selective copying and induction heads, as well as the use of state space models (SSMs) like S4 and Mamba, can enhance the ability to manage context compression efficiently. These models can maintain a fixed-size state, resulting in constant time inference and linear-time training.\n\n- **Tiling Strategies**: Hardware-efficient tiling and memory access patterns, such as those used in **CHELA** and **GLA Transformers**, can enhance performance on long sequences. These strategies optimize memory access and reduce computational overhead, making them essential for efficient autoregressive language models[Analysis].\n\nBy integrating these techniques\u2014dynamic memory compression methods like PyramidKV, hierarchical attention structures such as those in CHELA and S6 models, and hardware-efficient implementations using memristor-based acceleration and gated linear attention transformers\u2014the researchers can significantly improve the efficiency, scalability, and performance of autoregressive language models.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for CompressedHierarchicalLinearAttention (CHLA) is supported by existing research and techniques that align with its objectives. The search results provide insights into dynamic memory compression, hierarchical attention structures, and hardware-efficient implementations, which are central to the proposal.\n\n### Useful References and Excerpts\n\n1. **CHELA: Short-Long Convolutions with Hardware-Efficient Linear Attention**:\n   - This paper introduces CHELA, which combines short-long convolutions with hardware-efficient linear attention. It addresses the efficiency bottleneck of linear attention and proposes a tiling approach to achieve theoretical performance.\n   - *Excerpt*: \"CHELA... replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.\"\n\n2. **LoMA: Lossless Compressed Memory Attention**:\n   - LoMA introduces a method for lossless compression of the KV cache, reducing memory and computational demands. This aligns with the proposal's dynamic memory compression aspect.\n   - *Excerpt*: \"LoMA... enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\"\n\n3. **Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference**:\n   - This paper discusses Dynamic Memory Compression (DMC) for online key-value cache compression, achieving significant throughput increases. It supports the proposal's focus on dynamic memory compression.\n   - *Excerpt*: \"DMC... achieves up to 7x throughput increase during auto-regressive inference.\"\n\n4. **Gated Linear Attention Transformers with Hardware-Efficient Training**:\n   - This paper describes a hardware-efficient algorithm for linear attention, relevant to the proposal's focus on hardware-efficient implementation.\n   - *Excerpt*: \"GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K.\"\n\n### Key Insights and Detailed Analysis\n\n- **Dynamic Memory Compression**: The proposal's use of learned compression ratios per head is innovative and aligns with existing research on memory-efficient attention mechanisms like LoMA and DMC. These methods demonstrate the feasibility and potential benefits of dynamic compression in reducing memory and computational overhead.\n\n- **Hierarchical Attention Structures**: The integration of short-long convolutions in hierarchical attention structures is promising for capturing both local and global patterns. The success of similar approaches in other domains suggests potential applicability to language models.\n\n- **Hardware-Efficient Implementation**: The proposal's emphasis on hardware-efficient tiling and memory access patterns is supported by research on hardware-efficient linear attention, such as GLA Transformers. These strategies can enhance performance on long sequences.\n\n### Future Search Plan\n\n1. Conduct further searches on dynamic memory compression techniques in attention mechanisms to explore additional methods and their effectiveness.\n2. Investigate hierarchical attention structures in more detail, focusing on their application in language models and potential improvements in expressiveness and efficiency.\n3. Explore hardware-efficient implementations in greater depth, particularly those involving tiling strategies and memory access optimization.\n\n### References\n\n- Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv.org.\n- Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv.org.\n- Nawrot, P., et al. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. arXiv.org.\n- Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv.org."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "dynamic memory compression, hierarchical attention, linear attention",
            "detail": "CompressedHierarchicalLinearAttention, dynamic memory compression, hierarchical attention structures, hardware-efficient tiling",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nCompressedHierarchicalLinearAttention, dynamic memory compression, hierarchical attention structures, hardware-efficient tiling\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.15)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 7/32 (Score: 0.22)*\n\n```\nThis so-called FFT convolution scales in linear complexity when dealing with sequences with length $L$. ## 3. Why Combining Long Conv and Attention? As mentioned in Section 1, both long convolution and linear attention mechanisms have their limitations in spite of their widespread use and impressive accomplishments in sequence modeling. By bridging their benefits together, we push the limitation of the hybrid model, which allows us to benefit from the strong inductive bias of structured memory while still having the ability to learn complex datadependent patterns in true linear time with respect to length. The following two issues are the two main obstacles to extreme performance in long-sequence modeling. Linear Attention is the Efficiency Bottleneck. We can see from Fig. 1 right that as the input sequence grows, the actual efficiency of linear attention is much lower than that of S4, despite the fact that the complexity of the S 4 model is $\\mathcal{O}(L \\log L)$ as for long convolution and $\\mathcal{O}(L)$ for linear attention. The two obstacles to achieving a practical wall-clock speedup are intensive GPU memory access and cumulative summation (cumsum) in a casual setting. Inspired by Flashattention (Dao et al., 2022; Dao, 2023), we effectively doubled the speed of linear attention. Long Convolution Needs Short Convolutions. To match the efficiency of hardware-efficient linear attention, the long convolution is considered as a simpler and faster SSM model for long sequence abstraction. However, it requires additional constraint regularizations for stable performance on different data types. This is because it is hard for a single long convolutional kernel to learn multiple frequencies simultaneously, i.e., token patterns that repeat a lot versus those that occur more sparsely. We thereby introduce multiresolution short convolutional kernels to share the learning pressure of long convolution. They can also be reparameterized by simple linear transformation into a single kernel to speed up the inference phase. ![](https://cdn.mathpix.com/cropped/2024_09_17_542e0cd768b54533ad80g-04.jpg?height=610&width=1446&top_left_y=222&top_left_x=270)\n\nFigure 2. CHELA-model architecture. Figure (a) shows the CHELA layer. Figure (b) is a simplified graphic demonstration of the key components of CHELA: Short-Long Convolutions and Flash Linear Attention. HBM and SRAM denote high bandwidth memory and static random-access memory in GPU, respectively. ## 4. CHELA: Short-Long Convolutions with Hardware-Efficient Linear Attention\n\nIn this section, motivated by the above issues, we propose CHELA, short-long convolutions with hardware-efficient linear attention, which fully enjoys the benefits of each. We first describe how the tiling method is used in linear attention to reach the theoretical performance. We then introduce a simple and effective module named short-long convolutions to enhance the ability of global abstraction in this hybrid design. The blocks of CHELA are demonstrated in detailed architecture, including feed-forward and normalization layers. Moreover, we also discuss the relationship between CHELA and two closely related hybrid models: MEGA (Ma et al., 2022), SPADE (Zuo et al., 2023). ### 4.1. Hardware-Efficient Linear Attention\n\nIn response to the first question, we follow a tiling approach from GLA (Yang et al., 2024), which employs the classic\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_542e0cd768b54533ad80g-04.jpg?height=419&width=777&top_left_y=1924&top_left_x=209)\n\nFigure 3. Illustration Short-long Module and the effect of Short Convolution.\n```\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.09)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 2. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.02)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.02)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n#### 3. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.01)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 2/34 (Score: 0.01)*\n\n```\nThe proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with $5 x$ fewer model parameters than that of the previous-best Transformer-based models. ## 1 Introduction\n\nLinearly combining information using contentbased weights, a method generically known as attention, is a key building block in many deep neural networks such as recurrent neural networks (RNN) (Luong et al., 2015), convolutional neural networks (CNN) (Bello et al., 2019) and graph convolutional networks (GCN) (Velickovic et al., 2018). One particular type of such attention, called multi-head scaled dot-product attention, is one of the main components of the Transformer architecture proposed by Vaswani et al. (2017), which has been shown to push the state-of-the-art (SOTA) performance for various understanding and generation tasks. These include standard natural language processing (NLP) tasks such as machine translation, document classification, entailment, summarization and question answering (Zaheer et al., 2020; Dai et al., 2019; Baevski and Auli, 2019), as well as music generation (Huang et al., 2018), image generation (Parmar et al., 2018; Chen et al., 2020) and genomics (Zaheer et al., 2020; Choromanski et al., 2020). The Transformer is also the backbone architecture for models such as BERT (Devlin et al., 2019) (and its numerous relatives) and GPT3 (Brown et al., 2020), which have delivered impressive performance across many NLP tasks. However, the standard attention mechanism of the Transformer has a run time and memory usage that scales quadratically with sequence length. Therefore, this quadratic complexity has become a critical bottleneck in processing long sequences (over 1,000 tokens), and has since motivated many new attention algorithms, see (Tay et al., 2020d) for a survey of such work. In this paper, we draw inspiration from two branches in numerical analysis: Hierarchical Matrix (H-Matrix) (Hackbusch, 1999, 2000) and Multigrid method (Briggs et al., 2000). We propose a hierarchical attention that has linear complexity in run time and memory, and only utilizes dense linear algebra operations optimized for GPUs or TPUs. We hypothesize that the inductive bias embodied by the proposed hierarchical structure for the attention matrix is effective in capturing the hierarchical structure in the sequences typically seen in many natural language processing and computer vision tasks. The main benchmark we use in this paper is the Long Range Arena (LRA) benchmark (Tay et al., 2020c), which has been specifically designed to evaluate and compare various sub-quadratic attention algorithms. Our new hierarchical attention mechanism achieves best average performance to-date on the LRA benchmark by more than 6 points over the previous-best BigBird algorithm (Zaheer et al., 2020), while pushing SOTA performance higher\nin 4 of the 5 successful tasks. Furthermore, using this new attention, a Transformer-based language model trained on the One-Billion Word dataset (Chelba et al., 2014) sets a new SOTA performance record by reducing the test perplexity by 1.55 points comparing to the previousbest Transformer-XL (Dai et al., 2019) with 5x more parameters. Overall, these empirical results both validate the soundness of our approximation method for computing attention weights, as well as the the appropriateness of the inductive bias present in the proposed hierarchical attention. ## 2 Related Works\n\nIt is well established in the NLP literature that the embeddings of nearby tokens tend to be more similar than the distant ones (Manning and Sch\u00fctze, 1999). This leads to the intuition that token similarity and hence the attention should decrease with the sequence distance between a query token and a key token ${ }^{1}$. This motivates the sliding-window local attention (Parmar et al., 2018; Ramachandran et al., 2019; Qiu et al., 2019) which amounts to truncating off-diagonal entries in the attention matrix beyond a user-specified sequence distance. A second approach is to keep $O(1)$ number of nonzeros per row in the attention matrix. The nonzero entry selection is either content-based (Kitaev et al., 2020; Roy et al., 2020; Tay et al., 2020b; Zhou et al., 2020), handcrafted (Beltagy et al., 2020; Brown et al., 2020; Child et al., 2019; Ho et al., 2019) or simply random (Zaheer et al., 2020). It is also well known in the NLP literature that long-range contextual information is necessary for many NLP tasks (Khandelwal et al., 2018; Liu and Lapata, 2019). So a set of global tokens are also considered. This adds $O(1)$ number of dense rows and columns to the attention matrix (Zaheer et al., 2020; Ainslie et al., 2020; Beltagy et al., 2020). A third approach is to approximate the attention matrix with a lowrank factored form (Choromanski et al., 2020; Wang et al., 2020; Tay et al., 2020a). The first two approaches are based on the premise that one needs to explicitly zero out entries in the attention matrix in order to reduce the quadratic complexity. Decades of\n\n[^0]research by the scientific computing and numerical analysis community has resulted in more sophisticated algorithms to sparsify matrices. A small set of samples of these algorithms and their engineering applications include Fast Multipole Method (Greengard and Rokhlin, 1987; Greengard, 1994; Nabors et al., 1994; Shi et al., 1998), Pre-corrected FFT (Phillips and White, 1997; Zhu et al., 2005), Hierarchical Singular Value Decomposition (SVD) (Kapur and Long, 1997) and Hierarchical Matrix (H-Matrix) (Hackbusch, 1999, 2000; Zhu and White, 2005). These are generally called Multilevel Methods (Brandt and Lubrecht, 1990). The hierarchical attention proposed in this paper is inspired by these Multilevel Methods in general and the H-Matrix in particular. The hierarchical matrix structure allows a linear complexity in both constructing and applying the attention matrix. ## 3 Definition and Notation\n\nGiven matrices $Q, K$ and $V$, with rows representing sequences of token embedding or feature vectors for query, key and value respectively, the output weighted by the scaled dot-product attention in the Transformer (Vaswani et al., 2017) is defined as\n\n$$\nZ=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d}}\\right) V\n$$\n\nwhere $Z, Q, K, V \\in R^{L \\times d}, L$ is the length of the sequences, and $d$ is the embedding or feature size. In a more compact matrix form, Eq. (1) can be written as\n\n$$\nZ=D^{-1} A V\n$$\n\nwhere\n\n$$\n\\begin{aligned}\nA & =e^{S} \\\\\nS_{i, j} & =\\frac{Q_{i} K_{j}^{T}}{\\sqrt{d}} \\\\\nD & =\\operatorname{diag}\\left\\{A \\cdot \\mathbf{1}_{L}\\right\\} \\\\\n1_{L} & =[1,1, \\ldots, 1]^{T}\n\\end{aligned}\n$$\n\nHere, $A, S \\in R^{L \\times L}, \\mathbf{1}_{L} \\in R^{L}$ is a vector with all ones, and $S_{i, j}$ represents the unnormalized cosine similarity between query embedding $Q_{i}$ (the $i$-th row in $Q$ ) and key embedding $K_{j}$ (the $j$-th row in $K)$. For the sake of clarity, we focus on the singlehead attention in the exposition of the proposed\nalgorithm. Extension to the multi-head case is straightforward since each attention head is computed independently (Vaswani et al., 2017). Computing the similarity matrix $S$ in Eq. (4) and the attention matrix $A$ in Eq. (3) takes $O\\left(L^{2} d\\right)$ time and $O\\left(L^{2}\\right)$ memory. Similarly, computing $A V$ in Eq. (2) takes $O\\left(L^{2} d\\right)$ time, and computing $A \\cdot \\mathbf{1}_{L}$ in Eq. (5) takes $O\\left(L^{2}\\right)$ time. The $O\\left(L^{2} d\\right)$ and $O\\left(L^{2}\\right)$ complexities are the bottlenecks for applying the attention mechanism over very long sequences. ## 4 Introduction on H-Matrix and Multigrid Method\n\n### 4.1 H-Matrix\n\nThe singular-value decomposition of the attention matrix $A$ in Eq.\n```\n\n#### 4. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.01)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 8/16 (Score: 0.01)*\n\n```\nSee Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic memory compression, hierarchical attention, linear attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming\n\n*From Search Query: dynamic memory compression*\n\n*Jinuk Kim, Yeonwoo Jeong, Deokjae Lee, Hyun Oh Song*\n\n**TL;DR:** A subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent Convolution operations for efficient end-to-end inference latency is proposed.\n\n**Abstract:** Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evaluate our methods and baselines by TensorRT for a fair inference latency comparison. Our method outperforms the baseline method with higher accuracy and faster inference speed in MobileNetV2 on the ImageNet dataset. Specifically, we achieve $1.41\\times$ speed-up with $0.11$\\%p accuracy gain in MobileNetV2-1.0 on the ImageNet.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Clustering the Sketch: Dynamic Compression for Embedding Tables\n\n*From Search Query: dynamic memory compression*\n\n*Henry Ling-Hei Tsang, Thomas Dybdahl Ahle*\n\n**TL;DR:** Clustered Compositional Embeddings (CCE) is suggested which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and CompositionalEmbeddings to achieve the best of both worlds.\n\n**Abstract:** Embedding tables are used by machine learning systems to work with categorical features. In modern Recommendation Systems, these tables can be very large, necessitating the development of new methods for fitting them in memory, even during training. We suggest Clustered Compositional Embeddings (CCE) which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and Compositional Embeddings (Shi et al., 2020). Experimentally CCE achieves the best of both worlds: The high compression rate of codebook-based quantization, but *dynamically* like hashing-based methods, so it can be used during training. Theoretically, we prove that CCE is guaranteed to converge to the optimal codebook and give a tight bound for the number of iterations required.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4299  (*Influential: 554*)\n\n#### 4. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n#### 5. HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level\n\n*From Search Query: hierarchical attention*\n\n*Haoran Luo, E. Haihong, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin*\n\n**TL;DR:** A novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention, that addresses the issue of HKG multi-position prediction for the first time and achieves state-of-the-art performance in link prediction tasks on HKG standard datasets.\n\n**Abstract:** Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs\u2019 representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\n\n*From Search Query: linear attention*\n\n*Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Franccois Fleuret*\n\n**TL;DR:** This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.\n\n**Abstract:** Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 1339  (*Influential: 177*)\n\n#### 7. Linear attention is (maybe) all you need (to understand transformer optimization)\n\n*From Search Query: linear attention*\n\n*Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, A. Jadbabaie, S. Sra*\n\n**TL;DR:** The results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.\n\n**Abstract:** Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 4*)\n\n### 5 related papers from ArXiv\n\n#### 1. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: dynamic memory compression*\n\n*Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti*\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Published:** 2024-03-14T17:59:26Z  (*Updated: 2024-07-23T17:55:30Z*)\n\n\n\n#### 2. DiaNet: BERT and Hierarchical Attention Multi-Task Learning of\n  Fine-Grained Dialect\n\n*From Search Query: hierarchical attention*\n\n*Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Arun Rajendran, Lyle Ungar*\n\n**Abstract:** Prediction of language varieties and dialects is an important language\nprocessing task, with a wide range of applications. For Arabic, the native\ntongue of ~ 300 million people, most varieties remain unsupported. To ease this\nbottleneck, we present a very large scale dataset covering 319 cities from all\n21 Arab countries. We introduce a hierarchical attention multi-task learning\n(HA-MTL) approach for dialect identification exploiting our data at the city,\nstate, and country levels. We also evaluate use of BERT on the three tasks,\ncomparing it to the MTL approach. We benchmark and release our data and models.\n\n**Published:** 2019-10-31T03:56:32Z  (*Updated: 2019-10-31T03:56:32Z*)\n\n\n\n#### 3. Bidirectional Context-Aware Hierarchical Attention Network for Document\n  Understanding\n\n*From Search Query: hierarchical attention*\n\n*Jean-Baptiste Remy, Antoine Jean-Pierre Tixier, Michalis Vazirgiannis*\n\n**Abstract:** The Hierarchical Attention Network (HAN) has made great strides, but it\nsuffers a major limitation: at level 1, each sentence is encoded in complete\nisolation. In this work, we propose and compare several modifications of HAN in\nwhich the sentence encoder is able to make context-aware attentional decisions\n(CAHAN). Furthermore, we propose a bidirectional document encoder that\nprocesses the document forwards and backwards, using the preceding and\nfollowing sentences as context. Experiments on three large-scale sentiment and\ntopic classification datasets show that the bidirectional version of CAHAN\noutperforms HAN everywhere, with only a modest increase in computation time.\nWhile results are promising, we expect the superiority of CAHAN to be even more\nevident on tasks requiring a deeper understanding of the input documents, such\nas abstractive summarization. Code is publicly available.\n\n**Published:** 2019-08-16T15:20:04Z  (*Updated: 2019-08-16T15:20:04Z*)\n\n\n\n#### 4. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry\n\n*From Search Query: linear attention*\n\n*Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher R\u00e9*\n\n**Abstract:** Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.\n\n**Published:** 2024-02-06T19:31:26Z  (*Updated: 2024-02-06T19:31:26Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: linear attention*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks\n\n*From Search Query: dynamic memory compression*\n\n*Lili Mou, Yanshuai Cao, Yongchang Hao*\n\n**Abstract:** The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.\n\n**Published:** 2024-10-28\n\n\n\n#### 2. Build a Deep Neural Network model using CPUs Builds a feed-forward multilayer artificial neural network on an H2OFrame\n\n*From Search Query: dynamic memory compression*\n\n*Viraj Parmar, Erin LeDell, Jessica Lanford, Arno Candel, Anisha Arora*\n\n**Abstract:** H2O is fast, scalable, open-source machine learning and deep learning for\r\nsmarter applications. With H2O, enterprises like PayPal, Nielsen Catalina,\r\nCisco, and others can use all their data without sampling to get accurate\r\npredictions faster. Advanced algorithms such as deep learning, boosting, and\r\nbagging ensembles are built-in to help application designers create smarter\r\napplications through elegant APIs. Some of our initial customers have built\r\npowerful domain-speci\fc predictive engines for recommendations, customer\r\nchurn, propensity to buy, dynamic pricing, and fraud detection for the insurance,\r\nhealthcare, telecommunications, ad tech, retail, and payment systems industries.\r\nUsing in-memory compression, H2O handles billions of data rows in-memory,\r\neven with a small cluster. To make it easier for non-engineers to create complete\r\nanalytic work\r\nows, H2O's platform includes interfaces for R, Python, Scala,\r\nJava, JSON, and Co\u000beeScript/JavaScript, as well as a built-in web interface,\r\nFlow. H2O is designed to run in standalone mode, on Hadoop, or within a\r\nSpark Cluster, and typically deploys within minutes.\r\nH2O includes many common machine learning algorithms, such as generalized\r\nlinear modeling (linear regression, logistic regression, etc.), Na\u007f\u0010ve Bayes, principal\r\ncomponents analysis, k-means clustering, and others. H2O also implements\r\nbest-in-class algorithms at scale, such as distributed random forest, gradient\r\nboosting and deep learning. Customers can build thousands of models and\r\ncompare the results to get the best predictions.\r\nH2O is nurturing a grassroots movement of physicists, mathematicians, and\r\ncomputer scientists to herald the new wave of discovery with data science by\r\ncollaborating closely with academic researchers and industrial data scientists.\r\nStanford university giants Stephen Boyd, Trevor Hastie, Rob Tibshirani advise\r\nthe H2O team on building scalable machine learning algorithms. With hundreds\r\nof meetups over the past three years, H2O has become a word-of-mouth\r\nphenomenon, growing amongst the data community by a hundred-fold, and\r\nis now used by 30,000+ users and is deployed using R, Python, Hadoop, and\r\nSpark in 2000+ corporations.\n\n**Proceeding:** h2o-ai-inc-2015-9\n\n**Published:** 2015-09-01\n\n\n\n#### 3. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: hierarchical attention*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 4. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 5. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\n\n*From Search Query: linear attention*\n\n*Mike Lewis, Noah A. Smith, Ofir Press*\n\n**Abstract:** Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi's inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.\n\n**Conference:** train-short-test-long-attention-with-linear-1\n\n**Published:** 2021-08-27\n\n\n\n#### 6. Linformer: Self-Attention with Linear Complexity\n\n*From Search Query: linear attention*\n\n*Madian Khabsa, Hao Ma, Sinong Wang, Belinda Z. Li, Han Fang*\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n**Published:** 2020-06-08\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design using the concepts of **CompressedHierarchicalLinearAttention (CHLA)**, here are some key findings and suggestions based on the provided sources and the detailed analysis:\n\n### Dynamic Memory Compression\n\n- The concept of dynamic memory compression in attention mechanisms is well-explored in recent research. For instance, the use of KV-Cache eviction strategies is a common approach to reduce memory overhead. The \"Attention-Gate\" (AG) method described in is particularly relevant, as it uses a lightweight, trainable module to determine which tokens in the KV-Cache should be retained or discarded. This approach generates binary flags for each token, guiding both the computation of attention scores and the management of the KV-Cache, which aligns with the idea of using learned compression ratios per head.\n- Other strategies like H2O and SpAtten, which use accumulative attention scores for adaptive eviction, also offer insights into dynamic memory compression. These methods dynamically identify important tokens, reducing computational complexity and memory usage.\n\n### Hierarchical Attention Structures\n\n- Hierarchical attention structures that integrate short-long convolutions to capture both local and global patterns are innovative and promising. While the provided sources do not explicitly discuss hierarchical attention in the context of language models, the concept of capturing local and global patterns is akin to the multi-hierarchical feature interaction networks seen in other domains, such as image reconstruction.\n- The idea of hierarchical attention can be inferred from the multi-attention mechanisms used in other models. For example, the dual-branch encoder-decoder structure with multi-attention mechanisms in suggests that hierarchical attention can effectively capture key features at different levels, which could be adapted for language models.\n\n### Hardware-Efficient Implementation\n\n- For hardware-efficient implementation, the use of memristor-based in-memory computing as described in offers a promising approach. This method integrates computation into memory, reducing the computational latency and memory access overhead associated with traditional attention mechanisms. The memristor crossbar array can perform matrix operations efficiently, which is crucial for attention mechanisms.\n- The concept of hardware-efficient tiling and memory access patterns is also important. Techniques like those discussed in, such as the Attention-Gate method, which operates before the Multi-Head Attention (MHA) layer, can help in optimizing memory access and computational efficiency.\n\n### Key Aspects for Further Investigation\n\n1. **Dynamic Memory Compression**:\n   - Investigate the effectiveness of Attention-Gate and other adaptive eviction strategies in reducing memory overhead and improving computational throughput.\n   - Compare the learned compression ratios per head with static and other dynamic compression methods to assess their impact on efficiency and performance.\n\n2. **Hierarchical Attention Structures**:\n   - Adapt the concept of multi-hierarchical feature interaction networks to language models, integrating short-long convolutions to capture local and global patterns effectively.\n   - Evaluate the success of similar hierarchical attention approaches in other domains and their potential applicability to language models.\n\n3. **Hardware-Efficient Implementation**:\n   - Explore the feasibility of using memristor-based in-memory computing for attention mechanisms in language models, focusing on the reduction of computational latency and memory access overhead.\n   - Optimize tiling strategies and memory access patterns to enhance performance on long sequences, leveraging insights from existing hardware-efficient attention mechanisms.\n\n### Novelty Assessment\n\n- The integration of dynamic memory compression with hierarchical attention structures within a linear attention framework appears novel. However, it is crucial to compare this proposal with existing research, such as Infini-attention and CHELA, to fully assess its novelty and potential impact.\n- The use of learned compression ratios per head and the specific hierarchical attention structure proposed in CHLA should be evaluated against existing methods to determine their unique contributions and advantages.\n",
            "analysis": "### Initial Analysis of the Proposal\n\nThe proposal introduces **CompressedHierarchicalLinearAttention (CHLA)**, an enhancement to LinearAttention that combines dynamic memory compression with hierarchical attention structures. The key innovations include:\n\n1. **Dynamic Memory Compression**: This aims to reduce memory requirements by compressing key-value caches using learned compression ratios per head. This could potentially improve memory efficiency and computational throughput.\n\n2. **Hierarchical Attention Structure**: By integrating short-long convolutions, the proposal seeks to capture both local and global patterns more effectively, which could enhance model expressiveness and performance on long sequences.\n\n3. **Hardware-Efficient Implementation**: The proposal emphasizes hardware-efficient tiling and memory access patterns, aiming to improve computational efficiency and scalability.\n\n### Key Aspects for Further Investigation\n\n1. **Dynamic Memory Compression**:\n   - Investigate existing methods for memory compression in attention mechanisms and their effectiveness.\n   - Explore the novelty of using learned compression ratios per head and its potential impact on efficiency and performance.\n\n2. **Hierarchical Attention Structures**:\n   - Examine similar approaches in hierarchical attention and their success in improving model expressiveness and efficiency.\n   - Assess the integration of short-long convolutions and its impact on capturing local and global patterns.\n\n3. **Hardware-Efficient Implementation**:\n   - Review existing strategies for hardware-efficient attention mechanisms and their scalability.\n   - Evaluate the proposed tiling strategies and their potential to enhance performance on long sequences.\n\n### Plan for Investigations\n\n1. Conduct a broad search to gather information on dynamic memory compression techniques in attention mechanisms and hierarchical attention structures.\n2. Perform a detailed search in the internal vector store to find specific papers or implementations related to the proposed innovations.\n3. Compare the proposal with existing research and previous design proposals to assess its novelty and potential impact.\n\n### Novelty Assessment\n\nThe proposal appears to be novel in its integration of dynamic memory compression with hierarchical attention structures within a linear attention framework. However, it is essential to verify this by comparing it with existing research and previous design proposals, particularly those related to Infini-attention, CHELA, and similar architectures."
        },
        {
            "ready": false,
            "query": "dynamic memory compression, hierarchical attention networks, hardware-efficient linear attention",
            "detail": "Dynamic Memory Compression in Attention Mechanisms, Hierarchical Attention Structures in Language Models, Hardware-Efficient Tiling Strategies",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nDynamic Memory Compression in Attention Mechanisms, Hierarchical Attention Structures in Language Models, Hardware-Efficient Tiling Strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.15)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.27)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n##### *Relevant Chunk: No. 8/16 (Score: 0.04)*\n\n```\nSee Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.\n```\n\n#### 2. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.04)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.04)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.03)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.03)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.01)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.01)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n\n\n---\n## Found 15 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic memory compression, hierarchical attention networks, hardware-efficient linear attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming\n\n*From Search Query: dynamic memory compression*\n\n*Jinuk Kim, Yeonwoo Jeong, Deokjae Lee, Hyun Oh Song*\n\n**TL;DR:** A subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent Convolution operations for efficient end-to-end inference latency is proposed.\n\n**Abstract:** Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evaluate our methods and baselines by TensorRT for a fair inference latency comparison. Our method outperforms the baseline method with higher accuracy and faster inference speed in MobileNetV2 on the ImageNet dataset. Specifically, we achieve $1.41\\times$ speed-up with $0.11$\\%p accuracy gain in MobileNetV2-1.0 on the ImageNet.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Clustering the Sketch: Dynamic Compression for Embedding Tables\n\n*From Search Query: dynamic memory compression*\n\n*Henry Ling-Hei Tsang, Thomas Dybdahl Ahle*\n\n**TL;DR:** Clustered Compositional Embeddings (CCE) is suggested which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and CompositionalEmbeddings to achieve the best of both worlds.\n\n**Abstract:** Embedding tables are used by machine learning systems to work with categorical features. In modern Recommendation Systems, these tables can be very large, necessitating the development of new methods for fitting them in memory, even during training. We suggest Clustered Compositional Embeddings (CCE) which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and Compositional Embeddings (Shi et al., 2020). Experimentally CCE achieves the best of both worlds: The high compression rate of codebook-based quantization, but *dynamically* like hashing-based methods, so it can be used during training. Theoretically, we prove that CCE is guaranteed to converge to the optimal codebook and give a tight bound for the number of iterations required.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Document-Level Neural Machine Translation with Hierarchical Attention Networks\n\n*From Search Query: hierarchical attention networks*\n\n*Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson*\n\n**TL;DR:** Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.\n\n**Abstract:** Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model\u2019s own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 258  (*Influential: 59*)\n\n#### 4. Sentence-Level Evidence Embedding for Claim Verification with Hierarchical Attention Networks\n\n*From Search Query: hierarchical attention networks*\n\n*Jing Ma, Wei Gao, Shafiq R. Joty, Kam-Fai Wong*\n\n**TL;DR:** Experimental results on three public benchmark datasets show that the proposed end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim outperforms a set of state-of-the-art baselines.\n\n**Abstract:** Claim verification is generally a task of verifying the veracity of a given claim, which is critical to many downstream applications. It is cumbersome and inefficient for human fact-checkers to find consistent pieces of evidence, from which solid verdict could be inferred against the claim. In this paper, we propose a novel end-to-end hierarchical attention network focusing on learning to represent coherent evidence as well as their semantic relatedness with the claim. Our model consists of three main components: 1) A coherence-based attention layer embeds coherent evidence considering the claim and sentences from relevant articles; 2) An entailment-based attention layer attends on sentences that can semantically infer the claim on top of the first attention; and 3) An output layer predicts the verdict based on the embedded evidence. Experimental results on three public benchmark datasets show that our proposed model outperforms a set of state-of-the-art baselines.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 84  (*Influential: 16*)\n\n#### 5. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware-efficient linear attention*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 6. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\n\n*From Search Query: hardware-efficient linear attention*\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n### 3 related papers from ArXiv\n\n#### 1. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: dynamic memory compression*\n\n*Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti*\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Published:** 2024-03-14T17:59:26Z  (*Updated: 2024-07-23T17:55:30Z*)\n\n\n\n#### 2. Bidirectional Context-Aware Hierarchical Attention Network for Document\n  Understanding\n\n*From Search Query: hierarchical attention networks*\n\n*Jean-Baptiste Remy, Antoine Jean-Pierre Tixier, Michalis Vazirgiannis*\n\n**Abstract:** The Hierarchical Attention Network (HAN) has made great strides, but it\nsuffers a major limitation: at level 1, each sentence is encoded in complete\nisolation. In this work, we propose and compare several modifications of HAN in\nwhich the sentence encoder is able to make context-aware attentional decisions\n(CAHAN). Furthermore, we propose a bidirectional document encoder that\nprocesses the document forwards and backwards, using the preceding and\nfollowing sentences as context. Experiments on three large-scale sentiment and\ntopic classification datasets show that the bidirectional version of CAHAN\noutperforms HAN everywhere, with only a modest increase in computation time.\nWhile results are promising, we expect the superiority of CAHAN to be even more\nevident on tasks requiring a deeper understanding of the input documents, such\nas abstractive summarization. Code is publicly available.\n\n**Published:** 2019-08-16T15:20:04Z  (*Updated: 2019-08-16T15:20:04Z*)\n\n\n\n#### 3. Pruning and Sparsemax Methods for Hierarchical Attention Networks\n\n*From Search Query: hierarchical attention networks*\n\n*Jo\u00e3o G. Ribeiro, Frederico S. Felisberto, Isabel C. Neto*\n\n**Abstract:** This paper introduces and evaluates two novel Hierarchical Attention Network\nmodels [Yang et al., 2016] - i) Hierarchical Pruned Attention Networks, which\nremove the irrelevant words and sentences from the classification process in\norder to reduce potential noise in the document classification accuracy and ii)\nHierarchical Sparsemax Attention Networks, which replace the Softmax function\nused in the attention mechanism with the Sparsemax [Martins and Astudillo,\n2016], capable of better handling importance distributions where a lot of words\nor sentences have very low probabilities. Our empirical evaluation on the IMDB\nReview for sentiment analysis datasets shows both approaches to be able to\nmatch the results obtained by the current state-of-the-art (without, however,\nany significant benefits). All our source code is made available\nathttps://github.com/jmribeiro/dsl-project.\n\n**Published:** 2020-04-08T17:56:58Z  (*Updated: 2020-04-08T17:56:58Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. NeuZip: Memory-Efficient Training and Inference with Dynamic Compression of Neural Networks\n\n*From Search Query: dynamic memory compression*\n\n*Lili Mou, Yanshuai Cao, Yongchang Hao*\n\n**Abstract:** The performance of neural networks improves when more parameters are used. However, the model sizes are constrained by the available on-device memory during training and inference. Although applying techniques like quantization can alleviate the constraint, they suffer from performance degradation. In this work, we introduce NeuZip, a new weight compression scheme based on the entropy of floating-point numbers in neural networks. With NeuZip, we are able to achieve memory-efficient training and inference without sacrificing performance. Notably, we significantly reduce the memory footprint of training a Llama-3 8B model from 31GB to less than 16GB, while keeping the training dynamics fully unchanged. In inference, our method can reduce memory usage by more than half while maintaining near-lossless performance. Our code is publicly available.\n\n**Published:** 2024-10-28\n\n\n\n#### 2. Build a Deep Neural Network model using CPUs Builds a feed-forward multilayer artificial neural network on an H2OFrame\n\n*From Search Query: dynamic memory compression*\n\n*Viraj Parmar, Erin LeDell, Jessica Lanford, Arno Candel, Anisha Arora*\n\n**Abstract:** H2O is fast, scalable, open-source machine learning and deep learning for\r\nsmarter applications. With H2O, enterprises like PayPal, Nielsen Catalina,\r\nCisco, and others can use all their data without sampling to get accurate\r\npredictions faster. Advanced algorithms such as deep learning, boosting, and\r\nbagging ensembles are built-in to help application designers create smarter\r\napplications through elegant APIs. Some of our initial customers have built\r\npowerful domain-speci\fc predictive engines for recommendations, customer\r\nchurn, propensity to buy, dynamic pricing, and fraud detection for the insurance,\r\nhealthcare, telecommunications, ad tech, retail, and payment systems industries.\r\nUsing in-memory compression, H2O handles billions of data rows in-memory,\r\neven with a small cluster. To make it easier for non-engineers to create complete\r\nanalytic work\r\nows, H2O's platform includes interfaces for R, Python, Scala,\r\nJava, JSON, and Co\u000beeScript/JavaScript, as well as a built-in web interface,\r\nFlow. H2O is designed to run in standalone mode, on Hadoop, or within a\r\nSpark Cluster, and typically deploys within minutes.\r\nH2O includes many common machine learning algorithms, such as generalized\r\nlinear modeling (linear regression, logistic regression, etc.), Na\u007f\u0010ve Bayes, principal\r\ncomponents analysis, k-means clustering, and others. H2O also implements\r\nbest-in-class algorithms at scale, such as distributed random forest, gradient\r\nboosting and deep learning. Customers can build thousands of models and\r\ncompare the results to get the best predictions.\r\nH2O is nurturing a grassroots movement of physicists, mathematicians, and\r\ncomputer scientists to herald the new wave of discovery with data science by\r\ncollaborating closely with academic researchers and industrial data scientists.\r\nStanford university giants Stephen Boyd, Trevor Hastie, Rob Tibshirani advise\r\nthe H2O team on building scalable machine learning algorithms. With hundreds\r\nof meetups over the past three years, H2O has become a word-of-mouth\r\nphenomenon, growing amongst the data community by a hundred-fold, and\r\nis now used by 30,000+ users and is deployed using R, Python, Hadoop, and\r\nSpark in 2000+ corporations.\n\n**Proceeding:** h2o-ai-inc-2015-9\n\n**Published:** 2015-09-01\n\n\n\n#### 3. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: hierarchical attention networks*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n#### 4. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention networks*\n\n*Xiaodong He, Chris Dyer, Zichao Yang, Alex Smola, Eduard Hovy, Diyi Yang*\n\n**Abstract:** \n\n**Proceeding:** naacl-2016-6\n\n**Published:** 2016-06-01\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware-efficient linear attention*\n\n*Yoon Kim, Rameswar Panda, Yikang Shen, Bailin Wang, Songlin Yang*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Published:** 2023-12-11\n\n\n\n#### 6. EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction\n\n*From Search Query: hardware-efficient linear attention*\n\n*Chuang Gan, Junyan Li, Muyan Hu, Song Han, Han Cai*\n\n**Abstract:** High-resolution dense prediction enables many appealing real-world applications, such as computational photography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art high-resolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of high-resolution vision models with novel multi-scale linear attention. Unlike prior high-resolution dense prediction models that rely on heavy softmax attention, hardware-inefficient large-kernel convolution, or complicated topology structure to obtain good performances, our multi-scale linear attention achieves the global receptive field and multi-scale learning (two desirable features for high-resolution dense prediction) with only lightweight and hardware-efficient operations. As such, EfficientViT delivers remarkable performance gains over previous state-of-the-art models with significant speedup on diverse hardware platforms, including mobile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 13.9$\\times$ and 6.2$\\times$ GPU latency reduction over SegFormer and SegNeXt, respectively. For super-resolution, EfficientViT delivers up to 6.4x speedup over Restormer while providing 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers 48.9x higher throughput on A100 GPU while achieving slightly better zero-shot instance segmentation performance on COCO.\n\n**Published:** 2022-05-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on dynamic memory compression, hierarchical attention networks, and hardware-efficient linear attention, here are some key insights and relevant techniques from the provided sources and additional context:\n\n## Dynamic Memory Compression\n\n- **PyramidKV and ZipVL**: These approaches introduce dynamic KV cache compression techniques that can significantly reduce memory usage and latency. PyramidKV uses a pyramidal information funneling approach, where the most important KV pairs are stored in a highly accessible format, and less important ones are compressed and stored in a broader base, reducing the memory footprint and latency.\n- **ZipVL**: This framework employs dynamic token sparsification and mixed-precision quantization for the KV cache. It adaptively determines the ratio of important tokens and quantizes the KV cache accordingly, reducing computational and memory bottlenecks.\n- **LoMA and DMC**: These methods involve lossless compression of the KV cache and dynamic online key-value cache compression, respectively. LoMA reduces memory and computational demands during autoregressive generation, while DMC achieves significant throughput increases during inference[Analysis Summary].\n\n## Hierarchical Attention Networks\n\n- **Hierarchical Attention Structures**: The concept of hierarchical attention networks, as seen in document classification tasks, can be applied to language models. This involves applying attention mechanisms at multiple levels (e.g., word and sentence levels) to capture both local and global patterns. Integrating short-long convolutions in these structures can enhance the model's ability to focus on long sequences[Analysis Summary].\n- **PyramidKV's Insights**: The observation that attention is scattered widely in lower layers and consolidates in higher layers supports the use of hierarchical attention structures. PyramidKV's pyramidal compression approach allocates more cache in lower layers and less in higher ones, aligning with the hierarchical attention concept.\n\n## Hardware-Efficient Linear Attention\n\n- **Memristor-Based Acceleration**: Using memristor crossbar arrays can significantly improve the efficiency of matrix operations in attention mechanisms. This approach integrates computation into memory, reducing power consumption and latency, which is crucial for hardware-efficient implementations.\n- **CHELA and GLA Transformers**: These models combine short-long convolutions with linear attention and implement hardware-efficient training algorithms. CHELA replaces self-attention mechanisms with short-long convolutions, and GLA Transformers are effective at length generalization, enabling models to handle long sequences efficiently[Analysis Summary].\n- **ZipVL's Efficient Inference**: ZipVL's dynamic token sparsification and mixed-precision quantization also contribute to hardware-efficient inference by reducing the computational and memory demands, making it suitable for high-resolution images or videos.\n\n## Key Insights and Future Directions\n\n- **Integrated Approach**: Combining dynamic memory compression techniques like PyramidKV and ZipVL with hierarchical attention structures can lead to more efficient and scalable language models. This integration can capture both local and global patterns while minimizing memory and computational overhead.\n- **Hardware Optimization**: Leveraging memristor-based acceleration and hardware-efficient algorithms like those in CHELA and GLA Transformers can further enhance the performance of these models, especially on long sequences.\n- **Adaptive Compression and Attention**: Using adaptive compression ratios and attention mechanisms that dynamically adjust based on the importance of tokens or KV pairs can optimize both memory usage and computational efficiency.\n\nBy incorporating these insights, the CompressedHierarchicalLinearAttention (CHLA) model can be significantly improved, offering better scalability, efficiency, and performance in autoregressive language model tasks.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for CompressedHierarchicalLinearAttention (CHLA) introduces innovative concepts by integrating dynamic memory compression with hierarchical attention structures within a linear attention framework. The search results provide insights into similar approaches and existing research that can inform the evaluation of this proposal.\n\n### Useful References and Excerpts\n\n1. **Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences**:\n   - This paper introduces CHELA, which combines short-long convolutions with hardware-efficient linear attention. It addresses the efficiency bottleneck of linear attention and proposes a tiling approach to achieve theoretical performance. This aligns with the proposal's aim to integrate hierarchical structures and hardware-efficient strategies.\n   - *Excerpt*: \"CHELA... replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.\"\n\n2. **LoMA: Lossless Compressed Memory Attention**:\n   - LoMA introduces a method for lossless compression of the KV cache, reducing memory and computational demands. This is relevant to the proposal's dynamic memory compression aspect.\n   - *Excerpt*: \"LoMA... enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\"\n\n3. **Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference**:\n   - This paper discusses Dynamic Memory Compression (DMC) for online key-value cache compression, achieving significant throughput increases. It supports the proposal's focus on dynamic memory compression.\n   - *Excerpt*: \"DMC... achieves up to 7x throughput increase during auto-regressive inference.\"\n\n4. **Hierarchical Attention Networks for Document Classification**:\n   - This work explores hierarchical attention networks, which can inform the hierarchical attention structures proposed in CHLA.\n   - *Excerpt*: \"We propose a hierarchical attention network for document classification... it has two levels of attention mechanisms applied at the word and sentence level.\"\n\n5. **Gated Linear Attention Transformers with Hardware-Efficient Training**:\n   - This paper describes a hardware-efficient algorithm for linear attention, relevant to the proposal's focus on hardware-efficient implementation.\n   - *Excerpt*: \"GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K.\"\n\n### Key Insights and Detailed Analysis\n\n- **Dynamic Memory Compression**: The proposal's use of learned compression ratios per head is innovative and aligns with existing research on memory-efficient attention mechanisms like LoMA and DMC. These methods demonstrate the feasibility and potential benefits of dynamic compression in reducing memory and computational overhead.\n\n- **Hierarchical Attention Structures**: The integration of short-long convolutions in hierarchical attention structures is promising for capturing both local and global patterns. The success of similar approaches in other domains suggests potential applicability to language models.\n\n- **Hardware-Efficient Implementation**: The proposal's emphasis on hardware-efficient tiling and memory access patterns is supported by research on hardware-efficient linear attention, such as GLA Transformers. These strategies can enhance performance on long sequences.\n\n### Future Search Plan\n\n1. Conduct further searches on dynamic memory compression techniques in attention mechanisms to explore additional methods and their effectiveness.\n2. Investigate hierarchical attention structures in more detail, focusing on their application in language models and potential improvements in expressiveness and efficiency.\n3. Explore hardware-efficient implementations in greater depth, particularly those involving tiling strategies and memory access optimization.\n\n### References\n\n- Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv.org.\n- Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv.org.\n- Nawrot, P., et al. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. arXiv.org.\n- Yang, Z., et al. (2016). Hierarchical Attention Networks for Document Classification. NAACL.\n- Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv.org."
        },
        {
            "ready": false,
            "query": "lossless memory compression, short-long convolutions, hardware-efficient attention",
            "detail": "Lossless Memory Compression in Attention Models, Short-Long Convolutions in Hierarchical Attention, Hardware-Efficient Linear Attention Techniques",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nLossless Memory Compression in Attention Models, Short-Long Convolutions in Hierarchical Attention, Hardware-Efficient Linear Attention Techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 1.00)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 7/32 (Score: 1.00)*\n\n```\nThis so-called FFT convolution scales in linear complexity when dealing with sequences with length $L$. ## 3. Why Combining Long Conv and Attention? As mentioned in Section 1, both long convolution and linear attention mechanisms have their limitations in spite of their widespread use and impressive accomplishments in sequence modeling. By bridging their benefits together, we push the limitation of the hybrid model, which allows us to benefit from the strong inductive bias of structured memory while still having the ability to learn complex datadependent patterns in true linear time with respect to length. The following two issues are the two main obstacles to extreme performance in long-sequence modeling. Linear Attention is the Efficiency Bottleneck. We can see from Fig. 1 right that as the input sequence grows, the actual efficiency of linear attention is much lower than that of S4, despite the fact that the complexity of the S 4 model is $\\mathcal{O}(L \\log L)$ as for long convolution and $\\mathcal{O}(L)$ for linear attention. The two obstacles to achieving a practical wall-clock speedup are intensive GPU memory access and cumulative summation (cumsum) in a casual setting. Inspired by Flashattention (Dao et al., 2022; Dao, 2023), we effectively doubled the speed of linear attention. Long Convolution Needs Short Convolutions. To match the efficiency of hardware-efficient linear attention, the long convolution is considered as a simpler and faster SSM model for long sequence abstraction. However, it requires additional constraint regularizations for stable performance on different data types. This is because it is hard for a single long convolutional kernel to learn multiple frequencies simultaneously, i.e., token patterns that repeat a lot versus those that occur more sparsely. We thereby introduce multiresolution short convolutional kernels to share the learning pressure of long convolution. They can also be reparameterized by simple linear transformation into a single kernel to speed up the inference phase. ![](https://cdn.mathpix.com/cropped/2024_09_17_542e0cd768b54533ad80g-04.jpg?height=610&width=1446&top_left_y=222&top_left_x=270)\n\nFigure 2. CHELA-model architecture. Figure (a) shows the CHELA layer. Figure (b) is a simplified graphic demonstration of the key components of CHELA: Short-Long Convolutions and Flash Linear Attention. HBM and SRAM denote high bandwidth memory and static random-access memory in GPU, respectively. ## 4. CHELA: Short-Long Convolutions with Hardware-Efficient Linear Attention\n\nIn this section, motivated by the above issues, we propose CHELA, short-long convolutions with hardware-efficient linear attention, which fully enjoys the benefits of each. We first describe how the tiling method is used in linear attention to reach the theoretical performance. We then introduce a simple and effective module named short-long convolutions to enhance the ability of global abstraction in this hybrid design. The blocks of CHELA are demonstrated in detailed architecture, including feed-forward and normalization layers. Moreover, we also discuss the relationship between CHELA and two closely related hybrid models: MEGA (Ma et al., 2022), SPADE (Zuo et al., 2023). ### 4.1. Hardware-Efficient Linear Attention\n\nIn response to the first question, we follow a tiling approach from GLA (Yang et al., 2024), which employs the classic\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_542e0cd768b54533ad80g-04.jpg?height=419&width=777&top_left_y=1924&top_left_x=209)\n\nFigure 3. Illustration Short-long Module and the effect of Short Convolution.\n```\n\n#### 2. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.98)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.99)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n##### *Relevant Chunk: No. 8/16 (Score: 0.98)*\n\n```\nSee Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.\n```\n\n#### 3. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.78)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.78)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 4. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.74)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.74)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: lossless memory compression, short-long convolutions, hardware-efficient attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. SSMF: Shifting Seasonal Matrix Factorization\n\n*From Search Query: lossless memory compression*\n\n*Koki Kawabata, Siddharth Bhatia, R. Liu, Mohit Wadhwa, Bryan Hooi*\n\n**TL;DR:** This paper proposes Shifting Seasonal Matrix Factorization approach, namely SSMF, that can adaptively learn multiple seasonal patterns (called regimes), as well as switching between them, and demonstrates that the algorithm outperforms state-of-the-art baseline methods by accurately forecasting upcoming events on three real-world data streams.\n\n**Abstract:** Given taxi-ride counts information between departure and destination locations, how can we forecast their future demands? In general, given a data stream of events with seasonal patterns that innovate over time, how can we effectively and efficiently forecast future events? In this paper, we propose Shifting Seasonal Matrix Factorization approach, namely SSMF, that can adaptively learn multiple seasonal patterns (called regimes), as well as switching between them. Our proposed method has the following properties: (a) it accurately forecasts future events by detecting regime shifts in seasonal patterns as the data stream evolves; (b) it works in an online setting, i.e., processes each observation in constant time and memory; (c) it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. We demonstrate that our algorithm outperforms state-of-the-art baseline methods by accurately forecasting upcoming events on three real-world data streams.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: short-long convolutions*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Short-Term Memory Convolutions\n\n*From Search Query: short-long convolutions*\n\n*Grzegorz Stefa\u0144ski, Krzysztof Arendt, P. Daniluk, Bartlomiej Jasik, Artur Szumaczuk*\n\n**TL;DR:** A novel method for minimization of inference time latency and memory consumption, called Short-Term Memory Convolution (STMC) and its transposed counterpart and the training of STMC-based models is faster and more stable as the method is based solely on convolutional neural networks (CNNs).\n\n**Abstract:** The real-time processing of time series signals is a critical issue for many real-life applications. The idea of real-time processing is especially important in audio domain as the human perception of sound is sensitive to any kind of disturbance in perceived signals, especially the lag between auditory and visual modalities. The rise of deep learning (DL) models complicated the landscape of signal processing. Although they often have superior quality compared to standard DSP methods, this advantage is diminished by higher latency. In this work we propose novel method for minimization of inference time latency and memory consumption, called Short-Term Memory Convolution (STMC) and its transposed counterpart. The main advantage of STMC is the low latency comparable to long short-term memory (LSTM) networks. Furthermore, the training of STMC-based models is faster and more stable as the method is based solely on convolutional neural networks (CNNs). In this study we demonstrate an application of this solution to a U-Net model for a speech separation task and GhostNet model in acoustic scene classification (ASC) task. In case of speech separation we achieved a 5-fold reduction in inference time and a 2-fold reduction in latency without affecting the output quality. The inference time for ASC task was up to 4 times faster while preserving the original accuracy.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Efficient and Information-Preserving Future Frame Prediction and Beyond\n\n*From Search Query: short-long convolutions*\n\n*Wei Yu, Y. Lu, S. Easterbrook, S. Fidler*\n\n**TL;DR:** This work proposes CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor and demonstrates the transferability of the self-supervised learning method by exploiting its learnt features for object detection on KITTI.\n\n**Abstract:** Applying resolution-preserving blocks is a common practice to maximize information preservation in video prediction, yet their high memory consumption greatly limits their application scenarios. We propose CrevNet, a Conditionally Reversible Network that uses reversible architectures to build a bijective two-way autoencoder and its complementary recurrent predictor. Our model enjoys the theoretically guaranteed property of no information loss during the feature extraction, much lower memory consumption and computational efficiency. The lightweight nature of our model enables us to incorporate 3D convolutions without concern of memory bottleneck, enhancing the model's ability to capture both short-term and long-term temporal dependencies. Our proposed approach achieves state-of-the-art results on Moving MNIST, Traffic4cast and KITTI datasets. We further demonstrate the transferability of our self-supervised learning method by exploiting its learnt features for object detection on KITTI. Our competitive results indicate the potential of using CrevNet as a generative pre-training strategy to guide downstream tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 95  (*Influential: 16*)\n\n#### 5. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n\n*From Search Query: hardware-efficient attention*\n\n*Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han*\n\n**TL;DR:** This work designs Hardware-Aware Transformers with neural architecture search, and trains a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing, and performs an evolutionary search with a hardware latency constraint.\n\n**Abstract:** Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 243  (*Influential: 45*)\n\n### 6 related papers from Papers with Code\n\n#### 1. SqueezeLLM: Dense-and-Sparse Quantization\n\n*From Search Query: lossless memory compression*\n\n*Kurt Keutzer, Michael W. Mahoney, Sheng Shen, Xiuyu Li, Zhen Dong, Amir Gholami, Coleman Hooper, Sehoon Kim*\n\n**Abstract:** Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint. Our framework incorporates two novel ideas: (i) sensitivity-based non-uniform quantization, which searches for the optimal bit precision assignment based on second-order information; and (ii) the Dense-and-Sparse decomposition that stores outliers and sensitive weight values in an efficient sparse format. When applied to the LLaMA models, our 3-bit quantization significantly reduces the perplexity gap from the FP16 baseline by up to 2.1x as compared to the state-of-the-art methods with the same memory requirement. Furthermore, when deployed on an A6000 GPU, our quantized models achieve up to 2.3x speedup compared to the baseline. Our code is available at https://github.com/SqueezeAILab/SqueezeLLM.\n\n**Published:** 2023-06-13\n\n\n\n#### 2. SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression\n\n*From Search Query: lossless memory compression*\n\n*Dan Alistarh, Torsten Hoefler, Alexander Borzunov, Saleh Ashkboos, Elias Frantar, Denis Kuznedelev, Vage Egiazarian, Ruslan Svirschevski, Tim Dettmers*\n\n**Abstract:** Recent advances in large language model (LLM) pretraining have led to high-quality LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4 bits per parameter, they can fit into memory-limited devices such as laptops and mobile phones, enabling personalized use. However, quantization down to 3-4 bits per parameter usually leads to moderate-to-high accuracy losses, especially for smaller models in the 1-10B parameter range, which are well-suited for edge deployments. To address this accuracy issue, we introduce the Sparse-Quantized Representation (SpQR), a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales, while reaching similar compression levels to previous methods. SpQR works by identifying and isolating outlier weights, which cause particularly-large quantization errors, and storing them in higher precision, while compressing all other weights to 3-4 bits, and achieves relative accuracy losses of less than 1% in perplexity for highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter LLM on a single 24 GB consumer GPU without any performance degradation at 15% speedup thus making powerful LLMs available to consumer without any downsides. SpQR comes with efficient algorithms for both encoding weights into its format, as well as decoding them efficiently at runtime. Specifically, we provide an efficient GPU inference algorithm for SpQR which yields faster inference than 16-bit baselines at similar accuracy, while enabling memory compression gains of more than 4x.\n\n**Published:** 2023-06-05\n\n\n\n#### 3. Insights into LSTM Fully Convolutional Networks for Time Series Classification\n\n*From Search Query: short-long convolutions*\n\n*Fazle Karim, Somshubra Majumdar, Houshang Darabi*\n\n**Abstract:** Long Short Term Memory Fully Convolutional Neural Networks (LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN) have shown to achieve state-of-the-art performance on the task of classifying time series signals on the old University of California-Riverside (UCR) time series repository. However, there has been no study on why LSTM-FCN and ALSTM-FCN perform well. In this paper, we perform a series of ablation tests (3627 experiments) on LSTM-FCN and ALSTM-FCN to provide a better understanding of the model and each of its sub-module. Results from the ablation tests on ALSTM-FCN and LSTM-FCN show that the LSTM and the FCN blocks perform better when applied in a conjoined manner. Two z-normalizing techniques, z-normalizing each sample independently and z-normalizing the whole dataset, are compared using a Wilcoxson signed-rank test to show a statistical difference in performance. In addition, we provide an understanding of the impact dimension shuffle has on LSTM-FCN by comparing its performance with LSTM-FCN when no dimension shuffle is applied. Finally, we demonstrate the performance of the LSTM-FCN when the LSTM block is replaced by a GRU, basic RNN, and Dense Block.\n\n**Published:** 2019-02-27\n\n\n\n#### 4. fMRI-S4: learning short- and long-range dynamic fMRI dependencies using 1D Convolutions and State Space Models\n\n*From Search Query: short-long convolutions*\n\n*Guido van Wingen, Rajat Mani Thomas, Ahmed El-Gazzar*\n\n**Abstract:** Single-subject mapping of resting-state brain functional activity to non-imaging phenotypes is a major goal of neuroimaging. The large majority of learning approaches applied today rely either on static representations or on short-term temporal correlations. This is at odds with the nature of brain activity which is dynamic and exhibit both short- and long-range dependencies. Further, new sophisticated deep learning approaches have been developed and validated on single tasks/datasets. The application of these models for the study of a different targets typically require exhaustive hyperparameter search, model engineering and trial and error to obtain competitive results with simpler linear models. This in turn limit their adoption and hinder fair benchmarking in a rapidly developing area of research. To this end, we propose fMRI-S4; a versatile deep learning model for the classification of phenotypes and psychiatric disorders from the timecourses of resting-state functional magnetic resonance imaging scans. fMRI-S4 capture short- and long- range temporal dependencies in the signal using 1D convolutions and the recently introduced state-space models S4. The proposed architecture is lightweight, sample-efficient and robust across tasks/datasets. We validate fMRI-S4 on the tasks of diagnosing major depressive disorder (MDD), autism spectrum disorder (ASD) and sex classifcation on three multi-site rs-fMRI datasets. We show that fMRI-S4 can outperform existing methods on all three tasks and can be trained as a plug&play model without special hyperpararameter tuning for each setting\n\n**Published:** 2022-08-08\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware-efficient attention*\n\n*Yoon Kim, Rameswar Panda, Yikang Shen, Bailin Wang, Songlin Yang*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Published:** 2023-12-11\n\n\n\n#### 6. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: hardware-efficient attention*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on lossless memory compression, short-long convolutions in hierarchical attention, and hardware-efficient attention techniques, here are some key insights and references:\n\n### Lossless Memory Compression\n\n- **LoMA (Lossless Compressed Memory Attention)**: This approach introduces a method for lossless compression of the KV cache in attention mechanisms, which can significantly reduce memory and computational demands. LoMA enables efficient processing while maintaining the accuracy of the model, making it a valuable technique for autoregressive language models.\n\n### Short-Long Convolutions in Hierarchical Attention\n\n- **Interleaved Attention Mechanisms**: The concept of combining global attention with local sliding window attention, as seen in models like Ministral, can be effective. This approach uses both global attention and local sliding windows to capture long-range dependencies efficiently. The \"interleaved\" aspect suggests alternating between these attention mechanisms, which can be particularly useful in large language models to avoid computational overhead.\n\n- **CHELA (Combining Short-Long Convolutions with Hardware-Efficient Linear Attention)**: This method replaces traditional self-attention mechanisms with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach helps in focusing on both local and global patterns, enhancing the efficiency and expressiveness of the model[Analysis Summary].\n\n### Hardware-Efficient Attention Techniques\n\n- **Gated Linear Attention Transformers**: These models are designed with hardware efficiency in mind, particularly for training and inference. They are effective at length generalization, allowing models trained on shorter sequences to generalize well to longer sequences. This is achieved through optimized tiling and memory access patterns, which are crucial for efficient hardware utilization[Analysis Summary].\n\n- **Weight Pruning and Compression**: Techniques such as fine-grained pruning and weight compression can significantly reduce the memory footprint and improve the inference performance of large language models. Using methods like SW weight decompression and leveraging sparsity in MatMul operations can optimize the model for hardware efficiency, especially in memory-bound deployment scenarios.\n\n### Additional Considerations\n\n- **Retrieval-Augmented Generation (RAG)**: While not directly related to the core attention mechanisms, RAG techniques can enhance the performance of language models by providing access to external information. Methods like RECOMP, which compress retrieved documents into summaries, can help in reducing the computational overhead associated with incorporating external knowledge.\n\n### Future Directions\n\n1. **Deep Dive into Dynamic Memory Compression**:\n   - Investigate various dynamic memory compression techniques, such as LoMA and DMC, to explore their applicability and effectiveness in different scenarios. This includes analyzing the impact of learned compression ratios per head and the potential for real-time compression during inference.\n\n2. **Hierarchical Attention Structures**:\n   - Further research on hierarchical attention structures, including the use of short-long convolutions, can help in understanding how these mechanisms capture both local and global patterns. This could involve experiments to optimize the balance between short-range and long-range attention.\n\n3. **Hardware-Efficient Implementations**:\n   - Explore advanced hardware-efficient techniques, such as optimized tiling strategies and memory access patterns, to enhance the performance of autoregressive language models on various hardware platforms. This includes leveraging tools like OpenVINO and NNCF for model optimization and deployment.\n\nBy integrating these insights and techniques, the researchers can develop an autoregressive language model that is not only efficient in terms of memory and computation but also highly effective in capturing complex patterns and dependencies in language data.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for CompressedHierarchicalLinearAttention (CHLA) aims to enhance linear attention mechanisms by integrating dynamic memory compression and hierarchical attention structures. The search results provide relevant insights and techniques that align with the proposal's objectives and can inform its evaluation.\n\n### Useful References and Excerpts\n\n1. **LoMA: Lossless Compressed Memory Attention**:\n   - LoMA introduces a method for lossless compression of the KV cache, reducing memory and computational demands. This aligns with the proposal's dynamic memory compression aspect.\n   - *Excerpt*: \"LoMA... enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\"\n\n2. **Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference**:\n   - This paper discusses Dynamic Memory Compression (DMC) for online key-value cache compression, achieving significant throughput increases. It supports the proposal's focus on dynamic memory compression.\n   - *Excerpt*: \"DMC... achieves up to 7x throughput increase during auto-regressive inference.\"\n\n3. **Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences**:\n   - CHELA combines short-long convolutions with hardware-efficient linear attention, addressing efficiency bottlenecks. This aligns with the proposal's hierarchical attention structures and hardware-efficient strategies.\n   - *Excerpt*: \"CHELA... replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.\"\n\n4. **Gated Linear Attention Transformers with Hardware-Efficient Training**:\n   - This paper describes a hardware-efficient algorithm for linear attention, relevant to the proposal's focus on hardware-efficient implementation.\n   - *Excerpt*: \"GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K.\"\n\n### Key Insights and Detailed Analysis\n\n- **Dynamic Memory Compression**: The proposal's use of learned compression ratios per head is innovative and aligns with existing research on memory-efficient attention mechanisms like LoMA and DMC. These methods demonstrate the feasibility and potential benefits of dynamic compression in reducing memory and computational overhead.\n\n- **Hierarchical Attention Structures**: The integration of short-long convolutions in hierarchical attention structures is promising for capturing both local and global patterns. The success of similar approaches in other domains suggests potential applicability to language models.\n\n- **Hardware-Efficient Implementation**: The proposal's emphasis on hardware-efficient tiling and memory access patterns is supported by research on hardware-efficient linear attention, such as GLA Transformers. These strategies can enhance performance on long sequences.\n\n### Future Search Plan\n\n1. Conduct further searches on dynamic memory compression techniques in attention mechanisms to explore additional methods and their effectiveness.\n2. Investigate hierarchical attention structures in more detail, focusing on their application in language models and potential improvements in expressiveness and efficiency.\n3. Explore hardware-efficient implementations in greater depth, particularly those involving tiling strategies and memory access optimization.\n\n### References\n\n- Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv.org.\n- Nawrot, P., et al. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. arXiv.org.\n- Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv.org.\n- Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv.org."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.9309330000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.6805725,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "LinearAttention",
    "proposal": "## Title: CompressedHierarchicalLinearAttention: Efficient Long-Sequence Processing with Dynamic Memory Compression and Hierarchical Structures\n\n### Motivation\nCurrent linear attention mechanisms face several challenges in processing long sequences efficiently:\n1. High memory requirements for storing KV caches\n2. Limited ability to capture both local and global patterns effectively\n3. Inefficient hardware utilization and memory access patterns\n4. Degraded performance with increasing sequence lengths\n\nThis proposal aims to address these limitations by combining dynamic memory compression with hierarchical attention structures and hardware-efficient processing strategies.\n\n### Problem Analysis\nTraditional linear attention mechanisms, while reducing computational complexity from quadratic to linear, still face significant challenges:\n1. Memory bottlenecks from storing full KV caches\n2. Inefficient processing of local and global contexts\n3. Suboptimal hardware utilization\n4. Limited ability to adapt to varying sequence lengths\n\n### Core Idea and Philosophy\nCompressedHierarchicalLinearAttention (CHLA) introduces three key innovations:\n1. Dynamic memory compression with learned compression ratios per head\n2. Hierarchical attention structure with short-long convolutions\n3. Hardware-efficient tiling and memory access patterns\n\nThe core philosophy is to maintain model expressiveness while significantly improving efficiency through:\n- Adaptive compression of KV caches\n- Multi-scale processing of sequences\n- Hardware-aware implementation strategies\n\n### Design Plan\n#### Selected GAU to Modify\n```selection\nLinearAttention\n```\n\n#### Modification Overview\nCHLA enhances LinearAttention through:\n1. Integration of Dynamic Memory Compression (DMC)\n2. Addition of hierarchical processing with short-long convolutions\n3. Implementation of hardware-efficient tiling strategies\n\n#### Detailed Modifications\n\n1. **Dynamic Memory Compression**:\n   ```python\n   class DynamicCompression(nn.Module):\n       def __init__(self, dim, num_heads):\n           super().__init__()\n           self.compression_ratios = nn.Parameter(torch.ones(num_heads))\n           self.importance_net = nn.Linear(dim, num_heads)\n           \n       def forward(self, kv_cache):\n           importance = self.importance_net(kv_cache)\n           compressed = self.compress(kv_cache, importance)\n           return compressed\n   ```\n\n2. **Hierarchical Processing**:\n   ```python\n   class HierarchicalProcessor(nn.Module):\n       def __init__(self, dim, num_levels=3):\n           super().__init__()\n           self.short_conv = nn.Conv1d(dim, dim, kernel_size=3)\n           self.long_conv = nn.Conv1d(dim, dim, kernel_size=7)\n           self.level_weights = nn.Parameter(torch.ones(num_levels))\n           \n       def forward(self, x):\n           short_out = self.short_conv(x)\n           long_out = self.long_conv(x)\n           return self.combine_levels(short_out, long_out)\n   ```\n\n3. **Hardware-Efficient Implementation**:\n   ```python\n   class CHLABlock(nn.Module):\n       def __init__(self, dim, num_heads, tile_size=256):\n           super().__init__()\n           self.compression = DynamicCompression(dim, num_heads)\n           self.hierarchy = HierarchicalProcessor(dim)\n           self.tile_size = tile_size\n           \n       def process_tile(self, tile):\n           compressed = self.compression(tile)\n           processed = self.hierarchy(compressed)\n           return processed\n           \n       def forward(self, x):\n           return self.process_with_tiling(x, self.tile_size)\n   ```\n\n### Implementation Guidelines\n\n1. **Dynamic Compression Implementation**:\n```python\ndef compress_kv_cache(self, cache, ratios):\n    # Apply learned compression ratios per head\n    importance = self.compute_importance(cache)\n    mask = importance > self.get_threshold(ratios)\n    return self.apply_compression(cache, mask)\n```\n\n2. **Hierarchical Processing**:\n```python\ndef process_hierarchical(self, x):\n    # Process at multiple scales\n    local_features = self.short_range_process(x)\n    global_features = self.long_range_process(x)\n    return self.adaptive_combine(local_features, global_features)\n```\n\n3. **Hardware-Efficient Tiling**:\n```python\ndef process_with_tiling(self, x, tile_size):\n    tiles = self.split_into_tiles(x, tile_size)\n    processed_tiles = [self.process_tile(t) for t in tiles]\n    return self.merge_tiles(processed_tiles)\n```\n\n### Mathematical Formulation\n\n1. **Dynamic Compression**:\n   For each attention head h:\n   ```\n   r_h = \u03c3(w_h \u00b7 x + b_h)  # Compression ratio\n   c_h = DMC(KV_cache, r_h)  # Compressed cache\n   ```\n\n2. **Hierarchical Processing**:\n   ```\n   y_local = Conv_s(x)  # Short-range convolution\n   y_global = Conv_l(x)  # Long-range convolution\n   y = \u03b1\u00b7y_local + (1-\u03b1)\u00b7y_global  # Adaptive combination\n   ```\n\n3. **Attention Computation**:\n   ```\n   A(Q,K,V) = softmax(QK^T/\u221ad)V  # Standard attention\n   A_compressed = A(Q,compress(K),compress(V))  # Compressed attention\n   ```\n\n### Research Summary\nKey findings from research:\n1. Dynamic Memory Compression can achieve up to 7x throughput increase\n2. Short-long convolutions improve efficiency for long sequences\n3. Hardware-aware implementations significantly impact performance\n4. Hierarchical structures enable better scaling with sequence length\n\n### Evolution of Design\nThe design evolved through:\n1. Initial focus on memory compression\n2. Integration of hierarchical processing\n3. Addition of hardware optimization\n4. Refinement of compression strategies\n\n### Theoretical Analysis\n\n1. **Complexity Analysis**:\n   - Time complexity: O(n) for linear attention\n   - Memory complexity: O(n/r) where r is compression ratio\n   - Computational savings: (1-1/r) * 100% reduction\n\n2. **Performance Benefits**:\n   - Reduced memory footprint through compression\n   - Improved processing efficiency via hierarchical structure\n   - Enhanced hardware utilization through tiling\n\n### Conclusion\nCompressedHierarchicalLinearAttention represents a significant advancement in efficient language model design by:\n1. Reducing memory requirements through dynamic compression\n2. Improving processing efficiency with hierarchical structures\n3. Enhancing hardware utilization through efficient implementation\n\nThe design maintains model expressiveness while significantly improving efficiency and scalability.\n\n### References\n\n[1] Nawrot, P., et al. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. ICML.\n\n[2] Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. ICML.\n\n[3] Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv:2312.06635.\n\n[4] Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv:2401.08541.\n\n[5] Qin, G., et al. (2023). Dodo: Dynamic Contextual Compression for Decoder-only LMs. ACL.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "gpt4o_0806",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "This proposal introduces CompressedHierarchicalLinearAttention (CHLA), a novel enhancement to LinearAttention that combines dynamic memory compression with hierarchical attention structures. By integrating lossless KV cache compression, short-long convolutions, and hardware-efficient hierarchical processing, CHLA achieves superior efficiency and performance scaling for long sequences while maintaining model expressiveness. The design leverages insights from Dynamic Memory Compression and CHELA architectures to create a more robust and hardware-aware attention mechanism.",
    "ideation": null,
    "modelname": "gpt_2_chla",
    "suggestions": "- **Empirical Validation:** Conduct comprehensive experiments to validate the proposed enhancements, including benchmarks comparing performance, memory usage, and efficiency against baseline models.\n- **Implementation Details:** Provide more detailed guidelines for integrating dynamic memory compression and hierarchical processing, including potential challenges and solutions.\n- **Hardware Optimization Strategies:** Outline specific strategies for optimizing the design for different hardware architectures, leveraging parallel processing capabilities and efficient tensor operations.\n- **Training Stability:** Investigate potential training stability issues and incorporate techniques such as gradient clipping or adaptive learning rates to ensure robust training dynamics.",
    "user_input": ""
}