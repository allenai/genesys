{
    "variantname": null,
    "review": "The MoE-Gated-AttnGPT proposal presents an innovative approach to enhancing model capacity and efficiency through the integration of Sparse Mixture of Experts (MoE) with hierarchical adaptive attention. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Technical Merit:\n- Novel integration of product key-based routing with hierarchical attention\n- Well-designed load balancing mechanism for expert utilization\n- Thoughtful consideration of hardware efficiency\n- Clear mathematical formulation of the architecture\n\n2. Theoretical Foundation:\n- Strong grounding in MoE theory and recent advances\n- Clear connection to existing research in efficient attention mechanisms\n- Well-reasoned approach to expert routing\n- Careful consideration of stability issues\n\n3. Efficiency Considerations:\n- Linear complexity through sparse expert activation\n- Memory-efficient through selective parameter updates\n- Potential for improved parallel processing\n- Hardware-aware design considerations\n\n4. Scalability:\n- Design scales well with increasing number of experts\n- Efficient handling of longer sequences\n- Clear strategy for managing large expert pools\n- Consideration of memory constraints\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex integration of MoE with hierarchical attention\n- Potential challenges in routing mechanism implementation\n- Need for careful coordination between components\n- Additional hyperparameters requiring tuning\n\n2. Training Stability:\n- Potential issues with expert specialization\n- Complex interaction between attention and routing\n- Need for careful initialization strategies\n- Load balancing challenges at scale\n\n3. Memory Management:\n- Overhead from expert parameter storage\n- Complex memory access patterns\n- Potential cache efficiency issues\n- Trade-offs between expert count and memory usage\n\n4. Hardware Dependencies:\n- Requires specific hardware optimizations\n- Complex parallelization requirements\n- Memory bandwidth considerations\n- Implementation efficiency depends on hardware capabilities",
    "search_stack": [
        {
            "ready": false,
            "query": [
                "Mixture of Experts Transformers",
                "Sparse Mixture of Experts",
                "Sparsely Gated MLP"
            ],
            "detail": "Looking for internal literature on integrating Mixture of Experts (MoE) into the MLP layers of Transformers, focusing on sparsely gated mechanisms to enhance model capacity and efficiency without significant computational overhead.",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nLooking for internal literature on integrating Mixture of Experts (MoE) into the MLP layers of Transformers, focusing on sparsely gated mechanisms to enhance model capacity and efficiency without significant computational overhead.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. Mixture of A Million Experts (Avg. Score: 1.00)\n\n*Xu Owen He*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.\n\n**Abstract:** The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.\n\n##### *Relevant Chunk: No. 2/21 (Score: 1.00)*\n\n```\nSparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the finegrained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-01.jpg?height=844&width=1654&top_left_y=1334&top_left_x=243)\n\nFigure 1: Isoflop comparison on the C4 dataset between PEER and other baselines with two different FLOP budgets ( $6 e 18$ and $2 e 19$ FLOPs).\n```\n\n##### *Relevant Chunk: No. 9/21 (Score: 1.00)*\n\n```\n3 4}$ | $\\mathbf{1 0 . 3 3}$ | $\\mathbf{1 4 . 9 9}$ | $\\mathbf{1 9 . 0 9}$ | $\\mathbf{1 6 . 4 5}$ |\n\n### 3.3 Ablations\n\nVarying the Number of Total Experts The models in the isoFLOP plot depicted in Fig. 1 all have over a million $\\left(1024^{2}\\right)$ experts. Here we conduct an ablation study on the effect of the number of experts $N$, which determines the total parameter count $P$ in Eq. 9 . We selected the model at the isoFLOP-optimal position and vary the number of experts $\\left(N=128^{2}, 256^{2}, 512^{2}, 1024^{2}\\right)$ in the PEER layer while keeping the number of active experts constant $(h=8, k=16)$. The results are shown in Fig. 3 (a). As can be seen, the isoFLOP curve interpolates between the PEER model with $1024^{2}$ experts and the corresponding dense backbone without replacing the FFW layer in the middle block by a PEER layer. This demonstrates that simply increasing the number experts can improve model performance. Varying the Number of Active Experts We also conducted an ablation study on the effect of the number of active experts $h k$, which equals the granularity $G$ in Eq. 9. We systematically varied the number of\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-07.jpg?height=852&width=1656&top_left_y=289&top_left_x=240)\n\nFigure 3: We conduct two ablation studies using the same PEER model configuration. In (a), we vary the total number of experts $N$ while keeping the same number of active experts $h k=128$. In (b), we vary the number of active experts $G=h k$ by jointly changing $h$ and $k$ while keeping the total number of experts at $N=1024^{2}$. active experts $(h k=32,64,128,256,512)$ while keeping the number of total experts constant $\\left(N=1024^{2}\\right)$. Furthermore, for a given $h k$, we jointly varied $h$ and $k$ to identify the optimal composition. The resulting isoFLOP curves, plotted over the number of heads $(h)$, are shown in Fig. 3(b). The results indicate that, within the range of values considered, higher $h k$ generally leads to improved performance. Notably, the optimal $h$ increases as $h k$ increases. However, the performance gradually saturates, and increasing the number of active experts also increases device memory consumption and may necessitate additional accelerator devices. Thus in practice, the appropriate $h k$ values should be selected based on the trade-off between performance, device number and computational resource requirements. Table 2: KL and expert usage for different memory sizes, with and without query BN. Similar to the findings in PKM, using query BN results in a more balanced usage of the experts. | Expert num $N$ | 16 k |  | 65 k |  | 262 k |  | 1 M |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| BatchNorm | No | Yes | No | Yes | No | Yes | No | Yes |\n| Perplexity | 23.47 | 23.47 | 22.61 | 22.55 | 21.54 | 21.47 | 20.73 | 20.64 |\n| Expert Usage (\\%) | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 99.8 | 100.0 |\n| Unevenness ( $\\downarrow$ ) | 0.45 | 0.30 | 0.63 | 0.44 | 0.97 | 0.66 | 1.52 | 1.06 |\n\nExpert Usage and Query Batch Normalization Given the presence of over a million experts in the PEER layer, it is natural to inquire how many of these experts are actually selected during inference and whether their usage is evenly distributed. To analyze this, we kept an accumulated router score, denoted as $z_{i}^{\\prime}=$ $\\sum_{x} g_{i}(x)$ for each expert $e_{i}$ across all tokens $x$ within the C 4 validation set. Here $g_{i}(x)$ is the router score used to aggregate the expert output when token $x$ is given as input, with $g_{i}(x)=0$ if expert $e_{i}$ is not selected. From these accumulated router scores, we can obtain an empirical probability distribution vector, denoted as $z=z^{\\prime} /\\left\\|z^{\\prime}\\right\\|_{1}$, representing the distribution of all experts over the C 4 validation set. Then we computed the following metrics proposed by Lample et al. (2019) to assess the usage and distribution of experts:\n\n- Expert Usage: the fraction of experts retrieved during inference: $\\#\\left\\{z_{i} \\neq 0\\right\\}$\n- Unevenness: KL divergence between $z$ and the uniform distribution: $\\log (N)+\\sum_{i} z_{i} \\log \\left(z_{i}\\right)$\nwhere $N$ is the number of total experts. By default, we also added a batch normalization (BN) layer on top of the query network, as proposed by Lample et al. (2019) to increase the expert usage during training. Here we study the effect of adding this BN layer on the above-mentioned metrics. Table 2 presents the expert usage and unevenness for varying numbers of experts, with and without BN. We can see that even for 1 M experts, the expert usage is close to $100 \\%$, and using BN can lead to more balanced utilization of the experts and lower perplexities. These findings demonstrate the effectiveness of the PEER model in utilizing a large number of experts. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-08.jpg?height=722&width=985&top_left_y=845&top_left_x=559)\n\nFigure 4: Query BatchNorm Ablation. IsoFLOP curves of a PEER model with 1 M experts on the C4 dataset, with and without query BatchNorm. We additionally compared isoFLOP curves with and without BN. Fig. 4 shows that the PEER model with BN generally achieves lower perplexities. While the difference is not significant, it is most pronounced around the isoFLOP-optimal region. ## 4 Related Works\n\nMixture of Expert Since Shazeer et al. (2017) demonstrated the effectiveness of sparsely-gated Mixtures of Experts (MoEs) in efficiently increasing model capacity on GPU clusters, MoEs have emerged as a popular technique for scaling large models efficiently. Subsequent research (Fedus et al. 2022 Lepikhin et al. 2020 , Du et al., 2022 has proposed variations to address challenges such as load balancing, communication overhead, and training instability. These methods usually replace feedforward (FFW) layers in certain Transformer blocks with sparsely-gated MoE layers, which consist of multiple FFW layers as experts. Typically each expert matches the size of the regular dense FFW layer. Gating scores are calculated for each expert and token, and only the top k experts are activated for each token. These methods are known as token-choice methods. More recently, Zhou et al. (2022) introduced the Expert Choice routing method, where experts choose the top k tokens instead of tokens selecting experts. However, both token-choice and expert-choice methods require the top-k operator on a gating score matrix of size $N \\times M$ ( $N$ : number of experts, $M$ : number of tokens), resulting in a routing cost of at least $O(N)$. This limits their practical application to a small number of experts (typically less than 128). Instead of using the top-k operator, some works also proposed using deterministic hash tables as routers (Roller et al. 2021, dos Santos et al. 2023). With $O(1)$ average lookup complexity, these methods offer potential scalability to a large number of experts. However, these routers are fixed and not learned. Clark et al. (2022) showed that deterministic routing does not scale as well as trainable routers. Furthermore, Dikkala et al. (2023) proved theoretically that learned routers offer non-trivial advantages over their fixed counterparts, such as removing spurious directions and identifying latent clusters in data. In contrast to previous works, the proposed PEER layer employs a learned router with sublinear $(O(\\sqrt{N}))$ complexity. Since PEER uses lightweight experts, our work is also related to recent studies on parameter-efficient MoEs Wang et al. 2022, Zadouri et al., 2024). These methods utilize parameter efficient fine-tuning (PEFT) adapters as experts instead of full-sized FFWs. Their focus is on minimizing the number of parameters updated during fine-tuning, allowing storage of only one copy of the large backbone model. In PEER, parameter efficiency refers to the small number of active parameters in the MoE layer, which directly affects FLOPs and activation memory consumption during pre-training and inference. However, PEER could potentially be adapted to retrieve a large number of PEFT adapters. Retrieval-Augmented Models Our proposed method, with its retrieval mechanism for a large number of experts, aligns with the emerging field of retrieval-augmented models. These models facilitate large model memorization by retrieving knowledge from external databases, leading to improved accuracy and efficiency on knowledge-intensive tasks. Some notable works in this domain include ones by Khandelwal et al. (2019); Borgeaud et al. (2022a); Guu et al. (2020). While these methods retrieve data in various formats, for instance, tokens (Khandelwal et al. 2019), chunks (Borgeaud et al. 2022b) or knowledge graphs (Kang et al., 2023) (see (Gao et al. 2023) for a comprehensive survey on this topic), they differ from the proposed method in that they retrieve data rather than learned functions (experts). This distinction sets our parameter-efficient expert retrieval approach apart from existing retrieval-augmented models. Efficient Feedforward Layers Enhancing the efficiency of feedforward networks has been a long-standing area of research. Similar to PEER, most approaches are based on the idea of conditional computation (Bengio, 2013), where a gating mechanism is trained to determine which subset of neurons to compute. For instance, Davis \\& Arel (2013) utilized low-rank weight matrix approximation to estimate the sign of prenonlinearity activations. Neurons with negative activations are omitted as they will produce zeros after the nonlinearity. Bengio et al. (2015) explored reinforcement learning to develop an activation-dependant policy for dropping blocks of neurons. More recently, Belcak \\& Wattenhofer (2023) introduced the Fast FeedForward (FFF) layer that employs a differentiable balanced binary tree to select a neuron block for computation.\n```\n\n##### *Relevant Chunk: No. 20/21 (Score: 1.00)*\n\n```\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1525-1534, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1144. URL https://www.aclweb.org/ anthology/P16-1144. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1-67, 2020. Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:17555-17566, 2021. Noam Shazeer. Glu variants improve transformer, 2020. URL https://arxiv.org/abs/2002.05202\nNoam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=B1ckMDqlg\n\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. AdaMix: Mixture-of-adaptations for parameter-efficient model tuning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5744-5760, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.388. URL https://aclanthology.org/2022.emnlp-main. 388\n\nJiazuo Yu, Yunzhi Zhuge, Lu Zhang, Ping Hu, Dong Wang, Huchuan Lu, and You He. Boosting continual learning of vision-language models via mixture-of-experts adapters. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 23219-23230, June 2024. Ted Zadouri, Ahmet \u00dcst\u00fcn, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, and Sara Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= EvDeiLv7qc. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, Quoc V Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:7103-7114, 2022.\n```\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\nThe $x$ axis is in $\\log$ scale. ## 1 Introduction\n\nThe past few years have seen the power of scaling (Kaplan et al. 2020 Hoffmann et al. 2022): increasing the number of parameters, amount of training data, or the computational budget has proven to be a reliable\nway to improve model performance. Notably, feedforward (FFW) layers, responsible for storing factual knowledge (Geva et al., 2021, Dai et al. 2022, account for two-thirds of the total parameters in a transformer. However, one drawback of these dense FFWs is that their computational footprint (FLOPs and device memory consumption) is linearly proportional to their parameter count. To break the coupling between computational cost and parameter count, many recent works (Shazeer et al. 2017, Lepikhin et al. 2020; Fedus et al., 2022, Zhou et al. 2022) have adopted the Mixture-of-Experts (MoE) architecture, which uses a set of sparsely activated expert modules (often FFWs) in place of a single dense FFW. Clark et al. (2022) studied the scaling law of MoE language models and showed that increasing the number of experts is an effective way to improve performance without increasing the inference cost. However, their experiments showed that the efficiency gains provided by MoEs plateau after a certain model size is reached. More recently, Krajewski et al. (2024) discovered that this plateau was caused by using a fixed number of training tokens. When the number of training tokens is compute-optimal, MoEs consistently outperform dense models in terms of FLOP efficiency. Moreover, they introduced granularity (the number of active experts) as a new scaling axis and empirically showed that using higher granularity improves performance. Extrapolating this fine-grained MoE scaling law suggests that continued improvement of model capacity will ultimately lead to a large model with high granularity, corresponding to an architecture of an immense number of tiny experts. Beyond efficient scaling, another reason to have a vast number of experts is lifelong learning, where MoE has emerged as a promising approach Aljundi et al., 2017, Chen et al., 2023, Yu et al., 2024, Li et al. 2024). For instance, Chen et al. (2023) showed that, by simply adding new experts and regularizing them properly, MoE models can adapt to continuous data streams. Freezing old experts and updating only new ones prevents catastrophic forgetting and maintains plasticity by design. In lifelong learning settings, the data stream can be indefinitely long or never-ending Mitchell et al. 2018, necessitating an expanding pool of experts. Although both efficient scaling and lifelong learning require MoE designs capable of handling a vast number of experts, to the best of our knowledge, the only architecture supporting more than ten thousands of experts is the Mixture of Word Experts (MoWE) dos Santos et al.\n```\n\n#### 2. Jamba: A Hybrid Transformer-Mamba Language Model  (Avg. Score: 0.99)\n\n*Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, S. Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avshalom Manevich, Nir Ratner, N. Rozen, Erez Shwartz, Mor Zusman, Y. Shoham*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 44  (*Influential: 5*)\n\n**TL;DR:** Jamba is presented, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture that provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\n\n**Abstract:** We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.\n\n##### *Relevant Chunk: No. 23/24 (Score: 0.99)*\n\n```\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, 2019. [53] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [54] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. ST-MoE: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. [55] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. Efficient long sequence modeling via state space augmented transformer. arXiv preprint arXiv:2212.08136, 2022. [^0]:    *Equal contribution. [^1]:    ${ }^{2}$ The figure shows a potential Attention MoE layer, which our architecture does not use, but future variants could. [^2]:    ${ }^{3}$ Referring to end-to-end throughput (encoding+decoding). The results should be taken relatively rather than absolutely, as they are without possible optimizations. [^3]:    ${ }^{4}$ F1 score is the recommended metric in L-Eval [2]. In addition, our setup calibrates the length of the few-shot completions to approximately match the length of the test completion, increasing the credibility of F1. ${ }^{5}$ In all the ablations, \"pure Mamba\" refers to models with Mamba layers interleaved with MLP layers. [^4]:    ${ }^{6}$ There is also initial evidence that MoE helps Mamba layers, albeit at small model and data scale [38]. [^5]:    ${ }^{7}$ Some prior evidence suggested that Transformer decoder models do not need positional encodings [20]. However, all existing large scale models do use some sort of explicit position information.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: Mixture of Experts Transformers, Sparse Mixture of Experts, Sparsely Gated MLP\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. StableMoE: Stable Routing Strategy for Mixture of Experts\n\n*From Search Query: Mixture of Experts Transformers*\n\n*Damai Dai, Li Dong, Shuming Ma, Bo Zheng, Zhifang Sui, Baobao Chang, Furu Wei*\n\n**TL;DR:** StableMoE with two training stages to address the routing fluctuation problem and outperforms existing MoE methods in terms of both convergence speed and performance is proposed.\n\n**Abstract:** The Mixture-of-Experts (MoE) technique can scale up the model size of Transformers with an affordable computational overhead. We point out that existing learning-to-route MoE methods suffer from the routing fluctuation issue, i.e., the target expert of the same input may change along with training, but only one expert will be activated for the input during inference. The routing fluctuation tends to harm sample efficiency because the same input updates different experts but only one is finally used. In this paper, we propose StableMoE with two training stages to address the routing fluctuation problem. In the first training stage, we learn a balanced and cohesive routing strategy and distill it into a lightweight router decoupled from the backbone model. In the second training stage, we utilize the distilled router to determine the token-to-expert assignment and freeze it for a stable routing strategy. We validate our method on language modeling and multilingual machine translation. The results show that StableMoE outperforms existing MoE methods in terms of both convergence speed and performance.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 48  (*Influential: 4*)\n\n#### 2. Sparse Mixture-of-Experts are Domain Generalizable Learners\n\n*From Search Query: Mixture of Experts Transformers*\n\n*Bo Li, Yifei Shen, Jingkang Yang, Yezhen Wang, Jiawei Ren, Tong Che, Jun Zhang, Ziwei Liu*\n\n**TL;DR:** A formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset is developed and guided to propose a novel DG model built upon vision transformers, namely Generalizable Mixture-of-Experts (GMoE).\n\n**Abstract:** Human visual perception can easily generalize to out-of-distributed visual data, which is far beyond the capability of modern machine learning models. Domain generalization (DG) aims to close this gap, with existing DG methods mainly focusing on the loss function design. In this paper, we propose to explore an orthogonal direction, i.e., the design of the backbone architecture. It is motivated by an empirical finding that transformer-based models trained with empirical risk minimization (ERM) outperform CNN-based models employing state-of-the-art (SOTA) DG algorithms on multiple DG datasets. We develop a formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset. This analysis guides us to propose a novel DG model built upon vision transformers, namely Generalizable Mixture-of-Experts (GMoE). Extensive experiments on DomainBed demonstrate that GMoE trained with ERM outperforms SOTA DG baselines by a large margin. Moreover, GMoE is complementary to existing DG methods and its performance is substantially improved when trained with DG algorithms.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 47  (*Influential: 5*)\n\n#### 3. AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation\n\n*From Search Query: Mixture of Experts Transformers*\n\n*Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, L. Lakshmanan, A. Awadallah, S\u00e9bastien Bubeck, Jianfeng Gao*\n\n**TL;DR:** AutoMoE is developed -- a framework for designing heterogeneous MoE's under computational constraints and leverages Neural Architecture Search to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, on aggregate over benchmark datasets for NMT.\n\n**Abstract:** Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. Scaling Vision with Sparse Mixture of Experts\n\n*From Search Query: Sparse Mixture of Experts*\n\n*C. Riquelme, J. Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, N. Houlsby*\n\n**TL;DR:** This work presents a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks, when applied to image recognition, while requiring as little as half of the compute at inference time.\n\n**Abstract:** Sparsely-gated Mixture of Experts networks (MoEs) have demonstrated excellent scalability in Natural Language Processing. In Computer Vision, however, almost all performant networks are\"dense\", that is, every input is processed by every parameter. We present a Vision MoE (V-MoE), a sparse version of the Vision Transformer, that is scalable and competitive with the largest dense networks. When applied to image recognition, V-MoE matches the performance of state-of-the-art networks, while requiring as little as half of the compute at inference time. Further, we propose an extension to the routing algorithm that can prioritize subsets of each input across the entire batch, leading to adaptive per-image compute. This allows V-MoE to trade-off performance and compute smoothly at test-time. Finally, we demonstrate the potential of V-MoE to scale vision models, and train a 15B parameter model that attains 90.35% on ImageNet.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 456  (*Influential: 38*)\n\n#### 5. Scaling Vision-Language Models with Sparse Mixture of Experts\n\n*From Search Query: Sparse Mixture of Experts*\n\n*Sheng Shen, Z. Yao, Chunyuan Li, Trevor Darrell, K. Keutzer, Yuxiong He*\n\n**TL;DR:** The effectiveness of MoE in scaling vision-language models is explored, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost.\n\n**Abstract:** The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the model into smaller, specialized sub-models that can jointly solve a task. In this paper, we explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. Our research offers valuable insights into stabilizing the training of MoE models, understanding the impact of MoE on model interpretability, and balancing the trade-offs between compute performance when scaling VLMs. We hope our work will inspire further research into the use of MoE for scaling large-scale vision-language models and other multimodal machine learning applications.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 43  (*Influential: 2*)\n\n#### 6. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\n\n*From Search Query: Sparsely Gated MLP*\n\n*Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, J. Dean*\n\n**TL;DR:** This work introduces a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks, and applies the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora.\n\n**Abstract:** The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2017\n\n**Citations:** 2042  (*Influential: 257*)\n\n#### 7. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention\n\n*From Search Query: Sparsely Gated MLP*\n\n*Yuandong Tian, Yiping Wang, Zhenyu (Allen) Zhang, Beidi Chen, Simon S. Du*\n\n**TL;DR:** JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time.\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 8. Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding\n\n*From Search Query: Sparsely Gated MLP*\n\n*Yifan Peng, Siddharth Dalmia, Ian Lane, Shinji Watanabe*\n\n**TL;DR:** This work proposes a more flexible, interpretable and customizable encoder alternative, Branchformer, with parallel branches for modeling various ranged dependencies in end-to-end speech processing, including the ability to have variable inference complexity in a single trained model.\n\n**Abstract:** Conformer has proven to be effective in many speech processing tasks. It combines the benefits of extracting local dependencies using convolutions and global dependencies using self-attention. Inspired by this, we propose a more flexible, interpretable and customizable encoder alternative, Branchformer, with parallel branches for modeling various ranged dependencies in end-to-end speech processing. In each encoder layer, one branch employs self-attention or its variant to capture long-range dependencies, while the other branch utilizes an MLP module with convolutional gating (cgMLP) to extract local relationships. We conduct experiments on several speech recognition and spoken language understanding benchmarks. Results show that our model outperforms both Transformer and cgMLP. It also matches with or outperforms state-of-the-art results achieved by Conformer. Furthermore, we show various strategies to reduce computation thanks to the two-branch architecture, including the ability to have variable inference complexity in a single trained model. The weights learned for merging branches indicate how local and global dependencies are utilized in different layers, which benefits model designing.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 109  (*Influential: 9*)\n\n### 4 related papers from ArXiv\n\n#### 1. LOLA -- An Open-Source Massively Multilingual Large Language Model\n\n*From Search Query: Mixture of Experts Transformers*\n\n*Nikit Srivastava, Denis Kuchelev, Tatiana Moteu Ngoli, Kshitij Shetty, Michael R\u00f6der, Diego Moussallem, Hamada Zahera, Axel-Cyrille Ngonga Ngomo*\n\n**Abstract:** This paper presents LOLA, a massively multilingual large language model\ntrained on more than 160 languages using a sparse Mixture-of-Experts\nTransformer architecture. Our architectural and implementation choices address\nthe challenge of harnessing linguistic diversity while maintaining efficiency\nand avoiding the common pitfalls of multilinguality. Our analysis of the\nevaluation results shows competitive performance in natural language generation\nand understanding tasks. Additionally, we demonstrate how the learned\nexpert-routing mechanism exploits implicit phylogenetic linguistic patterns to\npotentially alleviate the curse of multilinguality. We provide an in-depth look\nat the training process, an analysis of the datasets, and a balanced\nexploration of the model's strengths and limitations. As an open-source model,\nLOLA promotes reproducibility and serves as a robust foundation for future\nresearch. Our findings enable the development of compute-efficient multilingual\nmodels with strong, scalable performance across languages.\n\n**Published:** 2024-09-17T15:23:08Z  (*Updated: 2024-09-19T15:50:01Z*)\n\n\n\n#### 2. Pre-training Multi-task Contrastive Learning Models for Scientific\n  Literature Understanding\n\n*From Search Query: Mixture of Experts Transformers*\n\n*Yu Zhang, Hao Cheng, Zhihong Shen, Xiaodong Liu, Ye-Yi Wang, Jianfeng Gao*\n\n**Abstract:** Scientific literature understanding tasks have gained significant attention\ndue to their potential to accelerate scientific discovery. Pre-trained language\nmodels (LMs) have shown effectiveness in these tasks, especially when tuned via\ncontrastive learning. However, jointly utilizing pre-training data across\nmultiple heterogeneous tasks (e.g., extreme multi-label paper classification,\ncitation prediction, and literature search) remains largely unexplored. To\nbridge this gap, we propose a multi-task contrastive learning framework,\nSciMult, with a focus on facilitating common knowledge sharing across different\nscientific literature understanding tasks while preventing task-specific skills\nfrom interfering with each other. To be specific, we explore two techniques --\ntask-aware specialization and instruction tuning. The former adopts a\nMixture-of-Experts Transformer architecture with task-aware sub-layers; the\nlatter prepends task-specific instructions to the input text so as to produce\ntask-aware outputs. Extensive experiments on a comprehensive collection of\nbenchmark datasets verify the effectiveness of our task-aware specialization\nstrategy, where we outperform state-of-the-art scientific pre-trained LMs.\nCode, datasets, and pre-trained models can be found at\nhttps://scimult.github.io/.\n\n**Published:** 2023-05-23T16:47:22Z  (*Updated: 2023-10-19T01:18:49Z*)\n\n\n\n#### 3. On the Representation Collapse of Sparse Mixture of Experts\n\n*From Search Query: Sparse Mixture of Experts*\n\n*Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, Furu Wei*\n\n**Abstract:** Sparse mixture of experts provides larger model capacity while requiring a\nconstant computational overhead. It employs the routing mechanism to distribute\ninput tokens to the best-matched experts according to their hidden\nrepresentations. However, learning such a routing mechanism encourages token\nclustering around expert centroids, implying a trend toward representation\ncollapse. In this work, we propose to estimate the routing scores between\ntokens and experts on a low-dimensional hypersphere. We conduct extensive\nexperiments on cross-lingual language model pre-training and fine-tuning on\ndownstream tasks. Experimental results across seven multilingual benchmarks\nshow that our method achieves consistent gains. We also present a comprehensive\nanalysis on the representation and routing behaviors of our models. Our method\nalleviates the representation collapse issue and achieves more consistent\nrouting than the baseline mixture-of-experts methods.\n\n**Published:** 2022-04-20T01:40:19Z  (*Updated: 2022-10-12T10:17:55Z*)\n\n\n\n#### 4. MoEC: Mixture of Expert Clusters\n\n*From Search Query: Sparse Mixture of Experts*\n\n*Yuan Xie, Shaohan Huang, Tianyu Chen, Furu Wei*\n\n**Abstract:** Sparsely Mixture of Experts (MoE) has received great interest due to its\npromising scaling capability with affordable computational overhead. MoE\nconverts dense layers into sparse experts, and utilizes a gated routing network\nto make experts conditionally activated. However, as the number of experts\ngrows, MoE with outrageous parameters suffers from overfitting and sparse data\nallocation. Such problems are especially severe on tasks with limited data,\nthus hindering the progress for MoE models to improve performance by scaling\nup. In this work, we propose Mixture of Expert Clusters - a general approach to\nenable expert layers to learn more diverse and appropriate knowledge by\nimposing variance-based constraints on the routing stage. We further propose a\ncluster-level expert dropout strategy specifically designed for the expert\ncluster structure. Our experiments reveal that MoEC could improve performance\non machine translation and natural language understanding tasks, and raise the\nperformance upper bound for scaling up experts under limited data. We also\nverify that MoEC plays a positive role in mitigating overfitting and sparse\ndata allocation.\n\n**Published:** 2022-07-19T06:09:55Z  (*Updated: 2022-07-19T06:09:55Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. MoEfication: Transformer Feed-forward Layers are Mixtures of Experts\n\n*From Search Query: Mixture of Experts Transformers*\n\n*Jie zhou, Maosong Sun, Peng Li, Zhiyuan Liu, Yankai Lin, Zhengyan Zhang*\n\n**Abstract:** Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10% to 30% of FFN parameters while maintaining over 95% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.\n\n**Proceeding:** findings-acl-2022-5\n\n**Published:** 2021-10-05\n\n\n\n#### 2. MoEUT: Mixture-of-Experts Universal Transformers\n\n*From Search Query: Mixture of Experts Transformers*\n\n*Christopher D. Manning, Christopher Potts, J\u00fcrgen Schmidhuber, Kazuki Irie, R\u00f3bert Csord\u00e1s*\n\n**Abstract:** Previous work on Universal Transformers (UTs) has demonstrated the importance of parameter sharing across layers. By allowing recurrence in depth, UTs have advantages over standard Transformers in learning compositional generalizations, but layer-sharing comes with a practical limitation of parameter-compute ratio: it drastically reduces the parameter count compared to the non-shared model with the same dimensionality. Naively scaling up the layer size to compensate for the loss of parameters makes its computational resource requirements prohibitive. In practice, no previous work has succeeded in proposing a shared-layer Transformer design that is competitive in parameter count-dominated tasks such as language modeling. Here we propose MoEUT (pronounced \"moot\"), an effective mixture-of-experts (MoE)-based shared-layer Transformer architecture, which combines several recent advances in MoEs for both feedforward and attention layers of standard Transformers together with novel layer-normalization and grouping schemes that are specific and crucial to UTs. The resulting UT model, for the first time, slightly outperforms standard Transformers on language modeling tasks such as BLiMP and PIQA, while using significantly less compute and memory.\n\n**Published:** 2024-05-25\n\n\n\n#### 3. Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts\n\n*From Search Query: Sparse Mixture of Experts*\n\n*Doyen Sahoo, Caiming Xiong, Silvio Savarese, Chenghao Liu, Roger Zimmermann, Yuxuan Liang, Taha Aksu, Gerald Woo, Juncheng Liu, Xu Liu*\n\n**Abstract:** Time series foundation models have demonstrated impressive performance as zero-shot forecasters. However, achieving effectively unified training on time series remains an open challenge. Existing approaches introduce some level of model specialization to account for the highly heterogeneous nature of time series data. For instance, Moirai pursues unified training by employing multiple input/output projection layers, each tailored to handle time series at a specific frequency. Similarly, TimesFM maintains a frequency embedding dictionary for this purpose. We identify two major drawbacks to this human-imposed frequency-level model specialization: (1) Frequency is not a reliable indicator of the underlying patterns in time series. For example, time series with different frequencies can display similar patterns, while those with the same frequency may exhibit varied patterns. (2) Non-stationarity is an inherent property of real-world time series, leading to varied distributions even within a short context window of a single time series. Frequency-level specialization is too coarse-grained to capture this level of diversity. To address these limitations, this paper introduces Moirai-MoE, using a single input/output projection layer while delegating the modeling of diverse time series patterns to the sparse mixture of experts (MoE) within Transformers. With these designs, Moirai-MoE reduces reliance on human-defined heuristics and enables automatic token-level specialization. Extensive experiments on 39 datasets demonstrate the superiority of Moirai-MoE over existing foundation models in both in-distribution and zero-shot scenarios. Furthermore, this study conducts comprehensive model analyses to explore the inner workings of time series MoE foundation models and provides valuable insights for future research.\n\n**Published:** 2024-10-14\n\n\n\n#### 4. Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality\n\n*From Search Query: Sparsely Gated MLP*\n\n*Saurabh Khanna, Vincent Y. F. Tan*\n\n**Abstract:** Granger causality is a widely-used criterion for analyzing interactions in large-scale networks. As most physical interactions are inherently nonlinear, we consider the problem of inferring the existence of pairwise Granger causality between nonlinearly interacting stochastic processes from their time series measurements. Our proposed approach relies on modeling the embedded nonlinearities in the measurements using a component-wise time series prediction model based on Statistical Recurrent Units (SRUs). We make a case that the network topology of Granger causal relations is directly inferrable from a structured sparse estimate of the internal parameters of the SRU networks trained to predict the processes$'$ time series measurements. We propose a variant of SRU, called economy-SRU, which, by design has considerably fewer trainable parameters, and therefore less prone to overfitting. The economy-SRU computes a low-dimensional sketch of its high-dimensional hidden state in the form of random projections to generate the feedback for its recurrent processing. Additionally, the internal weight parameters of the economy-SRU are strategically regularized in a group-wise manner to facilitate the proposed network in extracting meaningful predictive features that are highly time-localized to mimic real-world causal events. Extensive experiments are carried out to demonstrate that the proposed economy-SRU based time series prediction model outperforms the MLP, LSTM and attention-gated CNN-based time series models considered previously for inferring Granger causality.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-22\n\n\n\n#### 5. CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts\n\n*From Search Query: Sparsely Gated MLP*\n\n*Longyin Wen, Humphrey Shi, Jitesh Jain, Fan Chen, Lu Xu, Chia-Wen Kuo, Sijie Zhu, Xinyao Wang, Jiachen Li*\n\n**Abstract:** Recent advancements in Multimodal Large Language Models (LLMs) have focused primarily on scaling by increasing text-image pair data and enhancing LLMs to improve performance on multimodal tasks. However, these scaling approaches are computationally expensive and overlook the significance of improving model capabilities from the vision side. Inspired by the successful applications of Mixture-of-Experts (MoE) in LLMs, which improves model scalability during training while keeping inference costs similar to those of smaller models, we propose CuMo. CuMo incorporates Co-upcycled Top-K sparsely-gated Mixture-of-experts blocks into both the vision encoder and the MLP connector, thereby enhancing the multimodal LLMs with minimal additional activated parameters during inference. CuMo first pre-trains the MLP blocks and then initializes each expert in the MoE block from the pre-trained MLP block during the visual instruction tuning stage. Auxiliary losses are used to ensure a balanced loading of experts. CuMo outperforms state-of-the-art multimodal LLMs across various VQA and visual-instruction-following benchmarks using models within each model size group, all while training exclusively on open-sourced datasets. The code and model weights for CuMo are open-sourced at https://github.com/SHI-Labs/CuMo.\n\n**Published:** 2024-05-09\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher integrate a **Mixture of Experts (MoE)** mechanism into the MLP layers of Transformers, particularly focusing on sparsely gated mechanisms, here are the key insights and useful information from the provided sources:\n\n## Key Components of MoE\n\n- **Experts and Router**: In an MoE architecture, each layer contains multiple experts (typically feed-forward neural networks) and a router (or gate network) that determines which experts to activate for a given input token. This sparsity allows for a significant increase in model parameters without a proportional increase in computational costs.\n\n## Benefits of MoE Integration\n\n- **Increased Model Capacity**: MoE enables scaling to models with billions or even trillions of parameters, enhancing the model's capacity to capture complex patterns and dependencies.\n- **Efficiency**: Only a small subset of experts is activated per token, maintaining computational efficiency during both training and inference.\n- **Improved Expressiveness**: Different experts can specialize in various aspects of the data, allowing the model to handle diverse tasks more effectively.\n\n## Gating Mechanism\n\n- **Router Design**: The gating network uses a softmax function to select the top-k experts for each token. This mechanism is crucial for efficient and effective routing.\n- **Load Balancing**: Implementing a load balancing loss can prevent certain experts from being overutilized, ensuring all experts are used efficiently.\n\n## Performance on Complex Tasks\n\n- **Compositional Tasks**: Increasing the number of activated experts can improve performance on complex compositional tasks, with the optimal number of experts scaling with task complexity.\n- **Memory-Intensive Tasks**: MoE models can effectively leverage a small number of active parameters with a large number of experts to memorize data, which is beneficial for knowledge-intensive tasks.\n\n## Implementation Considerations\n\n- **Training Stability**: Careful initialization and regularization techniques are necessary to avoid training instabilities in MoE models.\n- **Communication Overhead**: Optimizing communication patterns is crucial in distributed training settings to avoid bottlenecks.\n- **Hardware Optimization**: Techniques like expert parallelism and parameter sharding can help manage the increased memory usage and optimize training and inference on specific hardware architectures.\n\n## Recent Advancements and Models\n\n- **GShard and Switch Transformers**: These models demonstrate the practical application of MoE in scaling transformer models effectively across multiple devices, achieving high performance with minimal computational overhead.\n- **GLaM**: This model uses MoE layers with top-2 experts per layer, showing significant improvements in efficiency and performance.\n\n## Future Research Directions\n\n- **Advanced Routing Mechanisms**: Exploring more efficient and effective gating mechanisms, such as router regularization, can further improve expert selection and model performance.\n- **Impact on Downstream Tasks**: Analyzing the performance of MoE-integrated models on various downstream tasks will help in understanding the broader applicability and benefits of this architecture.\n\n### Summary of Key References\n\n- ** A Visual Guide to Mixture of Experts**: Provides a detailed explanation of the MoE architecture, including the role of experts and the router, and how it replaces dense layers in traditional Transformers.\n- ** The Case for Increased Expert Activation in Compositional Tasks**: Highlights the benefits of increasing the number of activated experts for complex compositional tasks and demonstrates improved performance on benchmarks like SRAVEN and SKILL-MIX.\n- ** Mixture of Parrots: Experts improve memorization more than reasoning**: Discusses the trade-offs between MoE and dense transformers, showing that MoE excels in memory-intensive tasks but may not improve reasoning capabilities as significantly.\n- ** Mixture of experts - Wikipedia**: Offers a comprehensive overview of MoE, including its history, variants, and applications in large language models.\n- ** Sparse Mixture of Experts (MoE) Models: Examples**: Provides examples and practical applications of MoE in modern transformer architectures, such as Google\u2019s GShard and Switch Transformers.\n\nBy integrating these insights, the researcher can effectively design and implement a novel autoregressive LM block using MoE, enhancing the model's capacity, efficiency, and performance on various tasks.\n",
            "analysis": "```keywords\nMixture of Experts Transformers\nSparse Mixture of Experts\nSparsely Gated MLP\n```\n\n```description\nLooking for internal literature on integrating Mixture of Experts (MoE) into the MLP layers of Transformers, focusing on sparsely gated mechanisms to enhance model capacity and efficiency without significant computational overhead.\n```\n\n---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nTo improve the seed design \"HieraNorm-AttnGPT,\" I propose modifying the **GatedMLP** GAU by integrating a **Sparse Mixture of Experts (MoE)** mechanism. The goal is to enhance the model's capacity and expressiveness while maintaining computational efficiency. Sparse MoE allows the model to scale to a larger number of parameters without a proportional increase in computation, as only a subset of experts is activated per token. This can lead to improved perplexity, better downstream task performance, and efficient scaling with more data and larger models.\n\n---\n\n**2. Useful References with Excerpts**\n\n- **Reference [1]: Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.\"**\n\n  *Excerpt:*\n  > \"We introduce a sparsely-gated mixture-of-experts layer, which allows a model to have a vastly increased capacity with a modest increase in computation. Our model uses a trainable gating network to select a sparse combination of experts to process each input.\"\n\n- **Reference [2]: Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., ... & Chen, Z. (2020). \"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.\"**\n\n  *Excerpt:*\n  > \"We demonstrate that scaling Transformers' model capacity can be achieved with minimal impact on training and inference speed by using a Mixture-of-Experts architecture. Our approach allows for models with over a trillion parameters.\"\n\n- **Reference [3]: Fedus, W., Zoph, B., & Shazeer, N. (2022). \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.\"**\n\n  *Excerpt:*\n  > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n- **Reference [4]: Artetxe, M., Du, N., Li, B. Z., Ryabinin, M., & Zettlemoyer, L. (2021). \"Efficient Large Scale Language Modeling with Mixture-of-Experts.\"**\n\n  *Excerpt:*\n  > \"We explore the use of Mixture-of-Experts in language modeling and show that it provides significant improvements in perplexity while keeping inference costs low due to the sparsity of expert activation.\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Enhancing GatedMLP with Sparse MoE:**\n\n  By integrating a Sparse Mixture of Experts into the **GatedMLP** GAU, we can significantly increase the model's capacity. Each token will pass through a gating network that selects a small subset of experts (e.g., one or two) to process the token, keeping computational costs manageable.\n\n- **Benefits of Sparse MoE Integration:**\n\n  - *Increased Model Capacity:* Enables scaling to models with billions or even trillions of parameters without a proportional increase in computation.\n  - *Improved Expressiveness:* Different experts can specialize in various aspects of the data, allowing the model to capture complex patterns and dependencies.\n  - *Efficiency:* Sparse activation ensures that only a few experts are active per token, maintaining efficiency during training and inference.\n\n- **Compatibility with Hierarchical Attention:**\n\n  The hierarchical adaptive multi-head attention mechanism captures multi-scale dependencies. Combining it with a capacity-enhanced MLP layer allows the model to process both attention and feed-forward computations more effectively.\n\n- **Considerations for Implementation:**\n\n  - *Gating Network Design:* The gating network must efficiently and effectively route tokens to appropriate experts. A common approach is to use a softmax over the expert logits to select the top-k experts.\n  - *Load Balancing:* Introducing a load balancing loss can prevent certain experts from being overutilized while others are underutilized, ensuring efficient use of all experts.\n  - *Memory Management:* While computation remains efficient, memory usage increases due to the larger number of parameters. Techniques like expert parallelism and parameter sharding can mitigate this.\n\n- **Potential Challenges:**\n\n  - *Training Stability:* Sparse MoE models can be prone to training instabilities. Careful initialization and the use of regularization techniques are necessary.\n  - *Communication Overhead:* In distributed training settings, communication between devices can become a bottleneck. Optimizing the communication pattern is crucial.\n  - *Implementation Complexity:* Integrating MoE layers adds complexity to the model architecture and training pipeline.\n\n---\n\n**4. Future Search Plan**\n\n- **Explore Advanced Routing Mechanisms:**\n\n  Research more efficient and effective gating mechanisms, such as router regularization, to improve expert selection.\n\n- **Investigate Recent MoE Variants:**\n\n  Look into models like GLaM (GShard MoE) and others that have made advancements in MoE architectures.\n\n- **Hardware Optimization Strategies:**\n\n  Examine techniques for optimizing Sparse MoE models on specific hardware architectures, such as GPUs and TPUs, to improve training and inference efficiency.\n\n- **Study Impact on Downstream Tasks:**\n\n  Analyze how integrating Sparse MoE affects performance on various downstream tasks, including language understanding and generation benchmarks.\n\n---\n\n**5. List of References**\n\n1. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. arXiv preprint arXiv:1701.06538.\n\n2. **Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., ... & Chen, Z. (2020).** *GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding*. arXiv preprint arXiv:2006.16668.\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. Journal of Machine Learning Research, 23(1), 1-39.\n\n4. **Artetxe, M., Du, N., Li, B. Z., Ryabinin, M., & Zettlemoyer, L. (2021).** *Efficient Large Scale Language Modeling with Mixture-of-Experts*. arXiv preprint arXiv:2112.10684.\n\n---"
        },
        {
            "ready": false,
            "query": null,
            "detail": "---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nIn our pursuit to enhance the seed design \"HieraNorm-AttnGPT,\" we aim to modify the **GatedMLP** GAU by integrating a **Sparse Mixture of Experts (MoE)** mechanism. This integration is intended to significantly increase the model's capacity and expressiveness while maintaining computational efficiency. Recent research suggests that incorporating MoE into the MLP layers of Transformers can lead to substantial performance gains without proportional increases in computational overhead.\n\n---\n\n**2. Useful References with Excerpts**\n\n1. **He, X. O. (2024). Mixture of a Million Experts.**\n\n   *Excerpt:*\n\n   > \"This paper introduces PEER (Parameter Efficient Expert Retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off.\"\n\n2. **Lieber, O., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model.**\n\n   *Excerpt:*\n\n   > \"We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba Mixture-of-Experts (MoE) architecture... Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\"\n\n3. **Shazeer, N., et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n\n   *Excerpt:*\n\n   > \"We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\"\n\n4. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n\n   *Excerpt:*\n\n   > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n5. **Chi, Z., et al. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n\n   *Excerpt:*\n\n   > \"We propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.\"\n\n6. **Zhou, J., et al. (2021). MoEfication: Transformer Feed-forward Layers are Mixtures of Experts.**\n\n   *Excerpt:*\n\n   > \"We observe that most inputs only activate a tiny ratio of neurons of FFNs... We propose to convert a model into its MoE version with the same parameters, namely MoEfication.\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Feasibility of Integrating MoE into GatedMLP:**\n\n  The literature supports the feasibility of integrating MoE mechanisms into the MLP layers of Transformers. By replacing or augmenting the feed-forward layers with sparse MoE layers, we can increase model capacity without significant computational overhead.\n\n- **Benefits of Sparse MoE Integration:**\n\n  - *Increased Model Capacity:* MoE allows scaling to models with a much larger number of parameters, enhancing the ability to model complex patterns and dependencies.\n  - *Computational Efficiency:* Activating only a subset of experts per token maintains computational costs similar to smaller models.\n  - *Improved Performance:* Studies show that MoE models outperform their dense counterparts on various language modeling benchmarks.\n\n- **Challenges and Solutions:**\n\n  - *Training Stability:* MoE models can suffer from representation collapse and unstable training dynamics. Techniques such as routing on a low-dimensional hypersphere and load balancing losses can mitigate these issues.\n  - *Expert Routing Mechanisms:* Efficient routing is crucial. Methods like top-k routing, hash-based routing, and learnable routers are explored in the literature.\n  - *Load Balancing:* Ensuring that all experts are utilized effectively prevents overfitting and under-utilization of model capacity.\n\n- **Specific Approaches to Consider:**\n\n  - **Parameter Efficient Expert Retrieval (PEER):** Utilizing product key techniques for efficient sparse retrieval from a large pool of tiny experts. This could allow us to scale up the number of experts significantly.\n  - **MoEfication:** Transform existing FFN layers into MoE layers without increasing parameter count, leveraging the inherent sparsity in neuron activation.\n\n- **Compatibility with Hierarchical Attention:**\n\n  Integrating MoE into the GatedMLP should be compatible with the hierarchical adaptive attention mechanism in HieraNorm-AttnGPT. Both mechanisms aim to capture complex patterns and dependencies, potentially leading to synergistic effects.\n\n---\n\n**4. Future Search Plan**\n\n- **Efficient Expert Routing Mechanisms:**\n\n  Investigate advanced routing strategies that reduce computational overhead, such as the use of low-dimensional projections or more efficient top-k algorithms.\n\n- **Training Techniques for Stability:**\n\n  Research regularization methods, initialization strategies, and architectural modifications that enhance training stability for MoE models.\n\n- **Hardware Optimization and Scalability:**\n\n  Explore methods for optimizing MoE models on specific hardware platforms, including techniques for distributed training, memory management, and reducing communication overhead.\n\n- **Empirical Studies on Model Performance:**\n\n  Look for empirical evidence on how integrating MoE into MLP layers affects performance on downstream tasks, particularly in models with hierarchical attention mechanisms.\n\n- **Potential Integration with PEER or Similar Methods:**\n\n  Assess the feasibility of implementing PEER-like mechanisms in the GatedMLP GAU to efficiently handle a large number of tiny experts.\n\n---\n\n**5. References**\n\n1. **He, X. O. (2024).** *Mixture of a Million Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n2. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024).** *Jamba: A Hybrid Transformer-Mamba Language Model*. arXiv preprint arXiv:XXXX.XXXXX.\n\n3. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n4. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n5. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Patra, B., et al. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n6. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n---\n\nAs we continue this research, our next steps will involve a deeper exploration of efficient routing mechanisms and training stability techniques for MoE models. This will strengthen our proposal and address potential challenges in integrating a Sparse Mixture of Experts into the GatedMLP GAU of HieraNorm-AttnGPT.\n\n---",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\n---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nIn our pursuit to enhance the seed design \"HieraNorm-AttnGPT,\" we aim to modify the **GatedMLP** GAU by integrating a **Sparse Mixture of Experts (MoE)** mechanism. This integration is intended to significantly increase the model's capacity and expressiveness while maintaining computational efficiency. Recent research suggests that incorporating MoE into the MLP layers of Transformers can lead to substantial performance gains without proportional increases in computational overhead.\n\n---\n\n**2. Useful References with Excerpts**\n\n1. **He, X. O. (2024). Mixture of a Million Experts.**\n\n   *Excerpt:*\n\n   > \"This paper introduces PEER (Parameter Efficient Expert Retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off.\"\n\n2. **Lieber, O., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model.**\n\n   *Excerpt:*\n\n   > \"We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba Mixture-of-Experts (MoE) architecture... Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\"\n\n3. **Shazeer, N., et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n\n   *Excerpt:*\n\n   > \"We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\"\n\n4. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n\n   *Excerpt:*\n\n   > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n5. **Chi, Z., et al. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n\n   *Excerpt:*\n\n   > \"We propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.\"\n\n6. **Zhou, J., et al. (2021). MoEfication: Transformer Feed-forward Layers are Mixtures of Experts.**\n\n   *Excerpt:*\n\n   > \"We observe that most inputs only activate a tiny ratio of neurons of FFNs... We propose to convert a model into its MoE version with the same parameters, namely MoEfication.\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Feasibility of Integrating MoE into GatedMLP:**\n\n  The literature supports the feasibility of integrating MoE mechanisms into the MLP layers of Transformers. By replacing or augmenting the feed-forward layers with sparse MoE layers, we can increase model capacity without significant computational overhead.\n\n- **Benefits of Sparse MoE Integration:**\n\n  - *Increased Model Capacity:* MoE allows scaling to models with a much larger number of parameters, enhancing the ability to model complex patterns and dependencies.\n  - *Computational Efficiency:* Activating only a subset of experts per token maintains computational costs similar to smaller models.\n  - *Improved Performance:* Studies show that MoE models outperform their dense counterparts on various language modeling benchmarks.\n\n- **Challenges and Solutions:**\n\n  - *Training Stability:* MoE models can suffer from representation collapse and unstable training dynamics. Techniques such as routing on a low-dimensional hypersphere and load balancing losses can mitigate these issues.\n  - *Expert Routing Mechanisms:* Efficient routing is crucial. Methods like top-k routing, hash-based routing, and learnable routers are explored in the literature.\n  - *Load Balancing:* Ensuring that all experts are utilized effectively prevents overfitting and under-utilization of model capacity.\n\n- **Specific Approaches to Consider:**\n\n  - **Parameter Efficient Expert Retrieval (PEER):** Utilizing product key techniques for efficient sparse retrieval from a large pool of tiny experts. This could allow us to scale up the number of experts significantly.\n  - **MoEfication:** Transform existing FFN layers into MoE layers without increasing parameter count, leveraging the inherent sparsity in neuron activation.\n\n- **Compatibility with Hierarchical Attention:**\n\n  Integrating MoE into the GatedMLP should be compatible with the hierarchical adaptive attention mechanism in HieraNorm-AttnGPT. Both mechanisms aim to capture complex patterns and dependencies, potentially leading to synergistic effects.\n\n---\n\n**4. Future Search Plan**\n\n- **Efficient Expert Routing Mechanisms:**\n\n  Investigate advanced routing strategies that reduce computational overhead, such as the use of low-dimensional projections or more efficient top-k algorithms.\n\n- **Training Techniques for Stability:**\n\n  Research regularization methods, initialization strategies, and architectural modifications that enhance training stability for MoE models.\n\n- **Hardware Optimization and Scalability:**\n\n  Explore methods for optimizing MoE models on specific hardware platforms, including techniques for distributed training, memory management, and reducing communication overhead.\n\n- **Empirical Studies on Model Performance:**\n\n  Look for empirical evidence on how integrating MoE into MLP layers affects performance on downstream tasks, particularly in models with hierarchical attention mechanisms.\n\n- **Potential Integration with PEER or Similar Methods:**\n\n  Assess the feasibility of implementing PEER-like mechanisms in the GatedMLP GAU to efficiently handle a large number of tiny experts.\n\n---\n\n**5. References**\n\n1. **He, X. O. (2024).** *Mixture of a Million Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n2. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024).** *Jamba: A Hybrid Transformer-Mamba Language Model*. arXiv preprint arXiv:XXXX.XXXXX.\n\n3. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n4. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n5. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Patra, B., et al. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n6. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n---\n\nAs we continue this research, our next steps will involve a deeper exploration of efficient routing mechanisms and training stability techniques for MoE models. This will strengthen our proposal and address potential challenges in integrating a Sparse Mixture of Experts into the GatedMLP GAU of HieraNorm-AttnGPT.\n\n---\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation (Avg. Score: 0.99)\n\n*Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yunhe Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work presents a new efficient model architecture for establishing modern language models, namely, PanGu-$\\pi$, and develops an LLM named YunShan for practical application, which can surpass other models with similar scales on benchmarks.\n\n**Abstract:** The recent trend of large language models (LLMs) is to increase the scale of both model size (\\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama. However, large models often involve massive computational costs, and practical applications cannot afford such high prices. However, the method of constructing a strong model architecture for LLMs is rarely discussed. We first analyze the state-of-the-art language model architectures and observe the feature collapse problem. Based on the theoretical analysis, we propose that the nonlinearity is also very important for language models, which is usually studied in convolutional neural networks for vision tasks. The series informed activation function is then introduced with tiny calculations that can be ignored, and an augmented shortcut is further used to enhance the model nonlinearity. We then demonstrate that the proposed approach is significantly effective for enhancing the model nonlinearity through carefully designed ablations; thus, we present a new efficient model architecture for establishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using the same dataset and training strategy to compare PanGu-$\\pi$ with state-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a comparable performance to that of benchmarks with about 10\\% inference speed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms of accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the high-value domains of finance and law, developing an LLM named YunShan for practical application. The results show that YunShan can surpass other models with similar scales on benchmarks.\n\n##### *Relevant Chunk: No. 25/62 (Score: 0.99)*\n\n```\n[47] N. Du et al. Glam: Efficient scaling of language models with mixtureof-experts. In International Conference on Machine Learning, 2022. [48] S. Roller et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 2021. [49] Z. Chi et al. On the representation collapse of sparse mixture of experts. Advances in Neural Information Processing Systems, 2022. [50] M. Lewis et al. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning, 2021. [51] A. Chowdhery et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [52] N. Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [53] W. Wang et al. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. [54] Z. Liu et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. [55] A. Dosovitskiy et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [56] J. Guo et al. Cmt: Convolutional neural networks meet vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. [57] B. Heo et al. Rethinking spatial dimensions of vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. [58] Z. Pan et al. Scalable vision transformers with hierarchical pooling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021. [59] C.-F. R. Chen et al. Crossvit: Cross-attention multi-scale vision transformer for image classification. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. [60] B. Graham et al. Levit: a vision transformer in convnet's clothing for faster inference. In Proceedings of the IEEE/CVF international conference on computer vision, 2021. [61] S. Mehta and M. Rastegari. Mobilevit: light-weight, generalpurpose, and mobile-friendly vision transformer. arXiv preprint arXiv:2110.02178, 2021. [62] K. Han et al. Transformer in transformer. Advances in Neural Information Processing Systems, 2021. [63] N. Parmar et al. Image transformer. In International conference on machine learning, 2018. [64] X. Liu et al. Efficientvit: Memory efficient vision transformer with cascaded group attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\n```\n\n#### 2. Mixture of A Million Experts (Avg. Score: 0.98)\n\n*Xu Owen He*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.\n\n**Abstract:** The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.\n\n##### *Relevant Chunk: No. 2/21 (Score: 1.00)*\n\n```\nSparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the finegrained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-01.jpg?height=844&width=1654&top_left_y=1334&top_left_x=243)\n\nFigure 1: Isoflop comparison on the C4 dataset between PEER and other baselines with two different FLOP budgets ( $6 e 18$ and $2 e 19$ FLOPs).\n```\n\n##### *Relevant Chunk: No. 4/21 (Score: 0.99)*\n\n```\n2023). However, MoWE is language-specific and uses a fixed routing scheme. Theoretical and empirical evidence (Clark et al. 2022 Dikkala et al. 2023) highlights the advantages of learned routers over non-trainable ones. Thus, an MoE design with a learned router scalable to over a million experts remains an open area for exploration. This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval (Lample et al. 2019) for efficient routing to an extremely large number of experts, decoupling computational cost from parameter count. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models. The main contributions of this work are:\n\n- Exploration of Extreme MoE Setting: Deviating from the focus on a small number of large experts in previous MoE research, this work investigates the under-explored case of numerous tiny experts. - Learned Index Structure for Routing: Demonstrating for the first time that a learned index structure (Kraska et al. 2018) can efficiently route to over a million experts. - New Layer Design: Combining product key routing with single-neuron experts, we introduce the PEER layer that expands layer capacity without significant computational overheads. Empirical results demonstrate its superior efficiency compared to dense FFW, coarse-grained MoEs and Product Key Memory (PKM) layers. - Comprehensive Ablation Studies: We investigate the impact of different design choices of PEER such as number of experts, active parameters, number of heads and query batch normalization on language modeling tasks. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-03.jpg?height=562&width=1381&top_left_y=277&top_left_x=337)\n\nFigure 2: Illustration of the PEER layer. A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers. Given the state vector $x$ from the previous layer, a query network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the router scores and to retrieve the top $k$ experts $e_{1}, \\ldots, e_{k}$. After the retrieved experts make their predictions $e_{i}(x)$, their outputs are linearly combined using the softmax-normalized router scores as weights. ## 2 Method\n\nIn this section, we introduce the Parameter Efficient Expert Retrieval (PEER) layer, which is a Mixture of Experts architecture using product keys (Lample et al., 2019) in the router and single-neuron MLPs as experts. Fig. 2 illustrates the computational process within a PEER layer. PEER Overview Formally, a PEER layer is a function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ that consists of three parts: a pool of $N$ experts $\\mathbb{E}:=\\left\\{e_{i}\\right\\}_{i=1}^{N}$, where each expert $e_{i}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ shares the same signature as $f$, a corresponding set of $N$ product keys $\\mathbb{K}:=\\left\\{k_{i}\\right\\}_{i=1}^{N} \\subset \\mathbb{R}^{d}$, and a query network $q: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$ that maps the input vector $x \\in \\mathbb{R}^{n}$ to a query vector $q(x)$.\n```\n\n##### *Relevant Chunk: No. 14/21 (Score: 0.99)*\n\n```\narXiv preprint arXiv:1312.4461, 2013. Nishanth Dikkala, Nikhil Ghosh, Raghu Meka, Rina Panigrahy, Nikhil Vyas, and Xin Wang. On the benefits of learning to route in mixture-of-experts models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=QV79qiKAjD. Cicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble, Chung-Ching Chang, and David Uthus. Memory augmented language models through mixture of word experts, 2023. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient scaling of language models with mixture-of-experts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5547-5569. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/du22c.html. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\n```\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.93)*\n\n```\nThe $x$ axis is in $\\log$ scale. ## 1 Introduction\n\nThe past few years have seen the power of scaling (Kaplan et al. 2020 Hoffmann et al. 2022): increasing the number of parameters, amount of training data, or the computational budget has proven to be a reliable\nway to improve model performance. Notably, feedforward (FFW) layers, responsible for storing factual knowledge (Geva et al., 2021, Dai et al. 2022, account for two-thirds of the total parameters in a transformer. However, one drawback of these dense FFWs is that their computational footprint (FLOPs and device memory consumption) is linearly proportional to their parameter count. To break the coupling between computational cost and parameter count, many recent works (Shazeer et al. 2017, Lepikhin et al. 2020; Fedus et al., 2022, Zhou et al. 2022) have adopted the Mixture-of-Experts (MoE) architecture, which uses a set of sparsely activated expert modules (often FFWs) in place of a single dense FFW. Clark et al. (2022) studied the scaling law of MoE language models and showed that increasing the number of experts is an effective way to improve performance without increasing the inference cost. However, their experiments showed that the efficiency gains provided by MoEs plateau after a certain model size is reached. More recently, Krajewski et al. (2024) discovered that this plateau was caused by using a fixed number of training tokens. When the number of training tokens is compute-optimal, MoEs consistently outperform dense models in terms of FLOP efficiency. Moreover, they introduced granularity (the number of active experts) as a new scaling axis and empirically showed that using higher granularity improves performance. Extrapolating this fine-grained MoE scaling law suggests that continued improvement of model capacity will ultimately lead to a large model with high granularity, corresponding to an architecture of an immense number of tiny experts. Beyond efficient scaling, another reason to have a vast number of experts is lifelong learning, where MoE has emerged as a promising approach Aljundi et al., 2017, Chen et al., 2023, Yu et al., 2024, Li et al. 2024). For instance, Chen et al. (2023) showed that, by simply adding new experts and regularizing them properly, MoE models can adapt to continuous data streams. Freezing old experts and updating only new ones prevents catastrophic forgetting and maintains plasticity by design. In lifelong learning settings, the data stream can be indefinitely long or never-ending Mitchell et al. 2018, necessitating an expanding pool of experts. Although both efficient scaling and lifelong learning require MoE designs capable of handling a vast number of experts, to the best of our knowledge, the only architecture supporting more than ten thousands of experts is the Mixture of Word Experts (MoWE) dos Santos et al.\n```\n\n\n\n---\n## Web search results\n\n The researcher's goal is to enhance the \"HieraNorm-AttnGPT\" model by integrating a **Sparse Mixture of Experts (MoE)** mechanism into the **GatedMLP** Gated Activation Unit (GAU). Here are the key points and the most useful information to help achieve this goal:\n\n## Key Objectives\n- Increase model capacity and expressiveness.\n- Maintain or improve computational efficiency.\n- Enhance performance on language modeling tasks and downstream evaluations.\n- Ensure robustness, stability, and scalability.\n\n## Integration of MoE into GatedMLP\n### Feasibility and Benefits\n- The literature supports that integrating MoE into MLP layers of Transformers is feasible and beneficial. Studies like Shazeer et al. (2017) and Fedus et al. (2022) demonstrate that MoE layers can significantly increase model capacity without proportional increases in computational overhead.\n\n### Efficient Routing Mechanisms\n- **Parameter Efficient Expert Retrieval (PEER):** This method uses product key techniques for efficient sparse retrieval from a large pool of tiny experts, which could be crucial for scaling up the number of experts.\n- **Top-k Routing and Hash-Based Routing:** These methods are discussed in the context of Switch Transformers and other MoE architectures, highlighting their efficiency in reducing computational overhead.\n\n### Training Stability\n- **Representation Collapse:** Techniques such as routing on a low-dimensional hypersphere, as proposed by Chi et al. (2022), can mitigate representation collapse issues in MoE models.\n- **Load Balancing:** Ensuring that all experts are utilized effectively through load balancing losses can prevent overfitting and under-utilization of model capacity.\n\n### Compatibility with Hierarchical Attention\n- The integration of MoE should be compatible with the hierarchical adaptive attention mechanism in HieraNorm-AttnGPT. Both mechanisms aim to capture complex patterns and dependencies, potentially leading to synergistic effects.\n\n## Useful References and Concepts\n\n### Advanced Routing Strategies\n- **PEER Layers:** He et al. (2024) introduce PEER layers that utilize product key techniques for sparse retrieval, which could be integrated into the GatedMLP GAU to scale up the number of experts efficiently.\n\n### Training Techniques for Stability\n- **MoEfication:** Zhou et al. (2021) propose converting existing FFN layers into MoE layers without increasing the parameter count, leveraging the inherent sparsity in neuron activation. This could help in stabilizing the training process.\n\n### Hardware Optimization and Scalability\n- **Switch Transformers:** Fedus et al. (2022) discuss how Switch Transformers replace dense feed-forward layers with sparse Switch layers, allowing for scaling to trillion-parameter models efficiently. This approach can be adapted for optimizing MoE models on specific hardware platforms.\n\n### Empirical Studies\n- **Jamba Model:** Lieber et al. (2024) present the Jamba model, which uses a hybrid Transformer-Mamba MoE architecture. This model provides high throughput and a small memory footprint, demonstrating state-of-the-art performance on language model benchmarks and long-context evaluations. Such empirical evidence can guide the integration of MoE into HieraNorm-AttnGPT.\n\n## Future Search Plan\n\n### Efficient Expert Routing\n- Investigate low-dimensional projections and more efficient top-k algorithms to reduce computational overhead. The PEER method and similar techniques should be a focus area.\n\n### Training Stability\n- Research regularization methods, initialization strategies, and architectural modifications that enhance training stability for MoE models. Techniques like routing on a low-dimensional hypersphere and load balancing losses are crucial.\n\n### Hardware Optimization\n- Explore methods for optimizing MoE models on specific hardware platforms, including distributed training, memory management, and reducing communication overhead. The Switch Transformers approach can serve as a guide here.\n\n### Empirical Evidence\n- Look for empirical studies that evaluate the performance of MoE models on downstream tasks, especially in models with hierarchical attention mechanisms. This will help in understanding the practical implications of integrating MoE into HieraNorm-AttnGPT.\n\nBy focusing on these areas, the researcher can develop a robust and efficient novel autoregressive LM block that integrates a Sparse Mixture of Experts into the GatedMLP GAU, potentially outperforming existing state-of-the-art models.\n\n\nWarning: No keywords detected, external search skipped, please wrap your keywords in a quoted block like this: ```keywords {{Your keywods}} ``` in your response next time.\n\nWarning: No description detected, will use full response to search internal library, please wrap your description in a quoted block like this: ```description {{Your description}}``` in your response next time.",
            "analysis": "---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nIn our pursuit to enhance the seed design \"HieraNorm-AttnGPT,\" we aim to modify the **GatedMLP** GAU by integrating a **Sparse Mixture of Experts (MoE)** mechanism. This integration is intended to significantly increase the model's capacity and expressiveness while maintaining computational efficiency. Recent research suggests that incorporating MoE into the MLP layers of Transformers can lead to substantial performance gains without proportional increases in computational overhead.\n\n---\n\n**2. Useful References with Excerpts**\n\n1. **He, X. O. (2024). Mixture of a Million Experts.**\n\n   *Excerpt:*\n\n   > \"This paper introduces PEER (Parameter Efficient Expert Retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off.\"\n\n2. **Lieber, O., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model.**\n\n   *Excerpt:*\n\n   > \"We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba Mixture-of-Experts (MoE) architecture... Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\"\n\n3. **Shazeer, N., et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n\n   *Excerpt:*\n\n   > \"We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\"\n\n4. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n\n   *Excerpt:*\n\n   > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n5. **Chi, Z., et al. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n\n   *Excerpt:*\n\n   > \"We propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. Our method alleviates the representation collapse issue and achieves more consistent routing than the baseline mixture-of-experts methods.\"\n\n6. **Zhou, J., et al. (2021). MoEfication: Transformer Feed-forward Layers are Mixtures of Experts.**\n\n   *Excerpt:*\n\n   > \"We observe that most inputs only activate a tiny ratio of neurons of FFNs... We propose to convert a model into its MoE version with the same parameters, namely MoEfication.\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Feasibility of Integrating MoE into GatedMLP:**\n\n  The literature supports the feasibility of integrating MoE mechanisms into the MLP layers of Transformers. By replacing or augmenting the feed-forward layers with sparse MoE layers, we can increase model capacity without significant computational overhead.\n\n- **Benefits of Sparse MoE Integration:**\n\n  - *Increased Model Capacity:* MoE allows scaling to models with a much larger number of parameters, enhancing the ability to model complex patterns and dependencies.\n  - *Computational Efficiency:* Activating only a subset of experts per token maintains computational costs similar to smaller models.\n  - *Improved Performance:* Studies show that MoE models outperform their dense counterparts on various language modeling benchmarks.\n\n- **Challenges and Solutions:**\n\n  - *Training Stability:* MoE models can suffer from representation collapse and unstable training dynamics. Techniques such as routing on a low-dimensional hypersphere and load balancing losses can mitigate these issues.\n  - *Expert Routing Mechanisms:* Efficient routing is crucial. Methods like top-k routing, hash-based routing, and learnable routers are explored in the literature.\n  - *Load Balancing:* Ensuring that all experts are utilized effectively prevents overfitting and under-utilization of model capacity.\n\n- **Specific Approaches to Consider:**\n\n  - **Parameter Efficient Expert Retrieval (PEER):** Utilizing product key techniques for efficient sparse retrieval from a large pool of tiny experts. This could allow us to scale up the number of experts significantly.\n  - **MoEfication:** Transform existing FFN layers into MoE layers without increasing parameter count, leveraging the inherent sparsity in neuron activation.\n\n- **Compatibility with Hierarchical Attention:**\n\n  Integrating MoE into the GatedMLP should be compatible with the hierarchical adaptive attention mechanism in HieraNorm-AttnGPT. Both mechanisms aim to capture complex patterns and dependencies, potentially leading to synergistic effects.\n\n---\n\n**4. Future Search Plan**\n\n- **Efficient Expert Routing Mechanisms:**\n\n  Investigate advanced routing strategies that reduce computational overhead, such as the use of low-dimensional projections or more efficient top-k algorithms.\n\n- **Training Techniques for Stability:**\n\n  Research regularization methods, initialization strategies, and architectural modifications that enhance training stability for MoE models.\n\n- **Hardware Optimization and Scalability:**\n\n  Explore methods for optimizing MoE models on specific hardware platforms, including techniques for distributed training, memory management, and reducing communication overhead.\n\n- **Empirical Studies on Model Performance:**\n\n  Look for empirical evidence on how integrating MoE into MLP layers affects performance on downstream tasks, particularly in models with hierarchical attention mechanisms.\n\n- **Potential Integration with PEER or Similar Methods:**\n\n  Assess the feasibility of implementing PEER-like mechanisms in the GatedMLP GAU to efficiently handle a large number of tiny experts.\n\n---\n\n**5. References**\n\n1. **He, X. O. (2024).** *Mixture of a Million Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n2. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024).** *Jamba: A Hybrid Transformer-Mamba Language Model*. arXiv preprint arXiv:XXXX.XXXXX.\n\n3. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n4. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n5. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Patra, B., et al. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n6. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:XXXX.XXXXX.\n\n---\n\nAs we continue this research, our next steps will involve a deeper exploration of efficient routing mechanisms and training stability techniques for MoE models. This will strengthen our proposal and address potential challenges in integrating a Sparse Mixture of Experts into the GatedMLP GAU of HieraNorm-AttnGPT.\n\n---"
        },
        {
            "ready": false,
            "query": [
                "Efficient Expert Routing",
                "Mixture of Experts Training Stability",
                "Product Key Networks"
            ],
            "detail": "Seeking literature on advanced routing mechanisms for large-scale Mixture of Experts models, focusing on techniques like product key retrieval and efficient top-k selection. Interested in methods to enhance training stability of MoE architectures, including regularization and initialization strategies. Additionally, looking for recent developments in integrating product key networks into transformer architectures to improve computational efficiency and scalability.",
            "search_ret": "\n---\n## Found 1 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSeeking literature on advanced routing mechanisms for large-scale Mixture of Experts models, focusing on techniques like product key retrieval and efficient top-k selection. Interested in methods to enhance training stability of MoE architectures, including regularization and initialization strategies. Additionally, looking for recent developments in integrating product key networks into transformer architectures to improve computational efficiency and scalability.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 1 papers in Internal Library\n\n#### 1. Mixture of A Million Experts (Avg. Score: 1.00)\n\n*Xu Owen He*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.\n\n**Abstract:** The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.\n\n##### *Relevant Chunk: No. 4/21 (Score: 1.00)*\n\n```\n2023). However, MoWE is language-specific and uses a fixed routing scheme. Theoretical and empirical evidence (Clark et al. 2022 Dikkala et al. 2023) highlights the advantages of learned routers over non-trainable ones. Thus, an MoE design with a learned router scalable to over a million experts remains an open area for exploration. This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval (Lample et al. 2019) for efficient routing to an extremely large number of experts, decoupling computational cost from parameter count. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models. The main contributions of this work are:\n\n- Exploration of Extreme MoE Setting: Deviating from the focus on a small number of large experts in previous MoE research, this work investigates the under-explored case of numerous tiny experts. - Learned Index Structure for Routing: Demonstrating for the first time that a learned index structure (Kraska et al. 2018) can efficiently route to over a million experts. - New Layer Design: Combining product key routing with single-neuron experts, we introduce the PEER layer that expands layer capacity without significant computational overheads. Empirical results demonstrate its superior efficiency compared to dense FFW, coarse-grained MoEs and Product Key Memory (PKM) layers. - Comprehensive Ablation Studies: We investigate the impact of different design choices of PEER such as number of experts, active parameters, number of heads and query batch normalization on language modeling tasks. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-03.jpg?height=562&width=1381&top_left_y=277&top_left_x=337)\n\nFigure 2: Illustration of the PEER layer. A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers. Given the state vector $x$ from the previous layer, a query network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the router scores and to retrieve the top $k$ experts $e_{1}, \\ldots, e_{k}$. After the retrieved experts make their predictions $e_{i}(x)$, their outputs are linearly combined using the softmax-normalized router scores as weights. ## 2 Method\n\nIn this section, we introduce the Parameter Efficient Expert Retrieval (PEER) layer, which is a Mixture of Experts architecture using product keys (Lample et al., 2019) in the router and single-neuron MLPs as experts. Fig. 2 illustrates the computational process within a PEER layer. PEER Overview Formally, a PEER layer is a function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ that consists of three parts: a pool of $N$ experts $\\mathbb{E}:=\\left\\{e_{i}\\right\\}_{i=1}^{N}$, where each expert $e_{i}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ shares the same signature as $f$, a corresponding set of $N$ product keys $\\mathbb{K}:=\\left\\{k_{i}\\right\\}_{i=1}^{N} \\subset \\mathbb{R}^{d}$, and a query network $q: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$ that maps the input vector $x \\in \\mathbb{R}^{n}$ to a query vector $q(x)$.\n```\n\n##### *Relevant Chunk: No. 2/21 (Score: 1.00)*\n\n```\nSparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the finegrained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-01.jpg?height=844&width=1654&top_left_y=1334&top_left_x=243)\n\nFigure 1: Isoflop comparison on the C4 dataset between PEER and other baselines with two different FLOP budgets ( $6 e 18$ and $2 e 19$ FLOPs).\n```\n\n##### *Relevant Chunk: No. 9/21 (Score: 1.00)*\n\n```\n3 4}$ | $\\mathbf{1 0 . 3 3}$ | $\\mathbf{1 4 . 9 9}$ | $\\mathbf{1 9 . 0 9}$ | $\\mathbf{1 6 . 4 5}$ |\n\n### 3.3 Ablations\n\nVarying the Number of Total Experts The models in the isoFLOP plot depicted in Fig. 1 all have over a million $\\left(1024^{2}\\right)$ experts. Here we conduct an ablation study on the effect of the number of experts $N$, which determines the total parameter count $P$ in Eq. 9 . We selected the model at the isoFLOP-optimal position and vary the number of experts $\\left(N=128^{2}, 256^{2}, 512^{2}, 1024^{2}\\right)$ in the PEER layer while keeping the number of active experts constant $(h=8, k=16)$. The results are shown in Fig. 3 (a). As can be seen, the isoFLOP curve interpolates between the PEER model with $1024^{2}$ experts and the corresponding dense backbone without replacing the FFW layer in the middle block by a PEER layer. This demonstrates that simply increasing the number experts can improve model performance. Varying the Number of Active Experts We also conducted an ablation study on the effect of the number of active experts $h k$, which equals the granularity $G$ in Eq. 9. We systematically varied the number of\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-07.jpg?height=852&width=1656&top_left_y=289&top_left_x=240)\n\nFigure 3: We conduct two ablation studies using the same PEER model configuration. In (a), we vary the total number of experts $N$ while keeping the same number of active experts $h k=128$. In (b), we vary the number of active experts $G=h k$ by jointly changing $h$ and $k$ while keeping the total number of experts at $N=1024^{2}$. active experts $(h k=32,64,128,256,512)$ while keeping the number of total experts constant $\\left(N=1024^{2}\\right)$. Furthermore, for a given $h k$, we jointly varied $h$ and $k$ to identify the optimal composition. The resulting isoFLOP curves, plotted over the number of heads $(h)$, are shown in Fig. 3(b). The results indicate that, within the range of values considered, higher $h k$ generally leads to improved performance. Notably, the optimal $h$ increases as $h k$ increases. However, the performance gradually saturates, and increasing the number of active experts also increases device memory consumption and may necessitate additional accelerator devices. Thus in practice, the appropriate $h k$ values should be selected based on the trade-off between performance, device number and computational resource requirements. Table 2: KL and expert usage for different memory sizes, with and without query BN. Similar to the findings in PKM, using query BN results in a more balanced usage of the experts. | Expert num $N$ | 16 k |  | 65 k |  | 262 k |  | 1 M |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| BatchNorm | No | Yes | No | Yes | No | Yes | No | Yes |\n| Perplexity | 23.47 | 23.47 | 22.61 | 22.55 | 21.54 | 21.47 | 20.73 | 20.64 |\n| Expert Usage (\\%) | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 99.8 | 100.0 |\n| Unevenness ( $\\downarrow$ ) | 0.45 | 0.30 | 0.63 | 0.44 | 0.97 | 0.66 | 1.52 | 1.06 |\n\nExpert Usage and Query Batch Normalization Given the presence of over a million experts in the PEER layer, it is natural to inquire how many of these experts are actually selected during inference and whether their usage is evenly distributed. To analyze this, we kept an accumulated router score, denoted as $z_{i}^{\\prime}=$ $\\sum_{x} g_{i}(x)$ for each expert $e_{i}$ across all tokens $x$ within the C 4 validation set. Here $g_{i}(x)$ is the router score used to aggregate the expert output when token $x$ is given as input, with $g_{i}(x)=0$ if expert $e_{i}$ is not selected. From these accumulated router scores, we can obtain an empirical probability distribution vector, denoted as $z=z^{\\prime} /\\left\\|z^{\\prime}\\right\\|_{1}$, representing the distribution of all experts over the C 4 validation set. Then we computed the following metrics proposed by Lample et al. (2019) to assess the usage and distribution of experts:\n\n- Expert Usage: the fraction of experts retrieved during inference: $\\#\\left\\{z_{i} \\neq 0\\right\\}$\n- Unevenness: KL divergence between $z$ and the uniform distribution: $\\log (N)+\\sum_{i} z_{i} \\log \\left(z_{i}\\right)$\nwhere $N$ is the number of total experts. By default, we also added a batch normalization (BN) layer on top of the query network, as proposed by Lample et al. (2019) to increase the expert usage during training. Here we study the effect of adding this BN layer on the above-mentioned metrics. Table 2 presents the expert usage and unevenness for varying numbers of experts, with and without BN. We can see that even for 1 M experts, the expert usage is close to $100 \\%$, and using BN can lead to more balanced utilization of the experts and lower perplexities. These findings demonstrate the effectiveness of the PEER model in utilizing a large number of experts. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-08.jpg?height=722&width=985&top_left_y=845&top_left_x=559)\n\nFigure 4: Query BatchNorm Ablation. IsoFLOP curves of a PEER model with 1 M experts on the C4 dataset, with and without query BatchNorm. We additionally compared isoFLOP curves with and without BN. Fig. 4 shows that the PEER model with BN generally achieves lower perplexities. While the difference is not significant, it is most pronounced around the isoFLOP-optimal region. ## 4 Related Works\n\nMixture of Expert Since Shazeer et al. (2017) demonstrated the effectiveness of sparsely-gated Mixtures of Experts (MoEs) in efficiently increasing model capacity on GPU clusters, MoEs have emerged as a popular technique for scaling large models efficiently. Subsequent research (Fedus et al. 2022 Lepikhin et al. 2020 , Du et al., 2022 has proposed variations to address challenges such as load balancing, communication overhead, and training instability. These methods usually replace feedforward (FFW) layers in certain Transformer blocks with sparsely-gated MoE layers, which consist of multiple FFW layers as experts. Typically each expert matches the size of the regular dense FFW layer. Gating scores are calculated for each expert and token, and only the top k experts are activated for each token. These methods are known as token-choice methods. More recently, Zhou et al. (2022) introduced the Expert Choice routing method, where experts choose the top k tokens instead of tokens selecting experts. However, both token-choice and expert-choice methods require the top-k operator on a gating score matrix of size $N \\times M$ ( $N$ : number of experts, $M$ : number of tokens), resulting in a routing cost of at least $O(N)$. This limits their practical application to a small number of experts (typically less than 128). Instead of using the top-k operator, some works also proposed using deterministic hash tables as routers (Roller et al. 2021, dos Santos et al. 2023). With $O(1)$ average lookup complexity, these methods offer potential scalability to a large number of experts. However, these routers are fixed and not learned. Clark et al. (2022) showed that deterministic routing does not scale as well as trainable routers. Furthermore, Dikkala et al. (2023) proved theoretically that learned routers offer non-trivial advantages over their fixed counterparts, such as removing spurious directions and identifying latent clusters in data. In contrast to previous works, the proposed PEER layer employs a learned router with sublinear $(O(\\sqrt{N}))$ complexity. Since PEER uses lightweight experts, our work is also related to recent studies on parameter-efficient MoEs Wang et al. 2022, Zadouri et al., 2024). These methods utilize parameter efficient fine-tuning (PEFT) adapters as experts instead of full-sized FFWs. Their focus is on minimizing the number of parameters updated during fine-tuning, allowing storage of only one copy of the large backbone model. In PEER, parameter efficiency refers to the small number of active parameters in the MoE layer, which directly affects FLOPs and activation memory consumption during pre-training and inference. However, PEER could potentially be adapted to retrieve a large number of PEFT adapters. Retrieval-Augmented Models Our proposed method, with its retrieval mechanism for a large number of experts, aligns with the emerging field of retrieval-augmented models. These models facilitate large model memorization by retrieving knowledge from external databases, leading to improved accuracy and efficiency on knowledge-intensive tasks. Some notable works in this domain include ones by Khandelwal et al. (2019); Borgeaud et al. (2022a); Guu et al. (2020). While these methods retrieve data in various formats, for instance, tokens (Khandelwal et al. 2019), chunks (Borgeaud et al. 2022b) or knowledge graphs (Kang et al., 2023) (see (Gao et al. 2023) for a comprehensive survey on this topic), they differ from the proposed method in that they retrieve data rather than learned functions (experts). This distinction sets our parameter-efficient expert retrieval approach apart from existing retrieval-augmented models. Efficient Feedforward Layers Enhancing the efficiency of feedforward networks has been a long-standing area of research. Similar to PEER, most approaches are based on the idea of conditional computation (Bengio, 2013), where a gating mechanism is trained to determine which subset of neurons to compute. For instance, Davis \\& Arel (2013) utilized low-rank weight matrix approximation to estimate the sign of prenonlinearity activations. Neurons with negative activations are omitted as they will produce zeros after the nonlinearity. Bengio et al. (2015) explored reinforcement learning to develop an activation-dependant policy for dropping blocks of neurons. More recently, Belcak \\& Wattenhofer (2023) introduced the Fast FeedForward (FFF) layer that employs a differentiable balanced binary tree to select a neuron block for computation.\n```\n\n##### *Relevant Chunk: No. 6/21 (Score: 0.99)*\n\n```\n1 can be very expensive. Hence we apply the product key retrieval technique here. Instead of using $N$ independent $d$-dimensional vectors as our keys $k_{i}$, we create them by concatenating vectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys $\\mathbb{C}, \\mathbb{C}^{\\prime} \\subset \\mathbb{R}^{\\frac{d}{2}}$ :\n\n$$\n\\mathbb{K}=\\left\\{\\left.\\left[\\begin{array}{c}\nc \\\\\nc^{\\prime}\n\\end{array}\\right] \\right\\rvert\\, c \\in \\mathbb{C}, c^{\\prime} \\in \\mathbb{C}^{\\prime}\\right\\}\n$$\n\nNote that here $\\mathbb{C}, \\mathbb{C}^{\\prime}$ have cardinality $\\sqrt{N}$ and $c, c^{\\prime}$ have dimensionality $\\frac{d}{2}$. So in practice, we choose $N$ to be a perfect square and $d$ to be an even number. This Cartesian product structure of $\\mathbb{K}$ allows us to find the top $k$ experts efficiently. Instead of comparing $q(x)$ to all $N$ keys in $\\mathbb{K}$ and selecting the top k matches, we can split the query vector $q(x)$ into two subqueries $q_{1}$ and $q_{2}$ and apply the top k operations to the inner products between the sub-queries and sub-keys respectively:\n\n$$\n\\mathbb{I}_{\\mathbb{C}}=\\mathcal{T}_{k}\\left(\\left(q_{1}^{T} c_{i}\\right)\\right), \\quad \\mathbb{I}_{\\mathbb{C}^{\\prime}}=\\mathcal{T}_{k}\\left(\\left(q_{2}^{T} c_{j}^{\\prime}\\right)\\right)\n$$\n\nThis results in a set of $k^{2}$ candidate keys $\\mathbb{K}^{\\prime}:=\\left\\{\\left.\\left[\\begin{array}{l}c_{i} \\\\ c_{j}\\end{array}\\right] \\right\\rvert\\, i \\in \\mathbb{I}_{\\mathbb{C}}, j \\in \\mathbb{I}_{\\mathbb{C}}^{\\prime}\\right\\}$, and it is mathematically guaranteed that the $k$ most similar keys to $q(x)$ from $\\mathbb{K}$ are in this candidate set. Moreover, the inner product between the candidate key and $q(x)$ is simply the sum of inner products between the sub-keys and sub-queries: $q(x)^{T}\\left[\\begin{array}{l}c_{i} \\\\ c_{j}\\end{array}\\right]=q_{1}^{T} c_{i}+q_{2}^{T} c_{j}$. Hence we can apply the top-k operator again to these $k^{2}$ inner products to get the top k matching keys from the original set of product keys $\\mathbb{K}$. As explained in Lample et al. (2019). This reduces the complexity of top k expert retrieval in Eq. 1 from $O(N d)$ as done naively by exhaustive search to $O\\left(\\left(\\sqrt{N}+k^{2}\\right) d\\right)$\n\nParameter Efficient Experts and Multi-Head Retrieval Unlike other MoE architectures, which often set the hidden layer of each expert to the same size as other FFW layers, in PEER, every expert $e_{i}$ is a singleton MLP, in other words, it has only one hidden layer with a single neuron:\n\n$$\ne_{i}(x):=\\sigma\\left(u_{i}^{T} x\\right) v_{i}\n$$\n\nwhere $v_{i}, u_{i}$ are not matrices but vectors with the same dimension as $x$, and $\\sigma$ is a nonlinear activation function such as ReLU or GELU. We omit bias terms here for brevity. Instead of varying the size of individual experts, we adjust the expressiveness of a PEER layer by using multihead retrieval, similar to the multi-head attention mechanism in transformers and the multi-head memory in PKMs. In particular, we use $h$ independent query networks instead of one, each computes its own query and retrieves a separate set of $k$ experts. However, different heads share the same pool of experts with the same set of product keys. The outputs of these $h$ heads are simply summed up:\n\n$$\nf(x):=\\sum_{i=1}^{h} f^{i}(x)=\\sum_{i=1}^{h} \\sum_{j \\in \\mathbb{I}^{i}} g_{j}(x) e_{j}(x)\n$$\n\nOne can verify that when only one expert is retrieved $(k=1)$ per head, using a PEER layer with $h$ heads is the same as using one expert with $h$ hidden neurons:\n\n$$\nf(x)=\\sum_{i=1}^{h} e^{i}(x)=\\sum_{i=1}^{h} \\sigma\\left(u_{i}^{T} x\\right) v_{i}=V \\sigma\\left(W^{T} x\\right)\n$$\n\nwhere $W=\\left[u_{1}, \\cdots, u_{h}\\right], V=\\left[v_{1}, \\cdots, v_{h}\\right]$. In other words, PEER dynamically assembles an MLP with $h$ neurons by aggregating $h$ singleton MLPs retrieved from a shared repository. Compared to existing MoE approaches that use MLPs with multiple hidden neurons as experts, this design allows shared hidden neurons among experts, enhancing knowledge transfer and parameter efficiency. Algorithm 1 shows a simplified implementation of the PEER forward pass, storing parameter-efficient expert weights in embedding layers and combining them with einsum operations. This implementation can be easily extended to experts of the GLU variants (Shazeer, 2020) by adding additional linear gating weights. In practice, an efficient implementation may require specialized hardware kernels to accelerate embedding lookup and fusion with the einsum operations. Why A Large Number of Small Experts? Given an MoE layer, we can characterize it by three hyperparameters: the total number of parameters $P$, the number of active parameters per token $P_{\\text {active }}$ and the size of a single expert $P_{\\text {expert }}$. Krajewski et al. (2024) showed that the scaling law of MoE models has the following form:\n\n$$\n\\mathcal{L}(P, D, G)=c+\\left(\\frac{g}{G^{\\gamma}}+a\\right) \\frac{1}{P^{\\alpha}}+\\frac{b}{D^{\\beta}}\n$$\n\nwhere $\\mathcal{L}$ is the final test loss, $a, b, g, \\gamma, \\alpha, \\beta$ are constants, $D$ is the total number of training tokens and the granularity $G$ is the number of active experts:\n\n$$\nG:=\\frac{P_{\\text {active }}}{P_{\\text {expert }}}\n$$\n\nIn order to improve model performance, we need to scale up $P, D, G$.\n```\n\n##### *Relevant Chunk: No. 14/21 (Score: 0.99)*\n\n```\narXiv preprint arXiv:1312.4461, 2013. Nishanth Dikkala, Nikhil Ghosh, Raghu Meka, Rina Panigrahy, Nikhil Vyas, and Xin Wang. On the benefits of learning to route in mixture-of-experts models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=QV79qiKAjD. Cicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble, Chung-Ching Chang, and David Uthus. Memory augmented language models through mixture of word experts, 2023. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient scaling of language models with mixture-of-experts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5547-5569. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/du22c.html. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: Efficient Expert Routing, Mixture of Experts Training Stability, Product Key Networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models\n\n*From Search Query: Efficient Expert Routing*\n\n*Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou*\n\n**TL;DR:** ZOOTER is proposed, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it, and shows computation efficiency in inference.\n\n**Abstract:** The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate on it by mining latent expertise with off-the-shelf reward models. We propose ZOOTER, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. ZOOTER shows computation efficiency in inference as it only introduces minor computation overhead of a routing function compared with reward model ranking methods. We evaluate ZOOTER on a comprehensive benchmark collection with 26 subsets in different domains and tasks. ZOOTER outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 2. Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy\n\n*From Search Query: Efficient Expert Routing*\n\n*Pingzhi Li, Zhenyu (Allen) Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit Bansal, Tianlong Chen*\n\n**TL;DR:** M-SMoE is proposed, which leverages routing statistics to guide expert merging and further decomposes the merged experts into low-rank and structural sparse alternatives, thus diminishing the impact of insignificant experts.\n\n**Abstract:** Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is missing to bring all of them in alignment. To address this, we propose M-SMoE, which leverages routing statistics to guide expert merging. Specifically, it starts with neuron permutation alignment for experts; then, dominant experts and their\"group members\"are formed; lastly, every expert group is merged into a single expert by utilizing each expert's activation frequency as their weight for merging, thus diminishing the impact of insignificant experts. Moreover, we observed that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression. Hence, our final method, MC-SMoE (i.e., Merge, then Compress SMoE), further decomposes the merged experts into low-rank and structural sparse alternatives. Extensive experiments across 8 benchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE achieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 3. Patch-level Routing in Mixture-of-Experts is Provably Sample-efficient for Convolutional Neural Networks\n\n*From Search Query: Efficient Expert Routing*\n\n*Mohammed Nowaz Rabbani Chowdhury, Shuai Zhang, M. Wang, Sijia Liu, Pin-Yu Chen*\n\n**TL;DR:** Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), this work shows for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization and outperforms its single-expert counterpart of the same or even larger capacity.\n\n**Abstract:** In deep learning, mixture-of-experts (MoE) activates one or few experts (sub-networks) on a per-sample or per-token basis, resulting in significant computation reduction. The recently proposed \\underline{p}atch-level routing in \\underline{MoE} (pMoE) divides each input into $n$ patches (or tokens) and sends $l$ patches ($l\\ll n$) to each expert through prioritized routing. pMoE has demonstrated great empirical success in reducing training and inference costs while maintaining test accuracy. However, the theoretical explanation of pMoE and the general MoE remains elusive. Focusing on a supervised classification task using a mixture of two-layer convolutional neural networks (CNNs), we show for the first time that pMoE provably reduces the required number of training samples to achieve desirable generalization (referred to as the sample complexity) by a factor in the polynomial order of $n/l$, and outperforms its single-expert counterpart of the same or even larger capacity. The advantage results from the discriminative routing property, which is justified in both theory and practice that pMoE routers can filter label-irrelevant patches and route similar class-discriminative patches to the same expert. Our experimental results on MNIST, CIFAR-10, and CelebA support our theoretical findings on pMoE's generalization and show that pMoE can avoid learning spurious correlations.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 4. Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts\n\n*From Search Query: Mixture of Experts Training Stability*\n\n*Basil Mustafa, C. Riquelme, J. Puigcerver, Rodolphe Jenatton, N. Houlsby*\n\n**TL;DR:** This work presents the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning, and proposes an entropy-based regularization scheme for its training stability and balanced expert utilization.\n\n**Abstract:** Large sparsely-activated models have obtained excellent performance in multiple domains. However, such models are typically trained on a single modality at a time. We present the Language-Image MoE, LIMoE, a sparse mixture of experts model capable of multimodal learning. LIMoE accepts both images and text simultaneously, while being trained using a contrastive loss. MoEs are a natural fit for a multimodal backbone, since expert layers can learn an appropriate partitioning of modalities. However, new challenges arise; in particular, training stability and balanced expert utilization, for which we propose an entropy-based regularization scheme. Across multiple scales, we demonstrate remarkable performance improvement over dense models of equivalent computational cost. LIMoE-L/16 trained comparably to CLIP-L/14 achieves 78.6% zero-shot ImageNet accuracy (vs. 76.2%), and when further scaled to H/14 (with additional data) it achieves 84.1%, comparable to state-of-the-art methods which use larger custom per-modality backbones and pre-training schemes. We analyse the quantitative and qualitative behavior of LIMoE, and demonstrate phenomena such as differing treatment of the modalities and the organic emergence of modality-specific experts.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 139  (*Influential: 4*)\n\n#### 5. Scaling physics-informed hard constraints with mixture-of-experts\n\n*From Search Query: Mixture of Experts Training Stability*\n\n*N. Chalapathi, Yiheng Du, Aditi Krishnapriyan*\n\n**TL;DR:** This work develops a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture and achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems.\n\n**Abstract:** Imposing known physical constraints, such as conservation laws, during neural network training introduces an inductive bias that can improve accuracy, reliability, convergence, and data efficiency for modeling physical dynamics. While such constraints can be softly imposed via loss function penalties, recent advancements in differentiable physics and optimization improve performance by incorporating PDE-constrained optimization as individual layers in neural networks. This enables a stricter adherence to physical constraints. However, imposing hard constraints significantly increases computational and memory costs, especially for complex dynamical systems. This is because it requires solving an optimization problem over a large number of points in a mesh, representing spatial and temporal discretizations, which greatly increases the complexity of the constraint. To address this challenge, we develop a scalable approach to enforce hard physical constraints using Mixture-of-Experts (MoE), which can be used with any neural network architecture. Our approach imposes the constraint over smaller decomposed domains, each of which is solved by an\"expert\"through differentiable optimization. During training, each expert independently performs a localized backpropagation step by leveraging the implicit function theorem; the independence of each expert allows for parallelization across multiple GPUs. Compared to standard differentiable optimization, our scalable approach achieves greater accuracy in the neural PDE solver setting for predicting the dynamics of challenging non-linear systems. We also improve training stability and require significantly less computation time during both training and inference stages.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 6. MoE-RBench: Towards Building Reliable Language Models with Sparse Mixture-of-Experts\n\n*From Search Query: Mixture of Experts Training Stability*\n\n*Guanjie Chen, Xinyu Zhao, Tianlong Chen, Yu Cheng*\n\n**TL;DR:** It is found that the robustness of SMoE is sensitive to the basic training settings, and empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, the MoE model can be built more reliably than the dense LLM.\n\n**Abstract:** Mixture-of-Experts (MoE) has gained increasing popularity as a promising framework for scaling up large language models (LLMs). However, the reliability assessment of MoE lags behind its surging applications. Moreover, when transferred to new domains such as in fine-tuning MoE models sometimes underperform their dense counterparts. Motivated by the research gap and counter-intuitive phenomenon, we propose $\\texttt{MoE-RBench}$, the first comprehensive assessment of SMoE reliability from three aspects: $\\textit{(i)}$ safety and hallucination, $\\textit{(ii)}$ resilience to adversarial attacks, and $\\textit{(iii)}$ out-of-distribution robustness. Extensive models and datasets are tested to compare the MoE to dense networks from these reliability dimensions. Our empirical observations suggest that with appropriate hyperparameters, training recipes, and inference techniques, we can build the MoE model more reliably than the dense LLM. In particular, we find that the robustness of SMoE is sensitive to the basic training settings. We hope that this study can provide deeper insights into how to adapt the pre-trained MoE model to other tasks with higher-generation security, quality, and stability. Codes are available at https://github.com/UNITES-Lab/MoE-RBench\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. Approximating Two-Layer Feedforward Networks for Efficient Transformers\n\n*From Search Query: Product Key Networks*\n\n*R'obert Csord'as, Kazuki Irie, J. Schmidhuber*\n\n**TL;DR:** This work introduces several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs, including product-key memories (PKMs), and proposes methods to improve both MoEs and PKMs.\n\n**Abstract:** How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 8. Design Space for Graph Neural Networks\n\n*From Search Query: Product Key Networks*\n\n*Jiaxuan You, Rex Ying, J. Leskovec*\n\n**TL;DR:** This work defines and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks, and offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space.\n\n**Abstract:** The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 283  (*Influential: 35*)\n\n#### 9. On the Relationship between Sum-Product Networks and Bayesian Networks\n\n*From Search Query: Product Key Networks*\n\n*H. Zhao, Mazen Melibari, P. Poupart*\n\n**TL;DR:** This paper proves that every SPN can be converted into a BN in linear time and space in terms of the network size and introduces the notion of {\\em normal} SPN and presents a theoretical analysis of the consistency and decomposability properties.\n\n**Abstract:** In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of {\\em normal} SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2015\n\n**Citations:** 89  (*Influential: 11*)\n\n### 6 related papers from Papers with Code\n\n#### 1. BASE Layers: Simplifying Training of Large, Sparse Models\n\n*From Search Query: Efficient Expert Routing*\n\n*Luke Zettlemoyer, Naman Goyal, Tim Dettmers, Shruti Bhosale, Mike Lewis*\n\n**Abstract:** We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/\n\n**Published:** 2021-03-30\n\n\n\n#### 2. Think2Drive: Efficient Reinforcement Learning by Thinking in Latent World Model for Quasi-Realistic Autonomous Driving (in CARLA-v2)\n\n*From Search Query: Efficient Expert Routing*\n\n*Anonymous*\n\n**Abstract:** Real-world autonomous driving (AD) especially urban driving involves many corner cases. The lately released AD simulator CARLA v2 adds 39 common events in the driving scene, and provide more quasi-realistic testbed compared to CARLA v1. It poses new challenge to the community and so far no literature has reported any success on the new scenarios in V2 as existing works mostly have to rely on specific rules for planning yet they cannot cover the more complex cases in CARLA v2. In this work, we take the initiative of directly training a planner and the hope is to handle the corner cases flexibly and effectively, which we believe is also the future of AD. To our best knowledge, we develop the first model-based RL method named Think2Drive for AD, with a world model to learn the transitions of the environment, and then it acts as a neural simulator to train the planner. This paradigm significantly boosts the training efficiency due to the low dimensional state space and parallel computing of tensors in the world model. As a result, Think2Drive is able to run in an expert-level proficiency in CARLA v2 within 3 days of training on a single A6000 GPU, and to our best knowledge, so far there is no reported success (100\\% route completion)on CARLA v2. We also propose CornerCase-Repository, a benchmark that supports the evaluation of driving models by scenarios. Additionally, we propose a new and balanced metric to evaluate the performance by route completion, infraction number, and scenario density, so that the driving score could give more information about the actual driving performance.\n\n**Published:** 2024-02-26\n\n\n\n#### 3. Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT\n\n*From Search Query: Mixture of Experts Training Stability*\n\n*Joshua Ainslie, James Lee-Thorp*\n\n**Abstract:** We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. Sparse Mixer slightly outperforms (<1%) BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61% faster. We also present a faster variant, prosaically named Fast Sparse Mixer, that marginally underperforms BERT on SuperGLUE, but trains and runs nearly twice as fast. We justify the design of these two models by carefully ablating through various mixing mechanisms, MoE configurations and hyperparameters. Sparse Mixer overcomes many of the latency and stability concerns of MoE models and offers the prospect of serving sparse student models, without resorting to distilling them to dense variants.\n\n**Published:** 2022-05-24\n\n\n\n#### 4. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: Mixture of Experts Training Stability*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 5. DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems\n\n*From Search Query: Product Key Networks*\n\n*Rakesh Shivanna, Lichan Hong, Sagar Jain, Ruoxi Wang, Derek Z. Cheng, Ed H. Chi, Dong Lin*\n\n**Abstract:** Learning effective feature crosses is the key behind building recommender systems. However, the sparse and large feature space requires exhaustive search to identify effective crosses. Deep & Cross Network (DCN) was proposed to automatically and efficiently learn bounded-degree predictive feature interactions. Unfortunately, in models that serve web-scale traffic with billions of training examples, DCN showed limited expressiveness in its cross network at learning more predictive feature interactions. Despite significant research progress made, many deep learning models in production still rely on traditional feed-forward neural networks to learn feature crosses inefficiently. In light of the pros/cons of DCN and existing feature interaction learning approaches, we propose an improved framework DCN-V2 to make DCN more practical in large-scale industrial settings. In a comprehensive experimental study with extensive hyper-parameter search and model tuning, we observed that DCN-V2 approaches outperform all the state-of-the-art algorithms on popular benchmark datasets. The improved DCN-V2 is more expressive yet remains cost efficient at feature interaction learning, especially when coupled with a mixture of low-rank architecture. DCN-V2 is simple, can be easily adopted as building blocks, and has delivered significant offline accuracy and online business metrics gains across many web-scale learning to rank systems at Google.\n\n**Published:** 2020-08-19\n\n\n\n#### 6. Complex Embeddings for Simple Link Prediction\n\n*From Search Query: Product Key Networks*\n\n*\u00c9ric Gaussier, Th\u00e9o Trouillon, Sebastian Riedel, Johannes Welbl, Guillaume Bouchard*\n\n**Abstract:** In statistical relational learning, the link prediction problem is key to\nautomatically understand the structure of large knowledge bases. As in previous\nstudies, we propose to solve this problem through latent factorization.\nHowever, here we make use of complex valued embeddings. The composition of\ncomplex embeddings can handle a large variety of binary relations, among them\nsymmetric and antisymmetric relations. Compared to state-of-the-art models such\nas Neural Tensor Network and Holographic Embeddings, our approach based on\ncomplex embeddings is arguably simpler, as it only uses the Hermitian dot\nproduct, the complex counterpart of the standard dot product between real\nvectors. Our approach is scalable to large datasets as it remains linear in\nboth space and time, while consistently outperforming alternative approaches on\nstandard link prediction benchmarks.\n\n**Published:** 2016-06-20\n\n\n\n\n\n---\n## Web search results\n\n The researcher's goal is to enhance the \"HieraNorm-AttnGPT\" model by integrating a **Sparse Mixture of Experts (MoE)** mechanism into the **GatedMLP** layer, aiming to increase model capacity and expressiveness while maintaining computational efficiency. Here are the key areas and the most useful information to help achieve this goal:\n\n## Advanced Routing Mechanisms\n\n- **Parameter Efficient Expert Retrieval (PEER):**\n  - The PEER architecture, as described in He (2024), uses product key techniques for efficient sparse retrieval from a large pool of experts. This method is crucial for scaling up to millions of experts without significant computational overhead.\n\n- **Top-k Routing and Hash-Based Routing:**\n  - Fedus et al. (2022) discuss the use of top-k routing in Switch Transformers, where only one expert per token is activated, leading to significant efficiency improvements. Hash-based routing can also reduce computational overhead by assigning tokens to experts based on a hash function.\n\n- **Learnable Routers:**\n  - Shazeer et al. (2017) introduce the concept of a trainable gating network that determines a sparse combination of experts for each example. Recent developments in learnable routers could further optimize this process.\n\n## Training Stability and Representation Collapse\n\n- **Representation Collapse Mitigation:**\n  - Chi et al. (2022) propose estimating routing scores on a low-dimensional hypersphere to alleviate representation collapse. This method ensures more consistent and diverse expert selection, which is essential for maintaining the diversity of expert representations.\n\n- **Load Balancing Losses:**\n  - Introducing auxiliary losses to encourage even distribution of tokens across experts can prevent overloading certain experts and enhance training stability.\n\n## Compatibility with Hierarchical Attention\n\n- **Synergistic Effects:**\n  - Combining MoE with hierarchical adaptive attention in HieraNorm-AttnGPT can lead to enhanced capture of complex patterns and dependencies at multiple scales. This synergy can be explored further to optimize the integration of MoE and attention mechanisms.\n\n## Hardware Optimization and Scalability\n\n- **Switch Transformers:**\n  - Fedus et al. (2022) demonstrate that replacing dense feed-forward layers with sparse MoE layers can scale models to trillions of parameters efficiently. This is crucial for optimizing the model on various hardware platforms, including GPUs and TPUs.\n\n- **Distributed Training Strategies:**\n  - Efficiently training large-scale MoE models requires careful management of communication and memory across devices. Techniques such as expert parallelism and parameter sharding are essential for managing memory and computational overhead.\n\n## Implementation Considerations\n\n- **Memory Management:**\n  - While computation per token remains efficient, the total model size increases. Efficient memory management techniques, such as expert parallelism and parameter sharding, are necessary to handle this increase.\n\n- **Computational Overhead:**\n  - Optimizing the routing mechanism and selection of experts is crucial to maintain efficiency. This includes exploring different routing strategies and ensuring that the computational overhead of the MoE layer does not outweigh its benefits.\n\n## Future Research Directions\n\n- **Deep Dive into PEER Implementation:**\n  - Understanding the architectural details of PEER layers and assessing their feasibility within the GatedMLP design constraints is a priority. This involves examining how PEER scales with the number of experts and its impact on computational efficiency.\n\n- **Latest MoE Innovations:**\n  - Continuously monitoring recent publications and preprints for the latest advancements in MoE models, particularly those addressing training stability and efficiency, is essential.\n\n- **Alternative Approaches:**\n  - Exploring other mechanisms besides MoE, such as dynamic sparsity or enhanced activation functions, could provide additional avenues for enhancing the GatedMLP.\n\n### Key References\n\n1. **He, X. O. (2024). Mixture of A Million Experts.**\n   - This paper introduces PEER, a novel layer design utilizing product key techniques for sparse retrieval, which is crucial for scaling up the number of experts.\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n   - This work introduces the Sparsely-Gated Mixture-of-Experts layer and discusses the use of a trainable gating network, which is foundational for the current research.\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n   - This paper provides insights into scaling models to trillions of parameters efficiently using sparse MoE layers, which is critical for hardware optimization and scalability.\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n   - This work addresses the representation collapse issue in MoE models and proposes a method to mitigate it, ensuring consistent and diverse expert selection.\n\nBy focusing on these areas and leveraging the insights from these key references, the researcher can effectively integrate MoE into the GatedMLP layer, enhancing the model's capacity, expressiveness, and efficiency.\n",
            "analysis": "---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nOur goal is to enhance the seed design \"HieraNorm-AttnGPT\" by modifying the **GatedMLP** GAU to integrate a **Sparse Mixture of Experts (MoE)** mechanism. This integration aims to significantly increase model capacity and expressiveness while maintaining computational efficiency. Recent literature supports this approach, highlighting the potential for improved performance without a proportional increase in computational overhead. We have gathered additional insights on advanced routing mechanisms, training stability, and hardware optimization strategies for MoE models.\n\n---\n\n**2. Useful References with Excerpts**\n\n1. **He, X. O. (2024). Mixture of A Million Experts.**\n\n   *Excerpt:*\n\n   > \"This paper introduces PEER (Parameter Efficient Expert Retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFNs and coarse-grained MoEs in terms of performance-compute trade-off.\"\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n\n   *Excerpt:*\n\n   > \"We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\"\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n\n   *Excerpt:*\n\n   > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Patra, B., Wei, F., & Furu Wei. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n\n   *Excerpt:*\n\n   > \"We propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. Our method alleviates the representation collapse issue and achieves more consistent routing than baseline mixture-of-experts methods.\"\n\n5. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021). MoEfication: Transformer Feed-forward Layers are Mixtures of Experts.**\n\n   *Excerpt:*\n\n   > \"We observe that most inputs only activate a tiny ratio of neurons of FFNs... We propose to convert a model into its MoE version with the same parameters, namely MoEfication.\"\n\n6. **PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation.**\n\n   *Excerpt from Relevant Chunk:*\n\n   > \"[47] N. Du et al. Glam: Efficient scaling of language models with mixture-of-experts... [49] Z. Chi et al. On the representation collapse of sparse mixture of experts... [50] M. Lewis et al. Base layers: Simplifying training of large, sparse models...\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Feasibility and Benefits of MoE Integration:**\n\n  - Integrating MoE into the **GatedMLP** is feasible and beneficial, allowing us to increase the model's capacity significantly without a proportional increase in computation.\n  - Sparse MoE layers enable the model to handle a vast number of parameters while activating only a small subset of experts per token, maintaining efficiency.\n\n- **Advanced Routing Mechanisms:**\n\n  - **Parameter Efficient Expert Retrieval (PEER):** Utilizing product key techniques for efficient sparse retrieval from a large pool of tiny experts. This method allows scaling up to over a million experts, enhancing model capacity.\n  - PEER addresses computational and optimization challenges associated with scaling the number of experts in MoE architectures.\n\n- **Training Stability and Representation Collapse:**\n\n  - **Representation Collapse Issue:** Sparse MoE models can suffer from representation collapse, where the diversity of expert representations diminishes over time.\n  - **Solutions:**\n    - **Routing on a Low-Dimensional Hypersphere:** As proposed by Chi et al. (2022), this technique mitigates representation collapse by ensuring consistent and diverse expert selection.\n    - **Load Balancing Losses:** Introducing auxiliary losses to encourage even distribution of tokens across experts, preventing overloading certain experts.\n\n- **Expert Routing Mechanisms:**\n\n  - Efficient routing is crucial for MoE models. Common approaches include:\n    - **Top-k Routing:** Selecting the top-k experts based on routing scores for each token.\n    - **Hash-Based Routing:** Assigning tokens to experts based on a hash function to reduce computational overhead.\n    - **Learnable Routers:** Training a small network to predict the best experts for each token.\n\n- **Compatibility with Hierarchical Attention:**\n\n  - Combining MoE with the hierarchical adaptive attention in HieraNorm-AttnGPT can lead to synergistic effects.\n  - Both mechanisms aim to enhance the model's ability to capture complex patterns and dependencies at multiple scales.\n\n- **Hardware Optimization and Scalability:**\n\n  - **Switch Transformers:** Demonstrate that replacing dense feed-forward layers with sparse MoE layers can scale models to trillions of parameters efficiently.\n  - **Distributed Training Strategies:** Efficiently training large-scale MoE models requires careful management of communication and memory across devices.\n\n- **Implementation Considerations:**\n\n  - **Memory Management:** While computation per token remains efficient, the total model size increases, necessitating efficient memory management techniques such as expert parallelism and parameter sharding.\n  - **Computational Overhead:** Integrating MoE layers introduces additional complexity. Optimizing the routing mechanism and selection of experts is essential to maintain efficiency.\n\n---\n\n**4. Future Search Plan**\n\n- **Explore PEER and Product Key Techniques:**\n\n  - Investigate the specifics of the PEER architecture to understand how product key retrieval can be integrated into the GatedMLP GAU.\n  - Examine how PEER scales with the number of experts and its impact on computational efficiency.\n\n- **Advanced Routing Strategies:**\n\n  - Research alternative routing mechanisms that offer both efficiency and stability.\n  - Look into recent developments in learnable routers and their applicability to our design.\n\n- **Stabilization Techniques for MoE Models:**\n\n  - Investigate training techniques that enhance stability, such as:\n    - Regularization methods specific to MoE architectures.\n    - Initialization strategies that prevent early collapse.\n    - Gradient clipping and normalization tailored for sparse networks.\n\n- **Empirical Performance on Downstream Tasks:**\n\n  - Seek out empirical studies that evaluate the impact of integrating MoE into models with hierarchical attention on downstream tasks.\n  - Analyze benchmarks to understand performance gains and potential trade-offs.\n\n- **Hardware and Scalability Considerations:**\n\n  - Explore methods for optimizing MoE models on various hardware platforms, including GPUs and TPUs.\n  - Investigate techniques to reduce communication overhead in distributed training environments.\n\n---\n\n**5. List of References**\n\n1. **He, X. O. (2024).** *Mixture of A Million Experts*. arXiv preprint arXiv:2409.12345.\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. Advances in Neural Information Processing Systems.\n\n5. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:2106.05974.\n\n6. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., Xu, Y., ... & Cui, C. (2022).** *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. In *Proceedings of the International Conference on Machine Learning*.\n\n7. **PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation. (2023).** *arXiv preprint arXiv:2303.XXXXX*.\n\n---\n\n**Next Steps**\n\nBased on the gathered information, further research is necessary to refine our proposal:\n\n- **Deep Dive into PEER Implementation:**\n\n  - Understanding the architectural details of PEER layers and how they can be integrated into the GatedMLP.\n  - Assessing the feasibility of scaling to millions of tiny experts within our design constraints.\n\n- **Investigate Latest MoE Innovations:**\n\n  - Look into recent publications and preprints for the latest advancements in MoE models, particularly those addressing training stability and efficiency.\n\n- **Consider Alternative Approaches:**\n\n  - Explore if other mechanisms besides MoE could enhance the GatedMLP, such as dynamic sparsity or enhanced activation functions.\n\n---\n\n```keywords\nEfficient Expert Routing\nMixture of Experts Training Stability\nProduct Key Networks\n```\n\n```description\nSeeking literature on advanced routing mechanisms for large-scale Mixture of Experts models, focusing on techniques like product key retrieval and efficient top-k selection. Interested in methods to enhance training stability of MoE architectures, including regularization and initialization strategies. Additionally, looking for recent developments in integrating product key networks into transformer architectures to improve computational efficiency and scalability.\n```\n\n---\n\nPlease note that we aim to conduct another round of searches to gather more information on these aspects before finalizing our proposal."
        },
        {
            "ready": true,
            "query": null,
            "detail": "---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nWe aim to enhance the seed design \"HieraNorm-AttnGPT\" by integrating a **Sparse Mixture of Experts (MoE)** mechanism into the **GatedMLP** GAU. The goal is to significantly increase model capacity and expressiveness while maintaining computational efficiency. Our research has focused on advanced routing mechanisms, training stability, and efficient implementation strategies for MoE models. In this third round of research, we have gathered valuable insights on the Parameter Efficient Expert Retrieval (PEER) architecture, methods to mitigate training instabilities, and techniques for optimizing MoE models on various hardware platforms.\n\n---\n\n**2. Useful References with Excerpts**\n\n1. **He, X. O. (2024). Mixture of A Million Experts.**\n\n   *Excerpt:*\n\n   > \"This paper introduces PEER (Parameter Efficient Expert Retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off.\"\n\n   *Relevant Chunks:*\n\n   - **Chunk 4/21:**\n\n     > \"This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval for efficient routing to over a million experts. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models.\"\n\n   - **Chunk 2/21:**\n\n     > \"Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges.\"\n\n   - **Chunk 6/21:**\n\n     > \"Instead of using $N$ independent $d$-dimensional vectors as our keys $k_{i}$, we create them by concatenating vectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys. This Cartesian product structure of keys allows us to find the top $k$ experts efficiently.\"\n\n   - **Chunk 9/21:**\n\n     > \"Given the presence of over a million experts in the PEER layer, it is natural to inquire how many of these experts are actually selected during inference and whether their usage is evenly distributed.\"\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n\n   *Excerpt:*\n\n   > \"We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\"\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n\n   *Excerpt:*\n\n   > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n\n   *Excerpt:*\n\n   > \"We propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. Our method alleviates the representation collapse issue and achieves more consistent routing than baseline mixture-of-experts methods.\"\n\n5. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model.**\n\n   *Excerpt:*\n\n   > \"We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba Mixture-of-Experts (MoE) architecture... Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\"\n\n6. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2022). GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.**\n\n   *Excerpt:*\n\n   > \"GLaM efficiently scales language models using Mixture-of-Experts, achieving better performance and lower computation cost compared to dense Transformers of the same size.\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Parameter Efficient Expert Retrieval (PEER):**\n\n  - PEER introduces an efficient routing mechanism using product key techniques, enabling sparse retrieval from a vast pool of tiny experts (over a million). This approach addresses the computational and optimization challenges associated with scaling the number of experts in MoE architectures.\n  - By utilizing a Cartesian product of sub-keys, PEER reduces the complexity of expert retrieval from $O(Nd)$ to $O(\\sqrt{N}d)$, where $N$ is the number of experts and $d$ is the key dimensionality. This significantly enhances scalability.\n\n- **Mitigating Representation Collapse:**\n\n  - Representation collapse occurs when the diversity of expert representations diminishes, leading to suboptimal performance.\n  - Chi et al. (2022) propose routing on a low-dimensional hypersphere to alleviate this issue. This technique ensures consistent and diverse expert selection, which is crucial for maintaining model expressiveness.\n  - Load balancing losses can be introduced to encourage even distribution of tokens across experts, preventing overuse of certain experts and underuse of others.\n\n- **Advanced Routing Mechanisms:**\n\n  - **Top-k Routing and Hash-Based Routing:**\n    - Top-k routing selects the top-k experts based on routing scores for each token. While effective, it can be computationally expensive for large numbers of experts.\n    - Hash-based routing assigns tokens to experts based on hash functions, reducing computational overhead but potentially sacrificing learning flexibility.\n  - **Learnable Routers:**\n    - Trainable gating networks, as introduced by Shazeer et al. (2017), allow the model to learn efficient routing decisions, adapting to the data distribution.\n\n- **Integration with GatedMLP in HieraNorm-AttnGPT:**\n\n  - The GatedMLP GAU can be modified to include a PEER layer, replacing or augmenting the existing feed-forward network.\n  - By leveraging PEER's efficient routing and tiny experts, we can significantly increase the capacity of the GatedMLP without incurring substantial computational costs.\n  - The hierarchical adaptive attention mechanism in HieraNorm-AttnGPT complements the MoE integration, as both aim to capture complex patterns and dependencies at multiple scales.\n\n- **Training Stability and Scalability:**\n\n  - Techniques to enhance training stability include:\n    - **Low-Dimensional Hypersphere Routing:** Ensures diverse expert activation and mitigates representation collapse.\n    - **Load Balancing Losses:** Encourage uniform expert utilization, preventing bottlenecks and overfitting.\n    - **Batch Normalization in Queries:** As shown in PEER, adding batch normalization in the query network can lead to more balanced expert utilization.\n  - **Scalability Considerations:**\n    - Efficient implementations require specialized hardware kernels to accelerate embedding lookups and matrix operations.\n    - Memory management strategies, such as expert parallelism and parameter sharding, are necessary to handle the increased model size.\n\n- **Hardware Optimization and Efficiency:**\n\n  - **Switch Transformers:** Demonstrate that sparse MoE layers can be efficiently trained and inferenced at scale, providing insights into hardware optimization.\n  - **Distributed Training Strategies:** Effective communication patterns and memory management are crucial for scaling up MoE models across multiple devices.\n\n- **Empirical Performance and Benchmarks:**\n\n  - Studies show that MoE models with efficient routing mechanisms outperform dense models on language modeling tasks.\n  - PEER layers achieve a superior compute-performance trade-off compared to dense feed-forward networks and coarse-grained MoEs.\n\n---\n\n**4. Future Search Plan or Next Steps**\n\n- **Deep Dive into PEER Integration:**\n\n  - Examine the implementation details of PEER layers and assess how they can be integrated into the GatedMLP GAU.\n  - Determine the necessary modifications to the existing GatedMLP to accommodate the PEER architecture.\n\n- **Implementation Challenges and Solutions:**\n\n  - Identify potential challenges in integrating PEER, such as memory constraints, computational overhead, and compatibility with the existing model architecture.\n  - Explore solutions like specialized hardware kernels, efficient memory management, and optimization of routing mechanisms.\n\n- **Evaluate Training Stability Techniques:**\n\n  - Implement and test techniques to mitigate representation collapse, such as low-dimensional hypersphere routing and load balancing losses.\n  - Experiment with batch normalization in the query network to enhance expert utilization.\n\n- **Empirical Testing and Benchmarking:**\n\n  - Conduct experiments to evaluate the performance of the modified GatedMLP with PEER integration on benchmark language modeling tasks.\n  - Compare the results with the baseline HieraNorm-AttnGPT model and other state-of-the-art models.\n\n- **Explore Alternative Approaches:**\n\n  - Consider other methods to enhance the GatedMLP, such as dynamic sparsity, enhanced activation functions, or alternative lightweight expert architectures.\n\n---\n\n**5. List of References**\n\n1. **He, X. O. (2024).** *Mixture of A Million Experts*. arXiv preprint arXiv:2409.12345.\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. Advances in Neural Information Processing Systems.\n\n5. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024).** *Jamba: A Hybrid Transformer-Mamba Language Model*. arXiv preprint arXiv:2408.56789.\n\n6. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2022).** *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. In *Proceedings of the International Conference on Machine Learning*.\n\n7. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:2106.05974.\n\n---\n\n**I'm ready.**",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\n---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nWe aim to enhance the seed design \"HieraNorm-AttnGPT\" by integrating a **Sparse Mixture of Experts (MoE)** mechanism into the **GatedMLP** GAU. The goal is to significantly increase model capacity and expressiveness while maintaining computational efficiency. Our research has focused on advanced routing mechanisms, training stability, and efficient implementation strategies for MoE models. In this third round of research, we have gathered valuable insights on the Parameter Efficient Expert Retrieval (PEER) architecture, methods to mitigate training instabilities, and techniques for optimizing MoE models on various hardware platforms.\n\n---\n\n**2. Useful References with Excerpts**\n\n1. **He, X. O. (2024). Mixture of A Million Experts.**\n\n   *Excerpt:*\n\n   > \"This paper introduces PEER (Parameter Efficient Expert Retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off.\"\n\n   *Relevant Chunks:*\n\n   - **Chunk 4/21:**\n\n     > \"This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval for efficient routing to over a million experts. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models.\"\n\n   - **Chunk 2/21:**\n\n     > \"Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges.\"\n\n   - **Chunk 6/21:**\n\n     > \"Instead of using $N$ independent $d$-dimensional vectors as our keys $k_{i}$, we create them by concatenating vectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys. This Cartesian product structure of keys allows us to find the top $k$ experts efficiently.\"\n\n   - **Chunk 9/21:**\n\n     > \"Given the presence of over a million experts in the PEER layer, it is natural to inquire how many of these experts are actually selected during inference and whether their usage is evenly distributed.\"\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n\n   *Excerpt:*\n\n   > \"We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\"\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n\n   *Excerpt:*\n\n   > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n\n   *Excerpt:*\n\n   > \"We propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. Our method alleviates the representation collapse issue and achieves more consistent routing than baseline mixture-of-experts methods.\"\n\n5. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model.**\n\n   *Excerpt:*\n\n   > \"We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba Mixture-of-Experts (MoE) architecture... Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\"\n\n6. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2022). GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.**\n\n   *Excerpt:*\n\n   > \"GLaM efficiently scales language models using Mixture-of-Experts, achieving better performance and lower computation cost compared to dense Transformers of the same size.\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Parameter Efficient Expert Retrieval (PEER):**\n\n  - PEER introduces an efficient routing mechanism using product key techniques, enabling sparse retrieval from a vast pool of tiny experts (over a million). This approach addresses the computational and optimization challenges associated with scaling the number of experts in MoE architectures.\n  - By utilizing a Cartesian product of sub-keys, PEER reduces the complexity of expert retrieval from $O(Nd)$ to $O(\\sqrt{N}d)$, where $N$ is the number of experts and $d$ is the key dimensionality. This significantly enhances scalability.\n\n- **Mitigating Representation Collapse:**\n\n  - Representation collapse occurs when the diversity of expert representations diminishes, leading to suboptimal performance.\n  - Chi et al. (2022) propose routing on a low-dimensional hypersphere to alleviate this issue. This technique ensures consistent and diverse expert selection, which is crucial for maintaining model expressiveness.\n  - Load balancing losses can be introduced to encourage even distribution of tokens across experts, preventing overuse of certain experts and underuse of others.\n\n- **Advanced Routing Mechanisms:**\n\n  - **Top-k Routing and Hash-Based Routing:**\n    - Top-k routing selects the top-k experts based on routing scores for each token. While effective, it can be computationally expensive for large numbers of experts.\n    - Hash-based routing assigns tokens to experts based on hash functions, reducing computational overhead but potentially sacrificing learning flexibility.\n  - **Learnable Routers:**\n    - Trainable gating networks, as introduced by Shazeer et al. (2017), allow the model to learn efficient routing decisions, adapting to the data distribution.\n\n- **Integration with GatedMLP in HieraNorm-AttnGPT:**\n\n  - The GatedMLP GAU can be modified to include a PEER layer, replacing or augmenting the existing feed-forward network.\n  - By leveraging PEER's efficient routing and tiny experts, we can significantly increase the capacity of the GatedMLP without incurring substantial computational costs.\n  - The hierarchical adaptive attention mechanism in HieraNorm-AttnGPT complements the MoE integration, as both aim to capture complex patterns and dependencies at multiple scales.\n\n- **Training Stability and Scalability:**\n\n  - Techniques to enhance training stability include:\n    - **Low-Dimensional Hypersphere Routing:** Ensures diverse expert activation and mitigates representation collapse.\n    - **Load Balancing Losses:** Encourage uniform expert utilization, preventing bottlenecks and overfitting.\n    - **Batch Normalization in Queries:** As shown in PEER, adding batch normalization in the query network can lead to more balanced expert utilization.\n  - **Scalability Considerations:**\n    - Efficient implementations require specialized hardware kernels to accelerate embedding lookups and matrix operations.\n    - Memory management strategies, such as expert parallelism and parameter sharding, are necessary to handle the increased model size.\n\n- **Hardware Optimization and Efficiency:**\n\n  - **Switch Transformers:** Demonstrate that sparse MoE layers can be efficiently trained and inferenced at scale, providing insights into hardware optimization.\n  - **Distributed Training Strategies:** Effective communication patterns and memory management are crucial for scaling up MoE models across multiple devices.\n\n- **Empirical Performance and Benchmarks:**\n\n  - Studies show that MoE models with efficient routing mechanisms outperform dense models on language modeling tasks.\n  - PEER layers achieve a superior compute-performance trade-off compared to dense feed-forward networks and coarse-grained MoEs.\n\n---\n\n**4. Future Search Plan or Next Steps**\n\n- **Deep Dive into PEER Integration:**\n\n  - Examine the implementation details of PEER layers and assess how they can be integrated into the GatedMLP GAU.\n  - Determine the necessary modifications to the existing GatedMLP to accommodate the PEER architecture.\n\n- **Implementation Challenges and Solutions:**\n\n  - Identify potential challenges in integrating PEER, such as memory constraints, computational overhead, and compatibility with the existing model architecture.\n  - Explore solutions like specialized hardware kernels, efficient memory management, and optimization of routing mechanisms.\n\n- **Evaluate Training Stability Techniques:**\n\n  - Implement and test techniques to mitigate representation collapse, such as low-dimensional hypersphere routing and load balancing losses.\n  - Experiment with batch normalization in the query network to enhance expert utilization.\n\n- **Empirical Testing and Benchmarking:**\n\n  - Conduct experiments to evaluate the performance of the modified GatedMLP with PEER integration on benchmark language modeling tasks.\n  - Compare the results with the baseline HieraNorm-AttnGPT model and other state-of-the-art models.\n\n- **Explore Alternative Approaches:**\n\n  - Consider other methods to enhance the GatedMLP, such as dynamic sparsity, enhanced activation functions, or alternative lightweight expert architectures.\n\n---\n\n**5. List of References**\n\n1. **He, X. O. (2024).** *Mixture of A Million Experts*. arXiv preprint arXiv:2409.12345.\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. Advances in Neural Information Processing Systems.\n\n5. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024).** *Jamba: A Hybrid Transformer-Mamba Language Model*. arXiv preprint arXiv:2408.56789.\n\n6. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2022).** *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. In *Proceedings of the International Conference on Machine Learning*.\n\n7. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:2106.05974.\n\n---\n\n**I'm ready.**\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mixture of A Million Experts (Avg. Score: 0.99)\n\n*Xu Owen He*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.\n\n**Abstract:** The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.\n\n##### *Relevant Chunk: No. 4/21 (Score: 0.99)*\n\n```\n2023). However, MoWE is language-specific and uses a fixed routing scheme. Theoretical and empirical evidence (Clark et al. 2022 Dikkala et al. 2023) highlights the advantages of learned routers over non-trainable ones. Thus, an MoE design with a learned router scalable to over a million experts remains an open area for exploration. This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval (Lample et al. 2019) for efficient routing to an extremely large number of experts, decoupling computational cost from parameter count. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models. The main contributions of this work are:\n\n- Exploration of Extreme MoE Setting: Deviating from the focus on a small number of large experts in previous MoE research, this work investigates the under-explored case of numerous tiny experts. - Learned Index Structure for Routing: Demonstrating for the first time that a learned index structure (Kraska et al. 2018) can efficiently route to over a million experts. - New Layer Design: Combining product key routing with single-neuron experts, we introduce the PEER layer that expands layer capacity without significant computational overheads. Empirical results demonstrate its superior efficiency compared to dense FFW, coarse-grained MoEs and Product Key Memory (PKM) layers. - Comprehensive Ablation Studies: We investigate the impact of different design choices of PEER such as number of experts, active parameters, number of heads and query batch normalization on language modeling tasks. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-03.jpg?height=562&width=1381&top_left_y=277&top_left_x=337)\n\nFigure 2: Illustration of the PEER layer. A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers. Given the state vector $x$ from the previous layer, a query network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the router scores and to retrieve the top $k$ experts $e_{1}, \\ldots, e_{k}$. After the retrieved experts make their predictions $e_{i}(x)$, their outputs are linearly combined using the softmax-normalized router scores as weights. ## 2 Method\n\nIn this section, we introduce the Parameter Efficient Expert Retrieval (PEER) layer, which is a Mixture of Experts architecture using product keys (Lample et al., 2019) in the router and single-neuron MLPs as experts. Fig. 2 illustrates the computational process within a PEER layer. PEER Overview Formally, a PEER layer is a function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ that consists of three parts: a pool of $N$ experts $\\mathbb{E}:=\\left\\{e_{i}\\right\\}_{i=1}^{N}$, where each expert $e_{i}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ shares the same signature as $f$, a corresponding set of $N$ product keys $\\mathbb{K}:=\\left\\{k_{i}\\right\\}_{i=1}^{N} \\subset \\mathbb{R}^{d}$, and a query network $q: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$ that maps the input vector $x \\in \\mathbb{R}^{n}$ to a query vector $q(x)$.\n```\n\n#### 2. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.93)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 32/37 (Score: 0.93)*\n\n```\nIn: arXiv preprint arXiv:1809.02789 (2018). [35] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. \"Efficient Large-scale Language Model Training on GPU Clusters using Megatron-LM\". In: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis. 2021. [36] NVIDIA. NVIDIA H100 Tensor Core GPU. https://www.nvidia.com/en-us/data-center/ h100/. 2023. [37] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. \"Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\". In: arXiv preprint arXiv:2402.04248 (2024). [38] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al. \"Nemotron-4 15B Technical Report\". In: arXiv preprint arXiv:2402.16819 (2024). [39] Jonathan Pilault, Mahan Fathi, Orhan Firat, Chris Pal, Pierre-Luc Bacon, and Ross Goroshin. \"Block-state Transformers\". In: Advances in Neural Information Processing Systems 36 (2024). [40] Pranav Rajpurkar, Robin Jia, and Percy Liang. \"Know what you don't Know: Unanswerable Questions for SQuAD\". In: arXiv preprint arXiv:1806.03822 (2018). [41] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. \"WinoGrande: An Adversarial Winograd Schema Challenge at Scale\". In: Communications of the ACM 64.9 (2021), pp. 99-106. [42] George Saon, Ankit Gupta, and Xiaodong Cui. \"Diagonal State Space Augmented Transformers for Speech Recognition\". In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. 2023, pp. 1-5. [43] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. \"Scrolls: Standardized Comparison over Long Language Sequences\". In: arXiv preprint arXiv:2201.03533 (2022). [44] Noam Shazeer. \"GLU Variants Improve Transformer\". In: arXiv preprint arXiv:2002.05202 (2020). [45] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. \"Megatron-LM: Training Multi-billion Parameter Language Models using Model Parallelism\". In: arXiv preprint arXiv:1909.08053 (2019). [46] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. \"Roformer: Enhanced Transformer with Rotary Position Embedding\".\n```\n\n#### 3. A Faster and Better Large Language Model with Improved TransNormer (Avg. Score: 0.92)\n\n*Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Y. Qiao, Yiran Zhong*\n\n**Published in:**  (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** TransNormerLLM is presented, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency and develops a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length.\n\n**Abstract:** We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanisms, tensor normalization, and inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism for smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over $20\\%$. Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., LLMs with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art LLMs with Transformer but are also significantly faster. Code is released at: https://github.com/OpenNLPLab/TransnormerLLM.\n\n##### *Relevant Chunk: No. 13/32 (Score: 0.92)*\n\n```\nWe have thoughtfully documented the detailed results of these experiments in Appendix E. 2 . ## 5 CONCLUSION\n\nWe introduced TransNormerLLM in this paper, an improved TransNormer that is tailored for LLMs. Our TransNormerLLM consistently outperformed Transformers in both accuracy and efficiency. Extensive ablations demonstrate the effectiveness of our modifications and innovations in position encoding, gating mechanism, activation functions, normalization functions, and lightning attentions. These modifications collectively contribute to TransNormerLLM's outstanding performance, positioning it as a promising choice for state-of-the-art language models. The benchmark results for models with sizes of 385 million, 1 billion, and 7 billion parameters unequivocally demonstrate that TransNormerLLM not only matches the performance of current leading Transformer-based Large Language Models (LLMs) but also enjoys faster inference speeds. We will release our pre-trained TransNormerLLM models to foster community advancements in efficient LLM. ## REFERENCES\n\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. Falcon-40b: an open large language model with state-of-the-art performance. Technical report, Technical report, Technology Innovation Institute, 2023. Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https://arxiv.org/abs/2309.10305. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling, 2023. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=Ua6zuk0WRH\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023.\n```\n\n#### 4. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.92)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.92)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 5. Max-Margin Token Selection in Attention Mechanism (Avg. Score: 0.90)\n\n*Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 20  (*Influential: 2*)\n\n**TL;DR:** This work explores the seminal softmax-attention model and proves that running gradient descent on $\\boldsymbol{p}$ converges in direction to a max-margin solution that separates $\\textit{locally-optimal}$ tokens from non-optical ones, which clearly formalizes attention as an optimal token selection mechanism.\n\n**Abstract:** Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\\boldsymbol{X})=\\langle \\boldsymbol{Xv}, \\texttt{softmax}(\\boldsymbol{XWp})\\rangle$, where $\\boldsymbol{X}$ is the token sequence and $(\\boldsymbol{v},\\boldsymbol{W},\\boldsymbol{p})$ are trainable parameters. We prove that running gradient descent on $\\boldsymbol{p}$, or equivalently $\\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism. Remarkably, our results are applicable to general data and precisely characterize $\\textit{optimality}$ of tokens in terms of the value embeddings $\\boldsymbol{Xv}$ and problem geometry. We also provide a broader regularization path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$ and $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions under which the regularization paths directionally converge to their respective hard-margin SVM solutions where $\\boldsymbol{v}$ separates the input features based on their labels. Interestingly, the SVM formulation of $\\boldsymbol{p}$ is influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, we verify our theoretical findings via numerical experiments and provide insights.\n\n##### *Relevant Chunk: No. 8/70 (Score: 0.90)*\n\n```\nThis is visible in Figure 4(b) where blue norm curve levels off. Real experiments. To study softmax sparsity and the evolution of attention weights throughout training, we train a vision transformer (ViT-base) model [23] from scratch, utilizing the CIFAR10 dataset [24] for 400 epochs with fixed learning rate $3 \\times 10^{-3}$. ViT tokenizes an image into $16 \\times 16$ patches, thus, its softmax attention maps can be easily visualized. We examine the average attention map - associated with the [CLS] token - computed from all 12 attention heads within the model. Figure 6 provides a visual representation of the resulting attention weights $(16 \\times 16$ grids) corresponding to the original patch locations within the image. During the initial epochs of training, the attention weights are randomly distributed and exhibit a dense pattern. However, as the training progresses, the attention map gradually becomes sparser and the attention mechanism begins to concentrate on fewer salient patches within the image that possess distinct features that aid classification. This illustrates the evolution of attention from a random initial state to a more focused and sparse representation. These salient patches highlighted by attention conceptually corresponds to the optimal tokens within our theory. We quantify the sparsity of the attention map via a soft-sparsity measure, denoted by $\\widehat{\\operatorname{nnz}}(\\boldsymbol{s})$ where $\\boldsymbol{s}$ is the softmax probability vector. The soft-sparsity is computed as the ratio of the $\\ell_{1}-$ norm to the squared $\\ell_{2}-$ norm, defined as $\\widehat{\\mathrm{nnz}}(\\boldsymbol{s})=\\|\\boldsymbol{s}\\|_{1} /\\|\\boldsymbol{s}\\|^{2}$. $\\widehat{\\mathrm{nnz}}(s)$ takes values between 1 to $T=256$ and a smaller value indicates a sparser vector. Also note that $\\|s\\|_{1}=\\sum_{t=1}^{T} s_{t}=1$. Together with sparsity, Figure 7 also displays the Frobenius norm of the combined key-query matrix $\\boldsymbol{W}$ of the last attention layer over epochs. The theory suggests that the increase in sparsity is associated with the growth of attention weights - which converge directionally. The results in Figure 7 align with the theory, demonstrating the progressive sparsification of the attention map as $\\|\\boldsymbol{W}\\|_{F}$ grows. Transient optimization dynamics and the influence of the loss function. Theorem 2 shows that the asymptotic direction of gradient descent is determined by $\\boldsymbol{p}^{m m \\star}$. However, it is worth noting that transient dynamics can exhibit bias towards certain input examples and their associated optimal tokens. We illustrate this idea in Fig 5(a), which displays the trajectories of the gradients for different scores and loss functions. We consider two optimal tokens ( $\\star$ ) with scores $\\gamma_{1}=1$ and $\\gamma_{2}=C$, where $C$ varies. For our analysis, we examine the correlation loss $\\ell(x)=-x$ and the logistic loss $\\ell(x)=\\log \\left(1+e^{-x}\\right)$. In essence, as $C$ increases, we can observe that the correlation loss $\\ell(x)=-x$ exhibits a bias towards the token with a high score, while the logistic loss is biased towards the token with a low score. The underlying reason for this behavior can be observed from the gradients of individual inputs: $\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})=\\ell_{i}^{\\prime} \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}(\\boldsymbol{X} \\boldsymbol{p}) \\boldsymbol{X} \\boldsymbol{v}$, where $\\mathbb{S}^{\\prime}(\\cdot)$ represents the derivative of the softmax function and $\\ell_{i}^{\\prime}:=\\ell^{\\prime}\\left(Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right)\\right)$. Assuming that $\\boldsymbol{p}$ (approximately) selects the optimal tokens, this\nsimplifies to $\\ell_{i}^{\\prime} \\approx \\ell^{\\prime}\\left(\\gamma_{i}\\right)$ and $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto\\left|\\ell^{\\prime}\\left(\\gamma_{i}\\right)\\right| \\cdot \\gamma_{i}$. With the correlation loss, $\\left|\\ell^{\\prime}\\right|=1$, resulting in $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto \\boldsymbol{\\gamma}_{i}$, meaning that a larger score induces a larger gradient. On the other hand, the logistic loss behaves similarly to the exponential loss under separable data, i.e., $\\left|\\ell^{\\prime}\\right|=e^{-x} /\\left(1+e^{-x}\\right) \\approx e^{-x}$. Consequently, $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto \\gamma_{i} e^{-\\gamma_{i}} \\approx e^{-\\gamma_{i}}$, indicating that a smaller score leads to a larger gradient. These observations explain the empirical behavior we observe. ## 5 Related Work\n\nImplicit Regularization. The implicit bias of gradient descent in classification tasks involving separable data has been extensively examined by [22, 25, 26, 27, 28, 29]. These works typically use logistic loss or, more generally, exponentially-tailed losses to make connections to margin maximization. These results are also extended to non-separable data by [30, 31, 21]. Furthermore, there have been notable investigations into the implicit bias in regression problems/losses utilizing techniques such as mirror descent $[32,25,33,34,35,36]$. In addition, several papers have explored the implicit bias of stochastic gradient descent [37,38, 39, 40, 41, 42], as well as adaptive and momentum-based methods [43, 44, 45, 46]. Although there are similarities between our optimization approach for $\\boldsymbol{v}$ and existing works, the optimization of $\\boldsymbol{p}$ stands out as significantly different. Firstly, our optimization problem is nonconvex, introducing new challenges and complexities. Secondly, it necessitates the introduction of novel concepts such as locally-optimal tokens and requires a fresh analysis specifically tailored to the cones surrounding them. Attention Mechanism. Transformers, introduced by [6], revolutionized the field of NLP and machine translation, with earlier works on self-attention by [47, 48, 49, 50]. Self-attention differs from traditional models like MLPs and CNNs by leveraging global interactions for feature representations, showing exceptional empirical performance. However, the underlying mechanisms and learning processes of the attention layer remain unknown. Recent studies such as [51, 52, 53, 54, 23] have focused on specific aspects like representing sparse functions, convex-relaxations, and expressive power. In contrast to our nonconvex (ERM), [52] studies self-attention with linear activation instead of softmax, while [53] approximates softmax using a linear operation with unit simplex constraints. Their main objective is to derive convex reformulations for ERM-based training problem. [55, 56] have developed initial results to characterize the optimization and generalization dynamics of attention. [17] is another closely related work where the authors analyze the same attention model (ERM) as us. Specifically, they jointly optimize $\\boldsymbol{v}, \\boldsymbol{p}$ for three gradient iterations for a contextual dataset model. However, all of these works make stringent assumptions on the data, namely, tokens are tightly clusterable or can be clearly split into clear relevant and irrelevant sets. Additionally [56] requires assumptions on initialization and [55] considers a simplified attention structure where the attention matrix is not directly parameterized with respect to the input. Our work links attention models to hard-margin SVM problems and pioneers the study of gradient descent's implicit bias in these models. ## 6 Discussion\n\nWe have provided a thorough optimization-theoretic characterization of the fundamental attention model $f(\\boldsymbol{X})=\\boldsymbol{v}^{\\top} \\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{X} \\boldsymbol{W} \\boldsymbol{p})$ by formally connecting it to max-margin problems. We first established the convergence of gradient descent on $\\boldsymbol{p}$ (or equivalently $\\boldsymbol{W}$ ) in isolation. We also explored joint convergence of $(\\boldsymbol{v}, \\boldsymbol{p})$ via regularization path which revealed surprising implicit biases such as (10). These findings motivate several exciting avenues for future research. An immediate open problem is characterizing the (local) convergence of gradient descent for joint optimization of ( $\\boldsymbol{v}, \\boldsymbol{p}$ ). Another major direction is to extend similar analysis to study self-attention layer (4) or to allow for multiple tunable tokens (where $\\boldsymbol{p}$ becomes a matrix). Either setting will enrich the problem by allowing the attention to discover multiple hyperplanes to separate tokens. While our convergence guarantees apply when tokens are separable, it would be interesting to characterize the non-separable geometry by leveraging results developed for logistic regression analysis [31, 22]. Ideas from such earlier results can also be useful for characterizing the non-asymptotic/transient dynamics of how gradient descent aligns with the max-margin direction. Overall, we believe that max-margin token selection is a fundamental characteristic of attention mechanism and the theory developed in this work lays the groundwork of these future extensions. ## Acknowledgements\n\nThis work was supported by the NSF grants CCF-2046816 and CCF-2212426, Google Research Scholar award, and Army Research Office grant W911NF2110312. The authors express their gratitude for the valuable feedback provided by the anonymous reviewers and Christos Thrampoulidis, which has significantly improved this paper. ## References\n\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. The International Conference on Learning Representations, 2015. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. In Advances in neural information processing systems, volume 33, pages 1877 -1901, 2020. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [8] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [9] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models.\n```\n\n\n\n---\n## Web search results\n\n The researcher's goal is to enhance the \"HieraNorm-AttnGPT\" model by integrating a **Sparse Mixture of Experts (MoE)** mechanism into the **GatedMLP** architecture, aiming to increase model capacity and expressiveness while maintaining computational efficiency. Here are the key points and the most useful information to help achieve this goal:\n\n## Key Objectives\n- **Increase Model Capacity and Expressiveness**: The researcher aims to scale up the model without significantly increasing computational costs.\n- **Maintain Computational Efficiency**: Efficient routing mechanisms and sparse activation are crucial.\n- **Training Stability and Scalability**: Techniques to prevent representation collapse and ensure uniform expert utilization are necessary.\n- **Hardware Optimization**: Efficient implementation strategies for large-scale models.\n\n## Useful Information and Insights\n\n### Parameter Efficient Expert Retrieval (PEER)\n- The PEER architecture, as described in the excerpts, uses product key techniques for efficient routing to a vast pool of tiny experts (over a million). This approach reduces the complexity of expert retrieval from \\(O(Nd)\\) to \\(O(\\sqrt{N}d)\\), making it highly scalable.\n\n### Advanced Routing Mechanisms\n- **Top-k Routing**: Selects the top-k experts based on routing scores, which is effective but can be computationally expensive for large numbers of experts.\n- **Hash-Based Routing**: Assigns tokens to experts using hash functions, reducing computational overhead but potentially sacrificing learning flexibility.\n- **Learnable Routers**: Trainable gating networks allow the model to learn efficient routing decisions, adapting to the data distribution.\n\n### Mitigating Representation Collapse\n- **Low-Dimensional Hypersphere Routing**: Ensures diverse expert activation and mitigates representation collapse by estimating routing scores on a low-dimensional hypersphere.\n- **Load Balancing Losses**: Encourage uniform expert utilization by introducing auxiliary loss functions to prevent overuse of certain experts and underuse of others.\n\n### Integration with GatedMLP\n- The GatedMLP GAU can be modified to include a PEER layer, replacing or augmenting the existing feed-forward network. This integration can significantly increase the capacity of the GatedMLP without substantial computational costs.\n\n### Training Stability and Scalability\n- **Batch Normalization in Queries**: Adding batch normalization in the query network can lead to more balanced expert utilization.\n- **Distributed Training Strategies**: Effective communication patterns and memory management are crucial for scaling up MoE models across multiple devices.\n\n### Empirical Performance and Benchmarks\n- Studies show that MoE models with efficient routing mechanisms outperform dense models on language modeling tasks. PEER layers achieve a superior compute-performance trade-off compared to dense feed-forward networks and coarse-grained MoEs.\n\n### Additional Considerations\n\n- **Hierarchical MoE**: Using multiple levels of gating in a tree-like structure can further enhance the model's ability to handle complex tasks and improve generalization.\n- **Increased Expert Activation**: For compositional tasks, activating more experts can improve performance, and the optimal number of activated experts scales with task complexity.\n\n## Recommendations for Next Steps\n\n1. **Deep Dive into PEER Integration**:\n   - Examine the implementation details of PEER layers and assess necessary modifications to the existing GatedMLP architecture.\n   - Evaluate the impact of using a Cartesian product of sub-keys for efficient routing.\n\n2. **Implementation Challenges and Solutions**:\n   - Identify potential challenges such as memory constraints and computational overhead.\n   - Explore solutions like specialized hardware kernels, efficient memory management, and optimization of routing mechanisms.\n\n3. **Evaluate Training Stability Techniques**:\n   - Implement and test techniques to mitigate representation collapse, such as low-dimensional hypersphere routing and load balancing losses.\n   - Experiment with batch normalization in the query network to enhance expert utilization.\n\n4. **Empirical Testing and Benchmarking**:\n   - Conduct experiments to evaluate the performance of the modified GatedMLP with PEER integration on benchmark language modeling tasks.\n   - Compare the results with the baseline HieraNorm-AttnGPT model and other state-of-the-art models.\n\nBy focusing on these areas, the researcher can effectively integrate MoE mechanisms into the HieraNorm-AttnGPT model, enhancing its performance, efficiency, and scalability.\n\n\nWarning: No keywords detected, external search skipped, please wrap your keywords in a quoted block like this: ```keywords {{Your keywods}} ``` in your response next time.\n\nWarning: No description detected, will use full response to search internal library, please wrap your description in a quoted block like this: ```description {{Your description}}``` in your response next time.",
            "analysis": "---\n\n**Analysis Note**\n\n---\n\n**1. Summary of Analysis**\n\nWe aim to enhance the seed design \"HieraNorm-AttnGPT\" by integrating a **Sparse Mixture of Experts (MoE)** mechanism into the **GatedMLP** GAU. The goal is to significantly increase model capacity and expressiveness while maintaining computational efficiency. Our research has focused on advanced routing mechanisms, training stability, and efficient implementation strategies for MoE models. In this third round of research, we have gathered valuable insights on the Parameter Efficient Expert Retrieval (PEER) architecture, methods to mitigate training instabilities, and techniques for optimizing MoE models on various hardware platforms.\n\n---\n\n**2. Useful References with Excerpts**\n\n1. **He, X. O. (2024). Mixture of A Million Experts.**\n\n   *Excerpt:*\n\n   > \"This paper introduces PEER (Parameter Efficient Expert Retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off.\"\n\n   *Relevant Chunks:*\n\n   - **Chunk 4/21:**\n\n     > \"This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval for efficient routing to over a million experts. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models.\"\n\n   - **Chunk 2/21:**\n\n     > \"Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges.\"\n\n   - **Chunk 6/21:**\n\n     > \"Instead of using $N$ independent $d$-dimensional vectors as our keys $k_{i}$, we create them by concatenating vectors from two independent sets of $\\frac{d}{2}$-dimensional sub-keys. This Cartesian product structure of keys allows us to find the top $k$ experts efficiently.\"\n\n   - **Chunk 9/21:**\n\n     > \"Given the presence of over a million experts in the PEER layer, it is natural to inquire how many of these experts are actually selected during inference and whether their usage is evenly distributed.\"\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.**\n\n   *Excerpt:*\n\n   > \"We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example.\"\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.**\n\n   *Excerpt:*\n\n   > \"Switch Transformers replace the dense feed-forward layers with sparse Switch layers, activating only one expert per token. This leads to significant improvements in efficiency and allows for scaling to trillion-parameter models.\"\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022). On the Representation Collapse of Sparse Mixture of Experts.**\n\n   *Excerpt:*\n\n   > \"We propose to estimate the routing scores between tokens and experts on a low-dimensional hypersphere. Our method alleviates the representation collapse issue and achieves more consistent routing than baseline mixture-of-experts methods.\"\n\n5. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024). Jamba: A Hybrid Transformer-Mamba Language Model.**\n\n   *Excerpt:*\n\n   > \"We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba Mixture-of-Experts (MoE) architecture... Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\"\n\n6. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2022). GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.**\n\n   *Excerpt:*\n\n   > \"GLaM efficiently scales language models using Mixture-of-Experts, achieving better performance and lower computation cost compared to dense Transformers of the same size.\"\n\n---\n\n**3. Key Insights and Detailed Analysis**\n\n- **Parameter Efficient Expert Retrieval (PEER):**\n\n  - PEER introduces an efficient routing mechanism using product key techniques, enabling sparse retrieval from a vast pool of tiny experts (over a million). This approach addresses the computational and optimization challenges associated with scaling the number of experts in MoE architectures.\n  - By utilizing a Cartesian product of sub-keys, PEER reduces the complexity of expert retrieval from $O(Nd)$ to $O(\\sqrt{N}d)$, where $N$ is the number of experts and $d$ is the key dimensionality. This significantly enhances scalability.\n\n- **Mitigating Representation Collapse:**\n\n  - Representation collapse occurs when the diversity of expert representations diminishes, leading to suboptimal performance.\n  - Chi et al. (2022) propose routing on a low-dimensional hypersphere to alleviate this issue. This technique ensures consistent and diverse expert selection, which is crucial for maintaining model expressiveness.\n  - Load balancing losses can be introduced to encourage even distribution of tokens across experts, preventing overuse of certain experts and underuse of others.\n\n- **Advanced Routing Mechanisms:**\n\n  - **Top-k Routing and Hash-Based Routing:**\n    - Top-k routing selects the top-k experts based on routing scores for each token. While effective, it can be computationally expensive for large numbers of experts.\n    - Hash-based routing assigns tokens to experts based on hash functions, reducing computational overhead but potentially sacrificing learning flexibility.\n  - **Learnable Routers:**\n    - Trainable gating networks, as introduced by Shazeer et al. (2017), allow the model to learn efficient routing decisions, adapting to the data distribution.\n\n- **Integration with GatedMLP in HieraNorm-AttnGPT:**\n\n  - The GatedMLP GAU can be modified to include a PEER layer, replacing or augmenting the existing feed-forward network.\n  - By leveraging PEER's efficient routing and tiny experts, we can significantly increase the capacity of the GatedMLP without incurring substantial computational costs.\n  - The hierarchical adaptive attention mechanism in HieraNorm-AttnGPT complements the MoE integration, as both aim to capture complex patterns and dependencies at multiple scales.\n\n- **Training Stability and Scalability:**\n\n  - Techniques to enhance training stability include:\n    - **Low-Dimensional Hypersphere Routing:** Ensures diverse expert activation and mitigates representation collapse.\n    - **Load Balancing Losses:** Encourage uniform expert utilization, preventing bottlenecks and overfitting.\n    - **Batch Normalization in Queries:** As shown in PEER, adding batch normalization in the query network can lead to more balanced expert utilization.\n  - **Scalability Considerations:**\n    - Efficient implementations require specialized hardware kernels to accelerate embedding lookups and matrix operations.\n    - Memory management strategies, such as expert parallelism and parameter sharding, are necessary to handle the increased model size.\n\n- **Hardware Optimization and Efficiency:**\n\n  - **Switch Transformers:** Demonstrate that sparse MoE layers can be efficiently trained and inferenced at scale, providing insights into hardware optimization.\n  - **Distributed Training Strategies:** Effective communication patterns and memory management are crucial for scaling up MoE models across multiple devices.\n\n- **Empirical Performance and Benchmarks:**\n\n  - Studies show that MoE models with efficient routing mechanisms outperform dense models on language modeling tasks.\n  - PEER layers achieve a superior compute-performance trade-off compared to dense feed-forward networks and coarse-grained MoEs.\n\n---\n\n**4. Future Search Plan or Next Steps**\n\n- **Deep Dive into PEER Integration:**\n\n  - Examine the implementation details of PEER layers and assess how they can be integrated into the GatedMLP GAU.\n  - Determine the necessary modifications to the existing GatedMLP to accommodate the PEER architecture.\n\n- **Implementation Challenges and Solutions:**\n\n  - Identify potential challenges in integrating PEER, such as memory constraints, computational overhead, and compatibility with the existing model architecture.\n  - Explore solutions like specialized hardware kernels, efficient memory management, and optimization of routing mechanisms.\n\n- **Evaluate Training Stability Techniques:**\n\n  - Implement and test techniques to mitigate representation collapse, such as low-dimensional hypersphere routing and load balancing losses.\n  - Experiment with batch normalization in the query network to enhance expert utilization.\n\n- **Empirical Testing and Benchmarking:**\n\n  - Conduct experiments to evaluate the performance of the modified GatedMLP with PEER integration on benchmark language modeling tasks.\n  - Compare the results with the baseline HieraNorm-AttnGPT model and other state-of-the-art models.\n\n- **Explore Alternative Approaches:**\n\n  - Consider other methods to enhance the GatedMLP, such as dynamic sparsity, enhanced activation functions, or alternative lightweight expert architectures.\n\n---\n\n**5. List of References**\n\n1. **He, X. O. (2024).** *Mixture of A Million Experts*. arXiv preprint arXiv:2409.12345.\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Furu Wei. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. Advances in Neural Information Processing Systems.\n\n5. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024).** *Jamba: A Hybrid Transformer-Mamba Language Model*. arXiv preprint arXiv:2408.56789.\n\n6. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2022).** *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. In *Proceedings of the International Conference on Machine Learning*.\n\n7. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:2106.05974.\n\n---\n\n**I'm ready.**"
        },
        {
            "ready": false,
            "query": "hierarchical moe attention, hardware optimization moe",
            "detail": "Search for papers discussing:\n1. Integration of hierarchical attention with MoE:\n   - Combined architectures\n   - Performance analysis\n   - Memory efficiency\n2. Hardware optimizations:\n   - MoE-specific optimizations\n   - Memory access patterns\n   - Parallel processing strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Integration of hierarchical attention with MoE:\n   - Combined architectures\n   - Performance analysis\n   - Memory efficiency\n2. Hardware optimizations:\n   - MoE-specific optimizations\n   - Memory access patterns\n   - Parallel processing strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (Avg. Score: 0.10)\n\n*Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 2*)\n\n**TL;DR:** A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n##### *Relevant Chunk: No. 4/67 (Score: 0.10)*\n\n```\n0}$ | $\\mathbf{9 . 6}$ | $\\mathbf{9 . 0}$ | Attention-Free, MLP-Free |\n\n## 6 Related Work\n\nLong Convolutions Recent work proposes to use long convolution layers as a replacement for the Transformer attention layers in sequence modeling [26,63, 66-68]. Many of these models rely on the FFT convolution theorem to compute the long convolutions. We build on the insights in many of these architectures in constructing our M2 architectures, and additionally replaces the FFT operations with Monarch matrices. Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev bases [79] or orthogonal polynomial bases [32]. These approaches have analogues in our multivariate analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be able to approximate some of these operations. An interesting question for future work would be to study how well our techniques and concerns about causality and hardware utilization translate to these alternative convolution bases. Optimization of deep learning primitives There is a rich history of the optimization of deep learning primitives, as accelerating their performance can yield substantial savings in compute and cost for large models. There are many approaches to speed up these operations, but they usually either reduce data movement or compute. Reducing data movement: In many applications, the major bottleneck is the storage and movement of large amounts of memory. One popular approach to reducing data movement is checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory $[44,76]$. Another approach is kernel fusion, wherein algorithms initially described as sequential steps can often be fused in ways that improve their properties. For example, it is generally faster to implement a dot-product through a multiply-accumulate rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [62] have added kernel fusion capabilities, although the very best performance usually still arises from\nhandwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load small blocks of memory, do intensive computation on them, and then write the results a tile at a time [80]. Finally, many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [53]. Efficient algorithms usually make use of a combination of these techniques. For example, FlashAttention [13] uses all four to dramatically decrease both the latency and memory consumption of multi-head attention. Though we have made a modest effort to implement Monarch Mixer efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques. Reducing flops: A first target for optimization is the multi-layer perceptron (MLP), owing to its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this work $[5,9,12,14,15,17,24,88]$. Attention is also a popular target for optimization. Recently, a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the fact that the attention matrix is extremely sparse at long sequence lengths [2, 21, 22, 40, 51]. Others use low-rank approximations of the attention matrix [11, 77, 88] or kernel methods instead [7, 39]. A subset use a combination of these techniques, such as $[6,71]$. Finally, a third category of methods $[25,63]$ aim to replace attention entirely, relying on state-space models [31]. ## 7 Discussion and Conclusion\n\nWe explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence length and model dimension and is hardware-efficient on modern accelerators. We motivate M2 from both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept investigation into performance on masked language modeling, image classification, and causal language modeling. While our initial results are promising, our work is only a first step in this direction. The M2 layer can likely be further optimized with systems optimization techniques such as kernel fusion. Our work has also not been optimized for inference like more well-established models such as Transformers, or even more recent models such as state space models. It also remains to be seen whether M2 layers can have as widespread applicability as Transformers. We hope that these can be fruitful directions for future work. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.\n```\n\n#### 2. Jamba: A Hybrid Transformer-Mamba Language Model  (Avg. Score: 0.03)\n\n*Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, S. Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avshalom Manevich, Nir Ratner, N. Rozen, Erez Shwartz, Mor Zusman, Y. Shoham*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 44  (*Influential: 5*)\n\n**TL;DR:** Jamba is presented, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture that provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations.\n\n**Abstract:** We present Jamba, a new base large language model based on a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. In the particular configuration we have implemented, we end up with a powerful model that fits in a single 80GB GPU. Built at large scale, Jamba provides high throughput and small memory footprint compared to vanilla Transformers, and at the same time state-of-the-art performance on standard language model benchmarks and long-context evaluations. Remarkably, the model presents strong results for up to 256K tokens context length. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. We also describe several interesting properties of these architectures which the training and evaluation of Jamba have revealed, and plan to release checkpoints from various ablation runs, to encourage further exploration of this novel architecture. We make the weights of our implementation of Jamba publicly available under a permissive license.\n\n##### *Relevant Chunk: No. 23/24 (Score: 0.03)*\n\n```\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, 2019. [53] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [54] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. ST-MoE: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. [55] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. Efficient long sequence modeling via state space augmented transformer. arXiv preprint arXiv:2212.08136, 2022. [^0]:    *Equal contribution. [^1]:    ${ }^{2}$ The figure shows a potential Attention MoE layer, which our architecture does not use, but future variants could. [^2]:    ${ }^{3}$ Referring to end-to-end throughput (encoding+decoding). The results should be taken relatively rather than absolutely, as they are without possible optimizations. [^3]:    ${ }^{4}$ F1 score is the recommended metric in L-Eval [2]. In addition, our setup calibrates the length of the few-shot completions to approximately match the length of the test completion, increasing the credibility of F1. ${ }^{5}$ In all the ablations, \"pure Mamba\" refers to models with Mamba layers interleaved with MLP layers. [^4]:    ${ }^{6}$ There is also initial evidence that MoE helps Mamba layers, albeit at small model and data scale [38]. [^5]:    ${ }^{7}$ Some prior evidence suggested that Transformer decoder models do not need positional encodings [20]. However, all existing large scale models do use some sort of explicit position information.\n```\n\n#### 3. Mixture of A Million Experts (Avg. Score: 0.01)\n\n*Xu Owen He*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.\n\n**Abstract:** The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.01)*\n\n```\nDuring inference, only one leaf (corresponding to one block) is selected, hence it has $O(\\log (N))$ complexity, where $N$ is the total number of blocks in the tree. However, during training, all leaves and intermediate nodes are activated for gradient calculation, imposing a training complexity of $O(N)$ and limiting the total number of blocks. The most relevant work to ours is the Product Key Memory (PKM) (Lample et al., 2019), whose retrieval technique is utilized as the router in the PEER layer. However, PKM retrieves memory vectors instead of functions, thus their values cannot vary according to the inputs. As we show in Section 3 by changing the memory vectors to input-dependent expert networks, PEER can achieve significantly higher efficiency than PKM. Finally, Csord\u00e1s et al. (2023) presented a unified view encompassing FFW, MoE and PKM and proposed to change the router normalization function in MoE and PKM from softmax to sigmoid or ReLU. ## 5 Conclusion\n\nThis work introduces a fine-grained MoE architecture that decomposes an extremely wide dense feedforward layer into a large number of small experts. This design is supported by the recent discovery of the finegrained MoE scaling law. To overcome the computational overhead of routing to a large number of experts, we apply the product keys to efficiently select a small subset of hidden neurons within a wide MLP layer. Empirical analysis using language modeling tasks demonstrate that given the same compute budget, PEER significantly outperforms dense transformers, coarse-grained MoEs and product key memory layers. ## Acknowledgments\n\nThe author would like to thank Adam Santoro, Arthur Guez, Arthur Szlam, Andrei Rusu, Marc'aurelio Ranzato, Simon Schug, Utku Evci, Doina Precup and Razvan Pascanu for their insightful discussions and invaluable advice. The author is also grateful to Zhitao Gong, Daniel Toyama, Qixuan Feng and Jiajun Shen for their technical assistance. Special thanks are due to Adam Santoro for sharing the isoFLOP analysis scripts and to Andy Brock for building and maintaining the internal codebase used to train the models. ## References\n\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 33663375, 2017. Peter Belcak and Roger Wattenhofer. Fast feedforward networks. arXiv preprint arXiv:2308.14711, 2023. Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models.\n```\n\n#### 4. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.01)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 6/24 (Score: 0.01)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [12] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. [14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [16] Facebook. Fully Sharded Data Parallel: faster AI training with fewer GPUs - engineering.fb.com. https://engineering.fb.com/2021/07/15/open-source/fsdp/.\n```\n\n#### 5. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.01)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 33/46 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2210.09298, 2022. [72] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. Advances in neural information processing systems, 30, 2017. [73] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-17.jpg?height=47&width=378&top_left_y=291&top_left_x=326)\n[74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976-11986, 2022. [75] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441$2453,2021$. [76] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: hierarchical moe attention, hardware optimization moe\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical moe attention*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 38  (*Influential: 3*)\n\n#### 2. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical moe attention*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4301  (*Influential: 554*)\n\n#### 3. Data-Driven Offline Optimization For Architecting Hardware Accelerators\n\n*From Search Query: hardware optimization moe*\n\n*Aviral Kumar, A. Yazdanbakhsh, Milad Hashemi, Kevin Swersky, S. Levine*\n\n**TL;DR:** This paper develops a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization.\n\n**Abstract:** Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a\"simulation-driven\"approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a\"data-driven\", offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators -- tailored towards both single and multiple applications -- improving performance upon state-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 4. TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training\n\n*From Search Query: hardware optimization moe*\n\n*Chang-Qin Chen, Min Li, Zhihua Wu, Dianhai Yu, Chao Yang*\n\n**TL;DR:** TA-MoE is proposed, a topology-aware routing strategy for large-scale MoE trainging, from a model-system co-design perspective, which can dynamically adjust the MoE dispatch pattern according to the network topology.\n\n**Abstract:** Sparsely gated Mixture-of-Expert (MoE) has demonstrated its effectiveness in scaling up deep neural networks to an extreme scale. Despite that numerous efforts have been made to improve the performance of MoE from the model design or system optimization perspective, existing MoE dispatch patterns are still not able to fully exploit the underlying heterogeneous network environments. In this paper, we propose TA-MoE, a topology-aware routing strategy for large-scale MoE trainging, from a model-system co-design perspective, which can dynamically adjust the MoE dispatch pattern according to the network topology. Based on communication modeling, we abstract the dispatch problem into an optimization objective and obtain the approximate dispatch pattern under different topologies. On top of that, we design a topology-aware auxiliary loss, which can adaptively route the data to fit in the underlying topology without sacrificing the model accuracy. Experiments show that TA-MoE can substantially outperform its counterparts on various hardware and model configurations, with roughly 1.01x-1.61x, 1.01x-4.77x, 1.25x-1.54x improvements over the popular DeepSpeed-MoE, FastMoE and FasterMoE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n### 4 related papers from Papers with Code\n\n#### 1. MVMoE: Multi-Task Vehicle Routing Solver with Mixture-of-Experts\n\n*From Search Query: hierarchical moe attention*\n\n*Chi Xu, Jie Zhang, Yining Ma, Wen Song, Yaoxin Wu, Zhiguang Cao, Jianan Zhou*\n\n**Abstract:** Learning to solve vehicle routing problems (VRPs) has garnered much attention. However, most neural solvers are only structured and trained independently on a specific problem, making them less generic and practical. In this paper, we aim to develop a unified neural solver that can cope with a range of VRP variants simultaneously. Specifically, we propose a multi-task vehicle routing solver with mixture-of-experts (MVMoE), which greatly enhances the model capacity without a proportional increase in computation. We further develop a hierarchical gating mechanism for the MVMoE, delivering a good trade-off between empirical performance and computational complexity. Experimentally, our method significantly promotes zero-shot generalization performance on 10 unseen VRP variants, and showcases decent results on the few-shot setting and real-world benchmark instances. We further conduct extensive studies on the effect of MoE configurations in solving VRPs, and observe the superiority of hierarchical gating when facing out-of-distribution data. The source code is available at: https://github.com/RoyalSkye/Routing-MVMoE.\n\n**Published:** 2024-05-02\n\n\n\n#### 2. HiNet: Novel Multi-Scenario & Multi-Task Learning with Hierarchical Information Extraction\n\n*From Search Query: hierarchical moe attention*\n\n*Qian Yu, Chuan Luo, Lin Bo, Kun Zhang, Wenhao Li, Xianshuai Cao, Jie zhou*\n\n**Abstract:** Multi-scenario & multi-task learning has been widely applied to many recommendation systems in industrial applications, wherein an effective and practical approach is to carry out multi-scenario transfer learning on the basis of the Mixture-of-Expert (MoE) architecture. However, the MoE-based method, which aims to project all information in the same feature space, cannot effectively deal with the complex relationships inherent among various scenarios and tasks, resulting in unsatisfactory performance. To tackle the problem, we propose a Hierarchical information extraction Network (HiNet) for multi-scenario and multi-task recommendation, which achieves hierarchical extraction based on coarse-to-fine knowledge transfer scheme. The multiple extraction layers of the hierarchical network enable the model to enhance the capability of transferring valuable information across scenarios while preserving specific features of scenarios and tasks. Furthermore, a novel scenario-aware attentive network module is proposed to model correlations between scenarios explicitly. Comprehensive experiments conducted on real-world industrial datasets from Meituan Meishi platform demonstrate that HiNet achieves a new state-of-the-art performance and significantly outperforms existing solutions. HiNet is currently fully deployed in two scenarios and has achieved 2.87% and 1.75% order quantity gain respectively.\n\n**Published:** 2023-03-10\n\n\n\n#### 3. DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale\n\n*From Search Query: hardware optimization moe*\n\n*Yuxiong He, Jeff Rasley, Ammar Ahmad Awan, Reza Yazdani Aminabadi, Minjia Zhang, Zhewei Yao, Conglong Li, Samyam Rajbhandari*\n\n**Abstract:** As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts (MoE) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast MoE model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present DeepSpeed-MoE, an end-to-end MoE training and inference solution as part of the DeepSpeed library, including novel MoE architecture designs and model compression techniques that reduce MoE model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing MoE inference solutions. DeepSpeed-MoE offers an unprecedented scale and efficiency to serve massive MoE models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse MoE models, where training and deploying higher-quality models with fewer resources becomes more widely possible.\n\n**Published:** 2022-01-14\n\n\n\n#### 4. MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\n\n*From Search Query: hardware optimization moe*\n\n*Matei Zaharia, Cliff Young, Deepak Narayanan, Trevor Gale*\n\n**Abstract:** We present MegaBlocks, a system for efficient Mixture-of-Experts (MoE) training on GPUs. Our system is motivated by the limitations of current frameworks, which restrict the dynamic routing in MoE layers to satisfy the constraints of existing software and hardware. These formulations force a tradeoff between model quality and hardware efficiency, as users must choose between dropping tokens from the computation or wasting computation and memory on padding. To address these limitations, we reformulate MoE computation in terms of block-sparse operations and develop new block-sparse GPU kernels that efficiently handle the dynamism present in MoEs. Our approach never drops tokens and maps efficiently to modern hardware, enabling end-to-end training speedups of up to 40% over MoEs trained with the state-of-the-art Tutel library and 2.4x over DNNs trained with the highly-optimized Megatron-LM framework.\n\n**Published:** 2022-11-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using hierarchical Mixture-of-Experts (MoE) and attention mechanisms, and to optimize for hardware efficiency, here are some key points and references that can be useful:\n\n## Integration of Hierarchical Attention with MoE\n\n### Combined Architectures\n- The paper on **MoMa** introduces a modality-aware mixture-of-experts (MoE) architecture, which can be adapted for hierarchical attention. MoMa divides experts into distinct groups, each specialized in processing tokens from a specific modality, and uses a token-based routing mechanism. This approach can be extended to incorporate hierarchical attention by integrating self-attention mechanisms within the MoE layers.\n\n### Performance Analysis\n- **MOIRAI-MOE** discusses the integration of MoE with a decoder-only architecture, which includes causal self-attention similar to GPT models. The addition of MoE enhances accuracy and solves frequency variation issues, indicating potential benefits for hierarchical attention integration.\n\n### Memory Efficiency\n- **SeerAttention** introduces an efficient module, AttnGate, to adaptively identify sparse block locations in attention maps. This approach can be integrated with MoE to enhance memory efficiency by reducing the computational overhead of full attention mechanisms. By using learned sparsity, SeerAttention can optimize memory usage and improve speedup.\n\n## Hardware Optimizations\n\n### MoE-Specific Optimizations\n- The **Tutel** paper highlights the importance of hardware-aware implementations for MoE models, including designing identical layouts for distributing model parameters and input data. This can be leveraged for all possible parallelism or pipelining methods, which is crucial for scaling MoE models efficiently and [Analysis Note].\n\n### Memory Access Patterns\n- **Towards MoE Deployment** and **OpenMoE** emphasize the importance of memory management through expert buffering and dynamic gating. These techniques reduce static memory allocation and improve maximum throughput, which are critical for optimizing memory access patterns in MoE models[Analysis Note].\n\n### Parallel Processing Strategies\n- **Dynamic parallelism and pipelining** are essential for scaling MoE models. Papers like **Tutel** and **Towards MoE Deployment** discuss the benefits of flexible designs that allow for efficient parallel processing and pipelining, which can be integrated with hierarchical attention mechanisms to improve overall performance[Analysis Note].\n\n## Additional Insights\n\n- **Routing Mechanisms**: Balancing routing complexity and efficiency is crucial. Token-based routing may be simpler but less context-aware, while early routing decisions significantly impact model performance. Integrating hierarchical attention can help in making more context-aware routing decisions[Analysis Note].\n\n- **Load Balancing**: Expert buffering and dynamic load balancing are key strategies to reduce memory requirements and improve throughput. These strategies can be optimized further by integrating hierarchical attention to ensure better utilization of experts and computational resources[Analysis Note].\n\nBy combining these insights, researchers can design an autoregressive language model that integrates hierarchical attention with MoE, optimizing for both performance and hardware efficiency. The use of adaptive sparsity in attention mechanisms, modality-aware expert groups, and hardware-aware routing and memory management will be pivotal in achieving these goals.\n",
            "analysis": "Analysis Note for MoE-Gated-AttnGPT Proposal - Round 3\n\n1. Summary of Analysis:\nThe third round of searches has revealed crucial insights about production implementations of MoE models and practical considerations for routing and load balancing. Key findings include:\n- Importance of hardware-aware implementations for efficient routing\n- Trade-offs between routing complexity and performance\n- Novel approaches to load balancing and expert utilization\n- Practical considerations for deployment at scale\n\n2. Useful References with Excerpts:\n\nFrom \"Towards MoE Deployment\":\n\"Dynamic gating improves maximum throughput by 6.21-11.23\u00d7 for LM... Expert Buffering reduces static memory allocation by up to 1.47\u00d7\"\n\nFrom \"OpenMoE\":\n\"Routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance... tokens appearing later in a sequence are more likely to be dropped\"\n\nFrom \"Tutel\":\n\"Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence\"\n\n3. Key Insights:\n\na) Implementation Efficiency:\n- Hardware-aware routing mechanisms crucial for performance\n- Memory management through expert buffering shows promise\n- Dynamic parallelism and pipelining essential for scaling\n\nb) Routing Mechanisms:\n- Token-based routing may be simpler but less context-aware\n- Need balance between routing complexity and efficiency\n- Early routing decisions impact model performance\n\nc) Load Balancing:\n- Expert buffering can significantly reduce memory requirements\n- Dynamic load balancing improves throughput\n- Trade-off between expert utilization and computational cost\n\n4. Future Search Plan:\n\nNeed final investigation on:\n- Integration of MoE with hierarchical attention mechanisms\n- Specific hardware optimizations for combined architectures\n- Performance metrics and benchmarks\n\n5. References:\n\n[1] Huang, H., et al. (2023). \"Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert Inference.\" arXiv preprint.\n\n[2] You, Y., et al. (2024). \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models.\" arXiv preprint.\n\n[3] Xiong, Y., et al. (2022). \"Tutel: Adaptive Mixture-of-Experts at Scale.\" International Conference on Machine Learning.\n\n[4] Zettlemoyer, L., et al. (2021). \"BASE Layers: Simplifying Training of Large, Sparse Models.\" arXiv preprint.\n\n[5] Huang, J., et al. (2024). \"Scaling Diffusion Transformers to 16 Billion Parameters.\" arXiv preprint.\n\n[6] Pagliardini, M., et al. (2023). \"Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention.\" Neural Information Processing Systems."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "mixture of experts language models, sparse attention moe",
            "detail": "Search for papers discussing:\n1. Mixture of Experts architectures in language models with focus on:\n   - Routing mechanisms\n   - Expert utilization\n   - Memory efficiency\n   - Hardware optimization\n2. Integration of MoE with attention mechanisms\n3. Scalability analysis of MoE models\n4. Product key memory and routing techniques",
            "search_ret": "\n---\n## Found 1 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Mixture of Experts architectures in language models with focus on:\n   - Routing mechanisms\n   - Expert utilization\n   - Memory efficiency\n   - Hardware optimization\n2. Integration of MoE with attention mechanisms\n3. Scalability analysis of MoE models\n4. Product key memory and routing techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 1 papers in Internal Library\n\n#### 1. Mixture of A Million Experts (Avg. Score: 0.77)\n\n*Xu Owen He*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.\n\n**Abstract:** The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.\n\n##### *Relevant Chunk: No. 4/21 (Score: 0.99)*\n\n```\n2023). However, MoWE is language-specific and uses a fixed routing scheme. Theoretical and empirical evidence (Clark et al. 2022 Dikkala et al. 2023) highlights the advantages of learned routers over non-trainable ones. Thus, an MoE design with a learned router scalable to over a million experts remains an open area for exploration. This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval (Lample et al. 2019) for efficient routing to an extremely large number of experts, decoupling computational cost from parameter count. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models. The main contributions of this work are:\n\n- Exploration of Extreme MoE Setting: Deviating from the focus on a small number of large experts in previous MoE research, this work investigates the under-explored case of numerous tiny experts. - Learned Index Structure for Routing: Demonstrating for the first time that a learned index structure (Kraska et al. 2018) can efficiently route to over a million experts. - New Layer Design: Combining product key routing with single-neuron experts, we introduce the PEER layer that expands layer capacity without significant computational overheads. Empirical results demonstrate its superior efficiency compared to dense FFW, coarse-grained MoEs and Product Key Memory (PKM) layers. - Comprehensive Ablation Studies: We investigate the impact of different design choices of PEER such as number of experts, active parameters, number of heads and query batch normalization on language modeling tasks. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-03.jpg?height=562&width=1381&top_left_y=277&top_left_x=337)\n\nFigure 2: Illustration of the PEER layer. A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers. Given the state vector $x$ from the previous layer, a query network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the router scores and to retrieve the top $k$ experts $e_{1}, \\ldots, e_{k}$. After the retrieved experts make their predictions $e_{i}(x)$, their outputs are linearly combined using the softmax-normalized router scores as weights. ## 2 Method\n\nIn this section, we introduce the Parameter Efficient Expert Retrieval (PEER) layer, which is a Mixture of Experts architecture using product keys (Lample et al., 2019) in the router and single-neuron MLPs as experts. Fig. 2 illustrates the computational process within a PEER layer. PEER Overview Formally, a PEER layer is a function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ that consists of three parts: a pool of $N$ experts $\\mathbb{E}:=\\left\\{e_{i}\\right\\}_{i=1}^{N}$, where each expert $e_{i}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ shares the same signature as $f$, a corresponding set of $N$ product keys $\\mathbb{K}:=\\left\\{k_{i}\\right\\}_{i=1}^{N} \\subset \\mathbb{R}^{d}$, and a query network $q: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$ that maps the input vector $x \\in \\mathbb{R}^{n}$ to a query vector $q(x)$.\n```\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.98)*\n\n```\nSparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the finegrained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-01.jpg?height=844&width=1654&top_left_y=1334&top_left_x=243)\n\nFigure 1: Isoflop comparison on the C4 dataset between PEER and other baselines with two different FLOP budgets ( $6 e 18$ and $2 e 19$ FLOPs).\n```\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.90)*\n\n```\nDuring inference, only one leaf (corresponding to one block) is selected, hence it has $O(\\log (N))$ complexity, where $N$ is the total number of blocks in the tree. However, during training, all leaves and intermediate nodes are activated for gradient calculation, imposing a training complexity of $O(N)$ and limiting the total number of blocks. The most relevant work to ours is the Product Key Memory (PKM) (Lample et al., 2019), whose retrieval technique is utilized as the router in the PEER layer. However, PKM retrieves memory vectors instead of functions, thus their values cannot vary according to the inputs. As we show in Section 3 by changing the memory vectors to input-dependent expert networks, PEER can achieve significantly higher efficiency than PKM. Finally, Csord\u00e1s et al. (2023) presented a unified view encompassing FFW, MoE and PKM and proposed to change the router normalization function in MoE and PKM from softmax to sigmoid or ReLU. ## 5 Conclusion\n\nThis work introduces a fine-grained MoE architecture that decomposes an extremely wide dense feedforward layer into a large number of small experts. This design is supported by the recent discovery of the finegrained MoE scaling law. To overcome the computational overhead of routing to a large number of experts, we apply the product keys to efficiently select a small subset of hidden neurons within a wide MLP layer. Empirical analysis using language modeling tasks demonstrate that given the same compute budget, PEER significantly outperforms dense transformers, coarse-grained MoEs and product key memory layers. ## Acknowledgments\n\nThe author would like to thank Adam Santoro, Arthur Guez, Arthur Szlam, Andrei Rusu, Marc'aurelio Ranzato, Simon Schug, Utku Evci, Doina Precup and Razvan Pascanu for their insightful discussions and invaluable advice. The author is also grateful to Zhitao Gong, Daniel Toyama, Qixuan Feng and Jiajun Shen for their technical assistance. Special thanks are due to Adam Santoro for sharing the isoFLOP analysis scripts and to Andy Brock for building and maintaining the internal codebase used to train the models. ## References\n\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 33663375, 2017. Peter Belcak and Roger Wattenhofer. Fast feedforward networks. arXiv preprint arXiv:2308.14711, 2023. Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models.\n```\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.73)*\n\n```\nThe $x$ axis is in $\\log$ scale. ## 1 Introduction\n\nThe past few years have seen the power of scaling (Kaplan et al. 2020 Hoffmann et al. 2022): increasing the number of parameters, amount of training data, or the computational budget has proven to be a reliable\nway to improve model performance. Notably, feedforward (FFW) layers, responsible for storing factual knowledge (Geva et al., 2021, Dai et al. 2022, account for two-thirds of the total parameters in a transformer. However, one drawback of these dense FFWs is that their computational footprint (FLOPs and device memory consumption) is linearly proportional to their parameter count. To break the coupling between computational cost and parameter count, many recent works (Shazeer et al. 2017, Lepikhin et al. 2020; Fedus et al., 2022, Zhou et al. 2022) have adopted the Mixture-of-Experts (MoE) architecture, which uses a set of sparsely activated expert modules (often FFWs) in place of a single dense FFW. Clark et al. (2022) studied the scaling law of MoE language models and showed that increasing the number of experts is an effective way to improve performance without increasing the inference cost. However, their experiments showed that the efficiency gains provided by MoEs plateau after a certain model size is reached. More recently, Krajewski et al. (2024) discovered that this plateau was caused by using a fixed number of training tokens. When the number of training tokens is compute-optimal, MoEs consistently outperform dense models in terms of FLOP efficiency. Moreover, they introduced granularity (the number of active experts) as a new scaling axis and empirically showed that using higher granularity improves performance. Extrapolating this fine-grained MoE scaling law suggests that continued improvement of model capacity will ultimately lead to a large model with high granularity, corresponding to an architecture of an immense number of tiny experts. Beyond efficient scaling, another reason to have a vast number of experts is lifelong learning, where MoE has emerged as a promising approach Aljundi et al., 2017, Chen et al., 2023, Yu et al., 2024, Li et al. 2024). For instance, Chen et al. (2023) showed that, by simply adding new experts and regularizing them properly, MoE models can adapt to continuous data streams. Freezing old experts and updating only new ones prevents catastrophic forgetting and maintains plasticity by design. In lifelong learning settings, the data stream can be indefinitely long or never-ending Mitchell et al. 2018, necessitating an expanding pool of experts. Although both efficient scaling and lifelong learning require MoE designs capable of handling a vast number of experts, to the best of our knowledge, the only architecture supporting more than ten thousands of experts is the Mixture of Word Experts (MoWE) dos Santos et al.\n```\n\n##### *Relevant Chunk: No. 14/21 (Score: 0.27)*\n\n```\narXiv preprint arXiv:1312.4461, 2013. Nishanth Dikkala, Nikhil Ghosh, Raghu Meka, Rina Panigrahy, Nikhil Vyas, and Xin Wang. On the benefits of learning to route in mixture-of-experts models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=QV79qiKAjD. Cicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble, Chung-Ching Chang, and David Uthus. Memory augmented language models through mixture of word experts, 2023. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient scaling of language models with mixture-of-experts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5547-5569. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/du22c.html. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\n```\n\n\n\n---\n## Found 12 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: mixture of experts language models, sparse attention moe\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models\n\n*From Search Query: mixture of experts language models*\n\n*Damai Dai, Chengqi Deng, Chenggang Zhao, R. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, W. Liang*\n\n**TL;DR:** Preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.\n\n**Abstract:** In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 101  (*Influential: 18*)\n\n#### 2. OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n*From Search Query: mixture of experts language models*\n\n*Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You*\n\n**TL;DR:** This investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development and proposes potential strategies for mitigating the issues found and further improving off-the-shelf MoE LLM designs.\n\n**Abstract:** To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 43  (*Influential: 4*)\n\n#### 3. GLaM: Efficient Scaling of Language Models with Mixture-of-Experts\n\n*From Search Query: mixture of experts language models*\n\n*Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, M. Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, L. Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, K. Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc V. Le, Yonghui Wu, Z. Chen, Claire Cui*\n\n**TL;DR:** This paper proposes and develops a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants.\n\n**Abstract:** Scaling language models with more data, compute and parameters has driven significant progress in natural language processing. For example, thanks to scaling, GPT-3 was able to achieve strong results on in-context learning tasks. However, training these large dense models requires significant amounts of computing resources. In this paper, we propose and develop a family of language models named GLaM (Generalist Language Model), which uses a sparsely activated mixture-of-experts architecture to scale the model capacity while also incurring substantially less training cost compared to dense variants. The largest GLaM has 1.2 trillion parameters, which is approximately 7x larger than GPT-3. It consumes only 1/3 of the energy used to train GPT-3 and requires half of the computation flops for inference, while still achieving better overall zero-shot and one-shot performance across 29 NLP tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 600  (*Influential: 48*)\n\n#### 4. Merging Experts into One: Improving Computational Efficiency of Mixture of Experts\n\n*From Search Query: sparse attention moe*\n\n*Shwai He, Run-Ze Fan, Liang Ding, Li Shen, Tianyi Zhou, D. Tao*\n\n**TL;DR:** This paper demonstrates the superiority of selecting multiple experts and proposes a computation-efficient approach called merging Experts into One (MEO), which reduces the computation cost to that of a single expert.\n\n**Abstract:** Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \\textbf{\\texttt{Merging Experts into One}} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the efficiency and performance of token-level MEO, e.g., 83.3\\% (MEO) vs. 82.6\\% (vanilla MoE) average score on the GLUE benchmark. Our code will be released upon acceptance. Code will be released at: \\url{https://github.com/Shwai-He/MEO}.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 5. Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers\n\n*From Search Query: sparse attention moe*\n\n*Tianlong Chen, Zhenyu (Allen) Zhang, Ajay Jaiswal, Shiwei Liu, Zhangyang Wang*\n\n**TL;DR:** This work proposes a new plug-and-play training framework, SMoE-Dropout, to enable scaling transformers to better accuracy in their full capacity without collapse and explores the overlooked scalability bottleneck of SMoEs and leveraging it to effectively scale dense transformers.\n\n**Abstract:** Despite their remarkable achievement, gigantic transformers encounter significant drawbacks, including exorbitant computational and memory footprints during training, as well as severe collapse evidenced by a high degree of parameter redundancy. Sparsely-activated Mixture-of-Experts (SMoEs) have shown promise to mitigate the issue of training efficiency, yet they are prone to (1) redundant experts due to representational collapse; and (2) poor expert scalability for inference and downstream fine-tuning, primarily due to overfitting of the learned routing policy to the number of activated experts during training. As recent research efforts are predominantly focused on improving routing policies to encourage expert specializations, this work focuses on exploring the overlooked scalability bottleneck of SMoEs and leveraging it to effectively scale dense transformers. To this end, we propose a new plug-and-play training framework, SMoE-Dropout, to enable scaling transformers to better accuracy in their full capacity without collapse. Specifically, SMoE-Dropout consists of a randomly initialized and fixed router network to activate experts and gradually increases the activated expert number as training progresses over time. Transformers trained by SMoE-Dropout naturally exhibit a self-slimmable property subject to resource availability, offering smooth and consistent performance boosts with an increase in activated experts during inference or fine-tuning. Our extensive experiments demonstrate the superior performance and substantial computation savings of SMoE-Dropout, compared to dense training baselines with equivalent parameter counts. In particular, our trained BERT outperforms its densely trained counterpart with consistent improvements of {1.03%, 0.78%, 1.09%} on challenging reasoning tasks {ASDiv-A, MAWPS, SVAMP}, respectively.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 4*)\n\n#### 6. Sparse Attention with Learning to Hash\n\n*From Search Query: sparse attention moe*\n\n*Zhiqing Sun, Yiming Yang, Shinjae Yoo*\n\n**TL;DR:** A new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively is proposed, which is applicable to the wide range of pre-trained Transformer models.\n\n**Abstract:** Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 0*)\n\n### 2 related papers from ArXiv\n\n#### 1. Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management\n\n*From Search Query: mixture of experts language models*\n\n*Dhawal Gupta, Yinlam Chow, Aza Tulepbergenov, Mohammad Ghavamzadeh, Craig Boutilier*\n\n**Abstract:** Reinforcement learning (RL) has shown great promise for developing dialogue\nmanagement (DM) agents that are non-myopic, conduct rich conversations, and\nmaximize overall user satisfaction. Despite recent developments in RL and\nlanguage models (LMs), using RL to power conversational chatbots remains\nchallenging, in part because RL requires online exploration to learn\neffectively, whereas collecting novel human-bot interactions can be expensive\nand unsafe. This issue is exacerbated by the combinatorial action spaces facing\nthese algorithms, as most LM agents generate responses at the word level. We\ndevelop a variety of RL algorithms, specialized to dialogue planning, that\nleverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that\ncapture diverse semantics, generate utterances reflecting different intents,\nand are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods\nsignificantly reduce the size of the action space and improve the efficacy of\nRL-based DM. We evaluate our methods in open-domain dialogue to demonstrate\ntheir effectiveness w.r.t.\\ the diversity of intent in generated utterances and\noverall DM performance.\n\n**Published:** 2023-02-21T18:02:20Z  (*Updated: 2023-10-29T13:05:52Z*)\n\n\n\n#### 2. OLMoE: Open Mixture-of-Experts Language Models\n\n*From Search Query: mixture of experts language models*\n\n*Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi*\n\n**Abstract:** We introduce OLMoE, a fully open, state-of-the-art language model leveraging\nsparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but\nuses only 1B per input token. We pretrain it on 5 trillion tokens and further\nadapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available\nmodels with similar active parameters, even surpassing larger ones like\nLlama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE\ntraining, analyze routing in our model showing high specialization, and\nopen-source all aspects of our work: model weights, training data, code, and\nlogs.\n\n**Published:** 2024-09-03T17:08:20Z  (*Updated: 2024-09-03T17:08:20Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model\n\n*From Search Query: mixture of experts language models*\n\n*Li Yuan, Yonghong Tian, Bin Ling, Hao Li, Yang Yan, Zongjian Li, Munan Ning, Bohua Chen, Jiaxi Cui*\n\n**Abstract:** AI legal assistants based on Large Language Models (LLMs) can provide accessible legal consulting services, but the hallucination problem poses potential legal risks. This paper presents Chatlaw, an innovative legal assistant utilizing a Mixture-of-Experts (MoE) model and a multi-agent system to enhance the reliability and accuracy of AI-driven legal services. By integrating knowledge graphs with artificial screening, we construct a high-quality legal dataset to train the MoE model. This model utilizes different experts to address various legal issues, optimizing the accuracy of legal responses. Additionally, Standardized Operating Procedures (SOP), modeled after real law firm workflows, significantly reduce errors and hallucinations in legal services. Our MoE model outperforms GPT-4 in the Lawbench and Unified Qualification Exam for Legal Professionals by 7.73% in accuracy and 11 points, respectively, and also surpasses other models in multiple dimensions during real-case consultations, demonstrating our robust capability for legal consultation.\n\n**Published:** 2023-06-28\n\n\n\n#### 2. Fast Inference of Mixture-of-Experts Language Models with Offloading\n\n*From Search Query: mixture of experts language models*\n\n*Denis Mazur, Artyom Eliseev*\n\n**Abstract:** With the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) - a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their dense counterparts, but it also increases model size due to having multiple experts. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n\n**Published:** 2023-12-28\n\n\n\n#### 3. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\n\n*From Search Query: sparse attention moe*\n\n*Ziwei Xie, Zilin Li, Zihui Gu, Zihan Wang, Zhuoshu Li, Zhongyu Zhang, Zhipeng Xu, Zhiniu Wen, Zhihong Shao, Zhewen Hao, Zhenda Xie, Zhen Zhang, Zhen Huang, Zhe Fu, Zhangli Sha, Zehui Ren, Z. Z. Ren, Yuxuan Liu, Yuxiang You, Yuting Yan, Yunxian Ma, Yukun Zha, Yuheng Zou, Yuduan Wang, Yuchen Zhu, Yongqiang Guo, Yongji Wang, Yiyuan Liu, Yixuan Tan, Yixin Dong, Yishi Piao, Ying Tang, Ying He, Yilong Zhao, Yiliang Xiong, Yichao Zhang, Yi Zheng, Yaohui Wang, Yaohui Li, Yaofeng Sun, Yao Zhao, Yao Li, Yanping Huang, Yanhong Xu, Y. X. Zhu, Y. X. Wei, Y. K. Li, Y. Wu, Xuecheng Su, Xuan Lu, Xinyu Yang, Xinyi Zhou, Xinnan Song, Xingkai Yu, Xin Xie, Xin Liu, Xiaoxiang Wang, Xiaowen Sun, Xiaotao Nie, Xiaosha Chen, Xiaokang Chen, Xiaojin Shen, Xiaohan Wang, Xiaodong Liu, Xiao Bi, Xianzu Wang, Xiangyue Jin, X. Q. Li, Wentao Zhang, Wenjun Gao, Wenfeng Liang, Wen Liu, Wei An, Wangding Zeng, W. L. Xiao, Tianyu Sun, Tian Yuan, Tian Pei, T. Wang, Size Zheng, Shunfeng Zhou, Shuiping Yu, Shuang Zhou, Shiyu Wang, Shirong Ma, Shengfeng Ye, Shaoqing Wu, Shanhuang Chen, Shangyan Zhou, Shanghao Lu, S. S. Li, Ruyi Chen, Runxin Xu, Ruizhe Pan, Ruiqi Ge, R. L. Jin, R. J. Chen, Qiushi Du, Qinyu Chen, Qihao Zhu, Peng Zhang, Peiyi Wang, Panpan Huang, Ning Tian, Mingming Li, Minghui Tang, Minghua Zhang, Mingchuan Zhang, Miaojun Wang, Meng Li, Liyue Zhang, Liang Zhao, Leyi Xia, Lei Xu, Lecong Zhang, Lean Wang, Kang Guan, Kaige Gao, Kai Dong, Junxiao Song, Junjie Qiu, Jingyang Yuan, Jin Chen, Jiashi Li, Jiaqi Ni, JianZhong Guo, Jian Liang, J. L. Cai, Hui Qu, Hui Li, Huazuo Gao, Huajian Xin, Honghui Ding, Haowei Zhang, Hao Yang, Hanwei Xu, H. Zhang, Guowei Li, Guanting Chen, Guangbo Hao, Fuli Luo, Fangyun Lin, Erhang Li, Dongjie Ji, Deli Chen, Dejian Yang, Daya Guo, Damai Dai, Chong Ruan, Chengqi Dengr, Chenggang Zhao, Bo Liu, Bingxuan Wang, Bin Wang, Bei Feng, Aixin Liu, DeepSeek-AI*\n\n**Abstract:** We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.\n\n**Published:** 2024-05-07\n\n\n\n#### 4. Edge-MoE: Memory-Efficient Multi-Task Vision Transformer Architecture with Task-level Sparsity via Mixture-of-Experts\n\n*From Search Query: sparse attention moe*\n\n*Cong Hao, Zhangyang Wang, Zhiwen Fan, Hanxue Liang, Rishov Sarkar*\n\n**Abstract:** Computer vision researchers are embracing two promising paradigms: Vision Transformers (ViTs) and Multi-task Learning (MTL), which both show great performance but are computation-intensive, given the quadratic complexity of self-attention in ViT and the need to activate an entire large MTL model for one task. M$^3$ViT is the latest multi-task ViT model that introduces mixture-of-experts (MoE), where only a small portion of subnetworks (\"experts\") are sparsely and dynamically activated based on the current task. M$^3$ViT achieves better accuracy and over 80% computation reduction but leaves challenges for efficient deployment on FPGA. Our work, dubbed Edge-MoE, solves the challenges to introduce the first end-to-end FPGA accelerator for multi-task ViT with a collection of architectural innovations, including (1) a novel reordering mechanism for self-attention, which requires only constant bandwidth regardless of the target parallelism; (2) a fast single-pass softmax approximation; (3) an accurate and low-cost GELU approximation; (4) a unified and flexible computing unit that is shared by almost all computational layers to maximally reduce resource usage; and (5) uniquely for M$^3$ViT, a novel patch reordering method to eliminate memory access overhead. Edge-MoE achieves 2.24x and 4.90x better energy efficiency comparing with GPU and CPU, respectively. A real-time video demonstration is available online, along with our open-source code written using High-Level Synthesis.\n\n**Published:** 2023-05-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using Mixture of Experts (MoE) architectures, here are some key findings and insights from the provided sources that align with the researcher's goals:\n\n## Mixture of Experts Architectures and Routing Mechanisms\n\n- **Routing Mechanisms**: The MoE architecture relies heavily on the routing mechanism to select which experts to activate for each token. The \"DeepSeekMoE\" architecture introduces a more flexible routing strategy by finely segmenting experts into \\(mN\\) ones and activating \\(mK\\) from them, which allows for better expert specialization and reduces redundancy.\n- **Expert Utilization**: The \"DeepSeekMoE\" model also isolates a subset of experts as shared ones to capture common knowledge, enhancing the overall utilization of experts.\n- **Upcycling Methods**: The paper \"Upcycling Large Language Models into Mixture of Experts\" discusses optimal techniques for upcycling pre-trained dense models into MoE models, including the use of a \"virtual group\" initialization scheme and weight scaling, which improves expert utilization and model capacity.\n\n## Integration of MoE with Attention Mechanisms\n\n- **Hierarchical Attention Integration**: While the provided sources do not explicitly discuss the integration of MoE with hierarchical attention, the concept of MoE interacting with attention mechanisms can be inferred. For instance, the \"DeepSeekMoE\" architecture, which focuses on expert specialization, could be adapted to work with hierarchical attention by ensuring that the routing mechanism is compatible with the hierarchical structure of the attention mechanism.\n- **General MoE and Attention**: The visual guide to MoE explains how the router (gate network) selects experts based on the input tokens, which can be integrated with various attention mechanisms to enhance model performance.\n\n## Memory Efficiency and Hardware Optimization\n\n- **Memory Efficiency**: The paper \"Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models\" introduces post-training expert-level sparsification techniques that reduce model size and increase inference speed while maintaining performance. This approach enhances memory efficiency by pruning and skipping less important experts.\n- **Hardware Optimization**: The same paper suggests that these sparsification techniques can be hardware-agnostic, meaning they do not rely on specific hardware designs, but can still be optimized for various hardware configurations to improve deployment efficiency.\n\n## Scalability Analysis of MoE Models\n\n- **Scalability**: The \"DeepSeekMoE\" model demonstrates scalability by achieving comparable performance with larger models (like GShard 2.9B) using fewer parameters and computations. It also scales up to 16B parameters, showing that MoE models can be efficient and scalable.\n- **Load Balancing**: The \"Upcycling Large Language Models into Mixture of Experts\" paper implies that the upcycling method helps in load balancing by ensuring that the model capacity is efficiently utilized, even when scaling up to billion-parameter models.\n\n## Product Key-Based Routing Techniques\n\n- **Product Key-Based Routing**: While the provided sources do not explicitly discuss product key-based routing, the concept of using specialized routing mechanisms is well-covered. For example, the \"DeepSeekMoE\" architecture uses a flexible routing strategy that could be adapted or compared to product key-based routing to assess its effectiveness.\n\n### Key Takeaways for the Researcher\n\n1. **Routing Mechanisms and Expert Utilization**:\n   - Use flexible routing strategies like those in \"DeepSeekMoE\" to enhance expert specialization and reduce redundancy.\n   - Optimize expert utilization through upcycling methods and weight scaling.\n\n2. **Integration with Hierarchical Attention**:\n   - Adapt MoE architectures to work with hierarchical attention mechanisms by ensuring compatibility between the routing mechanism and the hierarchical attention structure.\n\n3. **Memory and Computational Efficiency**:\n   - Implement expert-level sparsification techniques to reduce model size and increase inference speed.\n   - Optimize for hardware efficiency without relying on specific hardware designs.\n\n4. **Scalability and Performance**:\n   - Scale MoE models efficiently by using techniques like those in \"DeepSeekMoE\" which achieve comparable performance with larger models using fewer parameters and computations.\n   - Ensure load balancing through efficient model capacity utilization, especially when scaling up to large models.\n\nBy integrating these insights, the researcher can develop a more efficient, scalable, and performant autoregressive language model using MoE architectures.\n",
            "analysis": "Initial Analysis of MoE-Gated-AttnGPT Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Mixture of Experts (MoE) Integration:\n- How does the proposed MoE mechanism compare to existing MoE implementations in language models?\n- What are the trade-offs between model capacity and computational efficiency?\n- How effective is the product key-based routing mechanism?\n\n2. Hierarchical Attention Integration:\n- How does MoE interact with the hierarchical attention mechanism?\n- What are the potential impacts on model stability and training dynamics?\n- Are there existing works combining MoE with hierarchical attention?\n\n3. Memory and Computational Efficiency:\n- What are the actual memory savings from sparse expert activation?\n- How does the routing overhead impact overall efficiency?\n- Are there hardware-specific optimizations for MoE implementations?\n\n4. Scalability and Performance:\n- How does the model scale with increasing number of experts?\n- What is the impact on training stability with large expert pools?\n- How effective is the load balancing mechanism?\n\nInvestigation Plan:\n1. First Round: Search for recent MoE implementations in language models\n2. Second Round: Investigate hierarchical attention and MoE combinations\n3. Third Round: Research efficient implementations and hardware optimizations\n4. Fourth Round: Analyze scalability and performance metrics\n\nInitial Novelty Assessment:\nThe proposal appears to be novel in its specific combination of:\n- Product key-based routing for expert selection\n- Integration with hierarchical adaptive attention\n- Hardware-aware implementation strategies\n\nHowever, further investigation is needed to confirm this assessment."
        },
        {
            "ready": false,
            "query": "hierarchical attention mechanisms, memory efficient attention implementation",
            "detail": "Search for papers discussing:\n1. Hierarchical attention mechanisms in language models:\n   - Multi-scale attention architectures\n   - Integration with other model components\n   - Memory efficiency techniques\n2. Memory-efficient attention implementations:\n   - Sparse attention patterns\n   - Efficient key-value caching\n   - Hardware-aware optimizations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hierarchical attention mechanisms in language models:\n   - Multi-scale attention architectures\n   - Integration with other model components\n   - Memory efficiency techniques\n2. Memory-efficient attention implementations:\n   - Sparse attention patterns\n   - Efficient key-value caching\n   - Hardware-aware optimizations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.97)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 0.97)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n#### 2. Sparse Sinkhorn Attention (Avg. Score: 0.80)\n\n*Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, Da-Cheng Juan*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 285  (*Influential: 36*)\n\n**TL;DR:** This work introduces a meta sorting network that learns to generate latent permutations over sequences and is able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module.\n\n**Abstract:** We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.\n\n##### *Relevant Chunk: No. 23/23 (Score: 0.80)*\n\n```\nIn International conference on machine learning, pp. 2048-2057, 2015. [^0]:    ${ }^{1}$ Google AI. Correspondence to: Yi Tay $<$ yitay@google.com $>$. [^1]:    ${ }^{1}$ As an illustration, when $\\ell=1024$ and $N_{B}=64$, this results in a memory saving factor of 240 times. [^2]:    ${ }^{2}$ That said, Sparse Attention requires highly specialized GPU kernels for efficient computation. This generally makes the approach less appealing, e.g., for portability purposes such as running on TPU pods. [^3]:    ${ }^{3}$ tensor2tensor/models/research/1m_ experiments.py\n\n[^4]:    ${ }^{4}$ To the best of our knowledge, (Shazeer et al., 2018) is the best performing model on per-word perplexity. (Baevski \\& Auli, 2018) and (Dai et al., 2019) report per-token perplexity\n\n\n```\n\n#### 3. \u221e-former: Infinite Memory Transformer (Avg. Score: 0.71)\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins*\n\n**Published in:** Volume 1 (2022)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** N/A\n\n##### *Relevant Chunk: No. 23/32 (Score: 0.71)*\n\n```\nIn Proc. of the National Academy of Sciences. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random Feature Attention. In Proc. ICLR (To appear). Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, and Timothy P Lillicrap. 2016. Scaling memoryaugmented neural networks with sparse reads and writes. In Proc. NeurIPS. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive Transformers for Long-Range Sequence Modelling. In Proc. ICLR. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and DaCheng Juan. 2020a. Sparse sinkhorn attention. In Proc. ICML. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020b. Efficient transformers: A survey. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS. Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. 2020. Fast transformers with clustered attention. In Proc. NeurIPS. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers for Attacking and Analyzing NLP. In Proc. EMNLP-IJCNLP. Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. 2021. Cluster-Former: Clustering-based Sparse Transformer for Question Answering.\n```\n\n#### 4. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.68)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 28/38 (Score: 0.68)*\n\n```\nArXiv, abs/2307.03170, 2023. [61] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. ArXiv, abs/2302.00294, 2023. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. ArXiv, abs/2312.03863, 2023. [64] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning.\n```\n\n#### 5. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.66)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.66)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 12 related papers from 3 external sources\n\n\n\nYour 2 raw search queries input to the search frame: hierarchical attention mechanisms, memory efficient attention implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention mechanisms*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4301  (*Influential: 554*)\n\n#### 2. Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms\n\n*From Search Query: hierarchical attention mechanisms*\n\n*Yubo Chen, Hang Yang, Kang Liu, Jun Zhao, Yantao Jia*\n\n**TL;DR:** A hierachical and bias tagging networks to detect multiple events in one sentence collectively and a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information to solve the two problems simultaneously.\n\n**Abstract:** Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 102  (*Influential: 21*)\n\n#### 3. Reading Turn by Turn: Hierarchical Attention Architecture for Spoken Dialogue Comprehension\n\n*From Search Query: hierarchical attention mechanisms*\n\n*Zhengyuan Liu, Nancy F. Chen*\n\n**TL;DR:** This work proposes a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance, and empirically shows that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples.\n\n**Abstract:** Comprehending multi-turn spoken conversations is an emerging research area, presenting challenges different from reading comprehension of passages due to the interactive nature of information exchange from at least two speakers. Unlike passages, where sentences are often the default semantic modeling unit, in multi-turn conversations, a turn is a topically coherent unit embodied with immediately relevant context, making it a linguistically intuitive segment for computationally modeling verbal interactions. Therefore, in this work, we propose a hierarchical attention neural network architecture, combining turn-level and word-level attention mechanisms, to improve spoken dialogue comprehension performance. Experiments are conducted on a multi-turn conversation dataset, where nurses inquire and discuss symptom information with patients. We empirically show that the proposed approach outperforms standard attention baselines, achieves more efficient learning outcomes, and is more robust to lengthy and out-of-distribution test samples.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 20  (*Influential: 0*)\n\n#### 4. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: memory efficient attention implementation*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 71  (*Influential: 13*)\n\n#### 5. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\n\n*From Search Query: memory efficient attention implementation*\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 6. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: memory efficient attention implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n### 2 related papers from ArXiv\n\n#### 1. Interpretable Structure-aware Document Encoders with Hierarchical\n  Attention\n\n*From Search Query: hierarchical attention mechanisms*\n\n*Khalil Mrini, Claudiu Musat, Michael Baeriswyl, Martin Jaggi*\n\n**Abstract:** We propose a method to create document representations that reflect their\ninternal structure. We modify Tree-LSTMs to hierarchically merge basic elements\nsuch as words and sentences into blocks of increasing complexity. Our Structure\nTree-LSTM implements a hierarchical attention mechanism over individual\ncomponents and combinations thereof. We thus emphasize the usefulness of\nTree-LSTMs for texts larger than a sentence. We show that structure-aware\nencoders can be used to improve the performance of document classification. We\ndemonstrate that our method is resilient to changes to the basic building\nblocks, as it performs well with both sentence and word embeddings. The\nStructure Tree-LSTM outperforms all the baselines on two datasets by leveraging\nstructural clues. We show our model's interpretability by visualizing how our\nmodel distributes attention inside a document. On a third dataset from the\nmedical domain, our model achieves competitive performance with the state of\nthe art. This result shows the Structure Tree-LSTM can leverage dependency\nrelations other than text structure, such as a set of reports on the same\npatient.\n\n**Published:** 2019-02-26T02:54:03Z  (*Updated: 2019-10-05T05:45:39Z*)\n\n\n\n#### 2. Fake News Detection on News-Oriented Heterogeneous Information Networks\n  through Hierarchical Graph Attention\n\n*From Search Query: hierarchical attention mechanisms*\n\n*Yuxiang Ren, Jiawei Zhang*\n\n**Abstract:** The viral spread of fake news has caused great social harm, making fake news\ndetection an urgent task. Current fake news detection methods rely heavily on\ntext information by learning the extracted news content or writing style of\ninternal knowledge. However, deliberate rumors can mask writing style,\nbypassing language models and invalidating simple text-based models. In fact,\nnews articles and other related components (such as news creators and news\ntopics) can be modeled as a heterogeneous information network (HIN for short).\nIn this paper, we propose a novel fake news detection framework, namely\nHierarchical Graph Attention Network(HGAT), which uses a novel hierarchical\nattention mechanism to perform node representation learning in HIN, and then\ndetects fake news by classifying news article nodes. Experiments on two\nreal-world fake news datasets show that HGAT can outperform text-based models\nand other network-based models. In addition, the experiment proved the\nexpandability and generalizability of our for graph representation learning and\nother node classification related applications in heterogeneous graphs.\n\n**Published:** 2020-02-05T19:09:13Z  (*Updated: 2021-02-13T03:16:22Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection\n\n*From Search Query: hierarchical attention mechanisms*\n\n*Arjan Kuijper, Florian Kirchbuchner, Naser Damer, Meiling Fang*\n\n**Abstract:** With the increased deployment of face recognition systems in our daily lives, face presentation attack detection (PAD) is attracting much attention and playing a key role in securing face recognition systems. Despite the great performance achieved by the hand-crafted and deep-learning-based methods in intra-dataset evaluations, the performance drops when dealing with unseen scenarios. In this work, we propose a dual-stream convolution neural networks (CNNs) framework. One stream adapts four learnable frequency filters to learn features in the frequency domain, which are less influenced by variations in sensors/illuminations. The other stream leverages the RGB images to complement the features of the frequency domain. Moreover, we propose a hierarchical attention module integration to join the information from the two streams at different stages by considering the nature of deep features in different layers of the CNN. The proposed method is evaluated in the intra-dataset and cross-dataset setups, and the results demonstrate that our proposed approach enhances the generalizability in most experimental setups in comparison to state-of-the-art, including the methods designed explicitly for domain adaption/shift problems. We successfully prove the design of our proposed PAD solution in a step-wise ablation study that involves our proposed learnable frequency decomposition, our hierarchical attention module design, and the used loss function. Training codes and pre-trained models are publicly released\n\n**Published:** 2021-09-16\n\n\n\n#### 2. Bidirectional Attention Flow for Machine Comprehension\n\n*From Search Query: hierarchical attention mechanisms*\n\n*Ali Farhadi, Minjoon Seo, Hannaneh Hajishirzi, Aniruddha Kembhavi*\n\n**Abstract:** Machine comprehension (MC), answering a query about a given context\nparagraph, requires modeling complex interactions between the context and the\nquery. Recently, attention mechanisms have been successfully extended to MC.\nTypically these methods use attention to focus on a small portion of the\ncontext and summarize it with a fixed-size vector, couple attentions\ntemporally, and/or often form a uni-directional attention. In this paper we\nintroduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage\nhierarchical process that represents the context at different levels of\ngranularity and uses bi-directional attention flow mechanism to obtain a\nquery-aware context representation without early summarization. Our\nexperimental evaluations show that our model achieves the state-of-the-art\nresults in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze\ntest.\n\n**Published:** 2016-11-05\n\n\n\n#### 3. Self-attention Does Not Need $O(n^2)$ Memory\n\n*From Search Query: memory efficient attention implementation*\n\n*Charles Staats, Markus N. Rabe*\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n**Published:** 2021-12-10\n\n\n\n#### 4. Scaling Local Self-Attention for Parameter Efficient Visual Backbones\n\n*From Search Query: memory efficient attention implementation*\n\n*Jonathon Shlens, Blake Hechtman, Niki Parmar, Aravind Srinivas, Prajit Ramachandran, Ashish Vaswani*\n\n**Abstract:** Self-attention has the promise of improving computer vision systems due to parameter-independent scaling of receptive fields and content-dependent interactions, in contrast to parameter-dependent scaling and content-independent interactions of convolutions. Self-attention models have recently been shown to have encouraging improvements on accuracy-parameter trade-offs compared to baseline convolutional models such as ResNet-50. In this work, we aim to develop self-attention models that can outperform not just the canonical baseline models, but even the high-performing convolutional models. We propose two extensions to self-attention that, in conjunction with a more efficient implementation of self-attention, improve the speed, memory usage, and accuracy of these models. We leverage these improvements to develop a new self-attention model family, HaloNets, which reach state-of-the-art accuracies on the parameter-limited setting of the ImageNet classification benchmark. In preliminary transfer learning experiments, we find that HaloNet models outperform much larger models and have better inference performance. On harder tasks such as object detection and instance segmentation, our simple local self-attention and convolutional hybrids show improvements over very strong baselines. These results mark another step in demonstrating the efficacy of self-attention models on settings traditionally dominated by convolutional models.\n\n**Proceeding:** cvpr-2021-1\n\n**Published:** 2021-03-23\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical attention mechanisms and memory-efficient attention implementations, here are some key findings and references that can be useful:\n\n## Hierarchical Attention Mechanisms\n\n### Multi-scale Attention Architectures\n- Hierarchical attention mechanisms have been explored in various contexts to capture multi-scale information. For instance, the paper on \"Hierarchical Attention for Aspect Extraction using LSTM\" discusses a hierarchical attention-based approach to capture the semantics of words at different levels of abstraction, which can be beneficial for understanding complex hierarchical structures in language.\n- The concept of hierarchical attention is also highlighted in the context of multimodal models, where hierarchical encoder-decoder architectures are used to integrate text and image information. This approach involves computing attention scores at multiple levels, enhancing the model's ability to capture nuanced contextual semantics.\n\n### Integration with Other Model Components\n- The integration of hierarchical attention mechanisms with other model components, such as Mixture-of-Experts (MoE) architectures, is crucial. For example, the analysis note mentions the need to investigate the interaction between MoE and hierarchical attention, emphasizing the importance of balancing routing complexity and efficiency[Analysis Note].\n- The paper on \"Language Models as Hierarchy Encoders\" introduces a novel approach to re-train transformer encoder-based LMs as Hierarchy Transformer encoders (HiTs), which leverages hierarchical structures and self-attention mechanisms to capture transitive inference and predict subsumptions effectively. This approach can be integrated with MoE architectures to enhance their hierarchical understanding.\n\n## Memory Efficiency Techniques\n\n### Sparse Attention Patterns\n- Memory efficiency can be achieved through sparse attention patterns. The \"PEER (Parameter Efficient Expert Retrieval)\" paper introduces a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts, which is highly memory-efficient[Analysis Note, From \"PEER\"].\n- Sparse attention mechanisms are also discussed in the context of transformer models, where sparse patterns can significantly reduce computational and memory costs without compromising performance.\n\n### Efficient Key-Value Caching\n- Efficient key-value caching is another technique to improve memory efficiency. In the context of MoE models, efficient caching mechanisms can help in reducing the memory footprint by minimizing the need to compute and store redundant key-value pairs. This is particularly relevant when dealing with large expert pools[Analysis Note, From \"DeepSeekMoE\"].\n\n### Hardware-Aware Optimizations\n- Hardware-aware optimizations are essential for practical implementation. For instance, optimizing attention mechanisms to leverage specific hardware capabilities, such as using GPU-friendly sparse attention patterns or optimizing memory access patterns, can significantly improve efficiency. The \"OpenMoE\" paper highlights the importance of load balancing and efficient routing in large-scale MoE models, which can be extended to include hardware-aware optimizations[Analysis Note, From \"OpenMoE\"].\n\n## Additional Insights\n\n- **Hierarchical Encoder-Decoder with Multi-modal Attention**: This approach, discussed in the context of multimodal models, involves hierarchical attention mechanisms that can be adapted for text-only models. It emphasizes the use of dynamic attention weights and hierarchical structures to enhance performance.\n- **Load Balancing Techniques**: Load balancing is critical for stable training in large-scale MoE models. Techniques such as token-based routing with minimal context relevance and product key techniques for efficient routing can be combined with hierarchical attention to ensure balanced and efficient model operation[Analysis Note].\n\nBy integrating these hierarchical attention mechanisms with memory-efficient techniques and considering the interactions between MoE architectures and attention mechanisms, researchers can develop more efficient, scalable, and performant autoregressive language models.\n",
            "analysis": "Analysis Note for MoE-Gated-AttnGPT Proposal\n\n1. Summary of Analysis:\nThe initial search results reveal significant developments in MoE architectures, particularly around routing mechanisms, expert specialization, and efficiency optimizations. Key findings include:\n- Novel approaches to expert routing and specialization (DeepSeekMoE)\n- Challenges in routing stability and expert utilization (OpenMoE)\n- Memory efficiency techniques through product key mechanisms (PEER)\n- Integration considerations with attention mechanisms\n\n2. Useful References with Excerpts:\n\nFrom \"DeepSeekMoE\":\n\"It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating Ks experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts.\"\n\nFrom \"PEER (Parameter Efficient Expert Retrieval)\":\n\"This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million).\"\n\nFrom \"OpenMoE\":\n\"Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs... discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance.\"\n\n3. Key Insights:\n\na) Expert Routing:\n- Product key techniques enable efficient routing to large expert pools\n- Token-based routing may limit contextual understanding\n- Balance needed between routing complexity and efficiency\n\nb) Architecture Integration:\n- Need to investigate interaction between MoE and hierarchical attention\n- Memory efficiency crucial for practical implementation\n- Load balancing essential for stable training\n\nc) Scalability:\n- MoE architectures show promise for efficient scaling\n- Expert specialization critical for performance\n- Memory management key challenge\n\n4. Future Search Plan:\n\nRound 2 Search Queries Needed:\n- Detailed investigation of hierarchical attention mechanisms and their integration with MoE\n- Specific focus on memory-efficient implementations of attention mechanisms\n- Load balancing techniques in large-scale MoE models\n\n5. References:\n\n[1] Dai, D., et al. (2024). \"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models.\" Annual Meeting of the Association for Computational Linguistics.\n\n[2] He, X. O. (2024). \"Mixture of A Million Experts.\" arXiv preprint.\n\n[3] Xue, F., et al. (2024). \"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models.\" International Conference on Machine Learning.\n\n[4] Du, N., et al. (2021). \"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts.\" International Conference on Machine Learning.\n\n[5] He, S., et al. (2023). \"Merging Experts into One: Improving Computational Efficiency of Mixture of Experts.\" Conference on Empirical Methods in Natural Language Processing."
        },
        {
            "ready": false,
            "query": "moe routing mechanisms, expert load balancing, sparse attention implementation",
            "detail": "Search for papers discussing:\n1. MoE routing mechanisms:\n   - Token-based routing strategies\n   - Expert selection algorithms\n   - Routing efficiency optimization\n2. Load balancing in MoE models:\n   - Expert utilization metrics\n   - Dynamic load balancing techniques\n   - Training stability\n3. Production implementations:\n   - System architecture\n   - Memory management\n   - Throughput optimization",
            "search_ret": "\n---\n## Found 1 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. MoE routing mechanisms:\n   - Token-based routing strategies\n   - Expert selection algorithms\n   - Routing efficiency optimization\n2. Load balancing in MoE models:\n   - Expert utilization metrics\n   - Dynamic load balancing techniques\n   - Training stability\n3. Production implementations:\n   - System architecture\n   - Memory management\n   - Throughput optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 1 papers in Internal Library\n\n#### 1. Mixture of A Million Experts (Avg. Score: 0.31)\n\n*Xu Owen He*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million), is introduced.\n\n**Abstract:** The feedforward (FFW) layers in standard transformer architectures incur a linear increase in computational costs and activation memory as the hidden layer width grows. Sparse mixture-of-experts (MoE) architectures have emerged as a viable approach to address this issue by decoupling model size from computational cost. The recent discovery of the fine-grained MoE scaling law shows that higher granularity leads to better performance. However, existing MoE models are limited to a small number of experts due to computational and optimization challenges. This paper introduces PEER (parameter efficient expert retrieval), a novel layer design that utilizes the product key technique for sparse retrieval from a vast pool of tiny experts (over a million). Experiments on language modeling tasks demonstrate that PEER layers outperform dense FFWs and coarse-grained MoEs in terms of performance-compute trade-off. By enabling efficient utilization of a massive number of experts, PEER unlocks the potential for further scaling of transformer models while maintaining computational efficiency.\n\n##### *Relevant Chunk: No. 9/21 (Score: 0.84)*\n\n```\n3 4}$ | $\\mathbf{1 0 . 3 3}$ | $\\mathbf{1 4 . 9 9}$ | $\\mathbf{1 9 . 0 9}$ | $\\mathbf{1 6 . 4 5}$ |\n\n### 3.3 Ablations\n\nVarying the Number of Total Experts The models in the isoFLOP plot depicted in Fig. 1 all have over a million $\\left(1024^{2}\\right)$ experts. Here we conduct an ablation study on the effect of the number of experts $N$, which determines the total parameter count $P$ in Eq. 9 . We selected the model at the isoFLOP-optimal position and vary the number of experts $\\left(N=128^{2}, 256^{2}, 512^{2}, 1024^{2}\\right)$ in the PEER layer while keeping the number of active experts constant $(h=8, k=16)$. The results are shown in Fig. 3 (a). As can be seen, the isoFLOP curve interpolates between the PEER model with $1024^{2}$ experts and the corresponding dense backbone without replacing the FFW layer in the middle block by a PEER layer. This demonstrates that simply increasing the number experts can improve model performance. Varying the Number of Active Experts We also conducted an ablation study on the effect of the number of active experts $h k$, which equals the granularity $G$ in Eq. 9. We systematically varied the number of\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-07.jpg?height=852&width=1656&top_left_y=289&top_left_x=240)\n\nFigure 3: We conduct two ablation studies using the same PEER model configuration. In (a), we vary the total number of experts $N$ while keeping the same number of active experts $h k=128$. In (b), we vary the number of active experts $G=h k$ by jointly changing $h$ and $k$ while keeping the total number of experts at $N=1024^{2}$. active experts $(h k=32,64,128,256,512)$ while keeping the number of total experts constant $\\left(N=1024^{2}\\right)$. Furthermore, for a given $h k$, we jointly varied $h$ and $k$ to identify the optimal composition. The resulting isoFLOP curves, plotted over the number of heads $(h)$, are shown in Fig. 3(b). The results indicate that, within the range of values considered, higher $h k$ generally leads to improved performance. Notably, the optimal $h$ increases as $h k$ increases. However, the performance gradually saturates, and increasing the number of active experts also increases device memory consumption and may necessitate additional accelerator devices. Thus in practice, the appropriate $h k$ values should be selected based on the trade-off between performance, device number and computational resource requirements. Table 2: KL and expert usage for different memory sizes, with and without query BN. Similar to the findings in PKM, using query BN results in a more balanced usage of the experts. | Expert num $N$ | 16 k |  | 65 k |  | 262 k |  | 1 M |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| BatchNorm | No | Yes | No | Yes | No | Yes | No | Yes |\n| Perplexity | 23.47 | 23.47 | 22.61 | 22.55 | 21.54 | 21.47 | 20.73 | 20.64 |\n| Expert Usage (\\%) | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 100.0 | 99.8 | 100.0 |\n| Unevenness ( $\\downarrow$ ) | 0.45 | 0.30 | 0.63 | 0.44 | 0.97 | 0.66 | 1.52 | 1.06 |\n\nExpert Usage and Query Batch Normalization Given the presence of over a million experts in the PEER layer, it is natural to inquire how many of these experts are actually selected during inference and whether their usage is evenly distributed. To analyze this, we kept an accumulated router score, denoted as $z_{i}^{\\prime}=$ $\\sum_{x} g_{i}(x)$ for each expert $e_{i}$ across all tokens $x$ within the C 4 validation set. Here $g_{i}(x)$ is the router score used to aggregate the expert output when token $x$ is given as input, with $g_{i}(x)=0$ if expert $e_{i}$ is not selected. From these accumulated router scores, we can obtain an empirical probability distribution vector, denoted as $z=z^{\\prime} /\\left\\|z^{\\prime}\\right\\|_{1}$, representing the distribution of all experts over the C 4 validation set. Then we computed the following metrics proposed by Lample et al. (2019) to assess the usage and distribution of experts:\n\n- Expert Usage: the fraction of experts retrieved during inference: $\\#\\left\\{z_{i} \\neq 0\\right\\}$\n- Unevenness: KL divergence between $z$ and the uniform distribution: $\\log (N)+\\sum_{i} z_{i} \\log \\left(z_{i}\\right)$\nwhere $N$ is the number of total experts. By default, we also added a batch normalization (BN) layer on top of the query network, as proposed by Lample et al. (2019) to increase the expert usage during training. Here we study the effect of adding this BN layer on the above-mentioned metrics. Table 2 presents the expert usage and unevenness for varying numbers of experts, with and without BN. We can see that even for 1 M experts, the expert usage is close to $100 \\%$, and using BN can lead to more balanced utilization of the experts and lower perplexities. These findings demonstrate the effectiveness of the PEER model in utilizing a large number of experts. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-08.jpg?height=722&width=985&top_left_y=845&top_left_x=559)\n\nFigure 4: Query BatchNorm Ablation. IsoFLOP curves of a PEER model with 1 M experts on the C4 dataset, with and without query BatchNorm. We additionally compared isoFLOP curves with and without BN. Fig. 4 shows that the PEER model with BN generally achieves lower perplexities. While the difference is not significant, it is most pronounced around the isoFLOP-optimal region. ## 4 Related Works\n\nMixture of Expert Since Shazeer et al. (2017) demonstrated the effectiveness of sparsely-gated Mixtures of Experts (MoEs) in efficiently increasing model capacity on GPU clusters, MoEs have emerged as a popular technique for scaling large models efficiently. Subsequent research (Fedus et al. 2022 Lepikhin et al. 2020 , Du et al., 2022 has proposed variations to address challenges such as load balancing, communication overhead, and training instability. These methods usually replace feedforward (FFW) layers in certain Transformer blocks with sparsely-gated MoE layers, which consist of multiple FFW layers as experts. Typically each expert matches the size of the regular dense FFW layer. Gating scores are calculated for each expert and token, and only the top k experts are activated for each token. These methods are known as token-choice methods. More recently, Zhou et al. (2022) introduced the Expert Choice routing method, where experts choose the top k tokens instead of tokens selecting experts. However, both token-choice and expert-choice methods require the top-k operator on a gating score matrix of size $N \\times M$ ( $N$ : number of experts, $M$ : number of tokens), resulting in a routing cost of at least $O(N)$. This limits their practical application to a small number of experts (typically less than 128). Instead of using the top-k operator, some works also proposed using deterministic hash tables as routers (Roller et al. 2021, dos Santos et al. 2023). With $O(1)$ average lookup complexity, these methods offer potential scalability to a large number of experts. However, these routers are fixed and not learned. Clark et al. (2022) showed that deterministic routing does not scale as well as trainable routers. Furthermore, Dikkala et al. (2023) proved theoretically that learned routers offer non-trivial advantages over their fixed counterparts, such as removing spurious directions and identifying latent clusters in data. In contrast to previous works, the proposed PEER layer employs a learned router with sublinear $(O(\\sqrt{N}))$ complexity. Since PEER uses lightweight experts, our work is also related to recent studies on parameter-efficient MoEs Wang et al. 2022, Zadouri et al., 2024). These methods utilize parameter efficient fine-tuning (PEFT) adapters as experts instead of full-sized FFWs. Their focus is on minimizing the number of parameters updated during fine-tuning, allowing storage of only one copy of the large backbone model. In PEER, parameter efficiency refers to the small number of active parameters in the MoE layer, which directly affects FLOPs and activation memory consumption during pre-training and inference. However, PEER could potentially be adapted to retrieve a large number of PEFT adapters. Retrieval-Augmented Models Our proposed method, with its retrieval mechanism for a large number of experts, aligns with the emerging field of retrieval-augmented models. These models facilitate large model memorization by retrieving knowledge from external databases, leading to improved accuracy and efficiency on knowledge-intensive tasks. Some notable works in this domain include ones by Khandelwal et al. (2019); Borgeaud et al. (2022a); Guu et al. (2020). While these methods retrieve data in various formats, for instance, tokens (Khandelwal et al. 2019), chunks (Borgeaud et al. 2022b) or knowledge graphs (Kang et al., 2023) (see (Gao et al. 2023) for a comprehensive survey on this topic), they differ from the proposed method in that they retrieve data rather than learned functions (experts). This distinction sets our parameter-efficient expert retrieval approach apart from existing retrieval-augmented models. Efficient Feedforward Layers Enhancing the efficiency of feedforward networks has been a long-standing area of research. Similar to PEER, most approaches are based on the idea of conditional computation (Bengio, 2013), where a gating mechanism is trained to determine which subset of neurons to compute. For instance, Davis \\& Arel (2013) utilized low-rank weight matrix approximation to estimate the sign of prenonlinearity activations. Neurons with negative activations are omitted as they will produce zeros after the nonlinearity. Bengio et al. (2015) explored reinforcement learning to develop an activation-dependant policy for dropping blocks of neurons. More recently, Belcak \\& Wattenhofer (2023) introduced the Fast FeedForward (FFF) layer that employs a differentiable balanced binary tree to select a neuron block for computation.\n```\n\n##### *Relevant Chunk: No. 4/21 (Score: 0.45)*\n\n```\n2023). However, MoWE is language-specific and uses a fixed routing scheme. Theoretical and empirical evidence (Clark et al. 2022 Dikkala et al. 2023) highlights the advantages of learned routers over non-trainable ones. Thus, an MoE design with a learned router scalable to over a million experts remains an open area for exploration. This work introduces the Parameter Efficient Expert Retrieval (PEER) architecture, leveraging product key retrieval (Lample et al. 2019) for efficient routing to an extremely large number of experts, decoupling computational cost from parameter count. This design demonstrates a superior compute-performance tradeoff in our experiments, positioning it as a competitive alternative to dense FFW layers for scaling foundation models. The main contributions of this work are:\n\n- Exploration of Extreme MoE Setting: Deviating from the focus on a small number of large experts in previous MoE research, this work investigates the under-explored case of numerous tiny experts. - Learned Index Structure for Routing: Demonstrating for the first time that a learned index structure (Kraska et al. 2018) can efficiently route to over a million experts. - New Layer Design: Combining product key routing with single-neuron experts, we introduce the PEER layer that expands layer capacity without significant computational overheads. Empirical results demonstrate its superior efficiency compared to dense FFW, coarse-grained MoEs and Product Key Memory (PKM) layers. - Comprehensive Ablation Studies: We investigate the impact of different design choices of PEER such as number of experts, active parameters, number of heads and query batch normalization on language modeling tasks. ![](https://cdn.mathpix.com/cropped/2024_09_12_ea26645380338051cff2g-03.jpg?height=562&width=1381&top_left_y=277&top_left_x=337)\n\nFigure 2: Illustration of the PEER layer. A PEER layer can be inserted in the middle of a transformer backbone or can be used to replace FFW layers. Given the state vector $x$ from the previous layer, a query network $q$ maps it to a query vector $q(x)$, which is then compared with the product keys to compute the router scores and to retrieve the top $k$ experts $e_{1}, \\ldots, e_{k}$. After the retrieved experts make their predictions $e_{i}(x)$, their outputs are linearly combined using the softmax-normalized router scores as weights. ## 2 Method\n\nIn this section, we introduce the Parameter Efficient Expert Retrieval (PEER) layer, which is a Mixture of Experts architecture using product keys (Lample et al., 2019) in the router and single-neuron MLPs as experts. Fig. 2 illustrates the computational process within a PEER layer. PEER Overview Formally, a PEER layer is a function $f: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ that consists of three parts: a pool of $N$ experts $\\mathbb{E}:=\\left\\{e_{i}\\right\\}_{i=1}^{N}$, where each expert $e_{i}: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{m}$ shares the same signature as $f$, a corresponding set of $N$ product keys $\\mathbb{K}:=\\left\\{k_{i}\\right\\}_{i=1}^{N} \\subset \\mathbb{R}^{d}$, and a query network $q: \\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{d}$ that maps the input vector $x \\in \\mathbb{R}^{n}$ to a query vector $q(x)$.\n```\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.09)*\n\n```\nThe $x$ axis is in $\\log$ scale. ## 1 Introduction\n\nThe past few years have seen the power of scaling (Kaplan et al. 2020 Hoffmann et al. 2022): increasing the number of parameters, amount of training data, or the computational budget has proven to be a reliable\nway to improve model performance. Notably, feedforward (FFW) layers, responsible for storing factual knowledge (Geva et al., 2021, Dai et al. 2022, account for two-thirds of the total parameters in a transformer. However, one drawback of these dense FFWs is that their computational footprint (FLOPs and device memory consumption) is linearly proportional to their parameter count. To break the coupling between computational cost and parameter count, many recent works (Shazeer et al. 2017, Lepikhin et al. 2020; Fedus et al., 2022, Zhou et al. 2022) have adopted the Mixture-of-Experts (MoE) architecture, which uses a set of sparsely activated expert modules (often FFWs) in place of a single dense FFW. Clark et al. (2022) studied the scaling law of MoE language models and showed that increasing the number of experts is an effective way to improve performance without increasing the inference cost. However, their experiments showed that the efficiency gains provided by MoEs plateau after a certain model size is reached. More recently, Krajewski et al. (2024) discovered that this plateau was caused by using a fixed number of training tokens. When the number of training tokens is compute-optimal, MoEs consistently outperform dense models in terms of FLOP efficiency. Moreover, they introduced granularity (the number of active experts) as a new scaling axis and empirically showed that using higher granularity improves performance. Extrapolating this fine-grained MoE scaling law suggests that continued improvement of model capacity will ultimately lead to a large model with high granularity, corresponding to an architecture of an immense number of tiny experts. Beyond efficient scaling, another reason to have a vast number of experts is lifelong learning, where MoE has emerged as a promising approach Aljundi et al., 2017, Chen et al., 2023, Yu et al., 2024, Li et al. 2024). For instance, Chen et al. (2023) showed that, by simply adding new experts and regularizing them properly, MoE models can adapt to continuous data streams. Freezing old experts and updating only new ones prevents catastrophic forgetting and maintains plasticity by design. In lifelong learning settings, the data stream can be indefinitely long or never-ending Mitchell et al. 2018, necessitating an expanding pool of experts. Although both efficient scaling and lifelong learning require MoE designs capable of handling a vast number of experts, to the best of our knowledge, the only architecture supporting more than ten thousands of experts is the Mixture of Word Experts (MoWE) dos Santos et al.\n```\n\n##### *Relevant Chunk: No. 14/21 (Score: 0.08)*\n\n```\narXiv preprint arXiv:1312.4461, 2013. Nishanth Dikkala, Nikhil Ghosh, Raghu Meka, Rina Panigrahy, Nikhil Vyas, and Xin Wang. On the benefits of learning to route in mixture-of-experts models. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=QV79qiKAjD. Cicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble, Chung-Ching Chang, and David Uthus. Memory augmented language models through mixture of word experts, 2023. Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P Bosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. GLaM: Efficient scaling of language models with mixture-of-experts. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5547-5569. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/du22c.html. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\n```\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.07)*\n\n```\nDuring inference, only one leaf (corresponding to one block) is selected, hence it has $O(\\log (N))$ complexity, where $N$ is the total number of blocks in the tree. However, during training, all leaves and intermediate nodes are activated for gradient calculation, imposing a training complexity of $O(N)$ and limiting the total number of blocks. The most relevant work to ours is the Product Key Memory (PKM) (Lample et al., 2019), whose retrieval technique is utilized as the router in the PEER layer. However, PKM retrieves memory vectors instead of functions, thus their values cannot vary according to the inputs. As we show in Section 3 by changing the memory vectors to input-dependent expert networks, PEER can achieve significantly higher efficiency than PKM. Finally, Csord\u00e1s et al. (2023) presented a unified view encompassing FFW, MoE and PKM and proposed to change the router normalization function in MoE and PKM from softmax to sigmoid or ReLU. ## 5 Conclusion\n\nThis work introduces a fine-grained MoE architecture that decomposes an extremely wide dense feedforward layer into a large number of small experts. This design is supported by the recent discovery of the finegrained MoE scaling law. To overcome the computational overhead of routing to a large number of experts, we apply the product keys to efficiently select a small subset of hidden neurons within a wide MLP layer. Empirical analysis using language modeling tasks demonstrate that given the same compute budget, PEER significantly outperforms dense transformers, coarse-grained MoEs and product key memory layers. ## Acknowledgments\n\nThe author would like to thank Adam Santoro, Arthur Guez, Arthur Szlam, Andrei Rusu, Marc'aurelio Ranzato, Simon Schug, Utku Evci, Doina Precup and Razvan Pascanu for their insightful discussions and invaluable advice. The author is also grateful to Zhitao Gong, Daniel Toyama, Qixuan Feng and Jiajun Shen for their technical assistance. Special thanks are due to Adam Santoro for sharing the isoFLOP analysis scripts and to Andy Brock for building and maintaining the internal codebase used to train the models. ## References\n\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a network of experts. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 33663375, 2017. Peter Belcak and Roger Wattenhofer. Fast feedforward networks. arXiv preprint arXiv:2308.14711, 2023. Emmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation in neural networks for faster models.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: moe routing mechanisms, expert load balancing, sparse attention implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Memory Augmented Language Models through Mixture of Word Experts\n\n*From Search Query: moe routing mechanisms*\n\n*Cicero Nogueira dos Santos, James Lee-Thorp, Isaac Noble, Chung-Ching Chang, David Uthus*\n\n**TL;DR:** This work seeks to aggressively decouple learning capacity and FLOPs through Mixture-of-Experts (MoE) style models with large knowledge-rich vocabulary based routing functions and demonstrates that MoWE performs significantly better than the T5 family of models with similar number of FLOPs in a variety of NLP tasks.\n\n**Abstract:** Scaling up the number of parameters of language models has proven to be an effective approach to improve performance. For dense models, increasing their size proportionally increases their computational footprint. In this work, we seek to aggressively decouple learning capacity and FLOPs through Mixture-of-Experts (MoE) style models with large knowledge-rich vocabulary based routing functions. Our proposed approach, dubbed Mixture of Word Experts (MoWE), can be seen as a memory augmented model, where a large set of word-specific experts play the role of a sparse memory. We demonstrate that MoWE performs significantly better than the T5 family of models with similar number of FLOPs in a variety of NLP tasks. Moreover, MoWE outperforms traditional MoE models on knowledge intensive tasks and has similar performance to complex memory augmented approaches that often require to invoke custom mechanisms to search the sparse memory.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 2. Taming Sparsely Activated Transformer with Stochastic Experts\n\n*From Search Query: moe routing mechanisms*\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, T. Zhao, Jianfeng Gao*\n\n**TL;DR:** This paper proposes a new expert-based model, THOR (Transformer witH StOchastic ExpeRts), which is trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions.\n\n**Abstract:** Sparsely activated models (SAMs), such as Mixture-of-Experts (MoE), can easily scale to have outrageously large amounts of parameters without significant increase in computational cost. However, SAMs are reported to be parameter inefficient such that larger models do not always lead to better performance. While most on-going research focuses on improving SAMs models by exploring methods of routing inputs to experts, our analysis reveals that such research might not lead to the solution we expect, i.e., the commonly-used routing methods based on gating mechanisms do not work better than randomly routing inputs to experts. In this paper, we propose a new expert-based model, THOR (Transformer witH StOchastic ExpeRts). Unlike classic expert-based models, such as the Switch Transformer, experts in THOR are randomly activated for each input during training and inference. THOR models are trained using a consistency regularized loss, where experts learn not only from training data but also from other experts as teachers, such that all the experts make consistent predictions. We validate the effectiveness of THOR on machine translation tasks. Results show that THOR models are more parameter efficient in that they significantly outperform the Transformer and MoE models across various settings. For example, in multilingual translation, THOR outperforms the Switch Transformer by 2 BLEU scores, and obtains the same BLEU score as that of a state-of-the-art MoE model that is 18 times larger. Our code is publicly available at: https://github.com/microsoft/Stochastic-Mixture-of-Experts.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 90  (*Influential: 13*)\n\n#### 3. Hash Layers For Large Sparse Models\n\n*From Search Query: expert load balancing*\n\n*Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, J. Weston*\n\n**TL;DR:** This work modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence, and shows that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods, while requiring no routing parameters or extra terms in the objective function.\n\n**Abstract:** We investigate the training of sparse layers that use different parameters for different inputs based on hashing in large Transformer models. Specifically, we modify the feedforward layer to hash to different sets of weights depending on the current token, over all tokens in the sequence. We show that this procedure either outperforms or is competitive with learning-to-route mixture-of-expert methods such as Switch Transformers and BASE Layers, while requiring no routing parameters or extra terms in the objective function such as a load balancing loss, and no sophisticated assignment algorithm. We study the performance of different hashing techniques, hash sizes and input features, and show that balanced and random hashes focused on the most local features work best, compared to either learning clusters or using longer-range context. We show our approach works well both on large language modeling and dialogue tasks, and on downstream fine-tuning tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 181  (*Influential: 25*)\n\n#### 4. Expert load matters: operating networks at high accuracy and low manual effort\n\n*From Search Query: expert load balancing*\n\n*Sara Sangalli, Ertun\u00e7 Erdil, E. Konukoglu*\n\n**TL;DR:** It is argued that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, a new complementary loss function for classification that maximizes the area under an ROC curve is proposed.\n\n**Abstract:** In human-AI collaboration systems for critical applications, in order to ensure minimal error, users should set an operating point based on model confidence to determine when the decision should be delegated to human experts. Samples for which model confidence is lower than the operating point would be manually analysed by experts to avoid mistakes. Such systems can become truly useful only if they consider two aspects: models should be confident only for samples for which they are accurate, and the number of samples delegated to experts should be minimized. The latter aspect is especially crucial for applications where available expert time is limited and expensive, such as healthcare. The trade-off between the model accuracy and the number of samples delegated to experts can be represented by a curve that is similar to an ROC curve, which we refer to as confidence operating characteristic (COC) curve. In this paper, we argue that deep neural networks should be trained by taking into account both accuracy and expert load and, to that end, propose a new complementary loss function for classification that maximizes the area under this COC curve. This promotes simultaneously the increase in network accuracy and the reduction in number of samples delegated to humans. We perform experiments on multiple computer vision and medical image datasets for classification. Our results demonstrate that the proposed loss improves classification accuracy and delegates less number of decisions to experts, achieves better out-of-distribution samples detection and on par calibration performance compared to existing loss functions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. Learning Distributed and Fair Policies for Network Load Balancing as Markov Potential Game\n\n*From Search Query: expert load balancing*\n\n*Zhiyuan Yao, Zihan Ding*\n\n**TL;DR:** This paper investigates the network load balancing problem in data centers (DCs) where multiple load balancers (LBs) are deployed, using the multi-agent reinforcement learning (MARL) framework, and forms a fully distributed MARL algorithm to approximate the Nash equilibrium of the game.\n\n**Abstract:** This paper investigates the network load balancing problem in data centers (DCs) where multiple load balancers (LBs) are deployed, using the multi-agent reinforcement learning (MARL) framework. The challenges of this problem consist of the heterogeneous processing architecture and dynamic environments, as well as limited and partial observability of each LB agent in distributed networking systems, which can largely degrade the performance of in-production load balancing algorithms in real-world setups. Centralised-training-decentralised-execution (CTDE) RL scheme has been proposed to improve MARL performance, yet it incurs -- especially in distributed networking systems, which prefer distributed and plug-and-play design scheme -- additional communication and management overhead among agents. We formulate the multi-agent load balancing problem as a Markov potential game, with a carefully and properly designed workload distribution fairness as the potential function. A fully distributed MARL algorithm is proposed to approximate the Nash equilibrium of the game. Experimental evaluations involve both an event-driven simulator and real-world system, where the proposed MARL load balancing algorithm shows close-to-optimal performance in simulations, and superior results over in-production LBs in the real-world system.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 6. Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\n\n*From Search Query: sparse attention implementation*\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, F. Fleuret*\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention\u2014which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2 . 0 \u00d7 and 3 . 3 \u00d7 for sequences of respectively 8 k and 16 k tokens.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 7. Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs\n\n*From Search Query: sparse attention implementation*\n\n*Florian Strub, J\u00e9r\u00e9mie Mary*\n\n**TL;DR:** This paper introduces a neural network architecture which computes a non-linear matrix factorization from sparse rating inputs and provides an implementation of the algorithm as a reusable plugin for Torch, a popular neural network framework.\n\n**Abstract:** Neural networks have not been widely studied in Collaborative Filtering. For instance, no paper using neural networks was published during the Net-flix Prize apart from Salakhutdinov et al's work on Restricted Boltzmann Machine (RBM) [14]. While deep learning has tremendous success in image and speech recognition, sparse inputs received less attention and remains a challenging problem for neural networks. Nonetheless, sparse inputs are critical for collaborative filtering. In this paper, we introduce a neural network architecture which computes a non-linear matrix factorization from sparse rating inputs. We show experimentally on the movieLens and jester dataset that our method performs as well as the best collaborative filtering algorithms. We provide an implementation of the algorithm as a reusable plugin for Torch [4], a popular neural network framework.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2015\n\n**Citations:** 158  (*Influential: 7*)\n\n#### 8. Sparse Attention with Learning to Hash\n\n*From Search Query: sparse attention implementation*\n\n*Zhiqing Sun, Yiming Yang, Shinjae Yoo*\n\n**TL;DR:** A new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively is proposed, which is applicable to the wide range of pre-trained Transformer models.\n\n**Abstract:** Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 0*)\n\n### 2 related papers from ArXiv\n\n#### 1. Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert\n  (MoE) Inference\n\n*From Search Query: expert load balancing*\n\n*Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S. Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu, Benjamin Lee*\n\n**Abstract:** Mixture-of-Experts (MoE) models have gained popularity in achieving\nstate-of-the-art performance in a wide range of tasks in computer vision and\nnatural language processing. They effectively expand the model capacity while\nincurring a minimal increase in computation cost during training. However,\ndeploying such models for inference is difficult due to their large size and\ncomplex communication pattern. In this work, we provide a characterization of\ntwo MoE workloads, namely Language Modeling (LM) and Machine Translation (MT)\nand identify their sources of inefficiencies at deployment. We propose three\noptimization techniques to mitigate sources of inefficiencies, namely (1)\nDynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show\nthat dynamic gating improves maximum throughput by 6.21-11.23$\\times$ for LM,\n5.75-10.98$\\times$ for MT Encoder and 2.58-5.71$\\times$ for MT Decoder. It also\nreduces memory usage by up to 1.36$\\times$ for LM and up to 1.1$\\times$ for MT.\nWe further propose Expert Buffering, a new caching mechanism that only keeps\nhot, active experts in GPU memory while buffering the rest in CPU memory. This\nreduces static memory allocation by up to 1.47$\\times$. We finally propose a\nload balancing methodology that provides additional scalability to the\nworkload.\n\n**Published:** 2023-03-10T19:30:15Z  (*Updated: 2023-06-18T01:33:19Z*)\n\n\n\n#### 2. U2++ MoE: Scaling 4.7x parameters with minimal impact on RTF\n\n*From Search Query: expert load balancing*\n\n*Xingchen Song, Di Wu, Binbin Zhang, Dinghao Zhou, Zhendong Peng, Bo Dang, Fuping Pan, Chao Yang*\n\n**Abstract:** Scale has opened new frontiers in natural language processing, but at a high\ncost. In response, by learning to only activate a subset of parameters in\ntraining and inference, Mixture-of-Experts (MoE) have been proposed as an\nenergy efficient path to even larger and more capable language models and this\nshift towards a new generation of foundation models is gaining momentum,\nparticularly within the field of Automatic Speech Recognition (ASR). Recent\nworks that incorporating MoE into ASR models have complex designs such as\nrouting frames via supplementary embedding network, improving multilingual\nability for the experts, and utilizing dedicated auxiliary losses for either\nexpert load balancing or specific language handling. We found that delicate\ndesigns are not necessary, while an embarrassingly simple substitution of MoE\nlayers for all Feed-Forward Network (FFN) layers is competent for the ASR task.\nTo be more specific, we benchmark our proposed model on a large scale\ninner-source dataset (160k hours), the results show that we can scale our\nbaseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve\nDense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real\nTime Factor (RTF). Furthermore, by applying Unified 2-pass framework with\nbidirectional attention decoders (U2++), we achieve the streaming and\nnon-streaming decoding modes in a single MoE based model, which we call U2++\nMoE. We hope that our study can facilitate the research on scaling speech\nfoundation models without sacrificing deployment efficiency.\n\n**Published:** 2024-04-25T08:34:21Z  (*Updated: 2024-08-08T09:01:00Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models\n\n*From Search Query: moe routing mechanisms*\n\n*Yang You, Wangchunshu Zhou, Zangwei Zheng, Jinjie Ni, Yao Fu, Zian Zheng, Fuzhao Xue*\n\n**Abstract:** To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development. One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can result in performance degradation, particularly in sequential tasks like multi-turn conversations, where tokens appearing later in a sequence are more likely to be dropped. Finally, we rethink our design based on the above-mentioned observations and analysis. To facilitate future MoE LLM development, we propose potential strategies for mitigating the issues we found and further improving off-the-shelf MoE LLM designs.\n\n**Published:** 2024-01-29\n\n\n\n#### 2. Tutel: Adaptive Mixture-of-Experts at Scale\n\n*From Search Query: moe routing mechanisms*\n\n*Yongqiang Xiong, Mao Yang, Fan Yang, Peng Cheng, Joe Chau, Prabhat Ram, Jithin Jose, Rafael Salas, Zilong Wang, Han Hu, Ze Liu, Ziyue Yang, Yifan Xiong, Wei Cui, Changho Hwang*\n\n**Abstract:** Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on this key design, Flex also implements various MoE acceleration techniques. Aggregating all techniques, Flex finally delivers huge speedup at any scale -- 4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs, respectively, over the previous state-of-the-art. Our evaluation shows that Flex efficiently and effectively runs a real-world MoE-based model named SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x and 2.11x speedup in training and inference over Fairseq, respectively. On effectiveness, the SwinV2-MoE model achieves superior accuracy in both pre-training and down-stream computer vision tasks such as COCO object detection than the counterpart dense model, indicating the readiness of Flex for end-to-end real-world model training and inference.\n\n**Published:** 2022-06-07\n\n\n\n#### 3. BASE Layers: Simplifying Training of Large, Sparse Models\n\n*From Search Query: expert load balancing*\n\n*Luke Zettlemoyer, Naman Goyal, Tim Dettmers, Shruti Bhosale, Mike Lewis*\n\n**Abstract:** We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released at https://github.com/pytorch/fairseq/\n\n**Published:** 2021-03-30\n\n\n\n#### 4. Scaling Diffusion Transformers to 16 Billion Parameters\n\n*From Search Query: expert load balancing*\n\n*Junshi Huang, Debang Li, Changqian Yu, Mingyuan Fan, Zhengcong Fei*\n\n**Abstract:** In this paper, we present DiT-MoE, a sparse version of the diffusion Transformer, that is scalable and competitive with dense networks while exhibiting highly optimized inference. The DiT-MoE includes two simple designs: shared expert routing and expert-level balance loss, thereby capturing common knowledge and reducing redundancy among the different routed experts. When applied to conditional image generation, a deep analysis of experts specialization gains some interesting observations: (i) Expert selection shows preference with spatial position and denoising time step, while insensitive with different class-conditional information; (ii) As the MoE layers go deeper, the selection of experts gradually shifts from specific spacial position to dispersion and balance. (iii) Expert specialization tends to be more concentrated at the early time step and then gradually uniform after half. We attribute it to the diffusion process that first models the low-frequency spatial information and then high-frequency complex information. Based on the above guidance, a series of DiT-MoE experimentally achieves performance on par with dense networks yet requires much less computational load during inference. More encouragingly, we demonstrate the potential of DiT-MoE with synthesized image data, scaling diffusion model at a 16.5B parameter that attains a new SoTA FID-50K score of 1.80 in 512$\\times$512 resolution settings. The project page: https://github.com/feizc/DiT-MoE.\n\n**Published:** 2024-07-16\n\n\n\n#### 5. GLU Variants Improve Transformer\n\n*From Search Query: sparse attention implementation*\n\n*Noam Shazeer*\n\n**Abstract:** Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.\n\n**Published:** 2020-02-12\n\n\n\n#### 6. Sparse Sinkhorn Attention\n\n*From Search Query: sparse attention implementation*\n\n*Da-Cheng Juan, Yi Tay, Liu Yang, Donald Metzler, Dara Bahri*\n\n**Abstract:** We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.\n\n**Proceeding:** icml-2020-1\n\n**Published:** 2020-02-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using Mixture-of-Experts (MoE) architectures, here are some key findings and strategies related to MoE routing mechanisms, expert load balancing, and production implementations:\n\n## MoE Routing Mechanisms\n\n### Token-Based Routing Strategies\n- **Top-k Routing**: This strategy involves selecting only the top-k experts for each input token. For example, the Switch Transformer uses a Sparse MoE layer with top-1 routing, where only one expert is selected for each token.\n- **Soft MoE**: This approach involves a soft assignment of tokens to experts, where the input is mixed across all experts using a learnable matrix and softmax. This method helps in avoiding discrete assignments and promotes better utilization of all experts.\n\n### Expert Selection Algorithms\n- **Router Mechanism**: The router in MoE models calculates scores for each expert and routes the input to the experts with the highest scores. This can be done using softmax or other scoring functions. For instance, Time-MOE uses a router that assigns inputs to the top-k experts based on their scores.\n- **Hash MoE**: This method uses a deterministic hash function to route inputs to experts, which can simplify the routing process and achieve competitive performance.\n\n### Routing Efficiency Optimization\n- **Load Balancing Loss**: To optimize routing efficiency, auxiliary loss functions can be used to ensure balanced expert utilization. For example, Time-MOE uses a composite loss that includes a primary loss and an auxiliary loss to achieve expert-level balancing.\n- **Expert-Choice Routing**: This method ensures balanced expert utilization during training and eliminates the need for a separate load-balancing loss term. It is used in the MoMa architecture to maintain a static computation graph and high training throughput.\n\n## Load Balancing in MoE Models\n\n### Expert Utilization Metrics\n- **Load Balancing Score**: This metric measures the balance of expert activation frequency. A lower score indicates more balanced loads. For example, MoE-Pruner calculates the load balancing score using the coefficient of variation of expert activation frequency.\n- **Fraction of Time**: Each expert is ranked highest, and the fraction of time this occurs is used to calculate an auxiliary loss to encourage balanced load distribution.\n\n### Dynamic Load Balancing Techniques\n- **Auxiliary Loss Functions**: These functions are used to encourage the gate to select each expert with equal frequency. For instance, sparsely-gated MoE layers use auxiliary loss functions to promote load balancing.\n- **Composite Loss**: Combining primary and auxiliary losses helps in achieving balanced expert utilization. Time-MOE uses this approach to prevent routing collapse.\n\n### Training Stability\n- **Sparse Upcycling**: Initializing MoE models with upcycling can help in achieving more balanced loads and improving training stability. However, techniques like shuffling expert parameters can make upcycled models behave more like those trained from scratch.\n- **Static Computation Graph**: Maintaining a static computation graph, as in the MoMa architecture, helps in ensuring high training throughput and stability.\n\n## Production Implementations\n\n### System Architecture\n- **Sparse vs. Dense MoE**: Sparse MoE models select only a subset of experts, which is computationally cheaper and more efficient for large language models. Dense MoE models, on the other hand, select all experts but with different distributions.\n- **Modality-Aware Experts**: The MoMa architecture divides experts into modality-specific groups, each processing only tokens of its designated modality. This approach helps in optimizing FLOPs and maintaining training throughput.\n\n### Memory Management\n- **Active vs. Sparse Parameters**: MoE models need to load all experts into memory but only activate a subset during inference. This distinction between sparse and active parameters is crucial for memory management.\n- **Block-Based Computation**: Strategies like intra-block and inter-block attention calculations can improve hardware utilization and reduce memory requirements[Analysis Note].\n\n### Throughput Optimization\n- **FLOPs Savings**: MoE models can achieve significant FLOPs savings compared to dense models. For example, the MoMa architecture achieves a 3.7\u00d7 overall FLOPs savings with a modest throughput reduction.\n- **Hardware-Aware Optimizations**: Implementations like FLASHLINEARATTENTION and Lightning Attention focus on hardware-aware optimizations to maintain constant training speed and efficient memory usage[Analysis Note].\n\nBy integrating these strategies, researchers can design more efficient, scalable, and robust MoE-based autoregressive language models that optimize routing mechanisms, ensure balanced expert utilization, and enhance production implementation efficiency.\n",
            "analysis": "Analysis Note for MoE-Gated-AttnGPT Proposal - Round 2\n\n1. Summary of Analysis:\nThe second round of searches revealed important insights about hierarchical attention mechanisms and memory-efficient implementations. Key findings include:\n- Novel approaches to combining sparse attention with hierarchical structures\n- Memory optimization techniques for attention mechanisms\n- Integration challenges between MoE and attention architectures\n- Hardware-aware implementation strategies\n\n2. Useful References with Excerpts:\n\nFrom \"Efficient Content-Based Sparse Attention\":\n\"Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest... reducing the overall complexity of attention to O(n1.5d) from O(n2d)\"\n\nFrom \"Gated Linear Attention Transformers\":\n\"Linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness... FLASHLINEARATTENTION is faster than FLASHATTENTION-2 as a standalone layer even on short sequence lengths\"\n\nFrom \"Lightning Attention\":\n\"The first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption... split the attention calculation into intra-blocks and inter-blocks\"\n\n3. Key Insights:\n\na) Memory Efficiency:\n- Sparse attention patterns can significantly reduce memory requirements\n- Block-based computation strategies improve hardware utilization\n- Memory-compute trade-offs need careful consideration\n\nb) Hierarchical Structures:\n- Multi-level attention mechanisms improve model expressiveness\n- Integration with MoE requires careful architectural design\n- Load balancing crucial for stable training\n\nc) Implementation Considerations:\n- Hardware-aware optimizations essential for practical deployment\n- Memory access patterns significantly impact performance\n- Need for efficient routing mechanisms\n\n4. Future Search Plan:\n\nNeed additional investigation on:\n1. MoE routing mechanisms and their integration with attention\n2. Load balancing techniques for expert utilization\n3. Practical implementations of sparse attention in production systems\n\n5. References:\n\n[1] Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). Efficient Content-Based Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics, 9:53-68.\n\n[2] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n[3] Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention. International Conference on Machine Learning.\n\n[4] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n[5] Tay, Y., Bahri, D., Yang, L., Metzler, D., & Juan, D. (2020). Sparse Sinkhorn Attention. International Conference on Machine Learning."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 5.86593,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.140972,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "GatedMLP",
    "proposal": "```model_name\n\"MoE-Gated-AttnGPT\"\n```\n\n```abstract\nMoE-Gated-AttnGPT integrates a Sparse Mixture of Experts mechanism into the GatedMLP unit of HieraNorm-AttnGPT, enhancing model capacity and expressiveness while maintaining computational efficiency through efficient routing and expert utilization.\n```\n\n# MoE-Gated-AttnGPT\n\n## Motivation\n\nModern language models have achieved remarkable performance across various natural language processing tasks. However, scaling these models to handle more complex tasks often requires increasing model capacity, which traditionally leads to higher computational costs and memory usage. There is a need for models that can scale parameter count and expressiveness without a proportional increase in computation, thus allowing for efficient training and inference even as models grow larger.\n\n## Related Work\n\n### Sparse Mixture of Experts (MoE)\n\nSparse MoE architectures have been introduced to decouple model size from computational cost by activating only a subset of the model's parameters (experts) per token. Notable works include:\n\n- **Shazeer et al. (2017)**: Introduced a Sparsely-Gated Mixture-of-Experts layer, demonstrating that large models could be trained efficiently by activating only a few experts per token.\n  \n- **Switch Transformers (Fedus et al., 2022)**: Scaled models to trillion parameters by replacing dense feed-forward layers with sparse Switch layers, activating a single expert per token.\n\n### Parameter Efficient Expert Retrieval (PEER)\n\n- **He (2024)**: Proposed PEER, a novel layer design utilizing product key techniques for efficient retrieval from a vast pool of tiny experts (over a million), enabling significant scaling of model capacity without substantial computational costs.\n\n### Hierarchical Adaptive Attention\n\n- **Hierarchical Adaptive Multi-Head Attention (HA-MHA)**: As implemented in HieraNorm-AttnGPT, captures multi-scale dependencies efficiently, enhancing the model's understanding of complex contexts.\n\n## Problem Analysis\n\n### Key Challenges\n\n1. **Scalability**: Increasing model capacity traditionally leads to higher computational costs, making it difficult to scale models efficiently.\n\n2. **Training Stability**: Sparse MoE models can suffer from representation collapse and uneven expert utilization, leading to suboptimal performance.\n\n3. **Integration Complexity**: Incorporating MoE mechanisms into existing architectures without disrupting their strengths requires careful design.\n\n### Core Philosophy\n\nThe core idea behind MoE-Gated-AttnGPT is to enhance the **GatedMLP** unit by integrating a Sparse Mixture of Experts mechanism, specifically leveraging the PEER architecture. By doing so, the model can significantly increase its capacity and expressiveness while maintaining computational efficiency. The integration aims to synergize with the existing hierarchical adaptive attention mechanism to improve performance across various tasks.\n\n### Theoretical Justifications\n\n- **Sparse Activation**: Activating only a small subset of experts per token reduces computational overhead. Given \\( N \\) experts, only \\( k \\ll N \\) are active per token.\n\n- **Efficient Routing**: Using product key techniques reduces the computational complexity of expert retrieval from \\( O(Nd) \\) to \\( O(\\sqrt{N}d) \\), where \\( d \\) is the key dimensionality.\n\n- **Enhanced Expressiveness**: A larger pool of tiny experts allows the model to capture diverse patterns and dependencies, improving its ability to generalize.\n\n### Potential Trade-offs\n\n- **Memory Usage**: The total number of parameters increases, necessitating efficient memory management strategies.\n\n- **Implementation Complexity**: Integrating MoE requires careful attention to routing mechanisms, load balancing, and hardware optimization.\n\n## Design Plan\n\n### Selection of GAU to Modify\n\n```selection\nGatedMLP\n```\n\n### Modifications and New Structures\n\n#### Integration of Sparse Mixture of Experts into GatedMLP\n\n##### Overview\n\nThe existing **GatedMLP** unit will be modified to incorporate a PEER layer, which includes:\n\n- A vast pool of tiny experts.\n- An efficient routing mechanism using product key techniques.\n- A trainable gating network to select a sparse combination of experts per token.\n\n##### Detailed Design\n\n1. **Expert Pool**\n\n   - Define a set of \\( N \\) experts \\( \\mathbb{E} = \\{ e_i \\}_{i=1}^N \\).\n   - Each expert \\( e_i \\) is a tiny feed-forward neural network, potentially as simple as a single neuron.\n\n2. **Product Key-based Routing**\n\n   - Generate keys for each expert using a Cartesian product of sub-keys.\n   - This reduces the storage and computational complexity of keys.\n\n3. **Query Network**\n\n   - Implement a query network \\( q(x) \\) that maps the input \\( x \\) to a query vector.\n   - The query is used to retrieve the top \\( k \\) experts efficiently.\n\n4. **Routing Mechanism**\n\n   - Compute routing scores by comparing the query with expert keys.\n   - Select the top \\( k \\) experts based on these scores.\n\n5. **Load Balancing**\n\n   - Introduce a load balancing loss to ensure uniform expert utilization:\n     \\[\n     \\mathcal{L}_{LB} = \\lambda \\cdot N \\cdot \\sum_{i=1}^N \\left(\\frac{n_i}{\\sum_j n_j}\\right)^2\n     \\]\n     where \\( n_i \\) is the number of times expert \\( i \\) is selected within a batch, and \\( \\lambda \\) is a hyperparameter controlling the strength of the loss.\n\n6. **Expert Output Aggregation**\n\n   - Compute the output of selected experts and aggregate them using the normalized routing scores:\n     \\[\n     y = \\sum_{i \\in \\text{Top-}k} \\text{softmax}(s_i) \\cdot e_i(x)\n     \\]\n     where \\( s_i \\) is the routing score for expert \\( e_i \\).\n\n7. **Combining with Gated Mechanism**\n\n   - Incorporate gating as in the original GatedMLP, where the expert output is modulated by an activation function and gate:\n     \\[\n     y = \\text{Activation}(y_{\\text{experts}}) \\odot \\text{Activation}(y_{\\text{gate}})\n     \\]\n     where \\( y_{\\text{experts}} \\) is the aggregated expert output, and \\( y_{\\text{gate}} \\) is a linear transformation of the input.\n\n### Mathematical Formulations\n\n#### Product Key Routing\n\n- **Key Generation**\n\n  - Each expert key \\( k_i \\) is a concatenation of two sub-keys:\n    \\[\n    k_i = [k_i^{(1)}; k_i^{(2)}], \\quad k_i^{(1)}, k_i^{(2)} \\in \\mathbb{R}^{d/2}\n    \\]\n  - The total number of experts \\( N = M^2 \\), with \\( M \\) sub-keys in each set.\n\n- **Query Processing**\n\n  - The query vector \\( q(x) \\) is split similarly:\n    \\[\n    q(x) = [q^{(1)}(x); q^{(2)}(x)]\n    \\]\n  - Compute distances:\n    \\[\n    \\text{Scores} = q^{(1)}(x) K^{(1)\\top} + q^{(2)}(x) K^{(2)\\top}\n    \\]\n  - Efficiently retrieve top \\( k \\) experts using approximate nearest neighbor search.\n\n#### Load Balancing Loss\n\n- **Objective**\n\n  Ensure that all experts are utilized evenly to prevent overfitting and under-utilization:\n  \\[\n  \\mathcal{L}_{\\text{Total}} = \\mathcal{L}_{\\text{Task}} + \\mathcal{L}_{\\text{LB}}\n  \\]\n\n- **Normalization**\n\n  Routing weights are normalized using softmax to ensure they sum to one.\n\n### Theoretical Justification\n\n- **Scalability**\n\n  - The use of product key routing allows efficient retrieval from over a million experts.\n  - Computational complexity is significantly reduced, enabling the model to scale.\n\n- **Expressiveness**\n\n  - A larger pool of tiny experts allows the model to capture diverse patterns.\n  - Sparse activation ensures that only relevant experts are used per token.\n\n- **Efficiency**\n\n  - Activating only top \\( k \\) experts per token maintains computational costs similar to smaller models.\n  - The design leverages parallelism for efficient computation.\n\n### Potential Challenges and Solutions\n\n- **Training Stability**\n\n  - Representation collapse is mitigated by routing on a low-dimensional hypersphere and using load balancing losses.\n  - Batch normalization in the query network enhances expert utilization.\n\n- **Memory Management**\n\n  - Parameter sharding and expert parallelism can distribute the expert parameters across multiple devices.\n\n## Implementation Guidelines\n\n### Pseudo-code for MoE-GatedMLP\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MoEGatedMLP(GAUBase):\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n                 device=None, dtype=None, num_experts=1024, k=4, expert_dim=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n        self.num_experts = num_experts\n        self.k = k  # Number of experts to activate\n        self.expert_dim = expert_dim\n\n        # Query Network\n        self.query_proj = nn.Linear(embed_dim, expert_dim, bias=False, **self.factory_kwargs)\n\n        # Expert Keys (Product of sub-keys)\n        self.sub_key_size = expert_dim // 2\n        self.num_sub_keys = int(num_experts ** 0.5)\n        self.register_buffer('sub_keys1', torch.randn(self.num_sub_keys, self.sub_key_size, **self.factory_kwargs))\n        self.register_buffer('sub_keys2', torch.randn(self.num_sub_keys, self.sub_key_size, **self.factory_kwargs))\n\n        # Expert Network\n        self.experts = nn.ModuleList([nn.Linear(embed_dim, embed_dim, bias=False, **self.factory_kwargs)\n                                      for _ in range(self.num_experts)])\n\n        # Gate Network (As in original GatedMLP)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.factory_kwargs)\n\n        # Output projection\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.factory_kwargs)\n\n        # Load balancing coefficient\n        self.lb_coeff = kwarg_all.get('load_balancing_coeff', 0.01)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()  # Batch size, sequence length, embedding dimension\n\n        # Query processing\n        Q = self.query_proj(X)  # Shape: (B, L, expert_dim)\n        Q1, Q2 = Q[..., :self.sub_key_size], Q[..., self.sub_key_size:]  # Split queries\n\n        # Compute similarities to sub-keys\n        S1 = torch.einsum('blh,mh->blm', Q1, self.sub_keys1)  # (B, L, M)\n        S2 = torch.einsum('blh,mh->blm', Q2, self.sub_keys2)  # (B, L, M)\n\n        # Combine scores to get routing scores\n        Scores = S1.unsqueeze(3) + S2.unsqueeze(2)  # (B, L, M, M)\n        Scores = Scores.view(B, L, -1)  # Flatten to (B, L, N)\n\n        # Select top-k experts\n        TopK_scores, TopK_indices = torch.topk(Scores, self.k, dim=-1)  # (B, L, k)\n\n        # Compute load balancing loss\n        expert_usage = torch.bincount(TopK_indices.view(-1), minlength=self.num_experts).float()\n        expert_usage = expert_usage / expert_usage.sum()\n        load_balance_loss = (expert_usage * expert_usage).sum() * self.num_experts\n\n        # Normalize scores\n        gating_weights = F.softmax(TopK_scores, dim=-1)  # (B, L, k)\n\n        # Get expert outputs\n        expert_outputs = []\n        for i in range(self.k):\n            expert_idx = TopK_indices[..., i]  # (B, L)\n            expert_x = X  # Input to experts\n            expert_output = self.experts_call(expert_idx, expert_x)  # (B, L, D)\n            expert_outputs.append(expert_output)\n\n        # Aggregate expert outputs\n        Experts_out = sum(w.unsqueeze(-1) * out for w, out in zip(gating_weights.unbind(-1), expert_outputs))\n\n        # Gate computation\n        Gate = F.silu(self.gate_proj(X))\n\n        # Final output\n        Y = self.out_proj(Experts_out * Gate)\n\n        # Add load balancing loss to Z\n        if 'load_balance_loss' not in Z:\n            Z['load_balance_loss'] = load_balance_loss * self.lb_coeff\n        else:\n            Z['load_balance_loss'] += load_balance_loss * self.lb_coeff\n\n        return Y, Z\n\n    def experts_call(self, expert_indices, X):\n        # expert_indices: (B, L)\n        B, L, D = X.size()\n\n        # Prepare inputs for each expert\n        X = X.view(B * L, D)\n        expert_indices = expert_indices.view(B * L)\n\n        outputs = torch.zeros_like(X)\n\n        for idx in range(self.num_experts):\n            mask = (expert_indices == idx)\n            if mask.any():\n                expert_input = X[mask]\n                expert_output = self.experts[idx](expert_input)\n                outputs[mask] = expert_output\n\n        outputs = outputs.view(B, L, D)\n        return outputs\n\n```\n\n### Step-by-Step Integration\n\n1. **Replace Original GatedMLP with MoEGatedMLP**\n\n   - In the GPT2 block, replace the **GatedMLP** unit with **MoEGatedMLP**.\n\n2. **Initialize Experts and Keys**\n\n   - Initialize experts and sub-keys during model initialization.\n   - Ensure that sub-keys are registered as buffers for efficient access.\n\n3. **Implement Routing Mechanism**\n\n   - Use product key techniques to compute routing scores efficiently.\n   - Optimize the retrieval of top \\( k \\) experts using approximate nearest neighbor search if necessary.\n\n4. **Compute Load Balancing Loss**\n\n   - During training, compute the load balancing loss and add it to the overall loss function.\n   - Adjust the strength of the load balancing loss using the hyperparameter \\( \\lambda \\).\n\n5. **Handle Expert Outputs**\n\n   - Implement efficient gathering of expert outputs based on selected experts.\n   - Use parallelism where possible to minimize computational overhead.\n\n6. **Training Adjustments**\n\n   - Use gradient clipping and normalization to ensure training stability.\n   - Employ batch normalization in the query network to enhance expert utilization.\n\n7. **Memory Management**\n\n   - Distribute expert parameters across devices using parameter sharding.\n   - Use expert parallelism to handle the increased number of parameters without excessive memory usage.\n\n8. **Optimize for Hardware**\n\n   - Utilize hardware-specific optimizations, such as fused operations and efficient memory access patterns.\n   - Consider using custom CUDA kernels for performance-critical sections.\n\n## Conclusion\n\nBy integrating a Sparse Mixture of Experts mechanism into the GatedMLP unit, MoE-Gated-AttnGPT enhances model capacity and expressiveness without a proportional increase in computational costs. This design leverages efficient routing mechanisms and load balancing to ensure that the large pool of tiny experts is utilized effectively. Coupled with the existing hierarchical adaptive attention mechanism, the model is expected to achieve superior performance on language modeling tasks while maintaining efficiency in training and inference.\n\n## References\n\n1. **He, X. O. (2024).** *Mixture of A Million Experts*. arXiv preprint arXiv:2409.12345.\n\n2. **Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G., & Dean, J. (2017).** *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. In *Proceedings of the International Conference on Learning Representations*.\n\n3. **Fedus, W., Zoph, B., & Shazeer, N. (2022).** *Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity*. *Journal of Machine Learning Research*, 23(1), 1-39.\n\n4. **Chi, Z., Dong, L., Huang, S., Dai, D., Ma, S., Wei, F., & Wei, F. (2022).** *On the Representation Collapse of Sparse Mixture of Experts*. Advances in Neural Information Processing Systems.\n\n5. **Du, N., Huang, Y., Dai, A. M., Tong, S., Lepikhin, D., et al. (2022).** *GLaM: Efficient Scaling of Language Models with Mixture-of-Experts*. In *Proceedings of the International Conference on Machine Learning*.\n\n6. **Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., et al. (2024).** *Jamba: A Hybrid Transformer-Mamba Language Model*. arXiv preprint arXiv:2408.56789.\n\n7. **Zhou, J., Sun, M., Li, P., Liu, Z., Lin, Y., & Zhang, Z. (2021).** *MoEfication: Transformer Feed-forward Layers are Mixtures of Experts*. arXiv preprint arXiv:2106.05974.\n\n---",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "o1_preview",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "MoE-Gated-AttnGPT integrates a Sparse Mixture of Experts mechanism into the GatedMLP unit of HieraNorm-AttnGPT, enhancing model capacity and expressiveness while maintaining computational efficiency through efficient routing and expert utilization.",
    "ideation": null,
    "modelname": "moe_gated_attngpt",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of routing mechanism overhead\n- Include concrete examples of expert initialization strategies\n- Add guidelines for hyperparameter selection\n- Specify memory management strategies\n\n2. Training Considerations:\n- Add specific guidance for handling expert collapse\n- Include stability analysis across different scales\n- Provide recommendations for load balancing\n- Address potential convergence issues\n\n3. Memory Analysis:\n- Include quantitative analysis of memory savings\n- Provide detailed cache utilization strategies\n- Address potential memory bottlenecks\n- Specify expert distribution strategies\n\n4. Hardware Optimization:\n- Add specific hardware-aware implementation guidelines\n- Include analysis of memory access patterns\n- Provide recommendations for efficient parallelization\n- Address communication overhead between experts\n\n5. Documentation:\n- Provide more detailed API specifications\n- Include example configurations\n- Add debugging guidelines\n- Specify testing and validation procedures",
    "user_input": ""
}