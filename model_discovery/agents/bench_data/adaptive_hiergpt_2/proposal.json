{
    "variantname": "AdaptiveHierarchicalRMSNorm",
    "review": "The AdaptiveHierarchicalRMSNorm proposal presents an innovative approach to enhancing normalization in language models through the integration of adaptive learning rates and state space models. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nCLARITY (4.5/5):\nThe proposal is exceptionally well-articulated, with:\n- Clear objectives for improving normalization efficiency and adaptivity\n- Detailed mathematical formulations\n- Well-structured implementation guidelines\n- Comprehensive theoretical justification\n\nINNOVATION (4.2/5):\nThe proposal demonstrates significant innovation through:\n- Novel integration of state space models with hierarchical normalization\n- Unique approach to adaptive initialization\n- Clear differentiation from existing approaches\n\nHowever, some aspects overlap with existing work:\n- Similar to HiPPO's hierarchical processing\n- Shares concepts with Mamba's state space modeling\n- Some adaptive mechanisms resemble existing approaches\n\nFEASIBILITY (4.0/5):\nImplementation appears feasible, supported by:\n- Detailed pseudo-code and mathematical formulations\n- Clear integration guidelines\n- Compatibility with existing frameworks\n\nChallenges include:\n- Complex coordination between SSM and normalization\n- Potential memory management issues\n- Need for careful parameter tuning\n\nSCALABILITY (4.3/5):\nThe design shows strong potential for scalability:\n- Linear complexity through SSM integration\n- Efficient memory usage through hierarchical processing\n- Adaptive mechanisms enabling better resource utilization\n\nACCURACY AND ROBUSTNESS (4.2/5):\nExpected improvements in:\n- Feature normalization through multi-scale processing\n- Enhanced stability through adaptive initialization\n- Better handling of varying sequence lengths\n\nEFFICIENCY (4.4/5):\nStrong efficiency benefits:\n- Memory optimization through hierarchical processing\n- Computational efficiency via SSM integration\n- Adaptive resource allocation\n\nSTRENGTHS:\n1. Innovation: Successfully combines SSMs with hierarchical normalization in a novel way\n2. Memory Efficiency: Effective use of hierarchical processing and adaptive mechanisms\n3. Scalability: Design scales well with sequence length and model size\n4. Theoretical Foundation: Strong mathematical justification and implementation details\n5. Adaptability: Flexible design accommodating various sequence lengths and data distributions\n\nCONCERNS:\n1. Implementation Complexity: Integration of SSMs with normalization requires sophisticated coordination\n2. Memory Management: Need for careful balance between adaptivity and memory usage\n3. Training Stability: Multiple adaptive components may affect convergence\n4. Parameter Sensitivity: Success may depend heavily on proper initialization and tuning\n5. Hardware Requirements: May need specific optimizations for efficient implementation",
    "search_stack": [
        {
            "ready": false,
            "query": "multi-scale normalization, adaptive initialization, memory optimization",
            "detail": "Hierarchical normalization techniques in language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nHierarchical normalization techniques in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.97)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.97)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 2. Hierarchical Transformers Are More Efficient Language Models (Avg. Score: 0.76)\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, H. Michalewski*\n\n**Published in:** NAACL-HLT (2021)\t**Cited by** 40  (*Influential: 4*)\n\n**TL;DR:** Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.\n\n##### *Relevant Chunk: No. 1/25 (Score: 0.76)*\n\n```\n# Hierarchical Transformers Are More Efficient Language Models \n\nPiotr Nawrot ${ }^{* 1}$, Szymon Tworkowski ${ }^{* 1}$, Micha\u0142 Tyrolski ${ }^{1}$, Lukasz Kaiser ${ }^{2}$,<br>Yuhuai Wu ${ }^{3}$, Christian Szegedy ${ }^{3}$, Henryk Michalewski ${ }^{3}$<br>${ }^{1}$ University of Warsaw, ${ }^{2}$ OpenAI, ${ }^{3}$ Google Research<br>\\{p.nawrot99, szy.tworkowski, michal.tyrolski, lukaszkaiser\\}@gmail.com,<br>\\{yuhuai, szegedy, henrykm\\}@google.com\n\n\n#### Abstract\n\nTransformer models yield impressive results on many NLP and sequence modeling tasks.\n```\n\n#### 3. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.57)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.57)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n#### 4. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.53)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.53)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n#### 5. Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation (Avg. Score: 0.44)\n\n*Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang, Di He, Jingjing Xu, Zhi Zhang, Hongxia Yang, Liwei Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** Theoretical analysis shows this disentanglement of positional information makes learning more effective and the empirical results show that the BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.\n\n**Abstract:** In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.\n\n##### *Relevant Chunk: No. 16/42 (Score: 0.44)*\n\n```\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, June 2019b. Eilenberg, S. Automata, languages, and machines. Academic press, 1974. Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L. Towards revealing the mystery behind chain of thought: A theoretical perspective. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Fine, S., Singer, Y., and Tishby, N. The hierarchical hidden markov model: Analysis and applications. Machine learning, 32:41-62, 1998. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac'h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/ 10256836 . Griffiths, T., Jordan, M., Tenenbaum, J., and Blei, D. Hierarchical topic models and the nested chinese restaurant process. Advances in neural information processing systems, 16, 2003. Halliday, M. A. K. and Matthiessen, C. M. Halliday's introduction to functional grammar. Routledge, 2013. Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S. Lm-infinite: Simple on-the-fly length generalization for large language models, 2023. Haviv, A., Ram, O., Press, O., Izsak, P., and Levy, O. Transformer language models without positional encodings still learn positional information. In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 1382-1390, December 2022. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419-1436, June 2021. Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.Y., Chen, H., and Hu, X. Llm maybe longlm: Selfextend llm context window without tuning.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: multi-scale normalization, adaptive initialization, memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting\n\n*From Search Query: multi-scale normalization*\n\n*Amin Shabani, A. Abdi, Li Meng, Tristan Sylvain*\n\n**TL;DR:** A general multi-scale framework that can be applied to the state-of-the-art transformer-based time series forecasting models (FEDformer, Autoformer, etc.), and demonstrates the effectiveness of each of its contributions across the architecture and methodology.\n\n**Abstract:** The performance of time series forecasting has recently been greatly improved by the introduction of transformers. In this paper, we propose a general multi-scale framework that can be applied to the state-of-the-art transformer-based time series forecasting models (FEDformer, Autoformer, etc.). By iteratively refining a forecasted time series at multiple scales with shared weights, introducing architecture adaptations, and a specially-designed normalization scheme, we are able to achieve significant performance improvements, from 5.5% to 38.5% across datasets and transformer architectures, with minimal additional computational overhead. Via detailed ablation studies, we demonstrate the effectiveness of each of our contributions across the architecture and methodology. Furthermore, our experiments on various public datasets demonstrate that the proposed improvements outperform their corresponding baseline counterparts. Our code is publicly available in https://github.com/BorealisAI/scaleformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 41  (*Influential: 2*)\n\n#### 2. SNIPER: Efficient Multi-Scale Training\n\n*From Search Query: multi-scale normalization*\n\n*Bharat Singh, Mahyar Najibi, L. Davis*\n\n**TL;DR:** SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct.\n\n**Abstract:** We present SNIPER, an algorithm for performing efficient multi-scale training in instance level visual recognition tasks. Instead of processing every pixel in an image pyramid, SNIPER processes context regions around ground-truth instances (referred to as chips) at the appropriate scale. For background sampling, these context-regions are generated using proposals extracted from a region proposal network trained with a short learning schedule. Hence, the number of chips generated per image during training adaptively changes based on the scene complexity. SNIPER only processes 30% more pixels compared to the commonly used single scale training at 800x1333 pixels on the COCO dataset. But, it also observes samples from extreme resolutions of the image pyramid, like 1400x2000 pixels. As SNIPER operates on resampled low resolution chips (512x512 pixels), it can have a batch size as large as 20 on a single GPU even with a ResNet-101 backbone. Therefore it can benefit from batch-normalization during training without the need for synchronizing batch-normalization statistics across GPUs. SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct. Our implementation based on Faster-RCNN with a ResNet-101 backbone obtains an mAP of 47.6% on the COCO dataset for bounding box detection and can process 5 images per second during inference with a single GPU. Code is available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 469  (*Influential: 35*)\n\n#### 3. Hierarchical Normalization for Robust Monocular Depth Estimation\n\n*From Search Query: multi-scale normalization*\n\n*Chi Zhang, Wei Yin, Zhibin Wang, Gang Yu, Bin Fu, Chunhua Shen*\n\n**TL;DR:** This paper proposes a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions and presents two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain.\n\n**Abstract:** In this paper, we address monocular depth estimation with deep neural networks. To enable training of deep monocular estimation models with various sources of datasets, state-of-the-art methods adopt image-level normalization strategies to generate affine-invariant depth representations. However, learning with image-level normalization mainly emphasizes the relations of pixel representations with the global statistic in the images, such as the structure of the scene, while the fine-grained depth difference may be overlooked. In this paper, we propose a novel multi-scale depth normalization method that hierarchically normalizes the depth representations based on spatial information and depth distributions. Compared with previous normalization strategies applied only at the holistic image level, the proposed hierarchical normalization can effectively preserve the fine-grained details and improve accuracy. We present two strategies that define the hierarchical normalization contexts in the depth domain and the spatial domain, respectively. Our extensive experiments show that the proposed normalization strategy remarkably outperforms previous normalization methods, and we set new state-of-the-art on five zero-shot transfer benchmark datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 4. Adaptive Universal Generalized PageRank Graph Neural Network\n\n*From Search Query: adaptive initialization*\n\n*Eli Chien, Jianhao Peng, Pan Li, O. Milenkovic*\n\n**TL;DR:** This work introduces a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic.\n\n**Abstract:** In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-of-the-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 598  (*Influential: 179*)\n\n#### 5. PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization\n\n*From Search Query: adaptive initialization*\n\n*Benyuan Sun, Hongxing Huo, Yi Yang, Bo Bai*\n\n**TL;DR:** A novel idea, PartialFed, which loads a subset of the global model\u2019s parameters rather than loading the entire model used in most previous works is proposed, which improves the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively and develops PartialFed-Adaptive, which automatically selects personalized loading strategy for each client.\n\n**Abstract:** The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data con\ufb01dentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a signi\ufb01cant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain characteristic, such as traf\ufb01c, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed , which loads a subset of the global model\u2019s parameters rather than loading the entire model used in most previous works. We \ufb01rst validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix . Then we develop PartialFed-Adaptive , which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classi\ufb01cation and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs signi\ufb01cantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 81  (*Influential: 6*)\n\n#### 6. Task-adaptive Label Dependency Transfer for Few-shot Named Entity Recognition\n\n*From Search Query: adaptive initialization*\n\n*Shan Zhang, Bin Cao, Tianming Zhang, Yuqi Liu, Jing Fan*\n\n**TL;DR:** The T ask-adaptive TLDT method is proposed to make label dependency transferable and effectively adapt to new tasks by a few samples and improves the existing optimization-based meta-learning methods by learning general initialization and individual parameter up-date rule for label dependency.\n\n**Abstract:** Named Entity Recognition (NER), as a crucial subtask in natural language processing (NLP), suffers from limited labeled samples (a.k.a. few-shot). Meta-learning methods are widely used for few-shot NER, but these existing methods overlook the importance of label dependency for NER, resulting in suboptimal performance. However, applying meta-learning methods to label dependency learning faces a special challenge, that is, due to the discrepancy of label sets in different domains, the label dependencies can not be transferred across domains. In this paper, we propose the T ask-adaptive L abel D ependency T ransfer (TLDT) method to make label dependency transferable and effectively adapt to new tasks by a few samples. TLDT improves the existing optimization-based meta-learning methods by learning general initialization and individual parameter up-date rule for label dependency. Extensive experiments show that TLDT achieves significant improvement over the state-of-the-art methods.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 7. H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training\n\n*From Search Query: memory optimization*\n\n*Yuzhong Wang, Xu Han, Weilin Zhao, Guoyang Zeng, Zhiyuan Liu, Maosong Sun*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Memory Optimization for Deep Networks\n\n*From Search Query: memory optimization*\n\n*Aashaka Shah, Chaoxia Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Krahenbuhl*\n\n**TL;DR:** MONeT is an automatic framework that minimizes both the memory footprint and computational overhead of deep networks, and is able to outperform all prior hand-tuned operations as well as automated checkpointing.\n\n**Abstract:** Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by 32x over the last five years, the total available memory only grew by 2.5x. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by 3x for various PyTorch models, with a 9-16% overhead in computation. For the same computation cost, MONeT requires 1.2-1.8x less memory than current state-of-the-art automated checkpointing frameworks. Our code is available at this https URL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 21  (*Influential: 0*)\n\n### 3 related papers from ArXiv\n\n#### 1. Towards Robust and Cost-Efficient Knowledge Unlearning for Large\n  Language Models\n\n*From Search Query: adaptive initialization*\n\n*Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee*\n\n**Abstract:** Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We also find\nthat combining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose two novel techniques for robust and efficient unlearning for LLMs.\nFirst, we introduce Inverted Hinge loss, which suppresses unwanted tokens while\nmaintaining fluency by boosting the probability of the next most likely token.\nSecond, we develop a data-adaptive initialization for LoRA adapters via\nlow-rank approximation weighted with relative Fisher information, thereby\nfocusing updates on parameters critical for removing targeted knowledge.\nExperiments on the Training Data Extraction Challenge dataset using GPT-Neo\nmodels as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models\ndemonstrate that our approach effectively removes sensitive information while\nmaintaining reasoning and generative capabilities with minimal impact.\n\n**Published:** 2024-08-13T04:18:32Z  (*Updated: 2024-10-13T19:03:38Z*)\n\n\n\n#### 2. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: memory optimization*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 3. L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models\n\n*From Search Query: memory optimization*\n\n*Hyesung Jeon, Yulhwa Kim, Jae-joon Kim*\n\n**Abstract:** Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically follow a two-step approach: first,\napplying post-training quantization (PTQ) to model weights, followed by PEFT on\nthe quantized model. However, recovering from the quantization error introduced\nby PTQ through fine-tuning has proven challenging. Additionally, most PTQ-based\nPEFT methods result in a mixture of low-precision quantized weights and\nhigh-precision adapter weights, limiting the efficiency of full quantization\nduring inference. While a previous method attempted to address these issues, it\nstill suffers from limited adaptability due to the constrained LoRA parameter\nstructure required to produce fully-quantized models. To overcome these\nchallenges, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA to effectively reduce quantization error. By employing\na memory-optimized layer design, L4Q significantly reduces QAT's memory\noverhead while producing fully-quantized weights, enabling effective adaptation\nto downstream tasks. Our experiments demonstrate that this combined approach to\nquantization and fine-tuning achieves superior accuracy compared to decoupled\nfine-tuning schemes, particularly in sub-4-bit quantization, positioning L4Q as\nan efficient QAT solution. Using the LLaMA model families and instructional\ndatasets, we showcase L4Q's capabilities in language tasks and few-shot\nlearning.\n\n**Published:** 2024-02-07T14:35:05Z  (*Updated: 2024-10-28T04:41:02Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture\n\n*From Search Query: multi-scale normalization*\n\n*David Eigen, Rob Fergus*\n\n**Abstract:** In this paper we address three different computer vision tasks using a single\nbasic architecture: depth prediction, surface normal estimation, and semantic\nlabeling. We use a multiscale convolutional network that is able to adapt\neasily to each task using only small modifications, regressing from the input\nimage to the output map directly. Our method progressively refines predictions\nusing a sequence of scales, and captures many image details without any\nsuperpixels or low-level segmentation. We achieve state-of-the-art performance\non benchmarks for all three tasks.\n\n**Conference:** predicting-depth-surface-normals-and-semantic-1\n\n**Published:** 2014-11-18\n\n\n\n#### 2. Normalizing Flows with Multi-Scale Autoregressive Priors\n\n*From Search Query: multi-scale normalization*\n\n*Bernt Schiele, Stefan Roth, Shweta Mahajan, Apratim Bhattacharyya, Mario Fritz*\n\n**Abstract:** Flow-based generative models are an important class of exact inference models that admit efficient inference and sampling for image synthesis. Owing to the efficiency constraints on the design of the flow layers, e.g. split coupling flow layers in which approximately half the pixels do not undergo further transformations, they have limited expressiveness for modeling long-range data dependencies compared to autoregressive models that rely on conditional pixel-wise generation. In this work, we improve the representational power of flow-based models by introducing channel-wise dependencies in their latent space through multi-scale autoregressive priors (mAR). Our mAR prior for models with split coupling flow layers (mAR-SCF) can better capture dependencies in complex multimodal data. The resulting model achieves state-of-the-art density estimation results on MNIST, CIFAR-10, and ImageNet. Furthermore, we show that mAR-SCF allows for improved image generation quality, with gains in FID and Inception scores compared to state-of-the-art flow-based models.\n\n**Conference:** normalizing-flows-with-multi-scale-1\n\n**Published:** 2020-04-08\n\n\n\n#### 3. An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios\n\n*From Search Query: adaptive initialization*\n\n*Junichi Yamagishi, Korin Richmond, Aidan Pine, Marc Tessier, Jianwu Dang, Longbiao Wang, Dan Wells, Mengzhe Geng, Chunyu Qiang, Xin Wang, Erica Cooper, Cheng Gong*\n\n**Abstract:** Self-supervised learning (SSL) representations from massively multilingual models offer a promising solution for low-resource language speech tasks. Despite advancements, language adaptation in TTS systems remains an open problem. This paper explores the language adaptation capability of ZMM-TTS, a recent SSL-based multilingual TTS system proposed in our previous work. We conducted experiments on 12 languages using limited data with various fine-tuning configurations. We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language's adaptation performance. Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability. Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data. Beyond speech intelligibility, our analysis covers speaker similarity, language identification, and predicted MOS.\n\n**Published:** 2024-06-13\n\n\n\n#### 4. I2I: Initializing Adapters with Improvised Knowledge\n\n*From Search Query: adaptive initialization*\n\n*Jesse Thomason, Mohammad Rostami, Furong Jia, Tejas Srinivasan*\n\n**Abstract:** Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.\n\n**Published:** 2023-04-04\n\n\n\n#### 5. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n\n*From Search Query: memory optimization*\n\n*Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He*\n\n**Abstract:** Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.\n\n**Published:** 2019-10-04\n\n\n\n#### 6. Memory-Efficient Adaptive Optimization\n\n*From Search Query: memory optimization*\n\n*Yoram Singer, Tomer Koren, Vineet Gupta, Rohan Anil*\n\n**Abstract:** Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of per-parameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-of-the-art.\n\n**Published:** 2019-01-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on hierarchical normalization techniques, adaptive initialization, and memory optimization, here are some key insights and relevant information from the provided sources and analysis:\n\n## Multi-scale Normalization\n\n- The concept of multi-scale processing is crucial for capturing hierarchical patterns and handling varying sequence lengths efficiently. This is supported by the idea of hierarchical attention mechanisms, such as those mentioned in the analysis note. For instance, hierarchical routing attention mechanisms like those in **HieraRoute-GPT** can combine content-based sparse attention with efficient clustering-based routing, which could be adapted for multi-scale normalization[Analysis Note].\n\n## Adaptive Initialization\n\n- Adaptive initialization based on scale-specific statistics can enhance training stability and model performance. The memory update mechanism in **HiPPO**, which involves online compression of continuous signals by projection onto polynomial bases, could inspire adaptive initialization strategies. This approach ensures that the initialization is dynamic and responsive to the data, potentially improving the model's adaptability and performance[Analysis Note].\n\n## Memory Optimization\n\n- Memory optimization is vital for scaling language models efficiently. Techniques such as bounded-memory control, as discussed in **Attention with Bounded-memory Control**, can improve inference time and space efficiency. This involves managing memory usage effectively, ensuring that the model does not compromise on performance while handling large sequences or complex tasks[Analysis Note].\n\n## Relevant Techniques and Frameworks\n\n- **Dynamic Memory Fusion Framework**: While primarily focused on multi-loss function optimization, this framework's use of historical loss values to dynamically adjust weights could be adapted to optimize normalization parameters in a multi-scale setting. The memory-based weighting strategies (variance-based, MAD-based, and Bayesian-based) could be applied to adjust normalization scales dynamically.\n\n- **Cellular Automaton-Driven Weight Perturbation**: This method introduces structured perturbations to the weight space, which could be adapted to enhance the exploration of normalization parameters at different scales. The use of cellular automata to produce intricate, emergent behaviors could help in navigating the complex space of normalization parameters more effectively.\n\n## Implementation Considerations\n\n- **Efficient Attention Mechanisms**: Techniques like those in **BlockSparseRetNet**, which integrate block-based sparse computation with adaptive granularity, can enhance computational efficiency and scalability. These methods can be applied to optimize the computational overhead of multi-scale normalization[Analysis Note].\n\n- **Hierarchical Processing**: The hierarchical routing attention mechanism and block-based sparse computation can be combined to process features at multiple resolutions efficiently. This approach ensures that the model captures hierarchical patterns while maintaining computational efficiency[Analysis Note].\n\nBy integrating these concepts, the researchers can develop a more robust and efficient autoregressive language model that leverages multi-scale normalization, adaptive initialization, and optimized memory usage. This would enhance the model's ability to handle long-range dependencies, improve training stability, and scale more effectively with larger datasets and models.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe seed design, HierarchicalRMSNorm, aims to enhance language model normalization by introducing multi-scale processing and adaptive initialization. The design focuses on addressing limitations of standard normalization techniques, which often fail to capture hierarchical patterns and efficiently handle varying sequence lengths. The proposal introduces a multi-scale normalization approach that processes features at multiple resolutions and adapts normalization parameters based on scale-specific statistics.\n\n#### 2. Useful References with Excerpts\n- **Reference 0: HieraRoute-GPT**\n  - \"HieraRoute-GPT introduces a novel hierarchical routing attention mechanism that combines the benefits of content-based sparse attention with efficient clustering-based routing.\"\n  - This reference provides insights into hierarchical processing and adaptive routing, which could be beneficial for enhancing multi-scale normalization.\n\n- **Reference 1: BlockSparseRetNet**\n  - \"BlockSparseRetNet integrates block-based sparse computation with adaptive granularity, enhancing computational efficiency and scalability.\"\n  - This reference discusses adaptive computation and memory-efficient processing, which are relevant to improving the efficiency of HierarchicalRMSNorm.\n\n- **Reference 2: HiPPO**\n  - \"HiPPO introduces a general framework for online compression of continuous signals by projection onto polynomial bases, yielding a new memory update mechanism.\"\n  - The memory update mechanism in HiPPO could inspire adaptive initialization strategies in HierarchicalRMSNorm.\n\n- **Reference 3: Attention with Bounded-memory Control**\n  - \"ABC reveals new, unexplored possibilities for efficient attention models, improving inference time and space efficiency.\"\n  - This reference provides insights into bounded-memory control, which could be applied to optimize memory usage in HierarchicalRMSNorm.\n\n#### 3. Key Insights and Detailed Analysis\n- **Multi-scale Processing**: The idea of processing features at multiple scales is supported by existing research in hierarchical attention and adaptive computation. This approach can improve the model's ability to capture hierarchical patterns and handle long-range dependencies efficiently.\n- **Adaptive Initialization**: Leveraging adaptive initialization based on scale-specific statistics can enhance training stability and model performance. This concept aligns with the memory update mechanisms discussed in HiPPO.\n- **Efficiency and Scalability**: References to efficient attention mechanisms and block-based sparse computation highlight the importance of optimizing computational efficiency and memory usage, which are crucial for scaling language models.\n\n#### 4. Future Search Plan or Next Steps\n- **Explore Adaptive Initialization**: Further investigate adaptive initialization techniques, possibly inspired by HiPPO's memory update mechanisms.\n- **Investigate Efficient Multi-scale Processing**: Look into efficient multi-scale processing methods, drawing from hierarchical attention and block-based sparse computation.\n- **Consider Memory Optimization**: Explore bounded-memory control techniques to optimize memory usage in HierarchicalRMSNorm.\n\n#### 5. List of References\n1. Roy, A., et al. (2020). Efficient Content-Based Sparse Attention with Routing Transformers. TACL.\n2. Qin et al. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models.\n3. Gu, A., et al. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. NeurIPS.\n4. Peng, H., et al. (2021). Attention with Bounded-memory Control. ACL."
        },
        {
            "ready": false,
            "query": "hierarchical processing, adaptive learning, memory optimization",
            "detail": "Hierarchical structures in language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nHierarchical structures in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Modeling Hierarchical Structures with Continuous Recursive Neural Networks  (Avg. Score: 0.98)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** International Conference on Machine Learning (2021)\t**Cited by** 11  (*Influential: 1*)\n\n**TL;DR:** This work proposes Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations of traditional RvNNs by incorporating a continuous relaxation to the induced structure.\n\n**Abstract:** Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases. However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference and ListOps. We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference.\n\n##### *Relevant Chunk: No. 1/40 (Score: 0.98)*\n\n```\n# Modeling Hierarchical Structures with Continuous Recursive Neural Networks \n\nJishnu Ray Chowdhury ${ }^{1}$ Cornelia Caragea ${ }^{1}$\n\n\n#### Abstract\n\nRecursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases.\n```\n\n#### 2. Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation (Avg. Score: 0.97)\n\n*Xiang Hu, Haitao Mi, Liang Li, Gerard de Melo*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2022)\t**Cited by** 10  (*Influential: 0*)\n\n**TL;DR:** This paper uses a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference and significantly improves the grammar induction quality and achieves competitive results in downstream tasks.\n\n**Abstract:** Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n\u00b3) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.However, its rule-based pruning process suffers from local optima and slow inference. In this paper, we propose a unified R2D2 method that overcomes these issues. We use a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference. Our parser casts parsing as a split point scoring task by first scoring all split points for a given sentence and then using the highest-scoring one to recursively split a span into two parts. The reverse order of the splits is considered as the order of pruning in the encoder. We optimize the unsupervised parser by minimizing the Kullback\u2013Leibler distance between tree probabilities from the parser and the R2D2 model.Our experiments show that our Fast-R2D2 significantly improves the grammar induction quality and achieves competitive results in downstream tasks.\n\n##### *Relevant Chunk: No. 14/26 (Score: 0.97)*\n\n```\nSpringer. Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-specific tree structures. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5094-5101. AAAI Press. Noam Chomsky. 1956. Three models for the description of language. IRE Trans. Inf. Theory, 2(3):113124 . Noam Chomsky. 2014. Aspects of the Theory of Syntax, volume 11. MIT press. Jishnu Ray Chowdhury and Cornelia Caragea. 2021. Modeling hierarchical structures with continuous recursive neural networks. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 1975-1988. PMLR. Alexander Clark. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning, CoNLL 2001, Toulouse, France, July 6-7, 2001.\n```\n\n#### 3. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.96)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.96)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n#### 4. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.94)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 25/46 (Score: 0.94)*\n\n```\nFrederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model.\n```\n\n#### 5. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.91)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 4/63 (Score: 0.91)*\n\n```\nThe formal proof is in Appendix B. 1 . ### 3.4 Difficulty of PARITY\n\nConversely, we next establish a design choice in SSMs which limits their power in emulating finitestate-automata, establishing - in the finite-precision setting - an even stronger separation between existing SSM variants and traditional RNNs than the circuit complexity arguments in Merrill et al. [49]. The language PARITY, the language of bitstrings with an even number of ones, is recognized by a finite-state automaton with 2 states, and is straightforwardly encoded into a traditional RNN, even a linear one, with finite precision. PARITY, while in principle expressible for transformers [9], is empirically very hard to learn for transformers [6, 15], as it can provably only be represented in sharp minima [26]. A sufficiently general SSM could easily recognize it at $d=1$ by setting $h_{0}=1$, $A\\left(e_{1}\\right)=-1, A\\left(e_{0}\\right)=0, B \\equiv 0$, so that the sign of the single entry of $h_{t}$ indicates the parity. Such an SSM would need to be non-time-invariant and require negative or complex gate values; i.e., satisfy neither TIME-INVARIANT nor NONNEGATIVE. Such properties are indeed provably needed:\nTheorem 2. No SSM satisfying NONNEGATIVE can recognize PARITY at arbitrary input lengths with finite precision. In particular, this applies to Mamba. The proof is in Appendix B.2 , it examines inputs of the form $1^{N}$ and shows that the activations $z_{N}$ converge as $N \\rightarrow \\infty$, and thus cannot reliably encode the parity of $N$. As we show in Theorem 13 . the same result holds even for SSMs evading NONNEGATIVE when they are TIME-INVARIANT, at least when the coefficients have rational angles in the complex planes. All extant SSMs we surveyed (Appendix, Section A) satisfy either NONnEGATIVE or TIME-INVARIANT. Hypothetical SSMs evading both NONNEGATIVE and TIME-INVARIANT would be strictly stronger and can represent not only PARITY, but all regular languages known to be in $\\mathrm{TC}^{0}$ (Theorem 21 . ### 3.5 Which Regular Languages can SSMs Model? We now combine Theorems 1 and 2 to derive an exact characterizations of the regular languages that modern non-time-invariant SSMs such as Mamba can recognize or predictively model - the two notions turn out to coincide here - in the finite-precision setting. The key insight is that $\\mathcal{L}_{F F}$ and PARITY are fundamental building blocks of two classes of regular languages: the star-free languages and their complement, the non-star-free languages [60, 46]:\nDefinition 3. A regular language is star-free if it can be defined using regular expressions involving only the empty set, the empty string, individual symbols, concatenation, and Boolean combinations - avoiding the Kleene star operation. $\\mathcal{L}_{F F}$ is star-free: there is a way to define it without Kleene star. PARITY is not star-free; any regular expression for it must involve the Kleene star. Even languages whose intuitive definition involves the Kleene star may turn out to be star-free. For example, $(01)^{*}$ has an expression avoiding the star ${ }^{1}$\n\n[^0]Equivalently, a language is star-free if it can be defined logically using only first-order quantifiers and the order relation [60]. Also, $\\mathcal{L}$ is non-star-free if and only if recognizing it involves counting modulo some $K$ [46]; PARITY being the simplest example. Modern non-time-invariant SSMs such as Mamba cannot perform modulo counting, but they can model all star-free languages:\n\nTheorem 4. The class of SSMs satisfying NONNEGATIVE can predictively model a regular language $\\mathcal{L}$ at finite precision if and only if $\\mathcal{L}$ is star-free. The proof in Appendix B. 3 uses the Krohn-Rhodes theorem [39] to reduce all star-free languages to flip flop-like state tracking. Importantly, there are well-known constructive criteria for deciding whether a given automaton defines a star-free language [60]; hence, we have a decidable criterion for the finite-state tracking problems that such SSMs satisfying NONNEGATIVE can solve. This is much simpler than the situation for transformers, where an exact characterization of their power within the regular languages is complicated: Angluin et al. [3] show that a certain formal abstraction of transformers (masked unique hard attention) also recognizes exactly the star-free languages, but constructions of realistic transformers via Krohn-Rhodes in Liu et al. [44] do not inherently length generalize, and empirical research indicates difficulty in generalization even for certain simple star-free languages [6, 45]. Known length-generalizing constructions are limited to very simple subclasses such as the piecewise testable languages [71]. In contrast, for SSMs we have a single model per language, at finite precision and for arbitrarily long inputs. Thus, we expect that the SSM architecture confers an advantage in star-free state tracking problems when compared to transformers - a prediction we will find supported experimentally (Figure 3). ### 3.6 Unbounded Counting\n\nHaving characterized the regular languages modeled by SSMs, we now consider languages requiring unbounded counting [20], specifically, languages recognized by keeping track of one or more counters, where each character causes a specific increment or decrement to each counter [38, 27, 70, 40]. A prime example is the Dyck-1 language of well-formed strings over \"(\" and \")\"; here a counter is incremented (decremented) whenever an opening (closing) bracket is encountered; a string is well-formed if and only if the counter is 0 at the end of the string. Some other relevant formal languages are Shuffle-Dyck- $k$ (the shuffles of multiple Dyck-1 languages), $a^{n} b^{n}$ - here, $a$ increments the counter and $b$ decrements it, and $a^{n} b^{n} c^{n}$ - here, there are two counters, one keeping track of $a^{n} b^{n}$ and one of $b^{n} c^{n}$ (See Appendix C.2). Such counter languages are fundamental as basic contextfree (Dyck-1, $a^{n} b^{n}$ ) or context-sensitive (e.g., $a^{n} b^{n} c^{n}$ ) languages [30], and have been the subject of studies of both transformers [6] and RNNs [70]. Many such languages are modeled by SSMs:\n\nTheorem 5. Consider the languages Dyck-1, Shuffle-Dyck-k, n-ary Boolean Expressions, $a^{n} b^{n}$, $a^{n} b^{n} c^{n}$, and $a^{n} b^{n} c^{n} d^{n}$, as defined in Appendix C.2 Then each of these is predictively modeled by an SSM. The proof is in Appendix B.4. Intuitively, an SSM can directly implement the required counters by setting $A \\equiv 1$ and by defining $B\\left(e_{\\sigma}\\right)$ to the be increment or decrement cased by $\\sigma$. In modeling such languages, SSMs pattern with both transformers [6] and LSTMs [70]. It may seem counterintuitive that NONNEGATIVE SSMs can perform unbounded counting but (by Theorem 2 not modular counting-the latter would seem to just require reading out the value of an unbounded counter. What is key is that, even though $h_{t}$ can encode unbounded counts, reading out the modular value of an unbounded integer is a formidable problem for typical neural network nonlinearities, in particular when the information has been pushed through normalization (2). We should note that there is a qualitative difference between this result and the preceding positive results about finite-state languages (Theorems 1 and 4), in that the construction in Theorem 5uses unboundedly large entries in the state $h_{t}$, whereas Theorems 1 and 4 use bounded values at finite precision. Indeed, we will find better length generalization in the finite-state case (Figure 3). A consequence of Theorem 5 is that SSMs can recognize some languages transcending the contextfree languages, as $a^{n} b^{n} c^{n}$ is not context-free. A second application of the theorem, of great linguistic interest, is to bounded hierarchical structure, as we discuss next. ### 3.7 Bounded Hierarchical Structure without Stacks\n\nIt is generally agreed that hierarchical structure is a key aspect of language, and that comprehending language at a human-like level requires the computational ability to process such structures [12, 43, 18]. The fundamental data structure for processing hierarchical structure is the stack, where information is stored and removed as one traverses to higher and lower levels of hierarchical embedding [30]. We now show that SSMs' counting ability can offer shortcuts even on such languages modeling hierarchical structure, eschewing the need for a stack. A useful abstraction of hierarchical structure as relevant to natural language is the family of Dyck languages. The bounded-depth Dyck language $D y c k_{K, h}$ with $K$ types of parentheses and depth $h$ is the language of well-bracketed strings over $(1,)_{1}, \\ldots,(K,)_{K}$, such that the number of yet unclosed brackets never exceeds $h$ in any prefix [28, 74]. The classical unbounded-depth Dyck languages - the limit where $h \\rightarrow \\infty$ - play a fundamental role as the backbone of context-free languages via the ChomskySch\u00fctzenberger theorem [12]. Bounding the depth reflects the fact that deep embedding is rare in natural language [36, 7]. Prior work has found that two-layer transformers [73] and traditional RNNs [28, 6] both model all $D y c k_{K, h}$ languages. The same turns out to hold for SSMs:\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_101ff39892b808f88e98g-07.jpg?height=424&width=442&top_left_y=371&top_left_x=1248)\n\nFigure 2: Test error on the validation set for $\\mathcal{L}_{F F}$, in the setup of Liu et al. [45]. On both the in-distribution (green) and out-of-distribution (orange) setting, Mamba achieves near-zero test error, in agreement with Theorem 1 , and avoiding the failures of transformers observed by Liu et al.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical processing, adaptive learning, memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\n\n*From Search Query: hierarchical processing*\n\n*C. Qi, L. Yi, Hao Su, L. Guibas*\n\n**TL;DR:** A hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set and proposes novel set learning layers to adaptively combine features from multiple scales to learn deep point set features efficiently and robustly.\n\n**Abstract:** Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 9591  (*Influential: 1898*)\n\n#### 2. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: hierarchical processing*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 193  (*Influential: 28*)\n\n#### 3. Compositional Foundation Models for Hierarchical Planning\n\n*From Search Query: hierarchical processing*\n\n*Anurag Ajay, Seung-Jun Han, Yilun Du, Shaung Li, Abhishek Gupta, T. Jaakkola, Josh Tenenbaum, L. Kaelbling, Akash Srivastava, Pulkit Agrawal*\n\n**TL;DR:** A foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks and enforce consistency between the models via iterative refinement is proposed.\n\n**Abstract:** To make effective decisions in novel environments with long-horizon goals, it is crucial to engage in hierarchical reasoning across spatial and temporal scales. This entails planning abstract subgoal sequences, visually reasoning about the underlying plans, and executing actions in accordance with the devised plan through visual-motor control. We propose Compositional Foundation Models for Hierarchical Planning (HiP), a foundation model which leverages multiple expert foundation model trained on language, vision and action data individually jointly together to solve long-horizon tasks. We use a large language model to construct symbolic plans that are grounded in the environment through a large video diffusion model. Generated video plans are then grounded to visual-motor control, through an inverse dynamics model that infers actions from generated videos. To enable effective reasoning within this hierarchy, we enforce consistency between the models via iterative refinement. We illustrate the efficacy and adaptability of our approach in three different long-horizon table-top manipulation tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 40  (*Influential: 8*)\n\n#### 4. MoMo: Momentum Models for Adaptive Learning Rates\n\n*From Search Query: adaptive learning*\n\n*Fabian Schaipp, Ruben Ohana, Michael Eickenberg, Aaron Defazio, Robert Mansel Gower*\n\n**TL;DR:** New Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well are developed.\n\n**Abstract:** Training a modern machine learning architecture on a new task requires extensive learning-rate tuning, which comes at a high computational cost. Here we develop new Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well. We first develop MoMo, a Momentum Model based adaptive learning rate for SGD-M (stochastic gradient descent with momentum). MoMo uses momentum estimates of the losses and gradients sampled at each iteration to build a model of the loss function. Our model makes use of any known lower bound of the loss function by using truncation, e.g. most losses are lower-bounded by zero. The model is then approximately minimized at each iteration to compute the next step. We show how MoMo can be used in combination with any momentum-based method, and showcase this by developing MoMo-Adam, which is Adam with our new model-based adaptive learning rate. We show that MoMo attains a $\\mathcal{O}(1/\\sqrt{K})$ convergence rate for convex problems with interpolation, needing knowledge of no problem-specific quantities other than the optimal value. Additionally, for losses with unknown lower bounds, we develop on-the-fly estimates of a lower bound, that are incorporated in our model. We show that MoMo and MoMo-Adam improve over SGD-M and Adam in terms of robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR, and Imagenet, for recommender systems on Criteo, for a transformer model on the translation task IWSLT14, and for a diffusion model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 5. H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training\n\n*From Search Query: memory optimization*\n\n*Yuzhong Wang, Xu Han, Weilin Zhao, Guoyang Zeng, Zhiyuan Liu, Maosong Sun*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Memory Optimization for Deep Networks\n\n*From Search Query: memory optimization*\n\n*Aashaka Shah, Chaoxia Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Krahenbuhl*\n\n**TL;DR:** MONeT is an automatic framework that minimizes both the memory footprint and computational overhead of deep networks, and is able to outperform all prior hand-tuned operations as well as automated checkpointing.\n\n**Abstract:** Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by 32x over the last five years, the total available memory only grew by 2.5x. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by 3x for various PyTorch models, with a 9-16% overhead in computation. For the same computation cost, MONeT requires 1.2-1.8x less memory than current state-of-the-art automated checkpointing frameworks. Our code is available at this https URL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 21  (*Influential: 0*)\n\n### 5 related papers from ArXiv\n\n#### 1. R2D2: Recursive Transformer based on Differentiable Tree for\n  Interpretable Hierarchical Language Modeling\n\n*From Search Query: hierarchical processing*\n\n*Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, Gerard de Melo*\n\n**Abstract:** Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.\n\n**Published:** 2021-07-02T11:00:46Z  (*Updated: 2022-03-03T05:22:59Z*)\n\n\n\n#### 2. Injecting Hierarchy with U-Net Transformers\n\n*From Search Query: hierarchical processing*\n\n*David Donahue, Vladislav Lialin, Anna Rumshisky*\n\n**Abstract:** The Transformer architecture has become increasingly popular over the past\ntwo years, owing to its impressive performance on a number of natural language\nprocessing (NLP) tasks. However, all Transformer computations occur at the\nlevel of word representations and therefore, it may be argued that Transformer\nmodels do not explicitly attempt to learn hierarchical structure which is\nwidely assumed to be integral to language. In the present work, we introduce\nhierarchical processing into the Transformer model, taking inspiration from the\nU-Net architecture, popular in computer vision for its hierarchical view of\nnatural images. We empirically demonstrate that the proposed architecture\noutperforms both the vanilla Transformer and some strong baselines in the\ndomain of chit-chat dialogue.\n\n**Published:** 2019-10-16T15:48:46Z  (*Updated: 2021-04-01T19:41:09Z*)\n\n\n\n#### 3. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: adaptive learning*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 4. No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for\n  Training Large Transformer Models\n\n*From Search Query: adaptive learning*\n\n*Chen Liang, Haoming Jiang, Simiao Zuo, Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Tuo Zhao*\n\n**Abstract:** Recent research has shown the existence of significant redundancy in large\nTransformer models. One can prune the redundant parameters without\nsignificantly sacrificing the generalization performance. However, we question\nwhether the redundant parameters could have contributed more if they were\nproperly trained. To answer this question, we propose a novel training strategy\nthat encourages all parameters to be trained sufficiently. Specifically, we\nadaptively adjust the learning rate for each parameter according to its\nsensitivity, a robust gradient-based measure reflecting this parameter's\ncontribution to the model performance. A parameter with low sensitivity is\nredundant, and we improve its fitting by increasing its learning rate. In\ncontrast, a parameter with high sensitivity is well-trained, and we regularize\nit by decreasing its learning rate to prevent further overfitting. We conduct\nextensive experiments on natural language understanding, neural machine\ntranslation, and image classification to demonstrate the effectiveness of the\nproposed schedule. Analysis shows that the proposed schedule indeed reduces the\nredundancy and improves generalization performance.\n\n**Published:** 2022-02-06T00:22:28Z  (*Updated: 2022-02-14T15:34:13Z*)\n\n\n\n#### 5. L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models\n\n*From Search Query: memory optimization*\n\n*Hyesung Jeon, Yulhwa Kim, Jae-joon Kim*\n\n**Abstract:** Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically follow a two-step approach: first,\napplying post-training quantization (PTQ) to model weights, followed by PEFT on\nthe quantized model. However, recovering from the quantization error introduced\nby PTQ through fine-tuning has proven challenging. Additionally, most PTQ-based\nPEFT methods result in a mixture of low-precision quantized weights and\nhigh-precision adapter weights, limiting the efficiency of full quantization\nduring inference. While a previous method attempted to address these issues, it\nstill suffers from limited adaptability due to the constrained LoRA parameter\nstructure required to produce fully-quantized models. To overcome these\nchallenges, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA to effectively reduce quantization error. By employing\na memory-optimized layer design, L4Q significantly reduces QAT's memory\noverhead while producing fully-quantized weights, enabling effective adaptation\nto downstream tasks. Our experiments demonstrate that this combined approach to\nquantization and fine-tuning achieves superior accuracy compared to decoupled\nfine-tuning schemes, particularly in sub-4-bit quantization, positioning L4Q as\nan efficient QAT solution. Using the LLaMA model families and instructional\ndatasets, we showcase L4Q's capabilities in language tasks and few-shot\nlearning.\n\n**Published:** 2024-02-07T14:35:05Z  (*Updated: 2024-10-28T04:41:02Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes\n\n*From Search Query: hierarchical processing*\n\n*Joaquim R. R. A. Martins, Joseph Morlier, John T. Hwang, Thierry Lefebvre, Jasper Bussemaker, Youssef Diouane, Nathalie Bartoli, Remi Lafage, Paul Saves*\n\n**Abstract:** The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.\n\n**Published:** 2023-05-23\n\n\n\n#### 2. Hierarchical Neural Memory Network for Low Latency Event Processing\n\n*From Search Query: hierarchical processing*\n\n*Ken Sakurada, Masaki Onishi, Yasutaka Furukawa, Ryuhei Hamaguchi*\n\n**Abstract:** This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/\n\n**Conference:** hierarchical-neural-memory-network-for-low\n\n**Published:** 2023-05-29\n\n\n\n#### 3. On the Variance of the Adaptive Learning Rate and Beyond\n\n*From Search Query: adaptive learning*\n\n*Jianfeng Gao, Weizhu Chen, Xiaodong Liu, Haoming Jiang, Liyuan Liu, Jiawei Han, Pengcheng He*\n\n**Abstract:** The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-08-08\n\n\n\n#### 4. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\n\n*From Search Query: adaptive learning*\n\n*Pieter Abbeel, Sergey Levine, Chelsea Finn*\n\n**Abstract:** We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies.\n\n**Conference:** model-agnostic-meta-learning-for-fast-1\n\n**Published:** 2017-03-09\n\n\n\n#### 5. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n\n*From Search Query: memory optimization*\n\n*Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He*\n\n**Abstract:** Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.\n\n**Published:** 2019-10-04\n\n\n\n#### 6. Memory-Efficient Adaptive Optimization\n\n*From Search Query: memory optimization*\n\n*Yoram Singer, Tomer Koren, Vineet Gupta, Rohan Anil*\n\n**Abstract:** Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of per-parameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-of-the-art.\n\n**Published:** 2019-01-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical processing, adaptive learning, and memory optimization, here are some key insights and references that can be beneficial:\n\n## Hierarchical Processing\n\n### Hierarchical Transformers\nThe concept of hierarchical transformers can significantly enhance the efficiency and performance of language models. For instance, the \"Hourglass\" model, a hierarchical Transformer language model, has shown improvements in language modeling efficiency by processing inputs at multiple scales.\n\n### Hierarchy Encoder Models\nThe \"Hierarchy Transformer encoders (HiTs)\" approach re-trains transformer encoder-based language models to explicitly encode hierarchical structures using hyperbolic space. This method has been shown to outperform standard fine-tuned LMs in tasks such as simulating transitive inference and predicting subsumptions, which are crucial for capturing hierarchical relationships in language.\n\n### Recurrent Attention Networks\nThe Recurrent Attention Network (RAN) model, designed for long-document encoding, incorporates hierarchical processing to extract global semantics at both token-level and document-level representations. This approach can be adapted to enhance multi-scale normalization in autoregressive language models[Analysis Note, Section 2].\n\n## Adaptive Learning\n\n### Adaptive Initialization Strategies\nThe \"Adaptive Universal Generalized PageRank Graph Neural Network\" (GPR-GNN) architecture demonstrates adaptive learning of weights to jointly optimize node feature and topological information extraction. This adaptive learning technique can be applied to develop adaptive initialization strategies for autoregressive language models, improving training stability and model performance[Analysis Note, Section 2].\n\n### Adaptive Multi-scale Refining\nThe \"Scaleformer\" model, which iteratively refines forecasts at multiple scales with shared weights, introduces architecture adaptations and a specially-designed normalization scheme. This approach can inform the design of adaptive multi-scale normalization techniques for autoregressive language models, enhancing their performance with minimal computational overhead[Analysis Note, Section 2].\n\n## Memory Optimization\n\n### MONeT Framework\nThe \"MONeT\" framework is designed to minimize both the memory footprint and computational overhead of deep networks. Techniques from this framework can be applied to optimize memory usage in autoregressive language models, ensuring efficient handling of large models and sequences[Analysis Note, Section 2].\n\n### Bounded-Memory Control\nImplementing bounded-memory control mechanisms can help in optimizing memory usage. This is particularly important for scaling language models efficiently, as it allows for the handling of large models and long sequences without significant memory overhead[Analysis Note, Section 3].\n\n## Additional Considerations\n\n### Hierarchical Neural Codec Language Models\nThe HALL-E model, which incorporates hierarchical tokens and multi-resolution residual vector quantization (MRVQ), demonstrates how hierarchical structures can be used to improve the efficiency and quality of language models, particularly in tasks like text-to-speech synthesis. This approach can inspire similar hierarchical token processing in autoregressive language models.\n\nBy integrating these concepts\u2014hierarchical processing, adaptive learning, and memory optimization\u2014the researcher can develop an autoregressive language model that is more efficient, scalable, and capable of capturing complex hierarchical structures in language.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe exploration of hierarchical normalization techniques, adaptive initialization, and memory optimization has provided several insights that can be leveraged to enhance the HierarchicalRMSNorm design. The focus is on improving multi-scale processing, training stability, and memory efficiency in language models.\n\n#### 2. Useful References with Excerpts\n- **Recurrent Attention Networks for Long-text Modeling**\n  - \"A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations.\"\n  - This reference highlights the importance of hierarchical processing in capturing global semantics, which can be applied to enhance multi-scale normalization.\n\n- **Hierarchical Transformers Are More Efficient Language Models**\n  - \"Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark.\"\n  - The hierarchical structure in Hourglass can inspire efficient multi-scale processing techniques for HierarchicalRMSNorm.\n\n- **Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting**\n  - \"By iteratively refining a forecasted time series at multiple scales with shared weights, introducing architecture adaptations, and a specially-designed normalization scheme, we are able to achieve significant performance improvements.\"\n  - This approach can inform the design of multi-scale normalization techniques that improve performance with minimal computational overhead.\n\n- **Adaptive Universal Generalized PageRank Graph Neural Network**\n  - \"A new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction.\"\n  - The adaptive learning of weights in GPR-GNN can be adapted for adaptive initialization strategies in HierarchicalRMSNorm.\n\n- **Memory Optimization for Deep Networks**\n  - \"MONeT is an automatic framework that minimizes both the memory footprint and computational overhead of deep networks.\"\n  - Techniques from MONeT can be applied to optimize memory usage in HierarchicalRMSNorm, ensuring efficient handling of large models.\n\n#### 3. Key Insights and Detailed Analysis\n- **Hierarchical Processing**: Hierarchical structures, as seen in RAN and Hourglass, are effective in capturing global semantics and improving efficiency. These can be adapted to enhance multi-scale normalization in HierarchicalRMSNorm.\n- **Adaptive Initialization**: The adaptive learning of weights, as demonstrated in GPR-GNN, can be utilized to develop adaptive initialization strategies that improve training stability and model performance.\n- **Memory Optimization**: Techniques from MONeT and bounded-memory control can be leveraged to optimize memory usage, crucial for scaling language models efficiently.\n\n#### 4. Future Search Plan or Next Steps\n- **Investigate Hierarchical Structures**: Further explore hierarchical structures and their application in multi-scale normalization.\n- **Explore Adaptive Techniques**: Look into adaptive learning and initialization techniques that can enhance model adaptability and performance.\n- **Optimize Memory Usage**: Continue researching memory optimization strategies to ensure efficient handling of large models and sequences.\n\n#### 5. List of References\n1. Li, X., et al. (2023). Recurrent Attention Networks for Long-text Modeling. ACL.\n2. Nawrot, P., et al. (2021). Hierarchical Transformers Are More Efficient Language Models. NAACL-HLT.\n3. Shabani, A., et al. (2022). Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting. ICLR.\n4. Chien, E., et al. (2020). Adaptive Universal Generalized PageRank Graph Neural Network. ICLR.\n5. Shah, A., et al. (2020). Memory Optimization for Deep Networks. ICLR."
        },
        {
            "ready": false,
            "query": "hierarchical structures, adaptive learning rates, memory-efficient frameworks",
            "detail": "Hierarchical structures and adaptive learning in language models",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nHierarchical structures and adaptive learning in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.82)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 16/49 (Score: 0.82)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016. [2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2018. [3] David H Bailey. FFTs in external or hierarchical memory. The journal of Supercomputing, 4(1):23-35, 1990 . [4] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. If you use this software, please cite it using these metadata. [5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [6] Willian L Brogan. Modern control theory, 1974. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [8] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches.\n```\n\n#### 2. Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation (Avg. Score: 0.72)\n\n*Xiang Hu, Haitao Mi, Liang Li, Gerard de Melo*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2022)\t**Cited by** 10  (*Influential: 0*)\n\n**TL;DR:** This paper uses a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference and significantly improves the grammar induction quality and achieves competitive results in downstream tasks.\n\n**Abstract:** Chart-based models have shown great potential in unsupervised grammar induction, running recursively and hierarchically, but requiring O(n\u00b3) time-complexity. The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.However, its rule-based pruning process suffers from local optima and slow inference. In this paper, we propose a unified R2D2 method that overcomes these issues. We use a top-down unsupervised parser as a model-guided pruning method, which also enables parallel encoding during inference. Our parser casts parsing as a split point scoring task by first scoring all split points for a given sentence and then using the highest-scoring one to recursively split a span into two parts. The reverse order of the splits is considered as the order of pruning in the encoder. We optimize the unsupervised parser by minimizing the Kullback\u2013Leibler distance between tree probabilities from the parser and the R2D2 model.Our experiments show that our Fast-R2D2 significantly improves the grammar induction quality and achieves competitive results in downstream tasks.\n\n##### *Relevant Chunk: No. 18/26 (Score: 0.75)*\n\n```\nACL. Joshua Goodman. 1998. Parsing inside-out. CoRR, cmp-lg/9805007. Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360, Online. Association for Computational Linguistics. Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics. Phu Mon Htut, Kyunghyun Cho, and Samuel Bowman. 2018. Grammar induction with neural language models: An unusual replication. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4998-5003, Brussels, Belgium. Association for Computational Linguistics. Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, and Gerard de Melo. 2021. R2D2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4897-4908, Online. Association for Computational Linguistics. Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization with gumbel-softmax. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.\n```\n\n##### *Relevant Chunk: No. 14/26 (Score: 0.70)*\n\n```\nSpringer. Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-specific tree structures. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5094-5101. AAAI Press. Noam Chomsky. 1956. Three models for the description of language. IRE Trans. Inf. Theory, 2(3):113124 . Noam Chomsky. 2014. Aspects of the Theory of Syntax, volume 11. MIT press. Jishnu Ray Chowdhury and Cornelia Caragea. 2021. Modeling hierarchical structures with continuous recursive neural networks. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 1975-1988. PMLR. Alexander Clark. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In Proceedings of the ACL 2001 Workshop on Computational Natural Language Learning, CoNLL 2001, Toulouse, France, July 6-7, 2001.\n```\n\n#### 3. Modeling Hierarchical Structures with Continuous Recursive Neural Networks  (Avg. Score: 0.67)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** International Conference on Machine Learning (2021)\t**Cited by** 11  (*Influential: 1*)\n\n**TL;DR:** This work proposes Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations of traditional RvNNs by incorporating a continuous relaxation to the induced structure.\n\n**Abstract:** Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases. However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference and ListOps. We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference.\n\n##### *Relevant Chunk: No. 1/40 (Score: 0.67)*\n\n```\n# Modeling Hierarchical Structures with Continuous Recursive Neural Networks \n\nJishnu Ray Chowdhury ${ }^{1}$ Cornelia Caragea ${ }^{1}$\n\n\n#### Abstract\n\nRecursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases.\n```\n\n#### 4. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.50)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.50)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical structures, adaptive learning rates, memory-efficient frameworks\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\n\n*From Search Query: hierarchical structures*\n\n*C. Qi, L. Yi, Hao Su, L. Guibas*\n\n**TL;DR:** A hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set and proposes novel set learning layers to adaptively combine features from multiple scales to learn deep point set features efficiently and robustly.\n\n**Abstract:** Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 9591  (*Influential: 1898*)\n\n#### 2. Learning Hierarchical Structures with Differentiable Nondeterministic Stacks\n\n*From Search Query: hierarchical structures*\n\n*Brian DuSell, David Chiang*\n\n**TL;DR:** This paper improves the performance of the recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes.\n\n**Abstract:** Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language models. Past work has shown that recurrent neural networks (RNNs) struggle to generalize on held-out algorithmic or syntactic patterns without supervision or some inductive bias. To remedy this, many papers have explored augmenting RNNs with various differentiable stacks, by analogy with finite automata and pushdown automata (PDAs). In this paper, we improve the performance of our recently proposed Nondeterministic Stack RNN (NS-RNN), which uses a differentiable data structure that simulates a nondeterministic PDA, with two important changes. First, the model now assigns unnormalized positive weights instead of probabilities to stack actions, and we provide an analysis of why this improves training. Second, the model can directly observe the state of the underlying PDA. Our model achieves lower cross-entropy than all previous stack RNNs on five context-free language modeling tasks (within 0.05 nats of the information-theoretic lower bound), including a task on which the NS-RNN previously failed to outperform a deterministic stack RNN baseline. Finally, we propose a restricted version of the NS-RNN that incrementally processes infinitely long sequences, and we present language modeling results on the Penn Treebank.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 12  (*Influential: 1*)\n\n#### 3. Modeling Hierarchical Structures with Continuous Recursive Neural Networks\n\n*From Search Query: hierarchical structures*\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**TL;DR:** This work proposes Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations of traditional RvNNs by incorporating a continuous relaxation to the induced structure.\n\n**Abstract:** Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases. However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference and ListOps. We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 12  (*Influential: 1*)\n\n#### 4. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\n\n*From Search Query: adaptive learning rates*\n\n*Noam M. Shazeer, Mitchell Stern*\n\n**TL;DR:** This work demonstrates empirically that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow, and proposes update clipping and a gradually increasing decay rate scheme as remedies.\n\n**Abstract:** In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 901  (*Influential: 159*)\n\n#### 5. MoMo: Momentum Models for Adaptive Learning Rates\n\n*From Search Query: adaptive learning rates*\n\n*Fabian Schaipp, Ruben Ohana, Michael Eickenberg, Aaron Defazio, Robert Mansel Gower*\n\n**TL;DR:** New Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well are developed.\n\n**Abstract:** Training a modern machine learning architecture on a new task requires extensive learning-rate tuning, which comes at a high computational cost. Here we develop new Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well. We first develop MoMo, a Momentum Model based adaptive learning rate for SGD-M (stochastic gradient descent with momentum). MoMo uses momentum estimates of the losses and gradients sampled at each iteration to build a model of the loss function. Our model makes use of any known lower bound of the loss function by using truncation, e.g. most losses are lower-bounded by zero. The model is then approximately minimized at each iteration to compute the next step. We show how MoMo can be used in combination with any momentum-based method, and showcase this by developing MoMo-Adam, which is Adam with our new model-based adaptive learning rate. We show that MoMo attains a $\\mathcal{O}(1/\\sqrt{K})$ convergence rate for convex problems with interpolation, needing knowledge of no problem-specific quantities other than the optimal value. Additionally, for losses with unknown lower bounds, we develop on-the-fly estimates of a lower bound, that are incorporated in our model. We show that MoMo and MoMo-Adam improve over SGD-M and Adam in terms of robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR, and Imagenet, for recommender systems on Criteo, for a transformer model on the translation task IWSLT14, and for a diffusion model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 6. Three Operator Splitting with Subgradients, Stochastic Gradients, and Adaptive Learning Rates\n\n*From Search Query: adaptive learning rates*\n\n*A. Yurtsever, Alex Gu, S. Sra*\n\n**TL;DR:** Two potentially valuable extensions of TOS permit using subgradients and stochastic gradients, and are shown to ensure a $\\mathcal{O}(1/\\sqrt{t})$ convergence rate, and a third extension endows TOS with adaptive step-sizes.\n\n**Abstract:** Three Operator Splitting (TOS) (Davis&Yin, 2017) can minimize the sum of multiple convex functions effectively when an efficient gradient oracle or proximal operator is available for each term. This requirement often fails in machine learning applications: (i) instead of full gradients only stochastic gradients may be available; and (ii) instead of proximal operators, using subgradients to handle complex penalty functions may be more efficient and realistic. Motivated by these concerns, we analyze three potentially valuable extensions of TOS. The first two permit using subgradients and stochastic gradients, and are shown to ensure a $\\mathcal{O}(1/\\sqrt{t})$ convergence rate. The third extension AdapTOS endows TOS with adaptive step-sizes. For the important setting of optimizing a convex loss over the intersection of convex sets AdapTOS attains universal convergence rates, i.e., the rate adapts to the unknown smoothness degree of the objective. We compare our proposed methods with competing methods on various applications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 7. Fourmer: An Efficient Global Modeling Paradigm for Image Restoration\n\n*From Search Query: memory-efficient frameworks*\n\n*Man Zhou, Jie Huang, Chunle Guo, Chongyi Li*\n\n**TL;DR:** The work presents an alternative approach to global modeling that is more efficient for image restoration that achieves competitive performance on common image restoration tasks such as image de-raining, image enhancement, image dehazing, and guided image super-resolution, while requiring fewer computational resources.\n\n**Abstract:** Global modeling-based image restoration frameworks have become popular. However, they often require a high memory footprint and do not consider task-specific degradation. Our work presents an alternative approach to global modeling that is more efficient for image restoration. The key insights which motivate our study are two-fold: 1) Fourier transform is capable of disentangling image degradation and content component to a certain extent, serving as the image degradation prior, and 2) Fourier domain innately embraces global properties, where each pixel in the Fourier space is involved with all spatial pixels. While adhering to the \u201cspatial interaction + channel evo-lution\u201d rule of previous studies, we customize the core designs with Fourier spatial interaction modeling and Fourier channel evolution. Our paradigm, Fourmer, achieves competitive performance on common image restoration tasks such as image de-raining, image enhancement, image dehazing, and guided image super-resolution, while requiring fewer computational resources. The code for Fourmer is publicly available at https://manman1995.github.io/ .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 43  (*Influential: 2*)\n\n#### 8. ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks\n\n*From Search Query: memory-efficient frameworks*\n\n*Jongseok Park, Kyungmin Bin, Gibum Park, Sangtae Ha, Kyunghan Lee*\n\n**TL;DR:** ASEN is a novel parallel computation solution for DNNs that allows fine-grained dynamic execution of DNNs, which removes the operator barriers and expresses DNNs in dataflow graphs of fine-grained tiles to expose the parallel computation opportunities across operators, and exploits these opportunities by dynamically locating and scheduling them in runtime.\n\n**Abstract:** Modern Deep Neural Network (DNN) frameworks use tensor operators as the main building blocks of DNNs. However, we observe that operator-based construction of DNNs incurs significant drawbacks in parallelism in the form of synchronization barriers . Synchronization barriers of operators confine the scope of parallel computation to each operator and obscure the rich parallel computation opportunities that exist across operators. To this end, we present ASPEN, a novel parallel computation solution for DNNs that allows fine-grained dynamic execution of DNNs , which (1) removes the operator barriers and expresses DNNs in dataflow graphs of fine-grained tiles to expose the parallel computation opportunities across operators, and (2) exploits these opportunities by dynamically locating and scheduling them in runtime. This novel approach of ASPEN enables opportunistic parallelism , a new class of parallelism for DNNs that is unavailable in the existing operator-based approaches. ASPEN also achieves high resource utilization and memory reuse by letting each resource asynchronously traverse depthwise in the DNN graph to its full computing potential. We provide challenges and solutions to our approach and show that our proof-of-concept implementation of ASPEN on CPU shows exceptional performance, outperforming state-of-the-art inference systems of TorchScript and TVM by up to 3.2 \u00d7 and 4.3 \u00d7 , respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 9. ED-Batch: Efficient Automatic Batching of Dynamic Neural Networks via Learned Finite State Machines\n\n*From Search Query: memory-efficient frameworks*\n\n*Siyuan Chen, Pratik Fegade, Tianqi Chen, Phillip B. Gibbons, T. Mowry*\n\n**TL;DR:** This paper provides an approach for batching dynamic DNNs based on finite state machines, which enables the automatic discovery of batching policies specialized for each DNN via reinforcement learning and finds that memory planning that is aware of the batching policy can save significant data movement overheads.\n\n**Abstract:** Batching has a fundamental influence on the efficiency of deep neural network (DNN) execution. However, for dynamic DNNs, efficient batching is particularly challenging as the dataflow graph varies per input instance. As a result, state-of-the-art frameworks use heuristics that result in suboptimal batching decisions. Further, batching puts strict restrictions on memory adjacency and can lead to high data movement costs. In this paper, we provide an approach for batching dynamic DNNs based on finite state machines, which enables the automatic discovery of batching policies specialized for each DNN via reinforcement learning. Moreover, we find that memory planning that is aware of the batching policy can save significant data movement overheads, which is automated by a PQ tree-based algorithm we introduce. Experimental results show that our framework speeds up state-of-the-art frameworks by on average 1.15x, 1.39x, and 2.45x for chain-based, tree-based, and lattice-based DNNs across CPU and GPU.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. Learning Syntax Without Planting Trees: Understanding When and Why\n  Transformers Generalize Hierarchically\n\n*From Search Query: hierarchical structures*\n\n*Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A. Smith, Navin Goyal, Yulia Tsvetkov*\n\n**Abstract:** Transformers trained on natural language data have been shown to learn its\nhierarchical structure and generalize to sentences with unseen syntactic\nstructures without explicitly encoding any structural bias. In this work, we\ninvestigate sources of inductive bias in transformer models and their training\nthat could cause such generalization behavior to emerge. We extensively\nexperiment with transformer models trained on multiple synthetic datasets and\nwith different training objectives and show that while other objectives e.g.\nsequence-to-sequence modeling, prefix language modeling, often failed to lead\nto hierarchical generalization, models trained with the language modeling\nobjective consistently learned to generalize hierarchically. We then conduct\npruning experiments to study how transformers trained with the language\nmodeling objective encode hierarchical structure. When pruned, we find joint\nexistence of subnetworks within the model with different generalization\nbehaviors (subnetworks corresponding to hierarchical structure and linear\norder). Finally, we take a Bayesian perspective to further uncover\ntransformers' preference for hierarchical generalization: We establish a\ncorrelation between whether transformers generalize hierarchically on a dataset\nand whether the simplest explanation of that dataset is provided by a\nhierarchical grammar compared to regular grammars exhibiting linear\ngeneralization.\n\n**Published:** 2024-04-25T07:10:29Z  (*Updated: 2024-05-31T23:47:15Z*)\n\n\n\n#### 2. Embedding Text in Hyperbolic Spaces\n\n*From Search Query: hierarchical structures*\n\n*Bhuwan Dhingra, Christopher J. Shallue, Mohammad Norouzi, Andrew M. Dai, George E. Dahl*\n\n**Abstract:** Natural language text exhibits hierarchical structure in a variety of\nrespects. Ideally, we could incorporate our prior knowledge of this\nhierarchical structure into unsupervised learning algorithms that work on text\ndata. Recent work by Nickel & Kiela (2017) proposed using hyperbolic instead of\nEuclidean embedding spaces to represent hierarchical data and demonstrated\nencouraging results when embedding graphs. In this work, we extend their method\nwith a re-parameterization technique that allows us to learn hyperbolic\nembeddings of arbitrarily parameterized objects. We apply this framework to\nlearn word and sentence embeddings in hyperbolic space in an unsupervised\nmanner from text corpora. The resulting embeddings seem to encode certain\nintuitive notions of hierarchy, such as word-context frequency and phrase\nconstituency. However, the implicit continuous hierarchy in the learned\nhyperbolic space makes interrogating the model's learned hierarchies more\ndifficult than for models that learn explicit edges between items. The learned\nhyperbolic embeddings show improvements over Euclidean embeddings in some --\nbut not all -- downstream tasks, suggesting that hierarchical organization is\nmore useful for some tasks than others.\n\n**Published:** 2018-06-12T03:25:15Z  (*Updated: 2018-06-12T03:25:15Z*)\n\n\n\n#### 3. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: adaptive learning rates*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 4. On the Variance of the Adaptive Learning Rate and Beyond\n\n*From Search Query: adaptive learning rates*\n\n*Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han*\n\n**Abstract:** The learning rate warmup heuristic achieves remarkable success in stabilizing\ntraining, accelerating convergence and improving generalization for adaptive\nstochastic optimization algorithms like RMSprop and Adam. Here, we study its\nmechanism in details. Pursuing the theory behind warmup, we identify a problem\nof the adaptive learning rate (i.e., it has problematically large variance in\nthe early stage), suggest warmup works as a variance reduction technique, and\nprovide both empirical and theoretical evidence to verify our hypothesis. We\nfurther propose RAdam, a new variant of Adam, by introducing a term to rectify\nthe variance of the adaptive learning rate. Extensive experimental results on\nimage classification, language modeling, and neural machine translation verify\nour intuition and demonstrate the effectiveness and robustness of our proposed\nmethod. All implementations are available at:\nhttps://github.com/LiyuanLucasLiu/RAdam.\n\n**Published:** 2019-08-08T20:51:17Z  (*Updated: 2021-10-26T02:48:30Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Hierarchical Graph Pooling with Structure Learning\n\n*From Search Query: hierarchical structures*\n\n*Zhen Zhang, Martin Ester, Zhi Yu, Jiajun Bu, Chengwei Yao, Jianfeng Zhang, Can Wang*\n\n**Abstract:** Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.\n\n**Published:** 2019-11-14\n\n\n\n#### 2. A Hierarchical Latent Structure for Variational Conversation Modeling\n\n*From Search Query: hierarchical structures*\n\n*Jaemin Cho, Yookoon Park, Gunhee Kim*\n\n**Abstract:** Variational autoencoders (VAE) combined with hierarchical RNNs have emerged\nas a powerful framework for conversation modeling. However, they suffer from\nthe notorious degeneration problem, where the decoders learn to ignore latent\nvariables and reduce to vanilla RNNs. We empirically show that this degeneracy\noccurs mostly due to two reasons. First, the expressive power of hierarchical\nRNN decoders is often high enough to model the data using only its decoding\ndistributions without relying on the latent variables. Second, the conditional\nVAE structure whose generation process is conditioned on a context, makes the\nrange of training targets very sparse; that is, the RNN decoders can easily\noverfit to the training data ignoring the latent variables. To solve the\ndegeneration problem, we propose a novel model named Variational Hierarchical\nConversation RNNs (VHCR), involving two key ideas of (1) using a hierarchical\nstructure of latent variables, and (2) exploiting an utterance drop\nregularization. With evaluations on two datasets of Cornell Movie Dialog and\nUbuntu Dialog Corpus, we show that our VHCR successfully utilizes latent\nvariables and outperforms state-of-the-art models for conversation generation.\nMoreover, it can perform several new utterance control tasks, thanks to its\nhierarchical latent structure.\n\n**Conference:** a-hierarchical-latent-structure-for-1\n\n**Published:** 2018-04-10\n\n\n\n#### 3. Adaptive Gradient Methods with Dynamic Bound of Learning Rate\n\n*From Search Query: adaptive learning rates*\n\n*Xu sun, Liangchen Luo, Yuanhao Xiong, Yan Liu*\n\n**Abstract:** Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been\nproposed to achieve a rapid training process with an element-wise scaling term\non learning rates. Though prevailing, they are observed to generalize poorly\ncompared with SGD or even fail to converge due to unstable and extreme learning\nrates. Recent work has put forward some algorithms such as AMSGrad to tackle\nthis issue but they failed to achieve considerable improvement over existing\nmethods. In our paper, we demonstrate that extreme learning rates can lead to\npoor performance. We provide new variants of Adam and AMSGrad, called AdaBound\nand AMSBound respectively, which employ dynamic bounds on learning rates to\nachieve a gradual and smooth transition from adaptive methods to SGD and give a\ntheoretical proof of convergence. We further conduct experiments on various\npopular tasks and models, which is often insufficient in previous work.\nExperimental results show that new variants can eliminate the generalization\ngap between adaptive methods and SGD and maintain higher learning speed early\nin training at the same time. Moreover, they can bring significant improvement\nover their prototypes, especially on complex deep networks. The implementation\nof the algorithm can be found at https://github.com/Luolc/AdaBound .\n\n**Conference:** adaptive-gradient-methods-with-dynamic-bound-1\n\n**Published:** 2019-02-26\n\n\n\n#### 4. A Graph Theoretic Framework of Recomputation Algorithms for Memory-Efficient Backpropagation\n\n*From Search Query: memory-efficient frameworks*\n\n*Takuya Akiba, Gentaro Watanabe, Mitsuru Kusumoto, Takuya Inoue, Masanori Koyama*\n\n**Abstract:** Recomputation algorithms collectively refer to a family of methods that aims to reduce the memory consumption of the backpropagation by selectively discarding the intermediate results of the forward propagation and recomputing the discarded results as needed. In this paper, we will propose a novel and efficient recomputation method that can be applied to a wider range of neural nets than previous methods. We use the language of graph theory to formalize the general recomputation problem of minimizing the computational overhead under a fixed memory budget constraint, and provide a dynamic programming solution to the problem. Our method can reduce the peak memory consumption on various benchmark networks by 36%~81%, which outperforms the reduction achieved by other methods.\n\n**Conference:** a-graph-theoretic-framework-of-recomputation-1\n\n**Published:** 2019-05-28\n\n\n\n#### 5. CooGAN: A Memory-Efficient Framework for High-Resolution Facial Attribute Editing\n\n*From Search Query: memory-efficient frameworks*\n\n*Qi Tian, Loc Truong, Yiliu Jiang, Ziang Liu, Naiyuan Liu, Bingbing Ni, Xuanhong Chen*\n\n**Abstract:** In contrast to great success of memory-consuming face editing methods at a low resolution, to manipulate high-resolution (HR) facial images, i.e., typically larger than 7682 pixels, with very limited memory is still challenging. This is due to the reasons of 1) intractable huge demand of memory; 2) inefficient multi-scale features fusion. To address these issues, we propose a NOVEL pixel translation framework called Cooperative GAN(CooGAN) for HR facial image editing. This framework features a local path for fine-grained local facial patch generation (i.e., patch-level HR, LOW memory) and a global path for global lowresolution (LR) facial structure monitoring (i.e., image-level LR, LOW memory), which largely reduce memory requirements. Both paths work in a cooperative manner under a local-to-global consistency objective (i.e., for smooth stitching). In addition, we propose a lighter selective transfer unit for more efficient multi-scale features fusion, yielding higher fidelity facial attributes manipulation. Extensive experiments on CelebAHQ well demonstrate the memory efficiency as well as the high image generation quality of the proposed framework.\n\n**Conference:** coogan-a-memory-efficient-framework-for-high\n\n**Published:** 2020-11-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on hierarchical structures, adaptive learning rates, and memory-efficient frameworks, here are some key insights and relevant information from the provided sources and additional context:\n\n## Hierarchical Structures\n- **Recursive Neural Networks (RvNNs)**: As mentioned in the analysis, RvNNs compose sequences according to their underlying hierarchical syntactic structure, which can be beneficial for capturing complex syntactic and semantic relationships in language models. This concept can be adapted to enhance multi-scale normalization in hierarchical models.\n- **Hierarchical Mixture of Experts (MoE)**: The paper on Hierarchical MoE discusses a framework that could be generalized to various tasks, including language modeling. This hierarchical approach can help in distributing the workload efficiently across different experts, potentially improving the model's ability to handle complex and diverse data.\n\n## Adaptive Learning Rates\n- **Cybersecurity Optimizer (CSO)**: Although the CSO is primarily discussed in the context of cybersecurity, its principles of integrating evolutionary computation with modern AI techniques to achieve faster convergence and better solution quality can be applied to optimize learning rates in language models. This adaptive optimization method could enhance training stability and model performance.\n- **Efficient Deep Learning Optimizers**: The guide on advances in deep learning efficiency highlights the importance of optimizers like Adam and AdamW, which are widely used in large language model training. However, it also suggests that optimizing these algorithms further to reduce memory load and improve convergence speed could be beneficial. This includes exploring new optimization techniques that balance exploration and exploitation efficiently.\n\n## Memory-Efficient Frameworks\n- **State Space Models (SSMs)**: SSMs are noted for their linear decoding efficiency and high parallelizability during training, which can be crucial for memory optimization. Incorporating SSMs or similar state-space approaches into hierarchical normalization techniques could help in reducing memory usage while maintaining or improving model performance.\n- **Memory Optimization in Deep Learning**: The guide on deep learning efficiency discusses several strategies for memory optimization, such as model pruning (e.g., the lottery ticket hypothesis) and efficient use of memory during training. These strategies can be applied to reduce the memory burden of large language models, making them more scalable.\n\n## Additional Considerations\n- **Transformer-XL and Segment-Level Recurrence**: The Transformer-XL model's segment-level recurrence mechanism allows it to learn dependencies beyond a fixed length without disrupting temporal coherence. This mechanism can inspire efficient handling of long-range dependencies in hierarchical normalization, which is crucial for autoregressive language models.\n- **Hierarchical Quantitative Analysis**: While the paper on hierarchical quantitative analysis of cerebrovascular systems is not directly related to language models, it demonstrates the use of hierarchical structures in analyzing complex data. This approach can be metaphorically applied to understand how hierarchical processing can be optimized in language models to handle multi-scale data efficiently.\n\nBy integrating these concepts, researchers can design autoregressive language models that leverage hierarchical structures for better syntactic and semantic understanding, employ adaptive learning rates for improved training stability and performance, and utilize memory-efficient frameworks to ensure scalability and efficiency.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe exploration of hierarchical processing, adaptive learning, and memory optimization has provided several insights that can be leveraged to enhance the HierarchicalRMSNorm design. The focus is on improving multi-scale processing, training stability, and memory efficiency in language models. Hierarchical structures, adaptive learning rates, and memory-efficient frameworks are key areas of interest.\n\n#### 2. Useful References with Excerpts\n- **Modeling Hierarchical Structures with Continuous Recursive Neural Networks**\n  - \"Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases.\"\n  - This reference highlights the importance of hierarchical processing in capturing syntactic structures, which can be applied to enhance multi-scale normalization.\n\n- **Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation**\n  - \"The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.\"\n  - This approach can inform the design of hierarchical structures that improve efficiency and scalability.\n\n- **Longhorn: State Space Models are Amortized Online Learners**\n  - \"State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training.\"\n  - Insights from SSMs can be used to enhance the efficiency of multi-scale processing in HierarchicalRMSNorm.\n\n- **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context**\n  - \"Transformer-XL enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\"\n  - The segment-level recurrence mechanism can inspire efficient handling of long-range dependencies in hierarchical normalization.\n\n- **The Expressive Capacity of State Space Models: A Formal Language Perspective**\n  - \"SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\"\n  - Understanding the strengths and limitations of SSMs can guide the development of more expressive hierarchical normalization techniques.\n\n#### 3. Key Insights and Detailed Analysis\n- **Hierarchical Processing**: Hierarchical structures, as seen in RvNNs and R2D2, are effective in capturing syntactic structures and improving efficiency. These can be adapted to enhance multi-scale normalization in HierarchicalRMSNorm.\n- **Adaptive Learning**: Techniques like adaptive learning rates and adaptive initialization can improve training stability and model performance, as demonstrated in GPR-GNN and MoMo.\n- **Memory Optimization**: Frameworks like MONeT and bounded-memory control mechanisms can optimize memory usage, crucial for scaling language models efficiently.\n\n#### 4. Future Search Plan or Next Steps\n- **Investigate Hierarchical Structures**: Further explore hierarchical structures and their application in multi-scale normalization.\n- **Explore Adaptive Techniques**: Look into adaptive learning and initialization techniques that can enhance model adaptability and performance.\n- **Optimize Memory Usage**: Continue researching memory optimization strategies to ensure efficient handling of large models and sequences.\n\n#### 5. List of References\n1. Chowdhury, J. R., & Caragea, C. (2021). Modeling Hierarchical Structures with Continuous Recursive Neural Networks. ICML.\n2. Hu, X., et al. (2022). Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation. EMNLP.\n3. Liu, B., et al. (2024). Longhorn: State Space Models are Amortized Online Learners.\n4. Dai, Z., et al. (2019). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. ACL.\n5. Sarrof, Y., et al. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv."
        },
        {
            "ready": true,
            "query": "state space models, recursive neural networks, adaptive learning rates",
            "detail": "State space models and adaptive learning in language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nState space models and adaptive learning in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.99)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.99)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.99)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.99)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n#### 3. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.98)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 13/32 (Score: 0.98)*\n\n```\n$A C L, 2019$. [EUD17] Stefan Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural Networks, 2017. [FDS ${ }^{+}$23] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [FZS22] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022. [GB22] Ankit Gupta and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 4. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.98)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 7/20 (Score: 0.98)*\n\n```\nSiivola and A. Honkela. A state-space method for language modeling. In 2003 IEEE Workshop on Automatic Speech Recognition and Understanding (IEEE Cat. No.03EX721), pages 548-553, St Thomas, VI, USA, 2003. IEEE. ISBN 978-0-7803-7980-0. doi: 10.1109/ ASRU.2003.1318499. [2] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections.\n```\n\n#### 5. Spectral State Space Models (Avg. Score: 0.98)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.98)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: state space models, recursive neural networks, adaptive learning rates\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 2. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: state space models*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 3. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: state space models*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 216  (*Influential: 38*)\n\n#### 4. Parsing Natural Scenes and Natural Language with Recursive Neural Networks\n\n*From Search Query: recursive neural networks*\n\n*R. Socher, Cliff Chiung-Yu Lin, A. Ng, Christopher D. Manning*\n\n**TL;DR:** A max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences is introduced.\n\n**Abstract:** Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2011\n\n**Citations:** 1451  (*Influential: 104*)\n\n#### 5. Recursive Neural Networks with Bottlenecks Diagnose (Non-)Compositionality\n\n*From Search Query: recursive neural networks*\n\n*Verna Dankers, Ivan Titov*\n\n**TL;DR:** This work demonstrates that compression through a bottleneck impacts non-compositional examples disproportionately and uses the bottleneck compositionality metric (BCM) to distinguish compositional from non-Compositional samples, yielding a compositionality ranking over a dataset.\n\n**Abstract:** A recent line of work in NLP focuses on the (dis)ability of models to generalise compositionally for artificial languages. However, when considering natural language tasks, the data involved is not strictly, or locally, compositional. Quantifying the compositionality of data is a challenging task, which has been investigated primarily for short utterances. We use recursive neural models (Tree-LSTMs) with bottlenecks that limit the transfer of information between nodes. We illustrate that comparing data's representations in models with and without the bottleneck can be used to produce a compositionality metric. The procedure is applied to the evaluation of arithmetic expressions using synthetic data, and sentiment classification using natural language data. We demonstrate that compression through a bottleneck impacts non-compositional examples disproportionately and then use the bottleneck compositionality metric (BCM) to distinguish compositional from non-compositional samples, yielding a compositionality ranking over a dataset.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 1*)\n\n#### 6. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\n\n*From Search Query: adaptive learning rates*\n\n*Noam M. Shazeer, Mitchell Stern*\n\n**TL;DR:** This work demonstrates empirically that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow, and proposes update clipping and a gradually increasing decay rate scheme as remedies.\n\n**Abstract:** In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 901  (*Influential: 159*)\n\n#### 7. MoMo: Momentum Models for Adaptive Learning Rates\n\n*From Search Query: adaptive learning rates*\n\n*Fabian Schaipp, Ruben Ohana, Michael Eickenberg, Aaron Defazio, Robert Mansel Gower*\n\n**TL;DR:** New Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well are developed.\n\n**Abstract:** Training a modern machine learning architecture on a new task requires extensive learning-rate tuning, which comes at a high computational cost. Here we develop new Polyak-type adaptive learning rates that can be used on top of any momentum method, and require less tuning to perform well. We first develop MoMo, a Momentum Model based adaptive learning rate for SGD-M (stochastic gradient descent with momentum). MoMo uses momentum estimates of the losses and gradients sampled at each iteration to build a model of the loss function. Our model makes use of any known lower bound of the loss function by using truncation, e.g. most losses are lower-bounded by zero. The model is then approximately minimized at each iteration to compute the next step. We show how MoMo can be used in combination with any momentum-based method, and showcase this by developing MoMo-Adam, which is Adam with our new model-based adaptive learning rate. We show that MoMo attains a $\\mathcal{O}(1/\\sqrt{K})$ convergence rate for convex problems with interpolation, needing knowledge of no problem-specific quantities other than the optimal value. Additionally, for losses with unknown lower bounds, we develop on-the-fly estimates of a lower bound, that are incorporated in our model. We show that MoMo and MoMo-Adam improve over SGD-M and Adam in terms of robustness to hyperparameter tuning for training image classifiers on MNIST, CIFAR, and Imagenet, for recommender systems on Criteo, for a transformer model on the translation task IWSLT14, and for a diffusion model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 8. Three Operator Splitting with Subgradients, Stochastic Gradients, and Adaptive Learning Rates\n\n*From Search Query: adaptive learning rates*\n\n*A. Yurtsever, Alex Gu, S. Sra*\n\n**TL;DR:** Two potentially valuable extensions of TOS permit using subgradients and stochastic gradients, and are shown to ensure a $\\mathcal{O}(1/\\sqrt{t})$ convergence rate, and a third extension endows TOS with adaptive step-sizes.\n\n**Abstract:** Three Operator Splitting (TOS) (Davis&Yin, 2017) can minimize the sum of multiple convex functions effectively when an efficient gradient oracle or proximal operator is available for each term. This requirement often fails in machine learning applications: (i) instead of full gradients only stochastic gradients may be available; and (ii) instead of proximal operators, using subgradients to handle complex penalty functions may be more efficient and realistic. Motivated by these concerns, we analyze three potentially valuable extensions of TOS. The first two permit using subgradients and stochastic gradients, and are shown to ensure a $\\mathcal{O}(1/\\sqrt{t})$ convergence rate. The third extension AdapTOS endows TOS with adaptive step-sizes. For the important setting of optimizing a convex loss over the intersection of convex sets AdapTOS attains universal convergence rates, i.e., the rate adapts to the unknown smoothness degree of the objective. We compare our proposed methods with competing methods on various applications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 6  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. LOCOST: State-Space Models for Long Document Abstractive Summarization\n\n*From Search Query: state space models*\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n\n**Published:** 2024-01-31T15:33:37Z  (*Updated: 2024-03-25T12:52:42Z*)\n\n\n\n#### 2. Mimetic Initialization Helps State Space Models Learn to Recall\n\n*From Search Query: state space models*\n\n*Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, Srinadh Bhojanapalli*\n\n**Abstract:** Recent work has shown that state space models such as Mamba are significantly\nworse than Transformers on recall-based tasks due to the fact that their state\nsize is constant with respect to their input sequence length. But in practice,\nstate space models have fairly large state sizes, and we conjecture that they\nshould be able to perform much better at these tasks than previously reported.\nWe investigate whether their poor copying and recall performance could be due\nin part to training difficulties rather than fundamental capacity constraints.\nBased on observations of their \"attention\" maps, we propose a structured\ninitialization technique that allows state space layers to more readily mimic\nattention. Across a variety of architecture settings, our initialization makes\nit substantially easier for Mamba to learn to copy and do associative recall\nfrom scratch.\n\n**Published:** 2024-10-14T23:17:46Z  (*Updated: 2024-10-14T23:17:46Z*)\n\n\n\n#### 3. Feature Weight Tuning for Recursive Neural Networks\n\n*From Search Query: recursive neural networks*\n\n*Jiwei Li*\n\n**Abstract:** This paper addresses how a recursive neural network model can automatically\nleave out useless information and emphasize important evidence, in other words,\nto perform \"weight tuning\" for higher-level representation acquisition. We\npropose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural\nNetwork (BENN), which automatically control how much one specific unit\ncontributes to the higher-level representation. The proposed model can be\nviewed as incorporating a more powerful compositional function for embedding\nacquisition in recursive neural networks. Experimental results demonstrate the\nsignificant improvement over standard neural models.\n\n**Published:** 2014-12-11T16:35:27Z  (*Updated: 2014-12-13T00:57:57Z*)\n\n\n\n#### 4. Improving the Expressiveness of Deep Learning Frameworks with Recursion\n\n*From Search Query: recursive neural networks*\n\n*Eunji Jeong, Joo Seong Jeong, Soojeong Kim, Gyeong-In Yu, Byung-Gon Chun*\n\n**Abstract:** Recursive neural networks have widely been used by researchers to handle\napplications with recursively or hierarchically structured data. However,\nembedded control flow deep learning frameworks such as TensorFlow, Theano,\nCaffe2, and MXNet fail to efficiently represent and execute such neural\nnetworks, due to lack of support for recursion. In this paper, we add recursion\nto the programming model of existing frameworks by complementing their design\nwith recursive execution of dataflow graphs as well as additional APIs for\nrecursive definitions. Unlike iterative implementations, which can only\nunderstand the topological index of each node in recursive data structures, our\nrecursive implementation is able to exploit the recursive relationships between\nnodes for efficient execution based on parallel computation. We present an\nimplementation on TensorFlow and evaluation results with various recursive\nneural network models, showing that our recursive implementation not only\nconveys the recursive nature of recursive neural networks better than other\nimplementations, but also uses given resources more effectively to reduce\ntraining and inference time.\n\n**Published:** 2018-09-04T08:31:21Z  (*Updated: 2018-09-04T08:31:21Z*)\n\n\n\n#### 5. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: adaptive learning rates*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 6. On the Variance of the Adaptive Learning Rate and Beyond\n\n*From Search Query: adaptive learning rates*\n\n*Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han*\n\n**Abstract:** The learning rate warmup heuristic achieves remarkable success in stabilizing\ntraining, accelerating convergence and improving generalization for adaptive\nstochastic optimization algorithms like RMSprop and Adam. Here, we study its\nmechanism in details. Pursuing the theory behind warmup, we identify a problem\nof the adaptive learning rate (i.e., it has problematically large variance in\nthe early stage), suggest warmup works as a variance reduction technique, and\nprovide both empirical and theoretical evidence to verify our hypothesis. We\nfurther propose RAdam, a new variant of Adam, by introducing a term to rectify\nthe variance of the adaptive learning rate. Extensive experimental results on\nimage classification, language modeling, and neural machine translation verify\nour intuition and demonstrate the effectiveness and robustness of our proposed\nmethod. All implementations are available at:\nhttps://github.com/LiyuanLucasLiu/RAdam.\n\n**Published:** 2019-08-08T20:51:17Z  (*Updated: 2021-10-26T02:48:30Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space models*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n#### 3. Rumor Detection on Twitter with Tree-structured Recursive Neural Networks\n\n*From Search Query: recursive neural networks*\n\n*Kam-Fai Wong, Wei Gao, Jing Ma*\n\n**Abstract:** Automatic rumor detection is technically very challenging. In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors. We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets. Results on two public Twitter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage.\n\n**Proceeding:** acl-2018-7\n\n**Published:** 2018-07-01\n\n\n\n#### 4. Template-based math word problem solvers with recursive neural networks\n\n*From Search Query: recursive neural networks*\n\n*Heng Tao Shen, Bing Tian Dai, Lianli Gao, Xing Xu, Jipeng Zhang, Dongxiang Zhang, Lei Wang*\n\n**Abstract:** The design of automatic solvers to arithmetic math word problems has attracted considerable attention in recent years and a large number of datasets and methods have been published. Among them, Math23K is the largest data corpus that is very helpful to evaluate the generality and robustness of a proposed solution. The best performer in Math23K is a seq2seq model based on LSTM to generate the math expression. However, the model suffers from performance degradation in large space of target expressions. In this paper, we propose a template-based solution based on recursive neural network for math expression construction. More specifically, we first apply a seq2seq model to predict a tree-structure template, with inferred numbers as leaf nodes and unknown operators as inner nodes. Then, we design a recursive neural network to encode the quantity with Bi-LSTM and self attention, and infer the unknown operator nodes in a bottom-up manner. The experimental results clearly establish the superiority of our new framework as we improve the accuracy by a wide margin in two of the largest datasets, ie, from 58. 1% to 66. 9% in Math23K and from 62. 8% to 66. 8% in MAWPS.\n\n**Proceeding:** aaai-2019-7\n\n**Published:** 2019-07-17\n\n\n\n#### 5. Adaptive Gradient Methods with Dynamic Bound of Learning Rate\n\n*From Search Query: adaptive learning rates*\n\n*Xu sun, Liangchen Luo, Yuanhao Xiong, Yan Liu*\n\n**Abstract:** Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been\nproposed to achieve a rapid training process with an element-wise scaling term\non learning rates. Though prevailing, they are observed to generalize poorly\ncompared with SGD or even fail to converge due to unstable and extreme learning\nrates. Recent work has put forward some algorithms such as AMSGrad to tackle\nthis issue but they failed to achieve considerable improvement over existing\nmethods. In our paper, we demonstrate that extreme learning rates can lead to\npoor performance. We provide new variants of Adam and AMSGrad, called AdaBound\nand AMSBound respectively, which employ dynamic bounds on learning rates to\nachieve a gradual and smooth transition from adaptive methods to SGD and give a\ntheoretical proof of convergence. We further conduct experiments on various\npopular tasks and models, which is often insufficient in previous work.\nExperimental results show that new variants can eliminate the generalization\ngap between adaptive methods and SGD and maintain higher learning speed early\nin training at the same time. Moreover, they can bring significant improvement\nover their prototypes, especially on complex deep networks. The implementation\nof the algorithm can be found at https://github.com/Luolc/AdaBound .\n\n**Conference:** adaptive-gradient-methods-with-dynamic-bound-1\n\n**Published:** 2019-02-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on state space models, adaptive learning rates, and memory-efficient frameworks, here are some key insights and references that can be beneficial:\n\n## State Space Models (SSMs)\n\n- **Efficiency and Performance**: State space models (SSMs) have shown promising results in language modeling, especially in handling long-context tasks efficiently. Models like Mamba and its variants (e.g., Mamba-2) offer constant memory usage and linear scaling in sequence length, which is a significant improvement over Transformers.\n- **Hybrid Architectures**: The introduction of hybrid architectures, such as Taipan, which combines Mamba-2 with Selective Attention Layers (SALs), balances the efficiency of SSMs with the performance of Transformers. This approach allows for accurate predictions over long context lengths while maintaining computational efficiency.\n- **Comparative Performance**: SSMs have been shown to outperform Transformers in tasks requiring long-context understanding, such as those in the Long Range Arena (LRA) benchmarks. This suggests that SSMs could be a viable alternative for tasks where Transformers are limited by their quadratic computational complexity.\n\n## Adaptive Learning Rates\n\n- **Adafactor and Similar Techniques**: Adaptive learning rate methods like Adafactor can significantly improve training stability and model performance. These methods adjust learning rates based on the gradient scale, which helps in avoiding large updates and ensures more stable training. This can be particularly useful in fine-tuning SSMs.\n- **Parameter-Efficient Fine-Tuning**: Research on parameter-efficient fine-tuning (PEFT) methods for SSMs, such as LoRA and its variants (e.g., SDLoRA), has shown that these methods can effectively tune SSM-based models without modifying the core SSM modules. This approach enhances the adaptability and performance of SSMs.\n\n## Hierarchical Structures and Recursive Neural Networks\n\n- **Recursive Neural Networks (RvNNs)**: RvNNs and their variants, such as Fast-R2D2, have demonstrated the importance of hierarchical processing in capturing syntactic structures. These models can be adapted to enhance multi-scale normalization in language models, improving their efficiency and scalability.\n- **Hierarchical Processing in SSMs**: While SSMs are not inherently hierarchical, incorporating hierarchical structures or using layers like Hungry Hungry Hippos (H3) can enhance their performance in language modeling tasks. H3 layers are designed to impact language modeling positively and have shown promising results in zero- and few-shot learning.\n\n## Memory Optimization\n\n- **Linear Scaling**: SSMs offer linear decoding efficiency and high parallelizability during training, which is crucial for optimizing memory usage. This makes them more scalable than Transformers, especially for long-context tasks.\n- **Efficient Fine-Tuning**: Methods like LoRA and SDLoRA for fine-tuning SSMs are designed to be memory-efficient, allowing for effective tuning without significantly increasing the model's parameter count or memory requirements.\n\n## Future Directions\n\n- **Hybrid Models**: Further exploration of hybrid architectures that combine the strengths of SSMs and other models (like Transformers or RvNNs) could lead to more efficient and expressive language models.\n- **Adaptive Techniques**: Investigating adaptive learning rate methods and their integration with SSMs can enhance training stability and model adaptability.\n- **Memory-Efficient Designs**: Continuing research on memory optimization strategies, such as those used in SSMs, will be crucial for scaling language models efficiently.\n\nBy integrating these insights, researchers can develop more efficient, scalable, and performant autoregressive language models that leverage the strengths of state space models, adaptive learning rates, and hierarchical structures.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe exploration of hierarchical structures, adaptive learning rates, and memory-efficient frameworks has provided several insights that can be leveraged to enhance the HierarchicalRMSNorm design. The focus is on improving multi-scale processing, training stability, and memory efficiency in language models. Hierarchical structures, adaptive learning rates, and memory-efficient frameworks are key areas of interest.\n\n#### 2. Useful References with Excerpts\n- **Hungry Hungry Hippos: Towards Language Modeling with State Space Models**\n  - \"A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\"\n  - This reference highlights the potential of state space models in improving language modeling efficiency and performance, which can be applied to enhance multi-scale normalization.\n\n- **Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation**\n  - \"The Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pretraining even with a complex tree encoder, by introducing a heuristic pruning method.\"\n  - This approach can inform the design of hierarchical structures that improve efficiency and scalability.\n\n- **Modeling Hierarchical Structures with Continuous Recursive Neural Networks**\n  - \"Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases.\"\n  - This reference highlights the importance of hierarchical processing in capturing syntactic structures, which can be applied to enhance multi-scale normalization.\n\n- **Longhorn: State Space Models are Amortized Online Learners**\n  - \"State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training.\"\n  - Insights from SSMs can be used to enhance the efficiency of multi-scale processing in HierarchicalRMSNorm.\n\n- **Adafactor: Adaptive Learning Rates with Sublinear Memory Cost**\n  - \"This work demonstrates empirically that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow, and proposes update clipping and a gradually increasing decay rate scheme as remedies.\"\n  - This reference provides insights into adaptive learning rates, which can enhance training stability and model performance.\n\n#### 3. Key Insights and Detailed Analysis\n- **Hierarchical Processing**: Hierarchical structures, as seen in RvNNs and R2D2, are effective in capturing syntactic structures and improving efficiency. These can be adapted to enhance multi-scale normalization in HierarchicalRMSNorm.\n- **Adaptive Learning Rates**: Techniques like adaptive learning rates and adaptive initialization can improve training stability and model performance. Adafactor's approach to adaptive learning rates can be particularly useful.\n- **Memory Optimization**: State space models and memory-efficient frameworks can optimize memory usage, crucial for scaling language models efficiently.\n\n#### 4. Future Search Plan or Next Steps\n- **Investigate Hierarchical Structures**: Further explore hierarchical structures and their application in multi-scale normalization.\n- **Explore Adaptive Techniques**: Look into adaptive learning and initialization techniques that can enhance model adaptability and performance.\n- **Optimize Memory Usage**: Continue researching memory optimization strategies to ensure efficient handling of large models and sequences.\n\n#### 5. List of References\n1. Dao, T., et al. (2022). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. ICLR.\n2. Hu, X., et al. (2022). Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation. EMNLP.\n3. Chowdhury, J. R., & Caragea, C. (2021). Modeling Hierarchical Structures with Continuous Recursive Neural Networks. ICML.\n4. Liu, B., et al. (2024). Longhorn: State Space Models are Amortized Online Learners.\n5. Shazeer, N. M., & Stern, M. (2018). Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. ICML."
        },
        {
            "ready": false,
            "query": "adaptive normalization training stability memory optimization",
            "detail": "Find papers discussing:\n1. Adaptive normalization techniques specifically in language models\n2. Training stability methods for hierarchical neural networks\n3. Memory optimization strategies in deep learning\n4. Integration of normalization with state space models\n5. Empirical studies on normalization in large language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Adaptive normalization techniques specifically in language models\n2. Training stability methods for hierarchical neural networks\n3. Memory optimization strategies in deep learning\n4. Integration of normalization with state space models\n5. Empirical studies on normalization in large language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Normalized Attention Without Probability Cage (Avg. Score: 0.82)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.82)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.78)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 27/74 (Score: 0.78)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: The International Conference on Learning Representations (ICLR). 2015. [6] David Balduzzi and Muhammad Ghifary. \"Strongly-typed Recurrent Neural Networks\". In: International Conference on Machine Learning. PMLR. 2016, pp. 1292-1300. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. \"Pythia: A Suite for Analyzing Large Language Models across Training and Scaling\".\n```\n\n#### 3. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.73)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.73)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 4. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.46)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.46)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 5. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.43)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 9/97 (Score: 0.43)*\n\n```\nIn D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 4331-4339. Curran Associates, Inc., 2016a. J. Ba, J. R. Kiros, and G. Hinton. Layer normalization. ArXiv, 1607.06450, 2016 b. A. Bau, Y. Belinkov, H. Sajjad, N. Durrani, F. Dalvi, and J. Glass. Identifying and controlling important neurons in neural machine translation. In International Conference on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?id=H1z-PsR5KX. Y. Bisk, R. Zellers, R. LeBras, J. Gao, and Y. Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, volume 34, pp. 7432-7439, 2020. S. L. Blodgett, L. Green, and B. O'Connor. Demographic dialectal variation in social media: A case study of African-American English. In Conference on Empirical Methods in Natural Language Processing, pp. 1119-1130, 2016. doi: 10.18653/v1/D16-1120. T. Brown, B. Mann, N. Ryder, et al. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.\n```\n\n\n\n---\n## Found 5 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive normalization training stability memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 3 related papers from Semantic Scholar\n\n#### 1. CAME: Confidence-guided Adaptive Memory Efficient Optimization\n\n*From Search Query: adaptive normalization training stability memory optimization*\n\n*Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, Yang You*\n\n**TL;DR:** This paper proposes CAME, a confidence-guided strategy to reduce the instability of existing memory efficient optimizers and proposes it to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods.\n\n**Abstract:** Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 2. Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions\n\n*From Search Query: adaptive normalization training stability memory optimization*\n\n*Vladimir Feinberg, Xinyi Chen, Y. Sun, Rohan Anil, Elad Hazan*\n\n**TL;DR:** A novel analysis which allows efficient interpolation between resource requirements and the degradation in regret guarantees with rank $k$ is presented, which matches full-matrix $d^2$ memory regret using only $dk$ memory up to additive error in the bottom eigenvalues of the gradient covariance.\n\n**Abstract:** Adaptive regularization methods that exploit more than the diagonal entries exhibit state of the art performance for many tasks, but can be prohibitive in terms of memory and running time. We find the spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace that changes throughout training, motivating a low-rank sketching approach. We describe a generic method for reducing memory and compute requirements of maintaining a matrix preconditioner using the Frequent Directions (FD) sketch. While previous approaches have explored applying FD for second-order optimization, we present a novel analysis which allows efficient interpolation between resource requirements and the degradation in regret guarantees with rank $k$: in the online convex optimization (OCO) setting over dimension $d$, we match full-matrix $d^2$ memory regret using only $dk$ memory up to additive error in the bottom $d-k$ eigenvalues of the gradient covariance. Further, we show extensions of our work to Shampoo, resulting in a method competitive in quality with Shampoo and Adam, yet requiring only sub-linear memory for tracking second moments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 3. Memory-Efficient Adaptive Optimization for Large-Scale Learning\n\n*From Search Query: adaptive normalization training stability memory optimization*\n\n*Rohan Anil, Vineet Gupta, Tomer Koren, Y. Singer*\n\n**TL;DR:** This work describes a novel, simple, and flexible adaptive optimization method with sublinear memory cost that retains the benefits of per-parameter adaptivity while allowing for larger models and mini-batches and gives convergence guarantees for the method.\n\n**Abstract:** Adaptive gradient-based optimizers such as AdaGrad and Adam are among the methods of choice in modern machine learning. These methods maintain second-order statistics of each parameter, thus doubling the memory footprint of the optimizer. In behemoth-size applications, this memory overhead restricts the size of the model being used as well as the number of examples in a mini-batch. We describe a novel, simple, and flexible adaptive optimization method with sublinear memory cost that retains the benefits of per-parameter adaptivity while allowing for larger models and mini-batches. We give convergence guarantees for our method and demonstrate its effectiveness in training very large deep models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 49  (*Influential: 11*)\n\n### 2 related papers from Papers with Code\n\n#### 1. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: adaptive normalization training stability memory optimization*\n\n*Xipeng Qiu, Haijun Lv, Qipeng Guo, Hang Yan, Kai Lv*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16\n\n\n\n#### 2. On-Device Training Under 256KB Memory\n\n*From Search Query: adaptive normalization training stability memory optimization*\n\n*Song Han, Chuang Gan, Wei-Chen Wang, Wei-Ming Chen, Ligeng Zhu, Ji Lin*\n\n**Abstract:** On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource does not allow full back-propagation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/0pUFZYdoMY8.\n\n**Published:** 2022-06-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive normalization, training stability, memory optimization, and integration with state space models, here are some key findings and references:\n\n## Adaptive Normalization Techniques in Language Models\n\n- The \"Liquid-Metal-Tinyllama-Test-1\" model incorporates several adaptive normalization techniques. It uses Adaptive RMSNorm, which normalizes the input based on the global context of each input batch, and Post-Attention Adaptive RMSNorm, which applies adaptive normalization after attention processing. These techniques help in maintaining stability in training and improving the model's ability to generalize across diverse inputs.\n\n- The \"Mixture-of-Depths (MoD)\" framework introduces an additional normalization module for each late layer, which is trained individually to ensure specific adaptation for each layer. This approach helps in adapting the outputs of the late layers to language modeling tasks without adding significant trainable parameters.\n\n## Training Stability Methods for Hierarchical Neural Networks\n\n- The MoD framework also addresses training stability by using an auxiliary distillation loss where the last layer output serves as the teacher. This method ensures that the late layers adapt to the predictions without interfering with the original model predictions, thus improving training stability.\n\n- Adaptive normalization techniques, such as those used in the \"Liquid-Metal-Tinyllama-Test-1\" model, contribute to training stability by smoothing out variations while retaining sensitivity to input-specific details. This helps in improved convergence and robustness, especially in complex tasks.\n\n## Memory Optimization Strategies in Deep Learning\n\n- The MoD framework achieves similar performance with significantly fewer trainable parameters by replacing traditional trainable modules with the MoD approach. This results in a minimal increase in trainable parameters (+0.04%) and demonstrates the potential of leveraging predictive power from intermediate representations during training, thus optimizing memory usage.\n\n- State Space Models (SSMs) are known for their linear computational complexity and high parallelizability during training, which can lead to memory efficiency. However, careful design is necessary to ensure stability and performance[Analysis Note].\n\n## Integration of Normalization with State Space Models\n\n- While the current literature does not provide direct examples of integrating normalization with state space models, the concept of adaptive normalization can be applied to SSMs. For instance, using adaptive normalization techniques like Adaptive RMSNorm or Context Normalization within the SSM architecture could enhance the model's ability to capture complex patterns and improve generalization.\n\n## Empirical Studies on Normalization in Large Language Models\n\n- The MoD framework has been empirically evaluated on arithmetic and commonsense reasoning tasks, showing consistent improvements in performance with minimal increases in trainable parameters. This demonstrates the effectiveness of adaptive normalization and ensemble methods in large language models.\n\n- The \"Cluster-Based Normalization (CBN)\" approach, although not specifically focused on language models, shows that multi-mode normalization techniques can outperform traditional normalization methods. This suggests that similar approaches could be beneficial in large language models, especially when dealing with complex, high-dimensional input data.\n\nBy integrating these adaptive normalization techniques, ensuring training stability through distillation and adaptive normalization, optimizing memory usage through efficient model design, and exploring the integration of normalization with state space models, researchers can significantly improve the performance and efficiency of autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about hierarchical normalization, state space models, and their integration:\n- Hierarchical structures can improve both efficiency and model performance\n- SSMs show promise for efficient sequence modeling but have limitations\n- Memory efficiency and training stability are critical considerations\n- Integration of adaptive mechanisms with normalization is relatively unexplored\n\n2. Useful References with Excerpts:\n\nFrom \"DenseMamba\":\n\"While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. [...] By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output.\"\n\nFrom \"Linear Transformers with Learnable Kernel Functions\":\n\"Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities.\"\n\nFrom \"Longhorn\":\n\"State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs.\"\n\n3. Key Insights:\n\na) State Space Models:\n- SSMs provide linear computational complexity\n- Need careful design for stability and performance\n- Can be enhanced through hierarchical structures\n- Integration with normalization requires consideration\n\nb) Hierarchical Processing:\n- Multi-scale approaches improve model robustness\n- Dense connections can enhance information flow\n- Need balance between complexity and efficiency\n- Memory management is crucial\n\nc) Normalization Strategies:\n- Layer placement affects training stability\n- Adaptive mechanisms show promise\n- Integration with SSMs is unexplored territory\n- Need careful consideration of initialization\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of adaptive normalization in language models\n2. Training stability techniques for hierarchical models\n3. Memory optimization strategies in SSM architectures\n4. Integration methods for combining SSMs with normalization\n\n5. References:\n\n[1] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[2] Aksenov, Y., Balagansky, N., Vaina, S. M. L. C., Shaposhnikov, B., Gorbatovski, A., & Gavrilov, D. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv preprint.\n\n[3] Liu, B., Wang, R., Wu, L., Feng, Y., Stone, P., & Liu, Q. (2024). Longhorn: State Space Models are Amortized Online Learners. arXiv preprint.\n\n[4] Gu, A., Johnson, I., Timalsina, A., Rudra, A., & R\u00e9, C. (2022). How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. ICLR.\n\n[5] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. ICML."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "adaptive normalization state space models",
            "detail": "Find papers discussing:\n1. Adaptive normalization techniques in neural networks\n2. Integration of state space models with normalization layers\n3. Multi-scale or hierarchical normalization approaches\n4. Memory-efficient implementations of normalization\n5. Training stability in adaptive normalization systems",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Adaptive normalization techniques in neural networks\n2. Integration of state space models with normalization layers\n3. Multi-scale or hierarchical normalization approaches\n4. Memory-efficient implementations of normalization\n5. Training stability in adaptive normalization systems\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.97)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.97)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 2. Normalized Attention Without Probability Cage (Avg. Score: 0.61)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.61)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 3. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.43)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.43)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 4. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.29)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 50/56 (Score: 0.29)*\n\n```\narXiv preprint arXiv:1609.08144, 2016. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.\n```\n\n#### 5. Linearizing Large Language Models (Avg. Score: 0.24)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.24)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive normalization state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Towards Defending against Adversarial Examples via Attack-Invariant Features\n\n*From Search Query: adaptive normalization state space models*\n\n*Dawei Zhou, Tongliang Liu, Bo Han, N. Wang, Chunlei Peng, Xinbo Gao*\n\n**TL;DR:** This paper introduces an adversarial feature learning mechanism to disentangle invariant features from adversarial noise and proposes a normalization term in the encoded space of the attack-invariant features to address the bias issue between the seen and unseen types of attacks.\n\n**Abstract:** Deep neural networks (DNNs) are vulnerable to adversarial noise. Their adversarial robustness can be improved by exploiting adversarial examples. However, given the continuously evolving attacks, models trained on seen types of adversarial examples generally cannot generalize well to unseen types of adversarial examples. To solve this problem, in this paper, we propose to remove adversarial noise by learning generalizable invariant features across attacks which maintain semantic classification information. Specifically, we introduce an adversarial feature learning mechanism to disentangle invariant features from adversarial noise. A normalization term has been proposed in the encoded space of the attack-invariant features to address the bias issue between the seen and unseen types of attacks. Empirical evaluations demonstrate that our method could provide better protection in comparison to previous state-of-the-art approaches, especially against unseen types of attacks and adaptive attacks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 2. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: adaptive normalization state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 3. Continuous-time identification of dynamic state-space models by deep subspace encoding\n\n*From Search Query: adaptive normalization state space models*\n\n*G. Beintema, M. Schoukens, R. T'oth*\n\n**TL;DR:** It is proved that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and it is shown that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.\n\n**Abstract:** Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 4. Liquid Structural State-Space Models\n\n*From Search Query: adaptive normalization state space models*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 5. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: adaptive normalization state space models*\n\n*Paul Mattes, Rainer Schlosser, R. Herbrich*\n\n**TL;DR:** Hieros is a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space that allows for more efficient training than RNN- based world models and more efficient imagination than Transformer-based world models.\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training\n\n*From Search Query: adaptive normalization state space models*\n\n*Hengshuang Zhao, Kaicheng Yu, Xihui Liu, Bohao Peng, Xin Wen, Zhuotao Tian, Xiaoyang Wu*\n\n**Abstract:** The rapid advancement of deep learning models often attributes to their ability to leverage massive training data. In contrast, such privilege has not yet fully benefited 3D deep learning, mainly due to the limited availability of large-scale 3D datasets. Merging multiple available data sources and letting them collaboratively train a single model is a potential solution. However, due to the large domain gap between 3D point cloud datasets, such mixed supervision could adversely affect the model's performance and lead to degenerated performance (i.e., negative transfer) compared to single-dataset training. In view of this challenge, we introduce Point Prompt Training (PPT), a novel framework for multi-dataset synergistic learning in the context of 3D representation learning that supports multiple pre-training paradigms. Based on this framework, we propose Prompt-driven Normalization, which adapts the model to different datasets with domain-specific prompts and Language-guided Categorical Alignment that decently unifies the multiple-dataset label spaces by leveraging the relationship between label text. Extensive experiments verify that PPT can overcome the negative transfer associated with synergistic learning and produce generalizable representations. Notably, it achieves state-of-the-art performance on each dataset using a single weight-shared model with supervised multi-dataset training. Moreover, when served as a pre-training framework, it outperforms other pre-training approaches regarding representation quality and attains remarkable state-of-the-art performance across over ten diverse downstream tasks spanning both indoor and outdoor 3D scenarios.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2023-08-18\n\n\n\n#### 2. Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation\n\n*From Search Query: adaptive normalization state space models*\n\n*Luc van Gool, Tim Br\u00f6dermann, Christos Sakaridis, David Bruggemann*\n\n**Abstract:** Standard unsupervised domain adaptation methods adapt models from a source to a target domain using labeled source data and unlabeled target data jointly. In model adaptation, on the other hand, access to the labeled source data is prohibited, i.e., only the source-trained model and unlabeled target data are available. We investigate normal-to-adverse condition model adaptation for semantic segmentation, whereby image-level correspondences are available in the target domain. The target set consists of unlabeled pairs of adverse- and normal-condition street images taken at GPS-matched locations. Our method -- CMA -- leverages such image pairs to learn condition-invariant features via contrastive learning. In particular, CMA encourages features in the embedding space to be grouped according to their condition-invariant semantic content and not according to the condition under which respective inputs are captured. To obtain accurate cross-domain semantic correspondences, we warp the normal image to the viewpoint of the adverse image and leverage warp-confidence scores to create robust, aggregated features. With this approach, we achieve state-of-the-art semantic segmentation performance for model adaptation on several normal-to-adverse adaptation benchmarks, such as ACDC and Dark Zurich. We also evaluate CMA on a newly procured adverse-condition generalization benchmark and report favorable results compared to standard unsupervised domain adaptation methods, despite the comparative handicap of CMA due to source data inaccessibility. Code is available at https://github.com/brdav/cma.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-03-09\n\n\n\n#### 3. HSurf-Net: Normal Estimation for 3D Point Clouds by Learning Hyper Surfaces\n\n*From Search Query: adaptive normalization state space models*\n\n*Zhizhong Han, Yi Fang, Cheng Wang, Jin-San Cheng, Yu-Shen Liu, Qing Li*\n\n**Abstract:** We propose a novel normal estimation method called HSurf-Net, which can accurately predict normals from point clouds with noise and density variations. Previous methods focus on learning point weights to fit neighborhoods into a geometric surface approximated by a polynomial function with a predefined order, based on which normals are estimated. However, fitting surfaces explicitly from raw point clouds suffers from overfitting or underfitting issues caused by inappropriate polynomial orders and outliers, which significantly limits the performance of existing methods. To address these issues, we introduce hyper surface fitting to implicitly learn hyper surfaces, which are represented by multi-layer perceptron (MLP) layers that take point features as input and output surface patterns in a high dimensional feature space. We introduce a novel space transformation module, which consists of a sequence of local aggregation layers and global shift layers, to learn an optimal feature space, and a relative position encoding module to effectively convert point clouds into the learned feature space. Our model learns hyper surfaces from the noise-less features and directly predicts normal vectors. We jointly optimize the MLP weights and module parameters in a data-driven manner to make the model adaptively find the most suitable surface pattern for various points. Experimental results show that our HSurf-Net achieves the state-of-the-art performance on the synthetic shape dataset, the real-world indoor and outdoor scene datasets. The code, data and pretrained models are publicly available.\n\n**Published:** 2022-10-13\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using adaptive normalization techniques, state space models, and hierarchical normalization, here are some key findings and suggestions based on the provided sources and the detailed analysis:\n\n## Adaptive Normalization Techniques\n- The paper on \"Cluster-Based Normalization Layer for Neural Networks\" introduces a novel approach called Cluster-Based Normalization (CBN) that normalizes features based on predefined contexts or clusters. This method can improve the performance of neural networks by handling complex underlying structures in the data, which is relevant for autoregressive models dealing with diverse linguistic features.\n- Another approach is the \"Unsupervised Adaptive Normalization (UAN)\" which uses clustering for normalization, integrating it with deep neural network learning. This method addresses the issue of dynamic distributions of neuron activations during training, which could be beneficial for stabilizing the training process of autoregressive models.\n\n## Integration of State Space Models with Normalization Layers\n- The concept of integrating state space models with neural networks is explored in papers like \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" and \"DenseMamba: State Space Models with Dense Hidden Connection\". These works show how state space models can be efficiently integrated into neural networks, which could be adapted to include normalization layers. This integration can potentially improve the handling of long-term dependencies and sequential data in autoregressive models[Initial References Found: 2, 3].\n\n## Multi-scale or Hierarchical Normalization Approaches\n- The \"Cluster-Based Normalization\" and \"Adaptive Context Normalization (ACN)\" methods discussed in the papers introduce multi-mode normalization techniques. These approaches normalize data based on different contexts or clusters, which can be seen as a form of hierarchical normalization. This can help in handling data with complex structures and multiple scales, which is crucial for autoregressive language models dealing with varied linguistic contexts.\n- The use of \"Mixture Normalization\" based on Gaussian Mixture Models (GMM) also provides a hierarchical approach to normalization, allowing the model to capture multiple components of the data distribution.\n\n## Memory-Efficient Implementations of Normalization\n- The \"Lightning Attention-2\" paper discusses efficient long-sequence processing and memory optimization techniques, which can be applied to normalization layers in autoregressive models to reduce memory usage. Techniques such as selective state spaces and dense hidden connections can help in achieving this efficiency[Initial References Found: 4].\n- The \"Unsupervised Adaptive Normalization (UAN)\" method also aims to reduce computational demands by dynamically building contexts in the latent space during training, which can be more memory-efficient compared to traditional normalization methods.\n\n## Training Stability in Adaptive Normalization Systems\n- The stability of adaptive normalization systems is a critical aspect. Papers like the one on \"Cluster-Based Normalization\" and \"Unsupervised Adaptive Normalization\" emphasize the importance of stable normalization techniques. For instance, the use of spectral normalization to constrain the network\u2019s Lipschitz constant can enhance stability during online adaptation, as mentioned in the context of self-supervised meta-learning.\n- The integration of adaptive learning rates and normalization techniques must be carefully balanced to ensure stability. The trade-offs between adaptivity and stability, as well as the impact on training dynamics, need to be thoroughly investigated to avoid unstable gradients and ensure robust training[Initial Analysis of AdaptiveHierarchicalRMSNorm Proposal].\n\nIn summary, the following approaches can be considered to improve the autoregressive language model design:\n\n- **Adaptive Normalization**: Implement Cluster-Based Normalization (CBN) or Unsupervised Adaptive Normalization (UAN) to handle complex data structures and dynamic distributions.\n- **State Space Models**: Integrate state space models like Mamba or DenseMamba into the normalization layers to improve handling of sequential data and long-term dependencies.\n- **Hierarchical Normalization**: Use multi-mode normalization techniques such as Adaptive Context Normalization (ACN) or Mixture Normalization to capture multiple scales and contexts.\n- **Memory Efficiency**: Apply efficient long-sequence processing and memory optimization techniques from papers like \"Lightning Attention-2\" to reduce memory usage.\n- **Training Stability**: Ensure stability by using techniques like spectral normalization and carefully balancing adaptive learning rates with normalization methods.\n\nThese approaches can help in designing a more robust, efficient, and scalable autoregressive language model.\n",
            "analysis": "Initial Analysis of AdaptiveHierarchicalRMSNorm Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Adaptive Learning Rate Integration\n- How does the adaptive learning rate mechanism compare to existing approaches?\n- What are the trade-offs between adaptivity and stability?\n- How does it affect training dynamics?\n\n2. State Space Model Integration\n- How effective are state space models in normalization layers?\n- What are the computational costs vs. benefits?\n- How does this compare to existing hierarchical approaches?\n\n3. Multi-scale Processing\n- What are existing approaches to multi-scale normalization?\n- How does the hierarchical structure affect model performance?\n- What are the memory-computation trade-offs?\n\nInvestigation Plan:\n\n1. First Round:\n- Search for papers on adaptive normalization techniques\n- Focus on state space models in normalization\n- Examine multi-scale normalization approaches\n\n2. Second Round:\n- Investigate specific implementations of adaptive learning rates\n- Look for papers combining state space models with normalization\n- Research memory efficiency in hierarchical models\n\n3. Third Round:\n- Study training stability in adaptive systems\n- Examine scalability of multi-scale approaches\n- Research initialization strategies for hierarchical models\n\nInitial References Found:\n\n1. \"Root Mean Square Layer Normalization\" by Zhang & Sennrich (2019)\n- Provides baseline RMSNorm implementation\n- Discusses normalization stability\n\n2. \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" by Gu & Dao (2023)\n- Introduces efficient state space models\n- Discusses integration with neural networks\n\n3. \"DenseMamba: State Space Models with Dense Hidden Connection\" by He et al. (2024)\n- Shows recent advances in state space models\n- Discusses efficiency improvements\n\n4. \"Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths\" by Qin et al. (2024)\n- Demonstrates efficient long-sequence processing\n- Discusses memory optimization techniques\n\nThe proposal appears to be novel in its specific combination of adaptive learning rates, state space models, and hierarchical normalization. However, further investigation is needed to confirm uniqueness and potential advantages over existing approaches."
        },
        {
            "ready": false,
            "query": "adaptive normalization language models training stability",
            "detail": "Find papers discussing:\n1. Adaptive normalization techniques specifically in language models\n2. Training stability methods for hierarchical neural networks\n3. Memory-efficient normalization implementations\n4. Integration of normalization with state space models\n5. Empirical studies on normalization in large language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Adaptive normalization techniques specifically in language models\n2. Training stability methods for hierarchical neural networks\n3. Memory-efficient normalization implementations\n4. Integration of normalization with state space models\n5. Empirical studies on normalization in large language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.87)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.87)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 2. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.82)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.82)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 3. Normalized Attention Without Probability Cage (Avg. Score: 0.81)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.81)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.81)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 27/74 (Score: 0.81)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: The International Conference on Learning Representations (ICLR). 2015. [6] David Balduzzi and Muhammad Ghifary. \"Strongly-typed Recurrent Neural Networks\". In: International Conference on Machine Learning. PMLR. 2016, pp. 1292-1300. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. \"Pythia: A Suite for Analyzing Large Language Models across Training and Scaling\".\n```\n\n#### 5. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.46)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 11/63 (Score: 0.46)*\n\n```\nE. Hinton. Layer normalization. stat, 1050:21, 2016. [5] D. A. M. Barrington, K. Compton, H. Straubing, and D. Th\u00e9rien. Regular languages in nc1. Journal of Computer and System Sciences, 44(3):478-499, 1992. [6] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096-7116, 2020. [7] D. Blasi, R. Cotterell, L. Wolf-Sonkin, S. Stoll, B. Bickel, and M. Baroni. On the distribution of deep clausal embeddings: A large cross-linguistic study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3938-3943, 2019. [8] J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2016. [9] D. Chiang and P. Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654-7664, 2022. [10] D. Chiang, P. Cholak, and A. Pillay. Tighter bounds on the expressivity of transformer encoders.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive normalization language models training stability\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Continual Pre-training of Language Models\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, Bin Liu*\n\n**TL;DR:** A novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances and contrasts the representations of the previously learned domain knowledge to achieve knowledge integration.\n\n**Abstract:** Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual pre-training of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their end-task performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 90  (*Influential: 6*)\n\n#### 2. Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Boxin Wang, Wei Ping, Chaowei Xiao, P. Xu, M. Patwary, Mohammad Shoeybi, Bo Li, Anima Anandkumar, Bryan Catanzaro*\n\n**TL;DR:** This work systematically explore domain-adaptive training to reduce the toxicity of language models and demonstrates that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.\n\n**Abstract:** Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we propose to leverage the generative power of LMs and generate nontoxic datasets for domain-adaptive training, which mitigates the exposure bias and is shown to be more data-efficient than using a curated pre-training corpus. We demonstrate that the self-generation method consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 1/3 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3x larger than GPT-3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to detoxify. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for the large-scale models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 54  (*Influential: 3*)\n\n#### 3. Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, H. Dai, D. Dou*\n\n**TL;DR:** A Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs and a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further.\n\n**Abstract:** Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8\\% in terms of accuracy) and efficiency (up to 97.59\\% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 36  (*Influential: 1*)\n\n#### 4. Stable and low-precision training for large-scale vision-language models\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Mitchell Wortsman, Tim Dettmers, Luke Zettlemoyer, Ari S. Morcos, Ali Farhadi, Ludwig Schmidt*\n\n**TL;DR:** SwitchBack is introduced, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date.\n\n**Abstract:** We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) For acceleration, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) For stability, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafactor hybrid which avoids loss spikes when training a CLIP ViT-Huge model and outperforms gradient clipping at the scales we test.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 3*)\n\n#### 5. QuIP: 2-Bit Quantization of Large Language Models With Guarantees\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Chris De Sa*\n\n**TL;DR:** This work introduces quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices that improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight.\n\n**Abstract:** This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights being even in magnitude and the directions in which it is important to round them accurately being unaligned with the coordinate axes. QuIP consists of two steps: (1) an adaptive rounding procedure minimizing a quadratic proxy objective; (2) efficient pre- and post-processing that ensures weight and Hessian incoherence via multiplication by random orthogonal matrices. We complement QuIP with the first theoretical analysis for an LLM-scale quantization algorithm, and show that our theory also applies to an existing method, OPTQ. Empirically, we find that our incoherence preprocessing improves several existing quantization algorithms and yields the first LLM quantization methods that produce viable results using only two bits per weight. Our code can be found at https://github.com/Cornell-RelaxML/QuIP.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 110  (*Influential: 19*)\n\n### 3 related papers from Papers with Code\n\n#### 1. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Xipeng Qiu, Haijun Lv, Qipeng Guo, Hang Yan, Kai Lv*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16\n\n\n\n#### 2. True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Bo An, Xinrun Wang, Longtao Zheng, Shanqi Liu, Wentao Zhang, Weihao Tan*\n\n**Abstract:** Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the actor and critic share one frozen LLM equipped with low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency and performance compared to the conventional RL method, PPO, and prompt tuning method, SayCan, in both classical decision-making environment, Overcooked, and simulated household environment, VirtualHome. ii) Benefiting from LLMs' open-vocabulary feature, TWOSOME shows superior generalization ability to unseen tasks. iii) Under our framework, there is no significant loss of the LLMs' original ability during online PPO finetuning.\n\n**Published:** 2024-01-25\n\n\n\n#### 3. Stabilizing Transformer Training by Preventing Attention Entropy Collapse\n\n*From Search Query: adaptive normalization language models training stability*\n\n*Josh Susskind, Jiatao Gu, Yizhe Zhang, Jason Ramapuram, Dan Busbridge, Etai Littwin, Tatiana Likhomanenko, Shuangfei Zhai*\n\n**Abstract:** Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\\textit{entropy collapse}$. As a remedy, we propose $\\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with $\\sigma$Reparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer {to competitive performance} without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at \\url{https://github.com/apple/ml-sigma-reparam}.\n\n**Published:** 2023-03-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive normalization techniques, training stability, memory efficiency, and integration with state space models, here are some key findings and references:\n\n## Adaptive Normalization Techniques in Language Models\n\n- The \"Liquid-Metal-Tinyllama-Test-1\" model incorporates several adaptive normalization techniques that can enhance the performance of language models. Specifically, it uses:\n  - **Adaptive RMSNorm**: This technique normalizes the input while applying an adaptive scaling based on the global context of each input batch, improving the model's ability to generalize across diverse inputs.\n  - **Token Mixing**: This layer blends information across tokens within each feature channel through depthwise convolution, capturing local dependencies and enhancing intra-sequence awareness.\n  - **SEBlock (Squeeze-and-Excitation Block)**: This block performs adaptive channel-wise recalibration, allowing the model to dynamically adjust which features are most relevant for each input.\n\n## Training Stability Methods for Hierarchical Neural Networks\n\n- **Layer Normalization** and **Instance Normalization** are discussed in the context of hybrid AI systems, which can be applied to hierarchical neural networks. Layer normalization normalizes across features instead of the batch dimension, which is particularly beneficial in transformer models and RNNs. Instance normalization, while primarily used in style transfer, can also help in scenarios with varying input styles.\n- The use of **adaptive normalization** techniques, such as those in the \"Liquid-Metal-Tinyllama-Test-1\" model, can help maintain stability in training by smoothing out variations while retaining sensitivity to input-specific details.\n\n## Memory-Efficient Normalization Implementations\n\n- **Cluster-Based Normalization (CBN)** is a novel approach that normalizes features separately based on their statistical properties, which can be more memory-efficient compared to traditional normalization techniques. This method allows the network to automatically discover the optimal grouping of features, reducing computational demands.\n- **Layer Normalization** is generally more memory-efficient than batch normalization because it normalizes across features rather than batches, making it suitable for models with hierarchical structures.\n\n## Integration of Normalization with State Space Models\n\n- While the references provided do not directly discuss the integration of normalization with state space models in language models, the concept of adaptive normalization can be extended to state space models. For instance, incorporating adaptive normalization layers within the state transition modules of Liquid Time-Constant (LTC) neural networks or Structured State Space Sequence (S4) models could enhance their adaptivity and stability.\n\n## Empirical Studies on Normalization in Large Language Models\n\n- The T5 model, which is a large language model, uses layer normalization with minor modifications such as placing the layer normalization outside the residual path and using relative positional embedding. These modifications contribute to the model's performance and stability during training.\n- The paper on \"A Comprehensive Overview of Large Language Models\" discusses various training objectives and techniques, including the use of normalization in large language models like GPT-3 and mT5. These models often employ layer normalization and other techniques to improve training stability and performance.\n\nBy integrating these adaptive normalization techniques, ensuring training stability through appropriate normalization methods, optimizing memory usage, and exploring the integration with state space models, researchers can significantly enhance the design and performance of autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe initial search reveals several key areas relevant to the AdaptiveHierarchicalRMSNorm proposal:\n- State space models (SSMs) show promise in sequence modeling with linear complexity\n- Hierarchical and adaptive normalization techniques are emerging in various domains\n- Memory efficiency and training stability are critical considerations\n- Integration of SSMs with normalization layers is relatively unexplored\n\n2. Useful References with Excerpts:\n\nFrom \"Liquid Structural State-Space Models\":\n\"LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference.\"\n\nFrom \"Structured State Space Models for In-Context Reinforcement Learning\":\n\"Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks.\"\n\nFrom \"Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\":\n\"Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination.\"\n\n3. Key Insights:\n\na) State Space Models:\n- SSMs show strong potential for long-range sequence modeling\n- Liquid time-constant (LTC) variants offer adaptive capabilities\n- Parallel training and iterative prediction are possible\n\nb) Hierarchical Processing:\n- Multi-scale approaches improve model robustness\n- Hierarchical structures enable efficient memory usage\n- Scale-specific processing can capture different levels of dependencies\n\nc) Adaptive Mechanisms:\n- Input-dependent adaptation shows promise\n- Need to balance adaptivity with stability\n- Integration with normalization requires careful consideration\n\n4. Future Search Plan:\n\nRound 2 Search Needs:\n- Specific implementations of adaptive normalization in language models\n- Training stability techniques for hierarchical models\n- Memory optimization strategies for normalization layers\n\n5. References:\n\n[1] Hasani, R. M., Lechner, M., Wang, T. H., Chahine, M., Amini, A., & Rus, D. (2022). Liquid Structural State-Space Models. International Conference on Learning Representations.\n\n[2] Lu, C. X., Schroecker, Y., Gu, A., Parisotto, E., Foerster, J., Singh, S., & Behbahani, F. M. P. (2023). Structured State Space Models for In-Context Reinforcement Learning. Neural Information Processing Systems.\n\n[3] Mattes, P., Schlosser, R., & Herbrich, R. (2023). Hieros: Hierarchical Imagination on Structured State Space Sequence World Models. International Conference on Machine Learning.\n\n[4] Beintema, G., Schoukens, M., & T'oth, R. (2022). Continuous-time identification of dynamic state-space models by deep subspace encoding. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "hierarchical normalization language models SSM",
            "detail": "Find papers discussing:\n1. Hierarchical normalization techniques in language models\n2. Integration of SSMs with normalization layers\n3. Memory-efficient implementations of hierarchical structures\n4. Training stability in hierarchical neural networks\n5. Adaptive normalization in state space models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical normalization techniques in language models\n2. Integration of SSMs with normalization layers\n3. Memory-efficient implementations of hierarchical structures\n4. Training stability in hierarchical neural networks\n5. Adaptive normalization in state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.96)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.96)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.73)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.73)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 3. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.68)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.68)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 4. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.38)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.38)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n#### 5. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.23)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.23)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical normalization language models SSM\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Strategies for Training Large Vocabulary Neural Language Models\n\n*From Search Query: hierarchical normalization language models SSM*\n\n*Wenlin Chen, David Grangier, Michael Auli*\n\n**TL;DR:** A systematic comparison of strategies to represent and train large vocabularies, includingsoftmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization, and extends selfnormalization to be a proper estimator of likelihood and introduce an efficient variant of softmax.\n\n**Abstract:** Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2015\n\n**Citations:** 137  (*Influential: 15*)\n\n#### 2. Revisiting Over-smoothing in BERT from the Perspective of Graph\n\n*From Search Query: hierarchical normalization language models SSM*\n\n*Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, J. Kwok*\n\n**TL;DR:** This work makes the attempt to analyze the over-smoothing problem of Transformer-based models from the perspective of graph, where such problem was first discovered and explored and finds that layer normalization plays a key role.\n\n**Abstract:** Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 57  (*Influential: 8*)\n\n#### 3. Spherical Topic Models\n\n*From Search Query: hierarchical normalization language models SSM*\n\n*J. Reisinger, Austin Waters, Bryan Silverthorn, R. Mooney*\n\n**TL;DR:** The Spherical Admixture Model is introduced, a Bayesian topic model for arbitrary l2 normalized data that maintains the same hierarchical structure as Latent Dirichlet Allocation, but models documents as points on a high-dimensional spherical manifold, allowing a natural likelihood parameterization in terms of cosine distance.\n\n**Abstract:** We introduce the Spherical Admixture Model (SAM), a Bayesian topic model for arbitrary l2 normalized data. SAM maintains the same hierarchical structure as Latent Dirichlet Allocation (LDA), but models documents as points on a high-dimensional spherical manifold, allowing a natural likelihood parameterization in terms of cosine distance. Furthermore, SAM can model word absence/presence at the document level, and unlike previous models can assign explicit negative weight to topic terms. Performance is evaluated empirically, both through human ratings of topic quality and through diverse classification tasks from natural language processing and computer vision. In these experiments, SAM consistently outperforms existing models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2010\n\n**Citations:** 103  (*Influential: 13*)\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hierarchical normalization language models SSM*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy\n\n*From Search Query: hierarchical normalization language models SSM*\n\n*Colin B. Clement, Shuai Lu, Xiaoyu Liu, Michele Tufano, Dawn Drain, Nan Duan, Neel Sundaresan, Alexey Svyatkovskiy*\n\n**TL;DR:** This work introduces an architecture-independent approach for leveraging the syntactic hierarchies of source code for incorporating entire file-level context into a fixed-length window.\n\n**Abstract:** Statistical language modeling and translation with transformers have found many successful applications in program understanding and generation tasks, setting high benchmarks for tools in modern software development environments. The finite context window of these neural models means, however, that they will be unable to leverage the entire relevant context of large files and packages for any given task. While there are many efforts to extend the context window, we introduce an architecture-independent approach for leveraging the syntactic hierarchies of source code for incorporating entire file-level context into a fixed-length window. Using concrete syntax trees of each source file we extract syntactic hierarchies and integrate them into context window by selectively removing from view more specific, less relevant scopes for a given task. We evaluate this approach on code generation tasks and joint translation of natural language and source code in Python programming language, achieving a new state-of-the-art in code completion and summarization for Python in the CodeXGLUE benchmark. We also introduce new CodeXGLUE benchmarks for user-experience-motivated tasks: code completion with normalized literals, method body completion/code summarization conditioned on file-level context.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 24  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using hierarchical normalization techniques, integration of State Space Models (SSMs) with normalization layers, and ensuring memory efficiency and training stability, here are some key points and references that can guide the research:\n\n## Hierarchical Normalization Techniques in Language Models\n\n- Hierarchical normalization can be achieved by applying normalization at different levels of the model architecture. For instance, using layer normalization or batch normalization at various hierarchical levels can help in stabilizing the training process and improving the model's performance. A study on \"Stable and low-precision training for large-scale vision-language models\" highlights the importance of careful placement of normalization layers to avoid loss spikes and ensure training stability.\n\n## Integration of SSMs with Normalization Layers\n\n- Integrating SSMs with normalization layers is a relatively unexplored area but holds significant potential. The concept of selective state spaces in SSMs, as discussed in \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces,\" can be combined with adaptive normalization techniques to handle long-range dependencies efficiently. This integration can address the weakness of SSMs in content-based reasoning by making the SSM parameters functions of the input.\n\n## Memory-Efficient Implementations of Hierarchical Structures\n\n- Memory efficiency is crucial for large-scale language models. Techniques such as selective state spaces and adaptive learning rates can help in reducing memory usage. \"AdaLomo: Low-memory Optimization with Adaptive Learning Rate\" provides insights into how adaptive learning rates can bridge the gap between high and low precision training, which is essential for memory-efficient implementations.\n\n## Training Stability in Hierarchical Neural Networks\n\n- Training stability is a critical aspect, especially in hierarchical models. Studies like \"Stable and low-precision training for large-scale vision-language models\" and \"Stabilizing Transformer Training by Preventing Attention Entropy Collapse\" emphasize the importance of avoiding loss spikes and preventing attention entropy collapse. These issues can be mitigated by careful gradient estimation and the strategic placement of normalization layers.\n\n## Adaptive Normalization in State Space Models\n\n- Adaptive normalization can significantly improve the performance of SSMs. By making the normalization parameters dependent on the input, the model can adapt to different contexts and improve its ability to handle long-range dependencies. This approach is aligned with the idea of input-dependent adaptation discussed in the analysis note, which highlights the potential of such mechanisms in improving model performance.\n\n### Additional Considerations\n\n- **Positional Encoding and Hierarchical Structures**: The Transformer architecture, which is widely used in autoregressive language models, relies on positional encoding to capture the sequence information. Integrating hierarchical normalization with positional encoding can enhance the model's ability to process long sequences efficiently.\n\n- **Unified Language Modeling**: Approaches like unified language modeling, which combine causal, non-causal, and masked language training objectives, can be beneficial when integrated with hierarchical normalization and SSMs. This can help in improving the model's performance across various tasks and contexts.\n\nBy focusing on these areas, researchers can develop more efficient, stable, and adaptive autoregressive language models that leverage hierarchical normalization and the strengths of SSMs.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about adaptive normalization, state space models, and training stability:\n- Adaptive normalization techniques show promise in improving model performance\n- Integration of SSMs with normalization is relatively unexplored but potentially valuable\n- Memory efficiency and training stability are critical considerations\n- Hierarchical approaches can improve both efficiency and performance\n\n2. Useful References with Excerpts:\n\nFrom \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements [...] letting the SSM parameters be functions of the input addresses their weakness\"\n\nFrom \"Stable and low-precision training for large-scale vision-language models\":\n\"We analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator\"\n\nFrom \"AdaLomo: Low-memory Optimization with Adaptive Learning Rate\":\n\"Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap\"\n\n3. Key Insights:\n\na) Adaptive Mechanisms:\n- Input-dependent adaptation shows strong potential\n- Need to balance adaptivity with stability\n- Integration with normalization requires careful consideration\n\nb) State Space Models:\n- SSMs can effectively handle long-range dependencies\n- Selective state spaces improve efficiency\n- Integration with normalization layers is promising but unexplored\n\nc) Training Stability:\n- Loss spikes correlate with gradient estimation errors\n- Adaptive learning rates are crucial for performance\n- Normalization placement affects stability\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of hierarchical normalization in language models\n2. Techniques for combining SSMs with normalization layers\n3. Memory-efficient adaptive normalization approaches\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n[2] Wortsman, M., et al. (2023). Stable and low-precision training for large-scale vision-language models. Neural Information Processing Systems.\n\n[3] Qiu, X., et al. (2023). AdaLomo: Low-memory Optimization with Adaptive Learning Rate. arXiv preprint arXiv:2310.10195.\n\n[4] Chee, J., et al. (2023). QuIP: 2-Bit Quantization of Large Language Models With Guarantees. Neural Information Processing Systems.\n\n[5] Susskind, J., et al. (2023). Stabilizing Transformer Training by Preventing Attention Entropy Collapse. arXiv preprint arXiv:2303.06296."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.52915,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.8847750000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalRMSNorm",
    "proposal": "# Proposal: AdaptiveHierarchicalRMSNorm\n\n## 1. Motivation\n\nCurrent language models face challenges in efficiently processing sequences of varying lengths and complexities. Standard normalization techniques often fail to capture hierarchical patterns and adapt to different feature scales. The proposed AdaptiveHierarchicalRMSNorm aims to address these issues by integrating adaptive learning rates and state space models into the HierarchicalRMSNorm framework, enhancing its ability to handle diverse inputs and improve training stability.\n\n## 2. Problem Analysis\n\nThe original HierarchicalRMSNorm, while innovative, has limitations:\n1. Fixed normalization parameters may not adapt well to varying data characteristics.\n2. Single-scale processing misses hierarchical feature relationships.\n3. Memory inefficiency in handling long sequences.\n4. Limited ability to handle varying sequence lengths efficiently.\n\n## 3. Core Idea and Philosophy\n\nAdaptiveHierarchicalRMSNorm introduces adaptive learning rates and state space models to enhance multi-scale normalization. This approach:\n- Utilizes adaptive initialization based on scale-specific statistics to improve training stability.\n- Incorporates state space models for efficient memory usage and scalability.\n- Processes features at multiple resolutions simultaneously, capturing hierarchical patterns more effectively.\n\n## 4. Design Plan\n\n### 4.1 Architecture Overview\n\nAdaptiveHierarchicalRMSNorm consists of three main components:\n1. **Adaptive Initialization**: Utilizes adaptive learning rates to initialize normalization parameters based on input characteristics.\n2. **State Space Models**: Integrates state space models to enhance memory efficiency and scalability.\n3. **Multi-scale Feature Decomposition**: Processes features at multiple resolutions to capture hierarchical patterns.\n\n### 4.2 Mathematical Formulation\n\n1. **Adaptive Initialization**:\n```python\nadaptive_gamma_s = init_adaptive_weights(s, data_stats)\nwhere:\ninit_adaptive_weights(s, stats) = base_gamma * adaptive_factor(s, stats)\n```\n\n2. **State Space Integration**:\nFor each scale s:\n```\nstate_s = state_space_model(x_s)\ny_s = state_s / rms_s(x) * adaptive_gamma_s\n```\n\n3. **Hierarchical Integration**:\n```python\ny = sum(upsample(y_s) * w_s for s, y_s in y_scales.items())\n```\n\n### 4.3 Implementation Details\n\n```python\nclass AdaptiveHierarchicalRMSNorm(nn.Module):\n    def __init__(self, embed_dim, scales=[1, 2, 4], eps=1e-5):\n        super().__init__()\n        self.scales = scales\n        self.eps = eps\n        self.adaptive_gammas = nn.ParameterDict({\n            f's{s}': nn.Parameter(torch.ones(embed_dim))\n            for s in scales\n        })\n        self.scale_weights = nn.Parameter(torch.ones(len(scales)))\n        self.state_space_models = {s: StateSpaceModel(embed_dim) for s in scales}\n        \n    def forward(self, x):\n        # Multi-scale decomposition\n        x_scales = self._decompose_scales(x)\n        \n        # Scale-specific normalization\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            state_s = self.state_space_models[s](x_s)\n            rms_s = torch.sqrt(x_s.pow(2).mean(-1, keepdim=True) + self.eps)\n            y_scales[s] = state_s / rms_s * self.adaptive_gammas[f's{s}']\n        \n        # Hierarchical integration\n        y = self._integrate_scales(y_scales)\n        return y\n        \n    def _decompose_scales(self, x):\n        return {s: self._downsample(x, s) for s in self.scales}\n        \n    def _integrate_scales(self, y_scales):\n        weights = F.softmax(self.scale_weights, dim=0)\n        return sum(self._upsample(y_s) * w for (s, y_s), w in zip(y_scales.items(), weights))\n```\n\n## 5. Implementation Guidelines\n\n1. **Adaptive Initialization**:\n   - Use data-driven adaptive factors to initialize gamma parameters.\n   - Implement a warm-up period for adaptive mixing.\n\n2. **State Space Models**:\n   - Integrate state space models for efficient memory usage.\n   - Use parallel processing for scalability.\n\n3. **Memory Optimization**:\n   - Implement in-place operations where possible.\n   - Use gradient checkpointing for long sequences.\n\n## 6. Theoretical Analysis\n\n1. **Computational Complexity**:\n   - Time: O(N log N) due to multi-scale processing.\n   - Memory: O(N) through efficient state space integration.\n   - Additional parameters: O(D * S) where D is dimension and S is number of scales.\n\n2. **Benefits**:\n   - Improved feature normalization across scales.\n   - Better handling of long-range dependencies.\n   - Reduced memory footprint through state space models.\n   - More stable training through adaptive initialization.\n\n3. **Limitations**:\n   - Slight increase in computation time.\n   - Additional hyperparameters to tune.\n   - Potential overhead in very short sequences.\n\n## 7. Research Summary\n\nKey findings from research:\n1. Multi-scale processing improves model robustness.\n2. Hierarchical structures enable efficient memory usage.\n3. Adaptive initialization enhances training stability.\n4. State space models provide scalable memory efficiency.\n\n## 8. Evolution of Design\n\nThe design evolved through several iterations:\n1. Initial focus on pure multi-scale processing.\n2. Addition of adaptive initialization.\n3. Integration of state space models for memory efficiency.\n4. Refinement of scale integration mechanism.\n\n## 9. References\n\n1. Gu, A., et al. (2023). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. ICLR.\n2. Shazeer, N. M., & Stern, M. (2018). Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. ICML.\n3. Dao, T., et al. (2022). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.\n4. Socher, R., et al. (2011). Parsing Natural Scenes and Natural Language with Recursive Neural Networks. ICML.\n\n## 10. Conclusion\n\nAdaptiveHierarchicalRMSNorm represents a significant advancement in normalization techniques for language models. By combining adaptive learning rates with state space models and multi-scale processing, it addresses key limitations of existing approaches while maintaining computational efficiency. The design's flexibility and scalability make it particularly suitable for modern large language models dealing with varying sequence lengths and complex feature distributions.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "gpt4o_0806",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "The proposal introduces AdaptiveHierarchicalRMSNorm, a novel enhancement to HierarchicalRMSNorm that incorporates adaptive learning rates and state space models for improved multi-scale normalization. This design leverages adaptive initialization and memory-efficient state space models to enhance training stability, scalability, and efficiency in language models.",
    "ideation": null,
    "modelname": "adaptive_hiergpt_2",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of SSM integration overhead\n- Include concrete examples of adaptive initialization\n- Add guidelines for parameter selection\n- Consider fallback mechanisms for stability\n\n2. Memory Analysis:\n- Include quantitative analysis of memory savings\n- Provide detailed cache utilization strategies\n- Address potential memory bottlenecks\n- Consider trade-offs in adaptive compression\n\n3. Training Considerations:\n- Add specific guidance for joint training\n- Include stability analysis across scales\n- Provide recommendations for initialization\n- Consider adaptive learning rate schedules\n\n4. Scalability Analysis:\n- Address model parallelism considerations\n- Include large-scale deployment guidelines\n- Add performance scaling analysis\n- Consider distributed implementation strategies\n\n5. Documentation:\n- Provide more detailed API specifications\n- Include example configurations\n- Add debugging guidelines\n- Consider failure mode analysis",
    "user_input": ""
}