{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = MetaTTT(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass MetaTTT(GAUBase):\n    \"\"\"\n    MetaTTT: A Test-Time Training GAU enhanced with meta-learning, gated mechanisms, and uncertainty-aware compression.\n    \n    This class implements the MetaTTT GAU as described in the proposal. MetaTTT enhances the TTT GAU by integrating meta-learning for dynamic hidden state adaptation, gated mechanisms for selective information flow, and uncertainty-aware compression for efficient memory usage.\n    \n    **Methods:**\n    \n    - **__init__**: Initializes the MetaTTT GAU, including the meta-learner, gating mechanisms, uncertainty estimation module, and compression gate.\n    - **_forward**: Implements the forward pass, including hidden state updates, gating mechanisms, uncertainty estimation, and compression.\n    \n    **Mathematical Formulation:**\n    \n    - **Meta-Learned Hidden State Update**:\n      \\\\( W_t = M(W_{t-1}, x_t; \theta_M) \\\\)\n    - **Gated Query and Key Modulation**:\n      \\\\( Q_t' = Q_t \\\\odot \\\\sigma(W_{GQ} W_t + b_{GQ}) \\\\)\n      \\\\( K_t' = K_t \\\\odot \\\\sigma(W_{GK} W_t + b_{GK}) \\\\)\n    - **Uncertainty-Aware Compression**:\n      \\\\( C_t = \\\\sigma(W_U \\\\cdot U(W_t) + b_U) \\\\)\n      \\\\( W_t' = W_t \\\\odot C_t \\\\)\n\n    **Args:**\n        embed_dim (int): The size of the input embeddings.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): The device to run the model on.\n        dtype (torch.dtype, optional): The data type for model parameters.\n        hidden_dim (int): The dimension of the hidden layer in the meta-learner. Default: 128.\n    \n    **Example:**\n        This is how you can use this function:\n    \n        meta_ttt = MetaTTT(embed_dim=768, block_loc=(0, 1), kwarg_all={})\n        X = torch.randn(32, 128, 768)\n        Y, Z = meta_ttt(X)\n    \n    **References:**\n    - Sun, Y. et al., 2024. Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n    - Other references as per the proposal.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_dim: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.meta_learner = nn.Sequential(nn.Linear(embed_dim + embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            hidden_dim, embed_dim, **self.factory_kwargs))\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.uncertainty_module = nn.Sequential(nn.Linear(embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.Softplus(), nn.Linear(\n            hidden_dim, 1, **self.factory_kwargs))\n        self.compress_gate = nn.Linear(1, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        W_prev = Z.get('hidden_state', torch.zeros(B, D, device=X.device,\n            dtype=X.dtype))\n        W_prev_expanded = W_prev.unsqueeze(1).expand(-1, L, -1)\n        meta_input = torch.cat([W_prev_expanded, X], dim=-1)\n        W_new = self.meta_learner(meta_input)\n        W_new_last = W_new[:, -1, :]\n        uncertainty = self.uncertainty_module(W_new)\n        compress_factor = torch.sigmoid(self.compress_gate(uncertainty))\n        W_compressed = W_new * compress_factor\n        W_compressed_last = W_compressed[:, -1, :]\n        Z['hidden_state'] = W_compressed_last\n        X_enhanced = X + W_compressed\n        Q = self.q_proj(X_enhanced)\n        K = self.k_proj(X_enhanced)\n        V = self.v_proj(X_enhanced)\n        G_Q = torch.sigmoid(self.gate_q(W_new))\n        G_K = torch.sigmoid(self.gate_k(W_new))\n        Q = Q * G_Q\n        K = K * G_K\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.embed_dim ** 0.5\n        mask = torch.tril(torch.ones(L, L, device=attn_scores.device)\n            ).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        Y = self.o_proj(attn_output)\n        return Y, Z\n\n\ngab_config = {'hidden_dim': 128}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = MetaTTT(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass MetaTTT(GAUBase):\n    \"\"\"\n    MetaTTT: A Test-Time Training GAU enhanced with meta-learning, gated mechanisms, and uncertainty-aware compression.\n    \n    This class implements the MetaTTT GAU as described in the proposal. MetaTTT enhances the TTT GAU by integrating meta-learning for dynamic hidden state adaptation, gated mechanisms for selective information flow, and uncertainty-aware compression for efficient memory usage.\n    \n    **Methods:**\n    \n    - **__init__**: Initializes the MetaTTT GAU, including the meta-learner, gating mechanisms, uncertainty estimation module, and compression gate.\n    - **_forward**: Implements the forward pass, including hidden state updates, gating mechanisms, uncertainty estimation, and compression.\n    \n    **Mathematical Formulation:**\n    \n    - **Meta-Learned Hidden State Update**:\n      \\\\( W_t = M(W_{t-1}, x_t; \theta_M) \\\\)\n    - **Gated Query and Key Modulation**:\n      \\\\( Q_t' = Q_t \\\\odot \\\\sigma(W_{GQ} W_t + b_{GQ}) \\\\)\n      \\\\( K_t' = K_t \\\\odot \\\\sigma(W_{GK} W_t + b_{GK}) \\\\)\n    - **Uncertainty-Aware Compression**:\n      \\\\( C_t = \\\\sigma(W_U \\\\cdot U(W_t) + b_U) \\\\)\n      \\\\( W_t' = W_t \\\\odot C_t \\\\)\n\n    **Args:**\n        embed_dim (int): The size of the input embeddings.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): The device to run the model on.\n        dtype (torch.dtype, optional): The data type for model parameters.\n        hidden_dim (int): The dimension of the hidden layer in the meta-learner. Default: 128.\n    \n    **Example:**\n        This is how you can use this function:\n    \n        meta_ttt = MetaTTT(embed_dim=768, block_loc=(0, 1), kwarg_all={})\n        X = torch.randn(32, 128, 768)\n        Y, Z = meta_ttt(X)\n    \n    **References:**\n    - Sun, Y. et al., 2024. Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n    - Other references as per the proposal.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_dim: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.meta_learner = nn.Sequential(nn.Linear(embed_dim + embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            hidden_dim, embed_dim, **self.factory_kwargs))\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.uncertainty_module = nn.Sequential(nn.Linear(embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.Softplus(), nn.Linear(\n            hidden_dim, 1, **self.factory_kwargs))\n        self.compress_gate = nn.Linear(1, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        W_prev = Z.get('hidden_state', torch.zeros(B, D, device=X.device,\n            dtype=X.dtype))\n        W_prev_expanded = W_prev.unsqueeze(1).expand(-1, L, -1)\n        meta_input = torch.cat([W_prev_expanded, X], dim=-1)\n        W_new = self.meta_learner(meta_input)\n        W_new_last = W_new[:, -1, :]\n        uncertainty = self.uncertainty_module(W_new)\n        compress_factor = torch.sigmoid(self.compress_gate(uncertainty))\n        W_compressed = W_new * compress_factor\n        W_compressed_last = W_compressed[:, -1, :]\n        Z['hidden_state'] = W_compressed_last\n        X_enhanced = X + W_compressed\n        Q = self.q_proj(X_enhanced)\n        K = self.k_proj(X_enhanced)\n        V = self.v_proj(X_enhanced)\n        G_Q = torch.sigmoid(self.gate_q(W_new))\n        G_K = torch.sigmoid(self.gate_k(W_new))\n        Q = Q * G_Q\n        K = K * G_K\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.embed_dim ** 0.5\n        mask = torch.tril(torch.ones(L, L, device=attn_scores.device)\n            ).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        Y = self.o_proj(attn_output)\n        return Y, Z\n\n\ngab_config = {'hidden_dim': 128}\n\n\n\nautoconfig = {\n    'd_model': 1536,\n    'n_block': 36\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = MetaTTT(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass MetaTTT(GAUBase):\n    \"\"\"\n    MetaTTT: A Test-Time Training GAU enhanced with meta-learning, gated mechanisms, and uncertainty-aware compression.\n    \n    This class implements the MetaTTT GAU as described in the proposal. MetaTTT enhances the TTT GAU by integrating meta-learning for dynamic hidden state adaptation, gated mechanisms for selective information flow, and uncertainty-aware compression for efficient memory usage.\n    \n    **Methods:**\n    \n    - **__init__**: Initializes the MetaTTT GAU, including the meta-learner, gating mechanisms, uncertainty estimation module, and compression gate.\n    - **_forward**: Implements the forward pass, including hidden state updates, gating mechanisms, uncertainty estimation, and compression.\n    \n    **Mathematical Formulation:**\n    \n    - **Meta-Learned Hidden State Update**:\n      \\\\( W_t = M(W_{t-1}, x_t; \theta_M) \\\\)\n    - **Gated Query and Key Modulation**:\n      \\\\( Q_t' = Q_t \\\\odot \\\\sigma(W_{GQ} W_t + b_{GQ}) \\\\)\n      \\\\( K_t' = K_t \\\\odot \\\\sigma(W_{GK} W_t + b_{GK}) \\\\)\n    - **Uncertainty-Aware Compression**:\n      \\\\( C_t = \\\\sigma(W_U \\\\cdot U(W_t) + b_U) \\\\)\n      \\\\( W_t' = W_t \\\\odot C_t \\\\)\n\n    **Args:**\n        embed_dim (int): The size of the input embeddings.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): The device to run the model on.\n        dtype (torch.dtype, optional): The data type for model parameters.\n        hidden_dim (int): The dimension of the hidden layer in the meta-learner. Default: 128.\n    \n    **Example:**\n        This is how you can use this function:\n    \n        meta_ttt = MetaTTT(embed_dim=768, block_loc=(0, 1), kwarg_all={})\n        X = torch.randn(32, 128, 768)\n        Y, Z = meta_ttt(X)\n    \n    **References:**\n    - Sun, Y. et al., 2024. Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n    - Other references as per the proposal.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_dim: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.meta_learner = nn.Sequential(nn.Linear(embed_dim + embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            hidden_dim, embed_dim, **self.factory_kwargs))\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.uncertainty_module = nn.Sequential(nn.Linear(embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.Softplus(), nn.Linear(\n            hidden_dim, 1, **self.factory_kwargs))\n        self.compress_gate = nn.Linear(1, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        W_prev = Z.get('hidden_state', torch.zeros(B, D, device=X.device,\n            dtype=X.dtype))\n        W_prev_expanded = W_prev.unsqueeze(1).expand(-1, L, -1)\n        meta_input = torch.cat([W_prev_expanded, X], dim=-1)\n        W_new = self.meta_learner(meta_input)\n        W_new_last = W_new[:, -1, :]\n        uncertainty = self.uncertainty_module(W_new)\n        compress_factor = torch.sigmoid(self.compress_gate(uncertainty))\n        W_compressed = W_new * compress_factor\n        W_compressed_last = W_compressed[:, -1, :]\n        Z['hidden_state'] = W_compressed_last\n        X_enhanced = X + W_compressed\n        Q = self.q_proj(X_enhanced)\n        K = self.k_proj(X_enhanced)\n        V = self.v_proj(X_enhanced)\n        G_Q = torch.sigmoid(self.gate_q(W_new))\n        G_K = torch.sigmoid(self.gate_k(W_new))\n        Q = Q * G_Q\n        K = K * G_K\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.embed_dim ** 0.5\n        mask = torch.tril(torch.ones(L, L, device=attn_scores.device)\n            ).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        Y = self.o_proj(attn_output)\n        return Y, Z\n\n\ngab_config = {'hidden_dim': 128}\n\n\n\nautoconfig = {\n    'd_model': 512,\n    'n_block': 7\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = MetaTTT(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass MetaTTT(GAUBase):\n    \"\"\"\n    MetaTTT: A Test-Time Training GAU enhanced with meta-learning, gated mechanisms, and uncertainty-aware compression.\n    \n    This class implements the MetaTTT GAU as described in the proposal. MetaTTT enhances the TTT GAU by integrating meta-learning for dynamic hidden state adaptation, gated mechanisms for selective information flow, and uncertainty-aware compression for efficient memory usage.\n    \n    **Methods:**\n    \n    - **__init__**: Initializes the MetaTTT GAU, including the meta-learner, gating mechanisms, uncertainty estimation module, and compression gate.\n    - **_forward**: Implements the forward pass, including hidden state updates, gating mechanisms, uncertainty estimation, and compression.\n    \n    **Mathematical Formulation:**\n    \n    - **Meta-Learned Hidden State Update**:\n      \\\\( W_t = M(W_{t-1}, x_t; \theta_M) \\\\)\n    - **Gated Query and Key Modulation**:\n      \\\\( Q_t' = Q_t \\\\odot \\\\sigma(W_{GQ} W_t + b_{GQ}) \\\\)\n      \\\\( K_t' = K_t \\\\odot \\\\sigma(W_{GK} W_t + b_{GK}) \\\\)\n    - **Uncertainty-Aware Compression**:\n      \\\\( C_t = \\\\sigma(W_U \\\\cdot U(W_t) + b_U) \\\\)\n      \\\\( W_t' = W_t \\\\odot C_t \\\\)\n\n    **Args:**\n        embed_dim (int): The size of the input embeddings.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): The device to run the model on.\n        dtype (torch.dtype, optional): The data type for model parameters.\n        hidden_dim (int): The dimension of the hidden layer in the meta-learner. Default: 128.\n    \n    **Example:**\n        This is how you can use this function:\n    \n        meta_ttt = MetaTTT(embed_dim=768, block_loc=(0, 1), kwarg_all={})\n        X = torch.randn(32, 128, 768)\n        Y, Z = meta_ttt(X)\n    \n    **References:**\n    - Sun, Y. et al., 2024. Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n    - Other references as per the proposal.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_dim: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.meta_learner = nn.Sequential(nn.Linear(embed_dim + embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            hidden_dim, embed_dim, **self.factory_kwargs))\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.uncertainty_module = nn.Sequential(nn.Linear(embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.Softplus(), nn.Linear(\n            hidden_dim, 1, **self.factory_kwargs))\n        self.compress_gate = nn.Linear(1, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        W_prev = Z.get('hidden_state', torch.zeros(B, D, device=X.device,\n            dtype=X.dtype))\n        W_prev_expanded = W_prev.unsqueeze(1).expand(-1, L, -1)\n        meta_input = torch.cat([W_prev_expanded, X], dim=-1)\n        W_new = self.meta_learner(meta_input)\n        W_new_last = W_new[:, -1, :]\n        uncertainty = self.uncertainty_module(W_new)\n        compress_factor = torch.sigmoid(self.compress_gate(uncertainty))\n        W_compressed = W_new * compress_factor\n        W_compressed_last = W_compressed[:, -1, :]\n        Z['hidden_state'] = W_compressed_last\n        X_enhanced = X + W_compressed\n        Q = self.q_proj(X_enhanced)\n        K = self.k_proj(X_enhanced)\n        V = self.v_proj(X_enhanced)\n        G_Q = torch.sigmoid(self.gate_q(W_new))\n        G_K = torch.sigmoid(self.gate_k(W_new))\n        Q = Q * G_Q\n        K = K * G_K\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.embed_dim ** 0.5\n        mask = torch.tril(torch.ones(L, L, device=attn_scores.device)\n            ).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        Y = self.o_proj(attn_output)\n        return Y, Z\n\n\ngab_config = {'hidden_dim': 128}\n\n\n\nautoconfig = {\n    'd_model': 2048,\n    'n_block': 37\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = MetaTTT(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass MetaTTT(GAUBase):\n    \"\"\"\n    MetaTTT: A Test-Time Training GAU enhanced with meta-learning, gated mechanisms, and uncertainty-aware compression.\n    \n    This class implements the MetaTTT GAU as described in the proposal. MetaTTT enhances the TTT GAU by integrating meta-learning for dynamic hidden state adaptation, gated mechanisms for selective information flow, and uncertainty-aware compression for efficient memory usage.\n    \n    **Methods:**\n    \n    - **__init__**: Initializes the MetaTTT GAU, including the meta-learner, gating mechanisms, uncertainty estimation module, and compression gate.\n    - **_forward**: Implements the forward pass, including hidden state updates, gating mechanisms, uncertainty estimation, and compression.\n    \n    **Mathematical Formulation:**\n    \n    - **Meta-Learned Hidden State Update**:\n      \\\\( W_t = M(W_{t-1}, x_t; \theta_M) \\\\)\n    - **Gated Query and Key Modulation**:\n      \\\\( Q_t' = Q_t \\\\odot \\\\sigma(W_{GQ} W_t + b_{GQ}) \\\\)\n      \\\\( K_t' = K_t \\\\odot \\\\sigma(W_{GK} W_t + b_{GK}) \\\\)\n    - **Uncertainty-Aware Compression**:\n      \\\\( C_t = \\\\sigma(W_U \\\\cdot U(W_t) + b_U) \\\\)\n      \\\\( W_t' = W_t \\\\odot C_t \\\\)\n\n    **Args:**\n        embed_dim (int): The size of the input embeddings.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): The device to run the model on.\n        dtype (torch.dtype, optional): The data type for model parameters.\n        hidden_dim (int): The dimension of the hidden layer in the meta-learner. Default: 128.\n    \n    **Example:**\n        This is how you can use this function:\n    \n        meta_ttt = MetaTTT(embed_dim=768, block_loc=(0, 1), kwarg_all={})\n        X = torch.randn(32, 128, 768)\n        Y, Z = meta_ttt(X)\n    \n    **References:**\n    - Sun, Y. et al., 2024. Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n    - Other references as per the proposal.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_dim: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.meta_learner = nn.Sequential(nn.Linear(embed_dim + embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            hidden_dim, embed_dim, **self.factory_kwargs))\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.uncertainty_module = nn.Sequential(nn.Linear(embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.Softplus(), nn.Linear(\n            hidden_dim, 1, **self.factory_kwargs))\n        self.compress_gate = nn.Linear(1, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        W_prev = Z.get('hidden_state', torch.zeros(B, D, device=X.device,\n            dtype=X.dtype))\n        W_prev_expanded = W_prev.unsqueeze(1).expand(-1, L, -1)\n        meta_input = torch.cat([W_prev_expanded, X], dim=-1)\n        W_new = self.meta_learner(meta_input)\n        W_new_last = W_new[:, -1, :]\n        uncertainty = self.uncertainty_module(W_new)\n        compress_factor = torch.sigmoid(self.compress_gate(uncertainty))\n        W_compressed = W_new * compress_factor\n        W_compressed_last = W_compressed[:, -1, :]\n        Z['hidden_state'] = W_compressed_last\n        X_enhanced = X + W_compressed\n        Q = self.q_proj(X_enhanced)\n        K = self.k_proj(X_enhanced)\n        V = self.v_proj(X_enhanced)\n        G_Q = torch.sigmoid(self.gate_q(W_new))\n        G_K = torch.sigmoid(self.gate_k(W_new))\n        Q = Q * G_Q\n        K = K * G_K\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.embed_dim ** 0.5\n        mask = torch.tril(torch.ones(L, L, device=attn_scores.device)\n            ).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        Y = self.o_proj(attn_output)\n        return Y, Z\n\n\ngab_config = {'hidden_dim': 128}\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 17\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = MetaTTT(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass MetaTTT(GAUBase):\n    \"\"\"\n    MetaTTT: A Test-Time Training GAU enhanced with meta-learning, gated mechanisms, and uncertainty-aware compression.\n    \n    This class implements the MetaTTT GAU as described in the proposal. MetaTTT enhances the TTT GAU by integrating meta-learning for dynamic hidden state adaptation, gated mechanisms for selective information flow, and uncertainty-aware compression for efficient memory usage.\n    \n    **Methods:**\n    \n    - **__init__**: Initializes the MetaTTT GAU, including the meta-learner, gating mechanisms, uncertainty estimation module, and compression gate.\n    - **_forward**: Implements the forward pass, including hidden state updates, gating mechanisms, uncertainty estimation, and compression.\n    \n    **Mathematical Formulation:**\n    \n    - **Meta-Learned Hidden State Update**:\n      \\\\( W_t = M(W_{t-1}, x_t; \theta_M) \\\\)\n    - **Gated Query and Key Modulation**:\n      \\\\( Q_t' = Q_t \\\\odot \\\\sigma(W_{GQ} W_t + b_{GQ}) \\\\)\n      \\\\( K_t' = K_t \\\\odot \\\\sigma(W_{GK} W_t + b_{GK}) \\\\)\n    - **Uncertainty-Aware Compression**:\n      \\\\( C_t = \\\\sigma(W_U \\\\cdot U(W_t) + b_U) \\\\)\n      \\\\( W_t' = W_t \\\\odot C_t \\\\)\n\n    **Args:**\n        embed_dim (int): The size of the input embeddings.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): The device to run the model on.\n        dtype (torch.dtype, optional): The data type for model parameters.\n        hidden_dim (int): The dimension of the hidden layer in the meta-learner. Default: 128.\n    \n    **Example:**\n        This is how you can use this function:\n    \n        meta_ttt = MetaTTT(embed_dim=768, block_loc=(0, 1), kwarg_all={})\n        X = torch.randn(32, 128, 768)\n        Y, Z = meta_ttt(X)\n    \n    **References:**\n    - Sun, Y. et al., 2024. Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n    - Other references as per the proposal.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_dim: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.meta_learner = nn.Sequential(nn.Linear(embed_dim + embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            hidden_dim, embed_dim, **self.factory_kwargs))\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.uncertainty_module = nn.Sequential(nn.Linear(embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.Softplus(), nn.Linear(\n            hidden_dim, 1, **self.factory_kwargs))\n        self.compress_gate = nn.Linear(1, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        W_prev = Z.get('hidden_state', torch.zeros(B, D, device=X.device,\n            dtype=X.dtype))\n        W_prev_expanded = W_prev.unsqueeze(1).expand(-1, L, -1)\n        meta_input = torch.cat([W_prev_expanded, X], dim=-1)\n        W_new = self.meta_learner(meta_input)\n        W_new_last = W_new[:, -1, :]\n        uncertainty = self.uncertainty_module(W_new)\n        compress_factor = torch.sigmoid(self.compress_gate(uncertainty))\n        W_compressed = W_new * compress_factor\n        W_compressed_last = W_compressed[:, -1, :]\n        Z['hidden_state'] = W_compressed_last\n        X_enhanced = X + W_compressed\n        Q = self.q_proj(X_enhanced)\n        K = self.k_proj(X_enhanced)\n        V = self.v_proj(X_enhanced)\n        G_Q = torch.sigmoid(self.gate_q(W_new))\n        G_K = torch.sigmoid(self.gate_k(W_new))\n        Q = Q * G_Q\n        K = K * G_K\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.embed_dim ** 0.5\n        mask = torch.tril(torch.ones(L, L, device=attn_scores.device)\n            ).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        Y = self.o_proj(attn_output)\n        return Y, Z\n\n\ngab_config = {'hidden_dim': 128}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = MetaTTT(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass MetaTTT(GAUBase):\n    \"\"\"\n    MetaTTT: A Test-Time Training GAU enhanced with meta-learning, gated mechanisms, and uncertainty-aware compression.\n    \n    This class implements the MetaTTT GAU as described in the proposal. MetaTTT enhances the TTT GAU by integrating meta-learning for dynamic hidden state adaptation, gated mechanisms for selective information flow, and uncertainty-aware compression for efficient memory usage.\n    \n    **Methods:**\n    \n    - **__init__**: Initializes the MetaTTT GAU, including the meta-learner, gating mechanisms, uncertainty estimation module, and compression gate.\n    - **_forward**: Implements the forward pass, including hidden state updates, gating mechanisms, uncertainty estimation, and compression.\n    \n    **Mathematical Formulation:**\n    \n    - **Meta-Learned Hidden State Update**:\n      \\\\( W_t = M(W_{t-1}, x_t; \theta_M) \\\\)\n    - **Gated Query and Key Modulation**:\n      \\\\( Q_t' = Q_t \\\\odot \\\\sigma(W_{GQ} W_t + b_{GQ}) \\\\)\n      \\\\( K_t' = K_t \\\\odot \\\\sigma(W_{GK} W_t + b_{GK}) \\\\)\n    - **Uncertainty-Aware Compression**:\n      \\\\( C_t = \\\\sigma(W_U \\\\cdot U(W_t) + b_U) \\\\)\n      \\\\( W_t' = W_t \\\\odot C_t \\\\)\n\n    **Args:**\n        embed_dim (int): The size of the input embeddings.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): The device to run the model on.\n        dtype (torch.dtype, optional): The data type for model parameters.\n        hidden_dim (int): The dimension of the hidden layer in the meta-learner. Default: 128.\n    \n    **Example:**\n        This is how you can use this function:\n    \n        meta_ttt = MetaTTT(embed_dim=768, block_loc=(0, 1), kwarg_all={})\n        X = torch.randn(32, 128, 768)\n        Y, Z = meta_ttt(X)\n    \n    **References:**\n    - Sun, Y. et al., 2024. Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n    - Other references as per the proposal.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_dim: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.meta_learner = nn.Sequential(nn.Linear(embed_dim + embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            hidden_dim, embed_dim, **self.factory_kwargs))\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.uncertainty_module = nn.Sequential(nn.Linear(embed_dim,\n            hidden_dim, **self.factory_kwargs), nn.Softplus(), nn.Linear(\n            hidden_dim, 1, **self.factory_kwargs))\n        self.compress_gate = nn.Linear(1, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        W_prev = Z.get('hidden_state', torch.zeros(B, D, device=X.device,\n            dtype=X.dtype))\n        W_prev_expanded = W_prev.unsqueeze(1).expand(-1, L, -1)\n        meta_input = torch.cat([W_prev_expanded, X], dim=-1)\n        W_new = self.meta_learner(meta_input)\n        W_new_last = W_new[:, -1, :]\n        uncertainty = self.uncertainty_module(W_new)\n        compress_factor = torch.sigmoid(self.compress_gate(uncertainty))\n        W_compressed = W_new * compress_factor\n        W_compressed_last = W_compressed[:, -1, :]\n        Z['hidden_state'] = W_compressed_last\n        X_enhanced = X + W_compressed\n        Q = self.q_proj(X_enhanced)\n        K = self.k_proj(X_enhanced)\n        V = self.v_proj(X_enhanced)\n        G_Q = torch.sigmoid(self.gate_q(W_new))\n        G_K = torch.sigmoid(self.gate_k(W_new))\n        Q = Q * G_Q\n        K = K * G_K\n        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.embed_dim ** 0.5\n        mask = torch.tril(torch.ones(L, L, device=attn_scores.device)\n            ).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_output = torch.bmm(attn_weights, V)\n        Y = self.o_proj(attn_output)\n        return Y, Z\n\n\ngab_config = {'hidden_dim': 128}\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 35\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}