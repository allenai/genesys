{
    "variantname": "SelectiveStateAttention",
    "review": "The SelectiveStateAttention (SSA) proposal presents an innovative approach to enhancing AdaptiveLinearAttention by integrating selective state space mechanisms with efficient linear attention computation. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nCLARITY (4.5/5):\n+ Well-structured presentation with clear objectives and motivation\n+ Detailed mathematical formulations and implementation guidelines\n+ Comprehensive pseudo-code implementation\n+ Clear explanation of architectural components\n- Some implementation details about state management could be more explicit\n\nINNOVATION (4.2/5):\n+ Novel integration of selective state mechanisms with linear attention\n+ Unique approach to hardware-efficient parallel processing\n+ Innovative dynamic memory management based on content importance\n- Some overlap with existing approaches (Mamba, CHELA)\n- Integration strategy shares similarities with previous work\n\nFEASIBILITY (4.0/5):\n+ Clear implementation path with detailed pseudo-code\n+ Practical considerations for hardware efficiency\n+ Reasonable memory requirements\nConcerns:\n- Complex integration of multiple components\n- Potential challenges in training stability\n- Need for careful parameter tuning\n\nSCALABILITY (4.3/5):\n+ Linear complexity with sequence length\n+ Efficient memory scaling through selective mechanisms\n+ Hardware-friendly implementation strategy\nConcerns:\n- Memory requirements for very long sequences\n- Potential bottlenecks in state management\n- Trade-offs between compression and model quality\n\nACCURACY AND ROBUSTNESS (4.0/5):\n+ Enhanced expressiveness through selective state mechanisms\n+ Improved handling of long-range dependencies\n+ Balance between local and global information processing\nConcerns:\n- Impact of compression on model quality needs validation\n- Stability under different input distributions requires testing\n- Trade-offs between efficiency and accuracy need empirical validation\n\nEFFICIENCY (4.4/5):\n+ Linear computational complexity\n+ Reduced memory usage through selective processing\n+ Hardware-aware design considerations\nLimitations:\n- Additional computation from state management\n- Memory overhead from state tracking\n- Implementation complexity may affect practical efficiency\n\nSTRENGTHS:\n1. Theoretical Foundation:\n- Strong theoretical grounding in state space models and attention mechanisms\n- Clear mathematical formulation of selective state updates\n- Well-justified architectural choices\n\n2. Implementation Design:\n- Detailed pseudo-code implementation\n- Hardware-aware optimization strategies\n- Efficient memory management techniques\n\n3. Innovation:\n- Novel integration of selective state mechanisms with linear attention\n- Unique approach to parallel processing\n- Innovative memory management strategy\n\nCONCERNS:\n1. Implementation Complexity:\n- Integration of multiple complex components\n- Potential challenges in training stability\n- Need for careful parameter tuning\n\n2. Empirical Validation:\n- Limited discussion of experimental validation\n- Need for comprehensive benchmarking\n- Impact on model quality requires testing\n\n3. Practical Efficiency:\n- Trade-offs between compression and accuracy\n- Memory overhead from state tracking\n- Implementation complexity considerations",
    "search_stack": [
        {
            "ready": false,
            "query": "state space models mamba longhorn",
            "detail": "Extract technical details about state space models, particularly focusing on Mamba and Longhorn architectures, their advantages over attention mechanisms, and implementation details.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nExtract technical details about state space models, particularly focusing on Mamba and Longhorn architectures, their advantages over attention mechanisms, and implementation details.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks (Avg. Score: 0.99)\n\n*Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 23  (*Influential: 4*)\n\n**TL;DR:** A hybrid model is introduced, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently, and suggests that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Abstract:** State-space models (SSMs), such as Mamba (Gu&Dao, 2023), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n##### *Relevant Chunk: No. 36/37 (Score: 0.99)*\n\n```\nIn International Conference on Machine Learning, pp. 35151-35174. PMLR, 2023a. 1\nvon Oswald, J., Niklasson, E., Schlegel, M., Kobayashi, S., Zucchet, N., Scherrer, N., Miller, N., Sandler, M., Vladymyrov, M., Pascanu, R., et al. Uncovering mesa-optimization algorithms in transformers. arXiv preprint arXiv:2309.05858, 2023b. URL https://arxiv.org/abs/2309. 05858. 1, 3\n\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. 3\n\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. 1\n\nXie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2021. 3, 4, 6, 13, 14\n\nYang, L., Lee, K., Nowak, R., and Papailiopoulos, D. Looped transformers are better at learning learning algorithms, 2023a. 1\n\nYang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023b. 2, 4\n\nYu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10819-10829, 2022. 10\n\nZhai, S., Talbott, W., Srivastava, N., Huang, C., Goh, H., Zhang, R., and Susskind, J. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. 2\n\nZhou, H., Bradley, A., Littwin, E., Razin, N., Saremi, O., Susskind, J., Bengio, S., and Nakkiran, P. What algorithms can transformers learn? a study in length generalization. arXiv preprint arXiv:2310.16028, 2023. 1\n\nZuo, S., Liu, X., Jiao, J., Charles, D., Manavoglu, E., Zhao, T., and Gao, J. Efficient long sequence modeling via state space augmented transformer. arXiv preprint arXiv:2212.08136, 2022. 12\n\n## A Experimental Setup\n\nIn this section, we describe our experimental design and configured setup. Our code and detailed implementations can be found in https://github.com/krafton-ai/mambaformer-icl. ## A. 1 Model architectures\n\nWe focus on decoder-only Transformer models, particularly those from the GPT-2 family (Radford et al., 2019), Mamba (Gu \\& Dao, 2023), and their hybrid variants, including Standard Hybrid and MambaFormer configurations. These models are evaluated across a range of sizes, as detailed in Table 6. Transformer layers consist of a Multi-Head Attention (MHA) block followed by a Multilayer Perceptron (MLP) block. Mamba models consist of two Mamba blocks per layer. The hybrid variants merge these approaches, combining a single MHA block with a Mamba block. For MHA blocks, we use 8 number of heads.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 4/86 (Score: 0.99)*\n\n```\nin the style of Megatron (Shoeybi et al. 2019)). The main ideas include introducing grouped-value attention (GVA) head structure, and moving all data-dependent projections to occur in parallel at the beginning of the block. The combination of the modified parallel Mamba block, together with using SSD as the inner SSM layer, results in the Mamba-2 architecture. We investigate Chinchilla scaling laws for Mamba-2 in the same setting as Mamba, finding that it Pareto dominates Mamba and Transformer++ in both perplexity and wall-clock time. We additionally train a family of\n\nMamba-2 models at varying sizes on the Pile, showing that it matches or outperforms Mamba and open source Transformers on standard downstream evaluations. For example, Mamba-2 with 2.7B parameters trained on 300B tokens on the Pile outperforms Mamba-2.8B, Pythia-2.8B and even Pythia-6.9B trained on the same dataset. Systems Optimizations. The SSD framework connects SSMs and Transformers, allowing us to leverage a rich body of work on systems optimizations developed for Transformers (Section 8). - For example, Tensor Parallelism (TP) is an important model parallelism technique to train large Transformer models by splitting each layer across GPUs on the same node. We design Mamba-2 to be TP-friendly, reducing the number of synchronization point per block by half. - For very long sequences whose activations do not fit on one device, sequence parallelism has been developed for the attention blocks. We describe how to train SSMs in general and Mamba-2 in particular with sequence parallelism, by passing the recurrent states between devices. - For finetuning with examples of different lengths, for best efficiency, Transformer requires sophisticated techniques to remove padding tokens and perform attention on variable length sequences. We show how Mamba-2 can be trained with variable sequence lengths efficiently, requiring no padding tokens. Section 9 empirically validates Mamba-2 on language modeling, training efficiency, and a difficult multi-query associative recall task (Arora, Eyuboglu, Zhang, et al. 2024). Finally, in Section 10, we provide an extended related work and discuss potential research directions opened up by our framework. Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 Background and Overview\n\n### 2.1 Structured State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional sequence $x \\in \\mathbb{R}^{\\top} \\mapsto y \\in \\mathbb{R}^{\\top}$ through an implicit latent state $h \\in \\mathbb{R}^{(\\top, N)}$. A general discrete form of structured SSMs takes the form of equation (1). $$\n\\begin{aligned}\n& h_{t}=A h_{t-1}+B x_{t} \\\\\n& y_{t}=C^{\\top} h_{t}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& h_{t}=A_{t} h_{t-1}+B_{t} x_{t} \\\\\n& y_{t}=C_{t}^{\\top} h_{t}\n\\end{aligned}\n$$\n\nwhere $A \\in \\mathbb{R}^{(N, N)}, B \\in \\mathbb{R}^{(N, 1)}, C \\in \\mathbb{R}^{(N, 1)}$. Structured SSMs are so named because the $A$ matrix controlling the temporal dynamics must be structured in order to compute this sequence-to-sequence transformation efficiently enough to be used in deep neural networks. The original structures introduced were diagonal plus low-rank (DPLR) (Gu, Goel, and R\u00e9 2022) and diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; J. T. Smith, Warrington, and Linderman 2023), which remains the most popular structure. In this work, we use the term state space model (SSM) to refer to structured SSMs. There are many flavors of such SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and convolutional models (Gu, Johnson, Goel, et al. 2021). We provide a brief overview below, and refer to prior work for more context and details (Gu 2023; Gu and Dao 2023). Continuous-time Models. The original structured SSMs originated as continuous-time maps on functions $x(t) \\in \\mathbb{R} \\mapsto$ $y(t) \\in \\mathbb{R}$, rather than operating directly on sequences. In the continuous-time perspective, in equation (1a) the matrices $(A, B)$ are not directly learned but generated from underlying parameters $(\\AA, B)$, along with a parameterized step size $\\Delta$. The \"continuous parameters\" $(\\Delta, \\AA, B)$ are converted to \"discrete parameters\" $(A, B)$ through fixed formulas $A=f_{A}(\\Delta, \\AA)$ and $B=f_{B}(\\Delta, B)$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Remark 1. While our main models adopt the same parameterization and discretization step as prior work (see Gu and Dao (2023) for details), for simplifying exposition and notation we omit it in the rest of this paper. We note that prior work on\nstructured SSMs referred to the continuous parameters $(\\AA$, have changed notation to simplify the presentation and focus directly on the discrete parameters, which govern the main SSM recurrence. Recurrent Models. Equations (1) and (2) take the form of a recurrence which is linear in its input $x$. Structured SSMs can therefore be viewed as types of recurrent neural networks (RNNs), where the linearity endows them with additional properties and allows them to avoid the sequential computation of traditional RNNs. Conversely, despite this simplification, SSMs are still fully expressive as sequence transformations (in the sense of universal approximation) (Kaul 2020; Orvieto et al. 2023; Shida Wang and Xue 2023). Convolutional Models. When the SSM's dynamics are constant through time as in equation (1), the model is called linear time-invariant (LTI). In this case, they are equivalent to convolutions. Thus, SSMs can also be viewed as types of CNNs, but where (i) the convolution kernels are implicitly parameterized through the SSM parameters $(A, B, C)$ and (ii) the convolution kernels are generally global instead of local. Conversely, through classical signal processing theory all sufficiently well-behaved convolutions can be represented as SSMs. Commonly, previous LTI SSMs would use the convolutional mode for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (1) for efficient autoregressive inference (where the inputs are seen one step at a time). Selective State Space Models. The form (2) where the parameters $(A, B, C)$ can also vary in time was introduced in Mamba as the selective SSM. Compared to the standard LTI formulation (1), this model can selectively choose to focus on or ignore inputs at every timestep. It was shown to perform much better than LTI SSMs on information-dense data such as language, especially as its state size N increases allowing for more information capacity. However, it can only be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be efficient. Even so, it is still less efficient than hardware-friendly models such as CNNs and Transformers because it does not leverage matrix multiplication units, which modern accelerators such as GPUs and TPUs are specialized for. While time-invariant SSMs are closely related to continuous, recurrent, and convolutional sequence models, they are not directly related to attention. In this paper, we show a deeper relationship between selective SSMs and attention, and use it to significantly improve the training speed of SSMs while simultaneously allowing for much larger state sizes N . ## Structured SSMs as Sequence Transformations. Definition 2.1. We use the term sequence transformation to refer to a parameterized map on sequences $Y=f_{\\theta}(X)$ where $X, Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ and $\\theta$ is an arbitrary collection of parameters. T represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_{t}, Y_{t} \\in \\mathbb{R}^{P}$. Sequence transformations (e.g. SSMs, or self-attention) are the cornerstone of deep sequence models, where they are incorporated into neural network architectures (e.g. Transformers). The SSM in (1) or (2) is a sequence transformation with $P=1$; it can be generalized to $P>1$ by simply broadcasting across this dimension (in other words, viewing the input as $P$ independent sequences and applying the SSM to each). One can think of $P$ as a head dimension, which we will elaborate on in Section 7. Definition 2.2. We define the SSM operator $\\operatorname{SSM}(A, B, C)=\\operatorname{SSM}\\left(A_{0: T}, B_{0: T}, C_{0: T}\\right)$ as the sequence transformation $X \\in$ $\\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})} \\mapsto Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ defined by equation (2). In SSMs, the $N$ dimension is a free parameter called the state size or state dimension. We also call it the state expansion factor, because it expands the size of the input/output by a factor of $N$, with implications for the computational efficiency of these models. Finally, we remark that many types of sequence transformations, such as attention, can be represented as a single matrix multiplication across the sequence dimension. Definition 2.3. We call a sequence transformation $Y=f_{\\theta}(X)$ a matrix transformation if it can be written in the form $Y=M_{\\theta} X$ where $M$ is a matrix depending on the parameters $\\theta$. We identify the sequence transformation with the matrix $M$, and often drop the dependence on $\\theta$ when clear from context. ### 2.2 Attention\n\nAttention broadly refers to a type of computation that assigns scores to every pair of positions in a sequence, allowing each element to \"attend\" to the rest. By far the most common and important variant of attention is softmax self-attention, which can be defined as\n\n$$\nY=\\operatorname{softmax}\\left(Q K^{\\top}\\right) \\cdot V\n$$\n\nfor $Q, K, V \\in \\mathbb{R}^{(\\top, \\mathrm{P})}$. The mechanism of pairwise comparisons (induced by materializing $Q K^{\\top}$ ) leads to the characteristic quadratic training cost of attention. Many variants of attention have been proposed, but all share the underlying core of these attention scores, with various approximations (Tay et al. 2022). The most important variant for this work is linear attention (Katharopoulos et al. 2020). Roughly speaking, this family of methods drops the softmax by folding it into a kernel feature map, and uses associativity of matrix multiplication to rewrite $\\left(Q K^{\\top}\\right) \\cdot V=Q \\cdot\\left(K^{\\top} V\\right)$. Moreover, in the important case of causal (autoregressive) attention, they show that when the causal mask is incorporated into the left-hand side as $\\left(L \\circ Q K^{\\top}\\right) \\cdot V$, where $L$ is the lower-triangular 1's matrix, then the right-hand side can be expanded as a recurrence.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 2/74 (Score: 1.00)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n##### *Relevant Chunk: No. 25/74 (Score: 0.98)*\n\n```\nThis validates our core motivation in Sections 3.1 and 3.3. ## 5 Discussion\n\nWe discuss related work, limitations, and some future directions. Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models. Table 9: (Ablations: Expressivity of $\\Delta$. ) The selection mechanism of $\\Delta$ constructs it with a projection of the input. Projecting it even to dim. 1 provides a large increase in performance; increasing it further provides further improvements at the cost of a modest increase in parameters. State size fixed to $N=16$. | Size of $\\Delta$ ProJ. | Params (M) | Perplexity |\n| :--- | :--- | :--- |\n| - | 358.9 | 9.12 |\n| 1 | 359.1 | 8.97 |\n| 2 | 359.3 | 8.97 |\n| 4 | 359.7 | 8.91 |\n| 8 | 360.5 | 8.83 |\n| 16 | 362.1 | 8.84 |\n| 32 | 365.2 | 8.80 |\n| 64 | 371.5 | 8.71 |\n\nTable 10: (Ablations: SSM state dimension.) (Top) Constant $\\boldsymbol{B}$ and $\\boldsymbol{C}$ (Bottom) Selective $B$ and $C$. Increasing the SSM state dimension $N$, which can be viewed as an expansion factor on the dimension of the recurrent state, can significantly improve performance for a negligible cost in parameters/FLOPs, but only when $\\boldsymbol{B}$ and $\\boldsymbol{C}$ are also selective. Size of $\\Delta$ projection fixed to 64 . No Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally defined as discretizations of continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeoff in more detail. Downstream Affordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as fine-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and affordances. Scaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper. ## 6 Conclusion\n\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for different domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model backbone. ## Acknowledgments\n\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft. ## References\n\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. \"Unitary Evolution Recurrent Neural Networks\". In: The International Conference on Machine Learning (ICML). 2016, pp. 1120-1128. [2] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \"Effective Gene Expression Prediction from Sequence by Integrating Long-range Interactions\".\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space models mamba longhorn\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: state space models mamba longhorn*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 357  (*Influential: 58*)\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space models mamba longhorn*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 158  (*Influential: 37*)\n\n#### 3. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: state space models mamba longhorn*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 4. The Illusion of State in State-Space Models\n\n*From Search Query: state space models mamba longhorn*\n\n*William Merrill, Jackson Petty, Ashish Sabharwal*\n\n**TL;DR:** Analysis of state-space models reveals that SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.\n\n**Abstract:** State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill&Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the\"state\"in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 5. Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\n\n*From Search Query: state space models mamba longhorn*\n\n*Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos*\n\n**TL;DR:** A hybrid model is introduced, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently, and suggests that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Abstract:** State-space models (SSMs), such as Mamba (Gu&Dao, 2023), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 43  (*Influential: 7*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using state space models (SSMs), particularly focusing on the Mamba and Longhorn architectures, here are the key technical details and advantages:\n\n## Mamba Architecture\n\n### Key Features\n- Mamba is a state space model that has shown promising results in sequence modeling tasks. It uses a state-space formulation to compress the history of a sequence into a state vector, which is then used for predicting the next token.\n- Mamba incorporates a separately parameterized forget gate, which is common in many SSMs. This gate helps in controlling the amount of information retained from previous states.\n\n### Limitations\n- Mamba, while efficient, still has some limitations such as the need for a separately parameterized forget gate, which can add to the model's complexity and parameter count.\n\n## Longhorn Architecture\n\n### Key Features\n- Longhorn is a novel SSM architecture that views the recurrent update of the state as solving an online learning objective. Specifically, it solves an online associative recall problem, which leads to a stable recurrent form without the need for a manually designed gating mechanism.\n- Longhorn does not require a separately parameterized forget gate, reducing the number of parameters, especially when the state size is large. This makes it more efficient and simpler compared to Mamba.\n- Longhorn demonstrates better performance than Mamba on both synthetic and large-scale sequence modeling tasks. It outperforms Mamba at the size of 1.3B parameters when trained on 100B tokens from the SlimPajama dataset.\n- Longhorn achieves a 1.8x enhancement in sampling efficiency compared to Mamba and maintains similar training speed by only replacing the SSM module in the Mamba architecture with Longhorn\u2019s recurrence.\n\n### Advantages Over Attention Mechanisms\n- **Linear Computational Complexity**: Both Mamba and Longhorn offer linear computational complexity with respect to sequence length, which is a significant improvement over the quadratic complexity of Transformer models.\n- **Efficient Handling of Long Sequences**: Longhorn, in particular, can extrapolate to 32K context length at inference time without a significant drop in perplexity, making it highly efficient for handling long sequences.\n- **Hardware-Efficient Implementations**: The design of Longhorn and Mamba allows for more efficient hardware utilization due to their simpler and more streamlined state transition mechanisms.\n\n## Implementation Details\n\n### State Transition Rules\n- In Longhorn, the state transition rules are derived from optimizing an online regression objective. This leads to an implicit online learning update, which is stable and does not require manual gating mechanisms.\n- For Mamba, the state transition involves a more traditional approach with a separately parameterized forget gate, which controls the retention of information from previous states.\n\n### Online Learning Objectives\n- Longhorn's design is based on solving an online associative recall problem, which naturally leads to a stable recurrent form. This approach aligns SSM design with online learning objectives, making the model more efficient and theoretically sound.\n\n## Integration and Optimization Strategies\n\n### Hybrid Architectures\n- Combining SSM strengths with selective attention mechanisms could be a promising direction. This could leverage the linear complexity and efficient state transitions of SSMs while incorporating the contextual understanding of attention mechanisms.\n\n### Adaptive State Updates\n- Incorporating adaptive state updates, as hinted by the online learning objectives in Longhorn, could further enhance the model's ability to handle both short and long-range dependencies.\n\n### Hardware-Specific Considerations\n- Optimizing SSMs for hardware efficiency is crucial. Longhorn's simpler state transition mechanism and the absence of a separately parameterized forget gate make it more hardware-friendly compared to Mamba.\n\nBy focusing on these aspects, researchers can develop more efficient, scalable, and robust autoregressive language models that outperform existing state-of-the-art models.\n",
            "analysis": "### Analysis Summary\n\nBased on the initial review of the seed design and references, I've identified several key areas for potential innovation in LM block design:\n\n1. The seed design (gpt_2_ala) focuses on linear attention mechanisms and context-adaptive encoding, while recent research shows promising results with state space models (SSMs) as alternatives to attention mechanisms.\n\n2. Recent works like Mamba and Longhorn demonstrate that SSMs can achieve:\n   - Linear computational complexity\n   - Better handling of long sequences\n   - Hardware-efficient implementations\n   - Strong performance on language modeling tasks\n\n3. The sibling designs (gpt_2_rsla and gpt_2_hgla) both focus on hierarchical and adaptive linear attention mechanisms, leaving room for innovation in the SSM direction.\n\n### Key References and Excerpts\n\n1. From \"Longhorn: State Space Models are Amortized Online Learners\":\n```\nThe paper introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Key features:\n- Links SSM design to online learning objectives\n- State transition rules derived from optimization\n- Outperforms Mamba on standard benchmarks\n```\n\n2. From \"MambaLRP: Explaining Selective State Space Models\":\n```\nIdentifies specific components in Mamba architecture that affect model transparency and explanation quality, suggesting areas for potential improvement in SSM designs.\n```\n\n3. From \"Coupled Mamba: Enhanced Multi-modal Fusion\":\n```\nIntroduces coupling between state chains while maintaining independence of intra-modality processes, demonstrating potential for enhanced information flow in SSMs.\n```\n\n### Key Insights\n\n1. SSM Architecture Benefits:\n   - Linear complexity in sequence length\n   - Efficient hardware utilization\n   - Strong theoretical foundations in online learning\n   - Potential for improved long-range dependency modeling\n\n2. Design Opportunities:\n   - Integration of online learning principles\n   - Enhanced state transition mechanisms\n   - Improved information flow between states\n   - Hardware-aware optimizations\n\n3. Potential Innovation Areas:\n   - Combining SSM strengths with selective attention mechanisms\n   - Incorporating adaptive state updates\n   - Optimizing for both short and long-range dependencies\n   - Enhancing parallelization capabilities\n\n### Future Search Plan\n\nNeed to investigate:\n1. Technical details of SSM implementations and optimizations\n2. Integration strategies for SSMs with existing architectures\n3. Hardware-specific considerations for efficient SSM deployment\n\nNext search queries should focus on:\n1. SSM optimization techniques and implementation details\n2. Hardware-efficient SSM variants\n3. Hybrid architectures combining SSMs with other mechanisms\n\n### References\n\n1. Liu, B., Wang, R., Wu, L., Feng, Y., Stone, P., & Liu, Q. (2024). Longhorn: State Space Models are Amortized Online Learners. arXiv preprint.\n\n2. Jafari, F., Montavon, G., M\u00fcller, K. R., & Eberle, O. (2024). MambaLRP: Explaining Selective State Space Models. arXiv preprint.\n\n3. Li, W., Zhou, H., Yu, J., Song, Z., & Yang, W. (2024). Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model. arXiv preprint.\n\n4. Hua, W., Dai, Z., Liu, H., & Le, Q. V. (2022). Transformer Quality in Linear Time. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "hybrid ssm attention architectures state tracking",
            "detail": "Search for specific techniques and architectures that combine SSMs with attention mechanisms or address state tracking limitations, focusing on implementation details and performance improvements.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific techniques and architectures that combine SSMs with attention mechanisms or address state tracking limitations, focusing on implementation details and performance improvements.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 1.00)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 1.00)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 57/74 (Score: 0.99)*\n\n```\n2019. [113] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. \"An Attention Free Transformer\". In: arXiv preprint arXiv:2105.14103 (2021). [114] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \"Effectively Modeling Time Series with Simple Discrete State Spaces\". In: The International Conference on Learning Representations (ICLR). 2023. [115] Lin Zheng, Chong Wang, and Lingpeng Kong. \"Linear complexity randomized self-attention mechanism\". In: International Conference on Machine Learning. PMLR. 2022, pp. 27011-27041. [116] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \"Efficient Long Sequence Modeling via State Space Augmented Transformer\". In: arXiv preprint arXiv:2212.08136 (2022). ## A Discussion: Selection Mechanism\n\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It can also be viewed as related to \"fast weights\" (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs with the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept that is worth clarifying. Gating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal through time and causes inputs to interact along the sequence length dimension. However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function). For example, elementwise multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta et al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of $R N N$ gating versus the popular usage of multiplicative gating actually have a very different semantic meaning. Hypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose recurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber 1992). Data-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data (Poli et al. 2023). Example: GLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear layer $y=D x$, where $D$ is a diagonal weight parameter. Now suppose that $D$ is itself generated from a linear transformation of $x$, with an optional nonlinearity: $D=\\sigma(\\boldsymbol{W} x)$. Since it is diagonal, the multiplication becomes an elementwise product: $y=\\sigma(W x) \\circ x$. This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative \"branch\"), hypernetworks (since the parameter $\\boldsymbol{D}$ is generated by another layer), and data-dependent (since $\\boldsymbol{D}$ depends on the data $x$ ). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer. Selection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructions-essentially anything with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as well-and we find it uninformative to think of them as such. Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of $\\Delta$ (Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term \"gating\" in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention. ## B Related Work\n\nWe overview several prior works related to our methods. We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV. ## B. 1 S4 Variants and Derivatives\n\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method. - S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020). - DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022). - S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the first S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO dimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism. Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where $\\bar{A}$ is manually set to 0 , instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries. - Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show that real-valued SSMs are empirically effective in certain settings or when combined with different architectural components. - Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI. - SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of S4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do fast autoregressive inference directly. Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant). ## B. 2 SSM Architectures\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer. - GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most importantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1. - Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient attention approximation. - H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures. - Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the input. While sharing the \"selection\" name, we consider this an architectural modification that is closer to architectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\nCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0 ).\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 4/86 (Score: 0.99)*\n\n```\nin the style of Megatron (Shoeybi et al. 2019)). The main ideas include introducing grouped-value attention (GVA) head structure, and moving all data-dependent projections to occur in parallel at the beginning of the block. The combination of the modified parallel Mamba block, together with using SSD as the inner SSM layer, results in the Mamba-2 architecture. We investigate Chinchilla scaling laws for Mamba-2 in the same setting as Mamba, finding that it Pareto dominates Mamba and Transformer++ in both perplexity and wall-clock time. We additionally train a family of\n\nMamba-2 models at varying sizes on the Pile, showing that it matches or outperforms Mamba and open source Transformers on standard downstream evaluations. For example, Mamba-2 with 2.7B parameters trained on 300B tokens on the Pile outperforms Mamba-2.8B, Pythia-2.8B and even Pythia-6.9B trained on the same dataset. Systems Optimizations. The SSD framework connects SSMs and Transformers, allowing us to leverage a rich body of work on systems optimizations developed for Transformers (Section 8). - For example, Tensor Parallelism (TP) is an important model parallelism technique to train large Transformer models by splitting each layer across GPUs on the same node. We design Mamba-2 to be TP-friendly, reducing the number of synchronization point per block by half. - For very long sequences whose activations do not fit on one device, sequence parallelism has been developed for the attention blocks. We describe how to train SSMs in general and Mamba-2 in particular with sequence parallelism, by passing the recurrent states between devices. - For finetuning with examples of different lengths, for best efficiency, Transformer requires sophisticated techniques to remove padding tokens and perform attention on variable length sequences. We show how Mamba-2 can be trained with variable sequence lengths efficiently, requiring no padding tokens. Section 9 empirically validates Mamba-2 on language modeling, training efficiency, and a difficult multi-query associative recall task (Arora, Eyuboglu, Zhang, et al. 2024). Finally, in Section 10, we provide an extended related work and discuss potential research directions opened up by our framework. Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 Background and Overview\n\n### 2.1 Structured State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional sequence $x \\in \\mathbb{R}^{\\top} \\mapsto y \\in \\mathbb{R}^{\\top}$ through an implicit latent state $h \\in \\mathbb{R}^{(\\top, N)}$. A general discrete form of structured SSMs takes the form of equation (1). $$\n\\begin{aligned}\n& h_{t}=A h_{t-1}+B x_{t} \\\\\n& y_{t}=C^{\\top} h_{t}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& h_{t}=A_{t} h_{t-1}+B_{t} x_{t} \\\\\n& y_{t}=C_{t}^{\\top} h_{t}\n\\end{aligned}\n$$\n\nwhere $A \\in \\mathbb{R}^{(N, N)}, B \\in \\mathbb{R}^{(N, 1)}, C \\in \\mathbb{R}^{(N, 1)}$. Structured SSMs are so named because the $A$ matrix controlling the temporal dynamics must be structured in order to compute this sequence-to-sequence transformation efficiently enough to be used in deep neural networks. The original structures introduced were diagonal plus low-rank (DPLR) (Gu, Goel, and R\u00e9 2022) and diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; J. T. Smith, Warrington, and Linderman 2023), which remains the most popular structure. In this work, we use the term state space model (SSM) to refer to structured SSMs. There are many flavors of such SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and convolutional models (Gu, Johnson, Goel, et al. 2021). We provide a brief overview below, and refer to prior work for more context and details (Gu 2023; Gu and Dao 2023). Continuous-time Models. The original structured SSMs originated as continuous-time maps on functions $x(t) \\in \\mathbb{R} \\mapsto$ $y(t) \\in \\mathbb{R}$, rather than operating directly on sequences. In the continuous-time perspective, in equation (1a) the matrices $(A, B)$ are not directly learned but generated from underlying parameters $(\\AA, B)$, along with a parameterized step size $\\Delta$. The \"continuous parameters\" $(\\Delta, \\AA, B)$ are converted to \"discrete parameters\" $(A, B)$ through fixed formulas $A=f_{A}(\\Delta, \\AA)$ and $B=f_{B}(\\Delta, B)$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Remark 1. While our main models adopt the same parameterization and discretization step as prior work (see Gu and Dao (2023) for details), for simplifying exposition and notation we omit it in the rest of this paper. We note that prior work on\nstructured SSMs referred to the continuous parameters $(\\AA$, have changed notation to simplify the presentation and focus directly on the discrete parameters, which govern the main SSM recurrence. Recurrent Models. Equations (1) and (2) take the form of a recurrence which is linear in its input $x$. Structured SSMs can therefore be viewed as types of recurrent neural networks (RNNs), where the linearity endows them with additional properties and allows them to avoid the sequential computation of traditional RNNs. Conversely, despite this simplification, SSMs are still fully expressive as sequence transformations (in the sense of universal approximation) (Kaul 2020; Orvieto et al. 2023; Shida Wang and Xue 2023). Convolutional Models. When the SSM's dynamics are constant through time as in equation (1), the model is called linear time-invariant (LTI). In this case, they are equivalent to convolutions. Thus, SSMs can also be viewed as types of CNNs, but where (i) the convolution kernels are implicitly parameterized through the SSM parameters $(A, B, C)$ and (ii) the convolution kernels are generally global instead of local. Conversely, through classical signal processing theory all sufficiently well-behaved convolutions can be represented as SSMs. Commonly, previous LTI SSMs would use the convolutional mode for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (1) for efficient autoregressive inference (where the inputs are seen one step at a time). Selective State Space Models. The form (2) where the parameters $(A, B, C)$ can also vary in time was introduced in Mamba as the selective SSM. Compared to the standard LTI formulation (1), this model can selectively choose to focus on or ignore inputs at every timestep. It was shown to perform much better than LTI SSMs on information-dense data such as language, especially as its state size N increases allowing for more information capacity. However, it can only be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be efficient. Even so, it is still less efficient than hardware-friendly models such as CNNs and Transformers because it does not leverage matrix multiplication units, which modern accelerators such as GPUs and TPUs are specialized for. While time-invariant SSMs are closely related to continuous, recurrent, and convolutional sequence models, they are not directly related to attention. In this paper, we show a deeper relationship between selective SSMs and attention, and use it to significantly improve the training speed of SSMs while simultaneously allowing for much larger state sizes N . ## Structured SSMs as Sequence Transformations. Definition 2.1. We use the term sequence transformation to refer to a parameterized map on sequences $Y=f_{\\theta}(X)$ where $X, Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ and $\\theta$ is an arbitrary collection of parameters. T represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_{t}, Y_{t} \\in \\mathbb{R}^{P}$. Sequence transformations (e.g. SSMs, or self-attention) are the cornerstone of deep sequence models, where they are incorporated into neural network architectures (e.g. Transformers). The SSM in (1) or (2) is a sequence transformation with $P=1$; it can be generalized to $P>1$ by simply broadcasting across this dimension (in other words, viewing the input as $P$ independent sequences and applying the SSM to each). One can think of $P$ as a head dimension, which we will elaborate on in Section 7. Definition 2.2. We define the SSM operator $\\operatorname{SSM}(A, B, C)=\\operatorname{SSM}\\left(A_{0: T}, B_{0: T}, C_{0: T}\\right)$ as the sequence transformation $X \\in$ $\\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})} \\mapsto Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ defined by equation (2). In SSMs, the $N$ dimension is a free parameter called the state size or state dimension. We also call it the state expansion factor, because it expands the size of the input/output by a factor of $N$, with implications for the computational efficiency of these models. Finally, we remark that many types of sequence transformations, such as attention, can be represented as a single matrix multiplication across the sequence dimension. Definition 2.3. We call a sequence transformation $Y=f_{\\theta}(X)$ a matrix transformation if it can be written in the form $Y=M_{\\theta} X$ where $M$ is a matrix depending on the parameters $\\theta$. We identify the sequence transformation with the matrix $M$, and often drop the dependence on $\\theta$ when clear from context. ### 2.2 Attention\n\nAttention broadly refers to a type of computation that assigns scores to every pair of positions in a sequence, allowing each element to \"attend\" to the rest. By far the most common and important variant of attention is softmax self-attention, which can be defined as\n\n$$\nY=\\operatorname{softmax}\\left(Q K^{\\top}\\right) \\cdot V\n$$\n\nfor $Q, K, V \\in \\mathbb{R}^{(\\top, \\mathrm{P})}$. The mechanism of pairwise comparisons (induced by materializing $Q K^{\\top}$ ) leads to the characteristic quadratic training cost of attention. Many variants of attention have been proposed, but all share the underlying core of these attention scores, with various approximations (Tay et al. 2022). The most important variant for this work is linear attention (Katharopoulos et al. 2020). Roughly speaking, this family of methods drops the softmax by folding it into a kernel feature map, and uses associativity of matrix multiplication to rewrite $\\left(Q K^{\\top}\\right) \\cdot V=Q \\cdot\\left(K^{\\top} V\\right)$. Moreover, in the important case of causal (autoregressive) attention, they show that when the causal mask is incorporated into the left-hand side as $\\left(L \\circ Q K^{\\top}\\right) \\cdot V$, where $L$ is the lower-triangular 1's matrix, then the right-hand side can be expanded as a recurrence.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid ssm attention architectures state tracking\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: hybrid ssm attention architectures state tracking*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking\n\n*From Search Query: hybrid ssm attention architectures state tracking*\n\n*Weizhe Lin, B-H Tseng, B. Byrne*\n\n**TL;DR:** A novel hybrid architecture is presented that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal, sequential prediction of slot values and captures inter-slot relationships and dependencies across domains that otherwise can be lost in sequential prediction.\n\n**Abstract:** Dialogue State Tracking is central to multi-domain task-oriented dialogue systems, responsible for extracting information from user utterances. We present a novel hybrid architecture that augments GPT-2 with representations derived from Graph Attention Networks in such a way to allow causal, sequential prediction of slot values. The model architecture captures inter-slot relationships and dependencies across domains that otherwise can be lost in sequential prediction. We report improvements in state tracking performance in MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified sparse training scenario in which DST models are trained only on session-level annotations but evaluated at the turn level. We further report detailed analyses to demonstrate the effectiveness of graph models in DST by showing that the proposed graph modules capture inter-slot dependencies and improve the predictions of values that are common to multiple domains.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 34  (*Influential: 4*)\n\n#### 3. Slot Attention with Value Normalization for Multi-domain Dialogue State Tracking\n\n*From Search Query: hybrid ssm attention architectures state tracking*\n\n*Ye Wang, Yi Guo, Siqi Zhu*\n\n**TL;DR:** A new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN, is proposed, which achieves the state-of-the-art joint accuracy and evaluation results show that even if only 30% ontology is used, VN can also contribute to the model.\n\n**Abstract:** Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST). Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence. In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN. Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value. SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span. VN is designed specifically for the use of ontology, which can convert supporting spans to the values. Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1. Besides, we evaluate VN with incomplete ontology. The results show that even if only 30% ontology is used, VN can also contribute to our model.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 27  (*Influential: 4*)\n\n#### 4. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid ssm attention architectures state tracking*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Pretraining Without Attention\n\n*From Search Query: hybrid ssm attention architectures state tracking*\n\n*Junxiong Wang, J. Yan, Albert Gu, Alexander M. Rush*\n\n**TL;DR:** This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs), and proposes a proposed model, Bidirectional Gated SSM (BiGS), that combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures.\n\n**Abstract:** Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://github.com/jxiw/BiGS.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 37  (*Influential: 0*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by combining State Space Models (SSMs) with attention mechanisms and addressing state tracking limitations, here are some specific techniques and architectures that are noteworthy:\n\n## Hybrid SSM and Attention Architectures\n\n### Taipan Architecture\nThe Taipan architecture is a hybrid model that combines the efficiency of Mamba (an SSM-based model) with enhanced long-range dependency handling through Selective Attention Layers (SALs). This architecture addresses the limitations of Mamba, which relies on the Markov assumption and can lead to information loss for tokens that need interactions with distant tokens. Taipan uses Sliding Window Attention (SWA) to maintain linear time complexity while leveraging the benefits of attention, ensuring that key information is preserved and enriched with attention outputs.\n\n## State Tracking Improvements\n\n### Mamba-2 and State Space Duality\nMamba-2, a variant of the Mamba architecture, introduces the concept of State Space Duality (SSD), which simplifies the matrix \\(A\\) into a scalar. This allows for a more efficient and scalable modeling of sequences by reformulating the state-space equations as a single matrix multiplication using semi-separable matrices. This approach achieves linear computational complexity and improves state tracking by enabling content-based interactions similar to attention mechanisms.\n\n## Combining SSMs with Attention Mechanisms\n\n### Selective State Mechanisms\nMamba and its variants incorporate selective state mechanisms that allow for content-based reasoning. These mechanisms enable SSM parameters to be functions of the input, facilitating selective propagation and improving the model's ability to capture complex dependencies. This integration helps in balancing the efficiency of SSMs with the expressiveness of attention-based models.\n\n## Addressing State Tracking Limitations\n\n### Bidirectional Modeling and Adaptive Stage-Wise Framework\nIn the context of single-cell ultra-long transcriptome data, SC-MAMBA2 introduces a bidirectional modeling approach within the state-space modules. This approach, combined with an adaptive stage-wise framework, helps in capturing intricate biological variations and regulatory elements that shorter models cannot accommodate. This design can be adapted to language models to improve state tracking by considering both forward and backward dependencies in the sequence.\n\n## Hardware-Efficient Implementations\n\n### Linear-Time Sequence Modeling\nMamba and its variants achieve linear computational complexity, which is crucial for hardware-efficient implementations. The use of structured state-space models and semi-separable matrices allows for efficient computation and scaling with sequence length, making these models more suitable for large-scale language modeling tasks.\n\n## Conclusion\n\nTo improve autoregressive language model design, the following strategies are recommended:\n\n- **Hybrid Architectures**: Use architectures like Taipan that combine the efficiency of SSMs with the long-range dependency handling of attention mechanisms.\n- **State Space Duality**: Utilize Mamba-2's State Space Duality to simplify state-space equations and improve state tracking.\n- **Selective State Mechanisms**: Incorporate content-based reasoning through selective state mechanisms.\n- **Bidirectional Modeling**: Implement bidirectional modeling approaches to capture both forward and backward dependencies.\n- **Hardware-Efficient Implementations**: Leverage linear-time sequence modeling and semi-separable matrices to optimize hardware utilization.\n\nThese approaches can help address the limitations of state tracking, improve the efficiency and expressiveness of the model, and enhance overall performance in autoregressive language modeling tasks.\n",
            "analysis": "### Analysis Summary\n\nThe initial search has revealed several key insights about state space models (SSMs) and their potential for improving language model design:\n\n1. Recent developments in SSMs (Mamba, Longhorn) show promising alternatives to attention-based architectures with:\n   - Linear computational complexity\n   - Efficient handling of long sequences\n   - Hardware-aware implementations\n   - Strong performance across multiple modalities\n\n2. Key architectural innovations:\n   - Selective state mechanisms for content-based reasoning\n   - Hardware-efficient parallel algorithms\n   - Integration with online learning principles\n   - Novel approaches to state compression and management\n\n### Key References and Excerpts\n\n1. From \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\":\n```\nKey weakness of subquadratic-time models is their inability to perform content-based reasoning. Improvements:\n- SSM parameters as functions of input for selective propagation\n- Hardware-aware parallel algorithm in recurrent mode\n- Simplified architecture without attention/MLP blocks\nResults: 5\u00d7 higher throughput than Transformers, linear scaling in sequence length\n```\n\n2. From \"Transformers are SSMs\":\n```\nDevelops rich framework of theoretical connections between SSMs and attention variants through structured semiseparable matrices. Mamba-2 achieves:\n- 2-8X faster than original Mamba\n- Competitive performance with Transformers\n- Improved hardware utilization\n```\n\n3. From \"The Illusion of State in State-Space Models\":\n```\nImportant limitations identified:\n- SSMs cannot express computation outside TC^0 complexity class\n- Struggles with certain state-tracking problems\n- Similar expressiveness limitations as non-recurrent models\n```\n\n### Key Insights for Design\n\n1. Architectural Opportunities:\n   - Combining selective state mechanisms with efficient compression\n   - Integrating online learning principles for adaptive state updates\n   - Developing hybrid approaches that leverage both SSM and attention strengths\n   - Hardware-aware optimizations for improved efficiency\n\n2. Critical Challenges:\n   - State tracking limitations need to be addressed\n   - Balance between expressiveness and efficiency\n   - Hardware utilization optimization\n   - Integration of content-based reasoning\n\n3. Design Considerations:\n   - Need for improved state representation\n   - Importance of hardware-aware implementation\n   - Balance between local and global context processing\n   - Efficient handling of long-range dependencies\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for improving state tracking capabilities\n2. Methods for combining SSM strengths with attention mechanisms\n3. Hardware optimization strategies for SSM implementations\n\nNext search queries should focus on:\n1. State tracking improvements and solutions\n2. Hybrid architectures combining SSMs with attention\n3. Hardware-efficient implementations of SSMs\n\n### References\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n2. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n3. Merrill, W., Petty, J., & Sabharwal, A. (2024). The Illusion of State in State-Space Models. arXiv preprint.\n\n4. Park, J., et al. (2024). Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks. arXiv preprint.\n\n5. Liu, B., et al. (2024). Longhorn: State Space Models are Amortized Online Learners. arXiv preprint."
        },
        {
            "ready": false,
            "query": "state compression memory management techniques",
            "detail": "Search for specific techniques and implementations related to state compression, memory management, and efficient state update mechanisms in SSMs and hybrid architectures.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific techniques and implementations related to state compression, memory management, and efficient state update mechanisms in SSMs and hybrid architectures.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.98)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.98)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.98)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.98)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.95)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.95)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 5. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.95)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.95)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state compression memory management techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Extreme Compression of Large Language Models via Additive Quantization\n\n*From Search Query: state compression memory management techniques*\n\n*Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, Dan Alistarh*\n\n**TL;DR:** AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime.\n\n**Abstract:** The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of\"extreme\"LLM compression-defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter-from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 45  (*Influential: 6*)\n\n#### 2. In defense of parameter sharing for model-compression\n\n*From Search Query: state compression memory management techniques*\n\n*Aditya Desai, Anshumali Shrivastava*\n\n**TL;DR:** It is demonstrated that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range.\n\n**Abstract:** When considering a model architecture, there are several ways to reduce its memory footprint. Historically, popular approaches included selecting smaller architectures and creating sparse networks through pruning. More recently, randomized parameter-sharing (RPS) methods have gained traction for model compression at start of training. In this paper, we comprehensively assess the trade-off between memory and accuracy across RPS, pruning techniques, and building smaller models. Our findings demonstrate that RPS, which is both data and model-agnostic, consistently outperforms/matches smaller models and all moderately informed pruning strategies, such as MAG, SNIP, SYNFLOW, and GRASP, across the entire compression range. This advantage becomes particularly pronounced in higher compression scenarios. Notably, even when compared to highly informed pruning techniques like Lottery Ticket Rewinding (LTR), RPS exhibits superior performance in high compression settings. This points out inherent capacity advantage that RPS enjoys over sparse models. Theoretically, we establish RPS as a superior technique in terms of memory-efficient representation when compared to pruning for linear models. This paper argues in favor of paradigm shift towards RPS based models. During our rigorous evaluation of RPS, we identified issues in the state-of-the-art RPS technique ROAST, specifically regarding stability (ROAST's sensitivity to initialization hyperparameters, often leading to divergence) and Pareto-continuity (ROAST's inability to recover the accuracy of the original model at zero compression). We provably address both of these issues. We refer to the modified RPS, which incorporates our improvements, as STABLE-RPS.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Compression-aware Training of Deep Networks\n\n*From Search Query: state compression memory management techniques*\n\n*J. \u00c1lvarez, M. Salzmann*\n\n**TL;DR:** It is shown that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.\n\n**Abstract:** In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training. We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 168  (*Influential: 21*)\n\n#### 4. Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy\n\n*From Search Query: state compression memory management techniques*\n\n*Asit K. Mishra, Debbie Marr*\n\n**TL;DR:** The approach, Apprentice, achieves state-of-the-art accuracies using ternary precision and 4-bit precision for variants of ResNet architecture on ImageNet dataset and presents three schemes using which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.\n\n**Abstract:** Deep learning networks have achieved state-of-the-art accuracies on computer vision workloads like image classification and object detection. The performant systems, however, typically involve big models with numerous parameters. Once trained, a challenging aspect for such top performing models is deployment on resource constrained inference systems - the models (often deep networks or wide networks or both) are compute and memory intensive. Low-precision numerics and model compression using knowledge distillation are popular techniques to lower both the compute requirements and memory footprint of these deployed models. In this paper, we study the combination of these two techniques and show that the performance of low-precision networks can be significantly improved by using knowledge distillation techniques. Our approach, Apprentice, achieves state-of-the-art accuracies using ternary precision and 4-bit precision for variants of ResNet architecture on ImageNet dataset. We present three schemes using which one can apply knowledge distillation techniques to various stages of the train-and-deploy pipeline.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2017\n\n**Citations:** 317  (*Influential: 29*)\n\n#### 5. QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks\n\n*From Search Query: state compression memory management techniques*\n\n*Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, Christopher De Sa*\n\n**TL;DR:** This work introduces QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes using three novel techniques, and introduces a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing.\n\n**Abstract:** Post-training quantization (PTQ) reduces the memory footprint of LLMs by quantizing their weights to low-precision. In this work, we introduce QuIP#, a weight-only PTQ method that achieves state-of-the-art results in extreme compression regimes ($\\le$ 4 bits per weight) using three novel techniques. First, QuIP# improves QuIP's (Chee et al., 2023) incoherence processing by using the randomized Hadamard transform, which is faster and has better theoretical properties. Second, QuIP# uses vector quantization to take advantage of the ball-shaped sub-Gaussian distribution that incoherent weights possess: specifically, we introduce a set of hardware-efficient codebooks based on the highly symmetric $E_8$ lattice, which achieves the optimal 8-dimension unit ball packing. Third, QuIP# uses fine-tuning to improve fidelity to the original model. Our experiments show that QuIP# outperforms existing PTQ methods, enables new behaviors in PTQ scaling, and supports fast inference. Our code can be found at https://github.com/Cornell-RelaxML/quip-sharp.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 43  (*Influential: 6*)\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on state compression, memory management, and efficient state update mechanisms, here are some specific techniques and implementations that can be beneficial:\n\n## State Compression Techniques\n\n### KV Cache Compression\nThe paper \"KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches\" introduces various KV cache compression methods for large language models (LLMs). This work evaluates techniques such as quantization, sparsity, and adaptive methods to reduce the memory footprint of KV caches, which are crucial for handling long-context tasks efficiently. The benchmark assesses the trade-offs between memory usage, accuracy, and computational efficiency, providing insights into balancing memory efficiency with performance.\n\n### Merkle Tree-Based State Compression\nIn the context of Solana, state compression using Merkle trees is a promising approach. The SPL State Compression Program utilizes concurrent Merkle trees to compress data, reducing the amount of data stored on-chain. This method involves hashing data pairs repeatedly to produce a single root hash, which is stored on-chain, while the raw data is stored off-chain. This approach is efficient for managing large amounts of data and ensures data integrity through verification against the on-chain root hash.\n\n## Memory Management Techniques\n\n### Efficient Memory Allocation and Deallocation\nEffective memory management techniques such as paging, segmentation, and virtual memory can be adapted for managing the memory footprint of LLMs. Paging divides memory into fixed-sized blocks, segmentation divides memory into variable-sized segments based on logical parts of a process, and virtual memory uses secondary storage as an extension of primary memory. These techniques can help in efficient allocation and deallocation of memory, enabling the system to multitask smoothly.\n\n### Off-Chain Data Storage\nSimilar to the state compression approach on Solana, storing raw data off-chain and using on-chain hashes for verification can significantly reduce the memory requirements for LLMs. This method ensures that only the necessary data is stored on-chain, while the bulk of the data is managed off-chain, reducing the overall memory footprint.\n\n## Efficient State Update Mechanisms\n\n### Concurrent Merkle Trees\nThe use of concurrent Merkle trees allows for multiple updates to be handled simultaneously without affecting transaction validity. This is particularly useful in scenarios where frequent state updates are necessary, as it ensures that the state can be updated efficiently while maintaining data integrity.\n\n### Selective State Mechanisms\nImplementing selective state mechanisms, as discussed in the context of State Space Models (SSMs), can enable content-based reasoning and improve state tracking capabilities. This involves making SSM parameters functions of the input for selective propagation, which can be integrated with attention mechanisms to enhance the model's ability to handle long contexts efficiently[Analysis Summary].\n\n## Hardware Optimization Strategies\n\n### Hardware-Aware Implementations\nHardware-aware designs are crucial for practical efficiency. Techniques such as tensor parallelism for model parallelism and sequence parallelism for handling long sequences can be leveraged to optimize the performance of hybrid architectures. This ensures that the model can scale efficiently while maintaining its performance[Analysis Summary].\n\nBy combining these state compression techniques, efficient memory management strategies, and hardware optimization methods, researchers can develop more efficient and scalable autoregressive language models that handle long-context tasks effectively while managing computational resources efficiently.\n",
            "analysis": "### Analysis Summary\n\nThe search results reveal several promising directions for enhancing LM block design by combining SSMs with attention mechanisms and addressing state tracking limitations:\n\n1. **Hybrid Architectures Evolution**:\n   - SSMs are showing competitive performance with Transformers while offering linear complexity\n   - Recent work demonstrates benefits of combining SSM strengths with attention mechanisms\n   - Hardware-aware implementations are crucial for practical efficiency\n\n2. **Key Technical Innovations**:\n   - State Space Duality (SSD) framework connects SSMs and attention variants\n   - Selective state mechanisms enable content-based reasoning\n   - Bidirectional modeling improves state tracking capabilities\n\n### Key References and Excerpts\n\n1. From \"Transformers are SSMs\":\n```\nThe SSD framework connects SSMs and Transformers, allowing leverage of systems optimizations developed for Transformers:\n- Tensor Parallelism (TP) for model parallelism\n- Sequence parallelism for long sequences\n- Variable sequence length handling without padding\n```\n\n2. From \"Mamba: Linear-Time Sequence Modeling\":\n```\nKey weakness of subquadratic-time models is inability to perform content-based reasoning. Improvements:\n- SSM parameters as functions of input for selective propagation\n- Hardware-aware parallel algorithm in recurrent mode\n- Simplified architecture without attention/MLP blocks\n```\n\n3. From \"The Expressive Capacity of State Space Models\":\n```\nSSMs and transformers have overlapping but distinct strengths:\n- SSMs excel at star-free state tracking\n- Can model bounded hierarchical structure efficiently\n- Current design choices limit expressive power\n```\n\n### Key Insights for Design\n\n1. **Architectural Integration**:\n   - Need to carefully balance SSM efficiency with attention expressiveness\n   - Hardware-aware design crucial for practical implementation\n   - State tracking capabilities can be enhanced through selective mechanisms\n\n2. **Performance Considerations**:\n   - Linear complexity achievable while maintaining model quality\n   - Content-based reasoning possible through selective state updates\n   - Bidirectional processing may improve state representation\n\n3. **Implementation Strategy**:\n   - Focus on hardware-efficient implementations\n   - Leverage tensor parallelism for scalability\n   - Consider variable sequence length handling\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient state compression and update mechanisms\n2. Methods for combining local and global context processing\n3. Hardware optimization strategies for hybrid architectures\n\nNext search queries should focus on:\n1. State compression and memory management techniques\n2. Local-global context integration methods\n3. Hardware-efficient hybrid architectures\n\n### References\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n3. Sarrof, Y., Veitsman, Y., & Hahn, M. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv preprint.\n\n4. He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hybrid attention ssm architectures memory efficient",
            "detail": "Search for specific techniques and implementations that combine attention mechanisms with SSMs in memory-efficient ways, focusing on hardware optimization and practical implementations.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific techniques and implementations that combine attention mechanisms with SSMs in memory-efficient ways, focusing on hardware optimization and practical implementations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 1.00)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n##### *Relevant Chunk: No. 4/86 (Score: 0.99)*\n\n```\nin the style of Megatron (Shoeybi et al. 2019)). The main ideas include introducing grouped-value attention (GVA) head structure, and moving all data-dependent projections to occur in parallel at the beginning of the block. The combination of the modified parallel Mamba block, together with using SSD as the inner SSM layer, results in the Mamba-2 architecture. We investigate Chinchilla scaling laws for Mamba-2 in the same setting as Mamba, finding that it Pareto dominates Mamba and Transformer++ in both perplexity and wall-clock time. We additionally train a family of\n\nMamba-2 models at varying sizes on the Pile, showing that it matches or outperforms Mamba and open source Transformers on standard downstream evaluations. For example, Mamba-2 with 2.7B parameters trained on 300B tokens on the Pile outperforms Mamba-2.8B, Pythia-2.8B and even Pythia-6.9B trained on the same dataset. Systems Optimizations. The SSD framework connects SSMs and Transformers, allowing us to leverage a rich body of work on systems optimizations developed for Transformers (Section 8). - For example, Tensor Parallelism (TP) is an important model parallelism technique to train large Transformer models by splitting each layer across GPUs on the same node. We design Mamba-2 to be TP-friendly, reducing the number of synchronization point per block by half. - For very long sequences whose activations do not fit on one device, sequence parallelism has been developed for the attention blocks. We describe how to train SSMs in general and Mamba-2 in particular with sequence parallelism, by passing the recurrent states between devices. - For finetuning with examples of different lengths, for best efficiency, Transformer requires sophisticated techniques to remove padding tokens and perform attention on variable length sequences. We show how Mamba-2 can be trained with variable sequence lengths efficiently, requiring no padding tokens. Section 9 empirically validates Mamba-2 on language modeling, training efficiency, and a difficult multi-query associative recall task (Arora, Eyuboglu, Zhang, et al. 2024). Finally, in Section 10, we provide an extended related work and discuss potential research directions opened up by our framework. Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 Background and Overview\n\n### 2.1 Structured State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional sequence $x \\in \\mathbb{R}^{\\top} \\mapsto y \\in \\mathbb{R}^{\\top}$ through an implicit latent state $h \\in \\mathbb{R}^{(\\top, N)}$. A general discrete form of structured SSMs takes the form of equation (1). $$\n\\begin{aligned}\n& h_{t}=A h_{t-1}+B x_{t} \\\\\n& y_{t}=C^{\\top} h_{t}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& h_{t}=A_{t} h_{t-1}+B_{t} x_{t} \\\\\n& y_{t}=C_{t}^{\\top} h_{t}\n\\end{aligned}\n$$\n\nwhere $A \\in \\mathbb{R}^{(N, N)}, B \\in \\mathbb{R}^{(N, 1)}, C \\in \\mathbb{R}^{(N, 1)}$. Structured SSMs are so named because the $A$ matrix controlling the temporal dynamics must be structured in order to compute this sequence-to-sequence transformation efficiently enough to be used in deep neural networks. The original structures introduced were diagonal plus low-rank (DPLR) (Gu, Goel, and R\u00e9 2022) and diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; J. T. Smith, Warrington, and Linderman 2023), which remains the most popular structure. In this work, we use the term state space model (SSM) to refer to structured SSMs. There are many flavors of such SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and convolutional models (Gu, Johnson, Goel, et al. 2021). We provide a brief overview below, and refer to prior work for more context and details (Gu 2023; Gu and Dao 2023). Continuous-time Models. The original structured SSMs originated as continuous-time maps on functions $x(t) \\in \\mathbb{R} \\mapsto$ $y(t) \\in \\mathbb{R}$, rather than operating directly on sequences. In the continuous-time perspective, in equation (1a) the matrices $(A, B)$ are not directly learned but generated from underlying parameters $(\\AA, B)$, along with a parameterized step size $\\Delta$. The \"continuous parameters\" $(\\Delta, \\AA, B)$ are converted to \"discrete parameters\" $(A, B)$ through fixed formulas $A=f_{A}(\\Delta, \\AA)$ and $B=f_{B}(\\Delta, B)$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Remark 1. While our main models adopt the same parameterization and discretization step as prior work (see Gu and Dao (2023) for details), for simplifying exposition and notation we omit it in the rest of this paper. We note that prior work on\nstructured SSMs referred to the continuous parameters $(\\AA$, have changed notation to simplify the presentation and focus directly on the discrete parameters, which govern the main SSM recurrence. Recurrent Models. Equations (1) and (2) take the form of a recurrence which is linear in its input $x$. Structured SSMs can therefore be viewed as types of recurrent neural networks (RNNs), where the linearity endows them with additional properties and allows them to avoid the sequential computation of traditional RNNs. Conversely, despite this simplification, SSMs are still fully expressive as sequence transformations (in the sense of universal approximation) (Kaul 2020; Orvieto et al. 2023; Shida Wang and Xue 2023). Convolutional Models. When the SSM's dynamics are constant through time as in equation (1), the model is called linear time-invariant (LTI). In this case, they are equivalent to convolutions. Thus, SSMs can also be viewed as types of CNNs, but where (i) the convolution kernels are implicitly parameterized through the SSM parameters $(A, B, C)$ and (ii) the convolution kernels are generally global instead of local. Conversely, through classical signal processing theory all sufficiently well-behaved convolutions can be represented as SSMs. Commonly, previous LTI SSMs would use the convolutional mode for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (1) for efficient autoregressive inference (where the inputs are seen one step at a time). Selective State Space Models. The form (2) where the parameters $(A, B, C)$ can also vary in time was introduced in Mamba as the selective SSM. Compared to the standard LTI formulation (1), this model can selectively choose to focus on or ignore inputs at every timestep. It was shown to perform much better than LTI SSMs on information-dense data such as language, especially as its state size N increases allowing for more information capacity. However, it can only be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be efficient. Even so, it is still less efficient than hardware-friendly models such as CNNs and Transformers because it does not leverage matrix multiplication units, which modern accelerators such as GPUs and TPUs are specialized for. While time-invariant SSMs are closely related to continuous, recurrent, and convolutional sequence models, they are not directly related to attention. In this paper, we show a deeper relationship between selective SSMs and attention, and use it to significantly improve the training speed of SSMs while simultaneously allowing for much larger state sizes N . ## Structured SSMs as Sequence Transformations. Definition 2.1. We use the term sequence transformation to refer to a parameterized map on sequences $Y=f_{\\theta}(X)$ where $X, Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ and $\\theta$ is an arbitrary collection of parameters. T represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_{t}, Y_{t} \\in \\mathbb{R}^{P}$. Sequence transformations (e.g. SSMs, or self-attention) are the cornerstone of deep sequence models, where they are incorporated into neural network architectures (e.g. Transformers). The SSM in (1) or (2) is a sequence transformation with $P=1$; it can be generalized to $P>1$ by simply broadcasting across this dimension (in other words, viewing the input as $P$ independent sequences and applying the SSM to each). One can think of $P$ as a head dimension, which we will elaborate on in Section 7. Definition 2.2. We define the SSM operator $\\operatorname{SSM}(A, B, C)=\\operatorname{SSM}\\left(A_{0: T}, B_{0: T}, C_{0: T}\\right)$ as the sequence transformation $X \\in$ $\\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})} \\mapsto Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ defined by equation (2). In SSMs, the $N$ dimension is a free parameter called the state size or state dimension. We also call it the state expansion factor, because it expands the size of the input/output by a factor of $N$, with implications for the computational efficiency of these models. Finally, we remark that many types of sequence transformations, such as attention, can be represented as a single matrix multiplication across the sequence dimension. Definition 2.3. We call a sequence transformation $Y=f_{\\theta}(X)$ a matrix transformation if it can be written in the form $Y=M_{\\theta} X$ where $M$ is a matrix depending on the parameters $\\theta$. We identify the sequence transformation with the matrix $M$, and often drop the dependence on $\\theta$ when clear from context. ### 2.2 Attention\n\nAttention broadly refers to a type of computation that assigns scores to every pair of positions in a sequence, allowing each element to \"attend\" to the rest. By far the most common and important variant of attention is softmax self-attention, which can be defined as\n\n$$\nY=\\operatorname{softmax}\\left(Q K^{\\top}\\right) \\cdot V\n$$\n\nfor $Q, K, V \\in \\mathbb{R}^{(\\top, \\mathrm{P})}$. The mechanism of pairwise comparisons (induced by materializing $Q K^{\\top}$ ) leads to the characteristic quadratic training cost of attention. Many variants of attention have been proposed, but all share the underlying core of these attention scores, with various approximations (Tay et al. 2022). The most important variant for this work is linear attention (Katharopoulos et al. 2020). Roughly speaking, this family of methods drops the softmax by folding it into a kernel feature map, and uses associativity of matrix multiplication to rewrite $\\left(Q K^{\\top}\\right) \\cdot V=Q \\cdot\\left(K^{\\top} V\\right)$. Moreover, in the important case of causal (autoregressive) attention, they show that when the causal mask is incorporated into the left-hand side as $\\left(L \\circ Q K^{\\top}\\right) \\cdot V$, where $L$ is the lower-triangular 1's matrix, then the right-hand side can be expanded as a recurrence.\n```\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.99)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.99)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid attention ssm architectures memory efficient\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hybrid attention ssm architectures memory efficient*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 2. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid attention ssm architectures memory efficient*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hybrid attention ssm architectures memory efficient*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. ABC: Attention with Bounded-memory Control\n\n*From Search Query: hybrid attention ssm architectures memory efficient*\n\n*Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, Noah A. Smith*\n\n**TL;DR:** This work shows that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and it outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.\n\n**Abstract:** Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 22  (*Influential: 3*)\n\n### 2 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: hybrid attention ssm architectures memory efficient*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: hybrid attention ssm architectures memory efficient*\n\n*ChengXiang Zhai, Chenguang Zhu, Yichong Xu, Shuohang Wang, Yang Liu, Liliang Ren*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.\n\n**Conference:** sparse-modular-activation-for-efficient\n\n**Published:** 2023-06-19\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by combining attention mechanisms with State Space Models (SSMs) in a memory-efficient manner, several approaches and techniques are noteworthy:\n\n## Hybrid Architectures\n\n### Taipan: Selective Attention Layers\nThe Taipan architecture combines the efficiency of Mamba (an SSM) with Selective Attention Layers (SALs) to handle long-range dependencies and in-context retrieval efficiently. This hybrid model selectively applies attention only to tokens that benefit from it, optimizing resource allocation and maintaining computational efficiency. This approach addresses the limitations of pure Mamba models in scenarios requiring precise memory recall and complex long-range dependencies.\n\n### E-Tamba: Layer Transplantation and Fine-Tuning\nE-Tamba introduces a method to create a Transformer-Mamba hybrid through layer transplantation and fine-tuning. This approach identifies non-critical Transformer layers and critical Mamba layers, merging them to leverage the memory efficiency of Mamba and the long-context capabilities of Transformers. E-Tamba achieves significant memory savings and competitive performance in downstream NLP tasks, making it a promising solution for memory-limited situations.\n\n## Memory-Efficient Techniques\n\n### Sequential Prefill in SSMs\nFor SSMs like Falcon Mamba, the Sequential Prefill method processes the prompt token by token or in larger chunks, mitigating the memory scaling issue seen with parallel prefill. This method allows SSMs to handle arbitrarily long prompts without the significant memory increase associated with parallel prefill, making it more memory-efficient.\n\n### Dense Hidden Connections\nDenseMamba enhances the flow of hidden information between layers by selectively integrating shallow-layer hidden states into deeper layers. This approach retains fine-grained information crucial for the final output, improving the model's expressive power without compromising memory efficiency[Analysis Summary].\n\n## Hardware Optimization\n\n### Hardware-Aware Parallel Algorithms\nImplementing hardware-aware parallel algorithms in recurrent mode can significantly optimize the performance of SSMs. This involves designing algorithms that maximize GPU utilization while minimizing memory usage, which is particularly important for long sequence generation[Analysis Summary].\n\n### Layer-Wise Importance Analysis\nE-Tamba's approach of performing layer-wise importance analysis to identify critical layers in both Transformer and Mamba models can help in optimizing the hybrid architecture for hardware efficiency. This method ensures that only the most critical layers are retained, reducing unnecessary computational and memory overhead.\n\n## Practical Implementations\n\n### Fine-Tuning Hybrid Models\nThe E-Tamba approach demonstrates that hybrid models can be effectively created and optimized through fine-tuning alone, without the need for extensive pre-training on massive datasets. This makes such hybrid models more accessible to researchers with limited resources.\n\n### Optimizing Existing Frameworks\nOptimizing existing inference frameworks to support SSMs, such as implementing Sequential Prefill, is crucial for fully leveraging the SSM architecture. This requires community support and further research to adapt frameworks like Hugging Face's transformers library to better handle SSMs.\n\nBy integrating these techniques\u2014selective attention, layer transplantation, memory-efficient prefill methods, dense hidden connections, and hardware-aware parallel algorithms\u2014researchers can develop autoregressive language models that are not only highly performant but also efficient in terms of memory and computational resources.\n",
            "analysis": "### Analysis Summary\n\nAfter analyzing the search results, several promising directions for improving LM block design have emerged, particularly around efficient state management and compression:\n\n1. **State Space Models (SSMs) Advantages**:\n   - Linear computational complexity\n   - Efficient hardware utilization\n   - Strong theoretical foundations in online learning\n   - Potential for improved long-range dependency modeling\n\n2. **Key Technical Challenges**:\n   - State tracking limitations in current SSMs\n   - Balance between compression and model quality\n   - Hardware-efficient implementation requirements\n   - Integration of content-based reasoning\n\n3. **Emerging Solutions**:\n   - Selective state mechanisms for content-based reasoning\n   - Hardware-aware parallel algorithms\n   - Dense hidden connections for improved information flow\n   - Hybrid approaches combining SSM and attention strengths\n\n### Key References and Excerpts\n\n1. From Mamba paper:\n```\nKey weakness of subquadratic-time models is their inability to perform content-based reasoning. Improvements:\n- SSM parameters as functions of input for selective propagation\n- Hardware-aware parallel algorithm in recurrent mode\n- Simplified architecture without attention/MLP blocks\n```\n\n2. From DenseMamba:\n```\nDenseSSM enhances flow of hidden information between layers by selectively integrating shallow-layer hidden states into deeper layers, retaining fine-grained information crucial for final output.\n```\n\n3. From \"The Expressive Capacity of State Space Models\":\n```\nSSMs and transformers have overlapping but distinct strengths:\n- SSMs excel at star-free state tracking\n- Can model bounded hierarchical structure efficiently\n- Current design choices limit expressive power\n```\n\n### Key Insights for Design\n\n1. **State Management**:\n   - Need for selective state propagation\n   - Balance between compression and information retention\n   - Importance of hardware-efficient implementation\n   - Integration of content-based reasoning\n\n2. **Architecture Integration**:\n   - Potential for hybrid approaches combining SSM and attention\n   - Dense connections for improved information flow\n   - Hardware-aware parallel processing\n   - Efficient memory management strategies\n\n3. **Implementation Considerations**:\n   - Focus on hardware-efficient designs\n   - Balance between local and global processing\n   - State compression without loss of critical information\n   - Efficient parallel computation support\n\n### Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for combining attention mechanisms with SSMs\n2. Memory-efficient attention variants\n3. Hardware optimization strategies for hybrid architectures\n\n### References\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n2. He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n3. Sarrof, Y., Veitsman, Y., & Hahn, M. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv preprint.\n\n4. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n5. Amo Alonso, C., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"training dynamics state space models\", \"memory optimization techniques\", \"hardware performance analysis\"",
            "detail": "Search for papers and implementations discussing:\n\n1. Training Dynamics:\n- Convergence analysis for hybrid models\n- Gradient flow optimization\n- Initialization strategies\n- Stability analysis\n\n2. Memory Optimization:\n- Compression techniques\n- Cache utilization strategies\n- Memory hierarchy design\n- Efficient state management\n\n3. Performance Analysis:\n- Benchmarking methodologies\n- Scaling characteristics\n- Hardware utilization metrics\n- Performance optimization techniques\n\nFocus on:\n- Empirical results\n- Implementation details\n- Hardware considerations\n- Performance comparisons",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations discussing:\n\n1. Training Dynamics:\n- Convergence analysis for hybrid models\n- Gradient flow optimization\n- Initialization strategies\n- Stability analysis\n\n2. Memory Optimization:\n- Compression techniques\n- Cache utilization strategies\n- Memory hierarchy design\n- Efficient state management\n\n3. Performance Analysis:\n- Benchmarking methodologies\n- Scaling characteristics\n- Hardware utilization metrics\n- Performance optimization techniques\n\nFocus on:\n- Empirical results\n- Implementation details\n- Hardware considerations\n- Performance comparisons\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Zamba: A Compact 7B SSM Hybrid Model (Avg. Score: 0.05)\n\n*Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam Ibrahim, Beren Millidge*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 6  (*Influential: 1*)\n\n**TL;DR:** Zamba is a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale and pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost.\n\n**Abstract:** In this technical report, we present Zamba, a novel 7B SSM-transformer hybrid model which achieves competitive performance against leading open-weight models at a comparable scale. Zamba is trained on 1T tokens from openly available datasets and is the best non-transformer model at this scale. Zamba pioneers a unique architecture combining a Mamba backbone with a single shared attention module, thus obtaining the benefits of attention at minimal parameter cost. Due to its architecture, Zamba is significantly faster at inference than comparable transformer models and requires substantially less memory for generation of long sequences. Zamba is pretrained in two phases: the first phase is based on existing web datasets, while the second one consists of annealing the model over high-quality instruct and synthetic datasets, and is characterized by a rapid learning rate decay. We open-source the weights and all checkpoints for Zamba, through both phase 1 and annealing phases.\n\n##### *Relevant Chunk: No. 18/31 (Score: 0.09)*\n\n```\nPeng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Du, X., Ferdinan, T., Hou, H., Kazienko, P., GV, K. K., Koco\u0144, J., Koptyra, B., Krishna, S., au2, R. M. J., Muennighoff, N., Obeid, F., Saito, A., Song, G., Tu, H., Wo\u017aniak, S., Zhang, R., Zhao, B., Zhao, Q., Zhou, P., Zhu, J., and Zhu, R.-J. (2024). Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. Poli, M., Thomas, A. W., Nguyen, E., Ponnusamy, P., Deiseroth, B., Kersting, K., Suzuki, T., Hie, B., Ermon, S., R\u00e9, C., et al. (2024). Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844. Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L. A., Rauh, M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X. L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d'Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., and Irving, G. (2022). Scaling language models: Methods, analysis \\& insights from training gopher. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.\n```\n\n##### *Relevant Chunk: No. 11/31 (Score: 0.00)*\n\n```\nAccessed: May 28, 2024. Anthony, Q., Tokpanov, Y., Glorioso, P., and Millidge, B. (2024). Blackmamba: Mixture of experts for state-space models. arXiv preprint arXiv:2402.01771. Bachmann, G., Anagnostidis, S., and Hofmann, T. (2023). Scaling mlps: A tale of inductive bias. Advances in Neural Information Processing Systems, 36. Bengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148. Biderman, S., Schoelkopf, H., Anthony, Q. G., Bradley, H., O\u2019Brien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. (2023). Pythia: A suite for analyzing large language models across training and scaling.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.01)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.01)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.01)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 39/51 (Score: 0.01)*\n\n```\nIn International conference on machine learning, pages 5156-5165. PMLR, 2020. [42] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [43] Louis Kirsch and J\u00fcrgen Schmidhuber. Meta learning backpropagation and improving it. Advances in Neural Information Processing Systems, 34:14122-14134, 2021. [44] Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural sequence models. In International Conference on Machine Learning, pages 2766-2775. PMLR, 2018. [45] Ben Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of transformer language models. arXiv preprint arXiv:1904.08378, 2019. [46] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611-626, 2023. [47] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. [48] Quoc V Le. Building high-level features using large scale unsupervised learning. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 8595-8598. IEEE, 2013. [49] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. [50] Xuan Luo, Jia-Bin Huang, Richard Szeliski, Kevin Matzen, and Johannes Kopf. Consistent video depth estimation. ACM Transactions on Graphics (ToG), 39(4):71-1, 2020. [51] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In International conference on machine learning, pages 2113-2122. PMLR, 2015. [52] Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein. Meta-learning update rules for unsupervised representation learning.\n```\n\n#### 4. Linear Attention Sequence Parallelism (Avg. Score: 0.00)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 21/24 (Score: 0.00)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open pre-trained transformer language models, 2022. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch FSDP: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, B., Liu, J., Sun, W., Chen, R., Tomlin, C. J., and Yuan, Y. pbSGD: Powered stochastic gradient descent methods for accelerated non-convex optimization. In IJCAI, pp. $3258-3266,2020$. ## A. Appendix\n\n## A.1. Hardware and Software\n\nHardware. Our experimental configuration involves a maximum of 16 DGX-A100 servers, each equipped with 8 A100 GPUs, these GPUs are interconnected through NVSwitch, ensuring an inter-GPU bandwidth of 600GBps. For inter-node communication, we employ RoCE (RDMA over Converged Ethernet) technology, utilizing 8 RoCE RDMA adapters in each server.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"training dynamics state space models\", \"memory optimization techniques\", \"hardware performance analysis\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series\n\n*From Search Query: \"training dynamics state space models\"*\n\n*Abdul Fatir Ansari, Alvin Heng, Andre Lim, Harold Soh*\n\n**TL;DR:** This work proposes the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations and proposes three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference.\n\n**Abstract:** Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved imputation and forecasting performance of NCDSSM over existing models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. Latent Matters: Learning Deep State-Space Models\n\n*From Search Query: \"training dynamics state space models\"*\n\n*Alexej Klushyn, Richard Kurle, Maximilian Soelch, Botond Cseke, Patrick van der Smagt*\n\n**TL;DR:** The EKVAE out-performs previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.\n\n**Abstract:** Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by max-imising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian \ufb01ltering/smoothing to model dynamics more accurately than RNN-based DSSMs. Our results show that the constrained optimisation framework signi\ufb01cantly improves system identi\ufb01cation and prediction accuracy on the example of established state-of-the-art DSSMs. The EKVAE out-performs previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 31  (*Influential: 3*)\n\n#### 3. Disentangled Generative Models for Robust Prediction of System Dynamics\n\n*From Search Query: \"training dynamics state space models\"*\n\n*Stathi Fotiadis, Mario Lino Valencia, Shunlong Hu, Stef Garasto, C. Cantwell, A. Bharath*\n\n**TL;DR:** Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data and can improve the long-term and out- of-distribution predictions of state-of-the-art models in video sequences.\n\n**Abstract:** Deep neural networks have become increasingly of interest in dynamical system prediction, but out-of-distribution generalization and long-term stability still remains challenging. In this work, we treat the domain parameters of dynamical systems as factors of variation of the data generating process. By leveraging ideas from supervised disentanglement and causal factorization, we aim to separate the domain parameters from the dynamics in the latent space of generative models. In our experiments we model dynamics both in phase space and in video sequences and conduct rigorous OOD evaluations. Results indicate that disentangled VAEs adapt better to domain parameters spaces that were not present in the training data. At the same time, disentanglement can improve the long-term and out-of-distribution predictions of state-of-the-art models in video sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. Symbolic Discovery of Optimization Algorithms\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le*\n\n**TL;DR:** Lion is a simple and effective optimization algorithm that requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function and is more memory-efficient than Adam as it only keeps track of the momentum.\n\n**Abstract:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 239  (*Influential: 40*)\n\n#### 5. Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen*\n\n**TL;DR:** This study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance, and introduces novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity.\n\n**Abstract:** In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 21  (*Influential: 3*)\n\n#### 6. Fast as CHITA: Neural Network Pruning with Combinatorial Optimization\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Riade Benbaki, Wenyu Chen, X. Meng, Hussein Hazimeh, N. Ponomareva, Zhe Zhao, R. Mazumder*\n\n**TL;DR:** This work proposes a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint, and extends the classical Optimal Brain Surgeon framework, CHITA, which leads to significantly better sparsity-accuracy tradeoffs than competing methods.\n\n**Abstract:** The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning. CHITA's main workhorse performs combinatorial optimization updates on a memory-friendly representation of local quadratic approximation(s) of the loss function. On a standard benchmark of pretrained models and datasets, CHITA leads to significantly better sparsity-accuracy tradeoffs than competing methods. For example, for MLPNet with only 2% of the weights retained, our approach improves the accuracy by 63% relative to the state of the art. Furthermore, when used in conjunction with fine-tuning SGD steps, our method achieves significant accuracy gains over the state-of-the-art approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 7. On The Fairness Impacts of Hardware Selection in Machine Learning\n\n*From Search Query: \"hardware performance analysis\"*\n\n*Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong Tran, Sara Hooker, Ferdinando Fioretto*\n\n**TL;DR:** It is demonstrated that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups.\n\n**Abstract:** In the machine learning ecosystem, hardware selection is often regarded as a mere utility, overshadowed by the spotlight on algorithms and data. This oversight is particularly problematic in contexts like ML-as-a-service platforms, where users often lack control over the hardware used for model deployment. How does the choice of hardware impact generalization properties? This paper investigates the influence of hardware on the delicate balance between model performance and fairness. We demonstrate that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups. Through both theoretical and empirical analysis, the paper not only identifies the underlying factors but also proposes an effective strategy for mitigating hardware-induced performance imbalances.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Multi Resolution Analysis (MRA) for Approximate Self-Attention\n\n*From Search Query: \"hardware performance analysis\"*\n\n*Zhanpeng Zeng, Sourav Pal, Jeffery Kline, G. Fung, Vikas Singh*\n\n**TL;DR:** This paper revisits classical Multiresolution Analysis concepts such as Wavelets and shows that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a MRA-based approach for self-attention with an excellent performance profile across most criteria of interest.\n\n**Abstract:** Transformers have emerged as a preferred model for many tasks in natural langugage processing and vision. Recent efforts on training and deploying Transformers more efficiently have identified many strategies to approximate the self-attention matrix, a key module in a Transformer architecture. Effective ideas include various prespecified sparsity patterns, low-rank basis expansions and combinations thereof. In this paper, we revisit classical Multiresolution Analysis (MRA) concepts such as Wavelets, whose potential value in this setting remains underexplored thus far. We show that simple approximations based on empirical feedback and design choices informed by modern hardware and implementation challenges, eventually yield a MRA-based approach for self-attention with an excellent performance profile across most criteria of interest. We undertake an extensive set of experiments and demonstrate that this multi-resolution scheme outperforms most efficient self-attention proposals and is favorable for both short and long sequences. Code is available at https://github.com/mlpen/mra-attention.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 9. Taming the Wild: A Unified Analysis of Hogwild-Style Algorithms\n\n*From Search Query: \"hardware performance analysis\"*\n\n*Christopher De Sa, Ce Zhang, K. Olukotun, C. R\u00e9*\n\n**TL;DR:** This work uses a martingale-based analysis to derive convergence rates for the convex case (Hogwild!) with relaxed assumptions on the sparsity of the problem and designs and analyzes an asynchronous SGD algorithm, called Buckwild!, that uses lower-precision arithmetic.\n\n**Abstract:** Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we use our new analysis in three ways: (1) we derive convergence rates for the convex case (Hogwild!) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called Buckwild!, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2015\n\n**Citations:** 203  (*Influential: 15*)\n\n### 6 related papers from ArXiv\n\n#### 1. Adaptive Fine-Tuning of Transformer-Based Language Models for Named\n  Entity Recognition\n\n*From Search Query: \"training dynamics state space models\"*\n\n*Felix Stollenwerk*\n\n**Abstract:** The current standard approach for fine-tuning transformer-based language\nmodels includes a fixed number of training epochs and a linear learning rate\nschedule. In order to obtain a near-optimal model for the given downstream\ntask, a search in optimization hyperparameter space is usually required. In\nparticular, the number of training epochs needs to be adjusted to the dataset\nsize. In this paper, we introduce adaptive fine-tuning, which is an alternative\napproach that uses early stopping and a custom learning rate schedule to\ndynamically adjust the number of training epochs to the dataset size. For the\nexample use case of named entity recognition, we show that our approach not\nonly makes hyperparameter search with respect to the number of training epochs\nredundant, but also leads to improved results in terms of performance,\nstability and efficiency. This holds true especially for small datasets, where\nwe outperform the state-of-the-art fine-tuning method by a large margin.\n\n**Published:** 2022-02-05T19:20:03Z  (*Updated: 2022-02-05T19:20:03Z*)\n\n\n\n#### 2. Dynamic Dialogue Policy for Continual Reinforcement Learning\n\n*From Search Query: \"training dynamics state space models\"*\n\n*Christian Geishauser, Carel van Niekerk, Nurul Lubis, Michael Heck, Hsien-Chin Lin, Shutong Feng, Milica Ga\u0161i\u0107*\n\n**Abstract:** Continual learning is one of the key components of human learning and a\nnecessary requirement of artificial intelligence. As dialogue can potentially\nspan infinitely many topics and tasks, a task-oriented dialogue system must\nhave the capability to continually learn, dynamically adapting to new\nchallenges while preserving the knowledge it already acquired. Despite the\nimportance, continual reinforcement learning of the dialogue policy has\nremained largely unaddressed. The lack of a framework with training protocols,\nbaseline models and suitable metrics, has so far hindered research in this\ndirection. In this work we fill precisely this gap, enabling research in\ndialogue policy optimisation to go from static to dynamic learning. We provide\na continual learning algorithm, baseline architectures and metrics for\nassessing continual learning models. Moreover, we propose the dynamic dialogue\npolicy transformer (DDPT), a novel dynamic architecture that can integrate new\nknowledge seamlessly, is capable of handling large state spaces and obtains\nsignificant zero-shot performance when being exposed to unseen domains, without\nany growth in network parameter size.\n\n**Published:** 2022-04-12T16:30:40Z  (*Updated: 2022-10-10T15:06:49Z*)\n\n\n\n#### 3. CompAct: Compressed Activations for Memory-Efficient LLM Training\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*\n\n**Abstract:** We introduce CompAct, a technique that reduces peak memory utilization on GPU\nby 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory\nis a major limiting factor in training LLMs, with various recent works aiming\nto reduce model memory. However most works don't target the largest component\nof allocated memory during training: the model's compute graph, which is stored\nfor the backward pass. By storing low-rank, compressed activations to be used\nin the backward pass we greatly reduce the required memory, unlike previous\nmethods which only reduce optimizer overheads or the number of trained\nparameters. Our compression uses random projection matrices, thus avoiding\nadditional memory overheads. Comparisons with previous techniques for either\npretraining or fine-tuning show that CompAct substantially improves existing\ncompute-performance tradeoffs. We expect CompAct's savings to scale even higher\nfor larger models.\n\n**Published:** 2024-10-20T10:24:38Z  (*Updated: 2024-10-20T10:24:38Z*)\n\n\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 5. Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM\n  Inference with Transferable Prompt\n\n*From Search Query: \"hardware performance analysis\"*\n\n*Zhaozhuo Xu, Zirui Liu, Beidi Chen, Yuxin Tang, Jue Wang, Kaixiong Zhou, Xia Hu, Anshumali Shrivastava*\n\n**Abstract:** While the numerous parameters in Large Language Models (LLMs) contribute to\ntheir superior performance, this massive scale makes them inefficient and\nmemory-hungry. Thus, they are hard to deploy on commodity hardware, such as one\nsingle GPU. Given the memory and power constraints of such devices, model\ncompression methods are widely employed to reduce both the model size and\ninference latency, which essentially trades off model quality in return for\nimproved efficiency. Thus, optimizing this accuracy-efficiency trade-off is\ncrucial for the LLM deployment on commodity hardware. In this paper, we\nintroduce a new perspective to optimize this trade-off by prompting compressed\nmodels. Specifically, we first observe that for certain questions, the\ngeneration quality of a compressed LLM can be significantly improved by adding\ncarefully designed hard prompts, though this isn't the case for all questions.\nBased on this observation, we propose a soft prompt learning method where we\nexpose the compressed model to the prompt learning process, aiming to enhance\nthe performance of prompts. Our experimental analysis suggests our soft prompt\nstrategy greatly improves the performance of the 8x compressed LLaMA-7B model\n(with a joint 4-bit quantization and 50% weight pruning compression), allowing\nthem to match their uncompressed counterparts on popular benchmarks. Also, we\ndemonstrate that these learned prompts can be transferred across various\ndatasets, tasks, and compression levels. Hence with this transferability, we\ncan stitch the soft prompt to a newly compressed model to improve the test-time\naccuracy in an ``in-situ'' way.\n\n**Published:** 2023-05-17T20:45:13Z  (*Updated: 2023-10-10T04:01:30Z*)\n\n\n\n#### 6. Full Stack Optimization of Transformer Inference: a Survey\n\n*From Search Query: \"hardware performance analysis\"*\n\n*Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, Amir Gholami*\n\n**Abstract:** Recent advances in state-of-the-art DNN architecture design have been moving\ntoward Transformer models. These models achieve superior accuracy across a wide\nrange of applications. This trend has been consistent over the past several\nyears since Transformer models were originally introduced. However, the amount\nof compute and bandwidth required for inference of recent Transformer models is\ngrowing at a significant rate, and this has made their deployment in\nlatency-sensitive applications challenging. As such, there has been an\nincreased focus on making Transformer models more efficient, with methods that\nrange from changing the architecture design, all the way to developing\ndedicated domain-specific accelerators. In this work, we survey different\napproaches for efficient Transformer inference, including: (i) analysis and\nprofiling of the bottlenecks in existing Transformer architectures and their\nsimilarities and differences with previous convolutional models; (ii)\nimplications of Transformer architecture on hardware, including the impact of\nnon-linear operations such as Layer Normalization, Softmax, and GELU, as well\nas linear operations, on hardware design; (iii) approaches for optimizing a\nfixed Transformer architecture; (iv) challenges in finding the right mapping\nand scheduling of operations for Transformer models; and (v) approaches for\noptimizing Transformer models by adapting the architecture using neural\narchitecture search. Finally, we perform a case study by applying the surveyed\noptimizations on Gemmini, the open-source, full-stack DNN accelerator\ngenerator, and we show how each of these approaches can yield improvements,\ncompared to previous benchmark results on Gemmini. Among other things, we find\nthat a full-stack co-design approach with the aforementioned methods can result\nin up to 88.7x speedup with a minimal performance degradation for Transformer\ninference.\n\n**Published:** 2023-02-27T18:18:13Z  (*Updated: 2023-02-27T18:18:13Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: \"training dynamics state space models\"*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 2. Simple random search provides a competitive approach to reinforcement learning\n\n*From Search Query: \"training dynamics state space models\"*\n\n*Horia Mania, Aurelia Guy, Benjamin Recht*\n\n**Abstract:** A common belief in model-free reinforcement learning is that methods based on\nrandom search in the parameter space of policies exhibit significantly worse\nsample complexity than those that explore the space of actions. We dispel such\nbeliefs by introducing a random search method for training static, linear\npolicies for continuous control problems, matching state-of-the-art sample\nefficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a\nnearly optimal controller for a challenging instance of the Linear Quadratic\nRegulator, a classical problem in control theory, when the dynamics are not\nknown. Computationally, our random search algorithm is at least 15 times more\nefficient than the fastest competing model-free methods on these benchmarks. We\ntake advantage of this computational efficiency to evaluate the performance of\nour method over hundreds of random seeds and many different hyperparameter\nconfigurations for each benchmark task. Our simulations highlight a high\nvariability in performance in these benchmark tasks, suggesting that commonly\nused estimations of sample efficiency do not adequately evaluate the\nperformance of RL algorithms.\n\n**Published:** 2018-03-19\n\n\n\n#### 3. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Shen Li, Ajit Mathews, Yuchen Hao, Geeta Chauhan, Pritam Damania, Bernard Nguyen, Can Balioglu, Alban Desmaison, Sam Shleifer, Myle Ott, Hamid Shojanazeri, Less Wright, Min Xu, Chien-chin Huang, Liang Luo, Rohan Varma, Andrew Gu, Yanli Zhao*\n\n**Abstract:** It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.\n\n**Published:** 2023-04-21\n\n\n\n#### 4. Elixir: Train a Large Language Model on a Small GPU Cluster\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Yang You, Shenggui Li, Hongxin Liu, Jiarui Fang, Haichen Huang*\n\n**Abstract:** In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art baseline. Our optimal configuration achieves up to a 3.4$\\times$ speedup on GPT-2 models compared with SOTA solutions. We hope that our work will benefit individuals who lack computing resources and expertise, granting them access to large models. The beta version of Elixir is now available at https://github.com/hpcaitech/ColossalAI/tree/feature/elixir.\n\n**Published:** 2022-12-10\n\n\n\n#### 5. Big Bird: Transformers for Longer Sequences\n\n*From Search Query: \"hardware performance analysis\"*\n\n*Anirudh Ravula, Santiago Ontanon, Manzil Zaheer, Chris Alberti, Avinava Dubey, Philip Pham, Joshua Ainslie, Amr Ahmed, Qifan Wang, Li Yang, Guru Guruganesh*\n\n**Abstract:** Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-07-28\n\n\n\n#### 6. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n\n*From Search Query: \"hardware performance analysis\"*\n\n*Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He*\n\n**Abstract:** Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.\n\n**Published:** 2019-10-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on training dynamics, memory optimization, and performance analysis, here are some key insights and references from the provided sources and additional context:\n\n### Training Dynamics\n\n#### Convergence Analysis for Hybrid Models\n- The paper on \"Selective and Simplified State Space Layers for Sequence Modeling\" introduces the S7 model, which incorporates input-dependent state transitions and reparameterization techniques. This model demonstrates stable training dynamics by controlling the gradient norm and ensuring that state transitions are well-behaved over time, which is crucial for convergence.\n\n#### Gradient Flow Optimization\n- The S7 model also employs reparameterization to ensure stable training. This technique helps in maintaining efficient training by preventing issues like exploding or vanishing gradients, which is essential for gradient flow optimization.\n\n#### Initialization Strategies\n- While the provided sources do not explicitly discuss initialization strategies, the stability analysis in the S7 model suggests that careful initialization of the state transition matrices and other parameters is crucial. Ensuring that the eigenvalues of the transition matrix have negative real parts (as mentioned in Assumption 3.2) can help in stable learning.\n\n#### Stability Analysis\n- The S7 model's stability is ensured through the reparameterization technique and the assumption that the mappings are Lipschitz continuous. This ensures small parameter changes lead to small changes in the state transition matrices, promoting stable learning and smooth transitions over time.\n\n### Memory Optimization\n\n#### Compression Techniques\n- The integration of selective state mechanisms with attention, as seen in models like Samba, involves selectively compressing sequences into recurrent hidden states. This approach can be seen as a form of compression that maintains the ability to recall memories precisely with the attention mechanism.\n\n#### Cache Utilization Strategies\n- Efficient memory management and cache utilization are highlighted in the context of hardware-aware implementations. For example, FlashAttention-2 emphasizes the importance of cache-aware implementations and optimal work partitioning to increase occupancy and efficiency.\n\n#### Memory Hierarchy Design\n- The design of hierarchical processing and divide-and-conquer approaches, as mentioned in the analysis summary, is crucial for efficient memory management. This involves designing cache-friendly memory access patterns and implementing efficient state updates.\n\n#### Efficient State Management\n- The S7 model's dynamic gating mechanism allows for selective updating of internal states based on the input signal and current state. This mechanism helps in balancing long-term and short-term dependencies and is an example of efficient state management.\n\n### Performance Analysis\n\n#### Benchmarking Methodologies\n- Benchmarking methodologies for state space models and their integration with attention mechanisms would involve evaluating performance on tasks such as the Long Range Arena benchmarks, as mentioned in the S7 model paper. This helps in comparing the performance of different models under standardized conditions.\n\n#### Scaling Characteristics\n- The scalability of state space models, particularly when integrated with attention mechanisms, is a key aspect. The S7 model demonstrates significant performance improvements across various sequence modeling tasks, indicating good scaling characteristics.\n\n#### Hardware Utilization Metrics\n- Hardware utilization metrics, such as those related to cache occupancy and thread block utilization, are important for optimizing performance. FlashAttention-2 provides insights into how to optimize these metrics through better work partitioning and parallelization strategies.\n\n#### Performance Optimization Techniques\n- Performance optimization techniques include designing cache-friendly memory access patterns, implementing efficient parallel processing strategies, and using hierarchical processing approaches. These techniques are crucial for optimizing the performance of autoregressive language models on hardware.\n\n### Additional Insights\n\n- **State Space Models & Attention Integration**: The theoretical connections between state space models and attention variants, as discussed in the analysis summary, provide a rich framework for integrating these models efficiently. This integration can lead to better expressiveness and computational efficiency.\n\n- **Hardware-Aware Implementations**: The importance of hardware-aware parallel processing optimization, as seen in FlashAttention-2, combines multiple approaches to optimize performance. This includes optimal work partitioning and parallelization across different thread blocks.\n\n- **Dynamic Memory Management**: The dynamic memory management based on content importance, as suggested in the analysis summary, shows potential for optimizing memory usage. This involves developing dynamic compression strategies and implementing efficient state updates.\n\nBy focusing on these aspects, researchers can develop autoregressive language models that are not only more efficient in training and inference but also scalable and robust, with improved performance on various tasks.\n",
            "analysis": "### Analysis Summary\n\nThe search results reveal important insights about integrating selective state space models with linear attention mechanisms and hardware-efficient implementations:\n\n1. **State Space Models & Attention Integration**\n- Mamba and Samba demonstrate successful integration of SSMs with attention\n- Theoretical connections between SSMs and attention variants through state space duality\n- Trade-offs between computational efficiency and expressiveness\n\n2. **Hardware Efficiency & Implementation**\n- FlashAttention-2 provides insights into optimal work partitioning and parallelization\n- CHELA shows benefits of hierarchical processing and divide-and-conquer approaches\n- Importance of cache-aware implementations and memory access patterns\n\n3. **Novel Elements in Proposal**\n- Integration of selective state mechanisms with linear attention appears promising\n- Hardware-aware parallel processing optimization combines multiple approaches\n- Dynamic memory management based on content importance shows potential\n\n### Key References with Excerpts\n\n1. From Transformers are SSMs (Dao & Gu, 2024):\n\"We develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices.\"\n\n2. From Samba (Ren et al., 2024):\n\"Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism.\"\n\n3. From FlashAttention-2 (Dao, 2023):\n\"We propose better work partitioning to address these issues... parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy.\"\n\n### Key Insights for Design\n\n1. **Architecture Integration**\n- Need careful design of interaction between selective state and attention mechanisms\n- Consider hierarchical processing for efficient memory management\n- Leverage theoretical foundations from SSM-attention duality\n\n2. **Hardware Optimization**\n- Implement efficient parallel processing strategies\n- Design cache-friendly memory access patterns\n- Consider work partitioning across thread blocks\n\n3. **Memory Management**\n- Develop dynamic compression strategies\n- Implement efficient state updates\n- Design cache-aware memory hierarchies\n\n### Future Search Needed\n\nNeed additional information on:\n\n1. Training Dynamics:\n- Convergence properties\n- Gradient flow optimization\n- Initialization strategies\n\n2. Memory Optimization:\n- Compression techniques\n- Cache utilization\n- Memory hierarchy design\n\n3. Performance Analysis:\n- Benchmarking methodologies\n- Scaling characteristics\n- Hardware utilization metrics\n\n### References\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. Ren, L., et al. (2024). Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arXiv preprint.\n\n3. Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. arXiv preprint.\n\n4. Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n5. Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"selective state space models\", \"linear attention mechanisms\", \"hardware efficient attention\"",
            "detail": "Search for papers and implementations that discuss:\n\n1. Integration of state space models with attention mechanisms:\n- Selective state updates\n- Memory compression techniques\n- Hardware-efficient implementations\n\n2. Linear attention optimizations:\n- Parallel processing strategies\n- Memory access patterns\n- Cache efficiency improvements\n\n3. Content-based selection mechanisms:\n- Dynamic pruning techniques\n- Adaptive computation\n- Memory management strategies\n\nFocus on:\n- Theoretical foundations\n- Implementation details\n- Performance comparisons\n- Hardware optimization techniques",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations that discuss:\n\n1. Integration of state space models with attention mechanisms:\n- Selective state updates\n- Memory compression techniques\n- Hardware-efficient implementations\n\n2. Linear attention optimizations:\n- Parallel processing strategies\n- Memory access patterns\n- Cache efficiency improvements\n\n3. Content-based selection mechanisms:\n- Dynamic pruning techniques\n- Adaptive computation\n- Memory management strategies\n\nFocus on:\n- Theoretical foundations\n- Implementation details\n- Performance comparisons\n- Hardware optimization techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.97)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.97)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.89)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 0.89)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 3. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.63)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.63)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.49)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.49)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 5. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.36)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.36)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"selective state space models\", \"linear attention mechanisms\", \"hardware efficient attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: \"selective state space models\"*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 158  (*Influential: 37*)\n\n#### 2. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: \"selective state space models\"*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: \"selective state space models\"*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 63  (*Influential: 6*)\n\n#### 4. Sparse Attention with Linear Units\n\n*From Search Query: \"linear attention mechanisms\"*\n\n*Biao Zhang, Ivan Titov, Rico Sennrich*\n\n**TL;DR:** This work introduces a novel, simple method for achieving sparsity in attention: it replaces the softmax activation with a ReLU, and shows that sparsity naturally emerges from such a formulation.\n\n**Abstract:** Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. \u2018switch off\u2019) for some queries, which is not possible with sparsified softmax alternatives.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 4*)\n\n#### 5. Rethinking Attention with Performers\n\n*From Search Query: \"linear attention mechanisms\"*\n\n*K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, Adrian Weller*\n\n**TL;DR:** Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity, without relying on any priors such as sparsity or low-rankness are introduced.\n\n**Abstract:** We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 1316  (*Influential: 183*)\n\n#### 6. Efficient Attention via Control Variates\n\n*From Search Query: \"linear attention mechanisms\"*\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 7. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han*\n\n**TL;DR:** This work designs Hardware-Aware Transformers with neural architecture search, and trains a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing, and performs an evolutionary search with a hardware latency constraint.\n\n**Abstract:** Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 243  (*Influential: 45*)\n\n### 6 related papers from ArXiv\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling\n\n*From Search Query: \"selective state space models\"*\n\n*Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11T17:50:51Z  (*Updated: 2024-06-11T17:50:51Z*)\n\n\n\n#### 2. MambaByte: Token-free Selective State Space Model\n\n*From Search Query: \"selective state space models\"*\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.\n\n**Published:** 2024-01-24T18:53:53Z  (*Updated: 2024-08-09T20:18:57Z*)\n\n\n\n#### 3. Luna: Linear Unified Nested Attention\n\n*From Search Query: \"linear attention mechanisms\"*\n\n*Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, Luke Zettlemoyer*\n\n**Abstract:** The quadratic computational and memory complexities of the Transformer's\nattention mechanism have limited its scalability for modeling long sequences.\nIn this paper, we propose Luna, a linear unified nested attention mechanism\nthat approximates softmax attention with two nested linear attention functions,\nyielding only linear (as opposed to quadratic) time and space complexity.\nSpecifically, with the first attention function, Luna packs the input sequence\ninto a sequence of fixed length. Then, the packed sequence is unpacked using\nthe second attention function. As compared to a more traditional attention\nmechanism, Luna introduces an additional sequence with a fixed length as input\nand an additional corresponding output, which allows Luna to perform attention\noperation linearly, while also storing adequate contextual information. We\nperform extensive evaluations on three benchmarks of sequence modeling tasks:\nlong-context sequence modeling, neural machine translation and masked language\nmodeling for large-scale pretraining. Competitive or even better experimental\nresults demonstrate both the effectiveness and efficiency of Luna compared to a\nvariety\n\n**Published:** 2021-06-03T01:47:26Z  (*Updated: 2021-11-02T20:23:09Z*)\n\n\n\n#### 4. Superiority of Softmax: Unveiling the Performance Edge Over Linear\n  Attention\n\n*From Search Query: \"linear attention mechanisms\"*\n\n*Yichuan Deng, Zhao Song, Tianyi Zhou*\n\n**Abstract:** Large transformer models have achieved state-of-the-art results in numerous\nnatural language processing tasks. Among the pivotal components of the\ntransformer architecture, the attention mechanism plays a crucial role in\ncapturing token interactions within sequences through the utilization of\nsoftmax function.\n  Conversely, linear attention presents a more computationally efficient\nalternative by approximating the softmax operation with linear complexity.\nHowever, it exhibits substantial performance degradation when compared to the\ntraditional softmax attention mechanism.\n  In this paper, we bridge the gap in our theoretical understanding of the\nreasons behind the practical performance gap between softmax and linear\nattention. By conducting a comprehensive comparative analysis of these two\nattention mechanisms, we shed light on the underlying reasons for why softmax\nattention outperforms linear attention in most scenarios.\n\n**Published:** 2023-10-18T03:17:57Z  (*Updated: 2023-10-18T03:17:57Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 6. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: \"selective state space models\"*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model for Semantic Segmentation of Remote Sensing Imagery\n\n*From Search Query: \"selective state space models\"*\n\n*Danfeng Hong, Xiaokang Zhang, Xiaoliang Meng, Sijun Dong, Dongxu Li, Libo Wang*\n\n**Abstract:** Semantic segmentation, as a basic tool for intelligent interpretation of remote sensing images, plays a vital role in many Earth Observation (EO) applications. Nowadays, accurate semantic segmentation of remote sensing images remains a challenge due to the complex spatial-temporal scenes and multi-scale geo-objects. Driven by the wave of deep learning (DL), CNN- and Transformer-based semantic segmentation methods have been explored widely, and these two architectures both revealed the importance of multi-scale feature representation for strengthening semantic information of geo-objects. However, the actual multi-scale feature fusion often comes with the semantic redundancy issue due to homogeneous semantic contents in pyramid features. To handle this issue, we propose a novel Mamba-based segmentation network, namely PyramidMamba. Specifically, we design a plug-and-play decoder, which develops a dense spatial pyramid pooling (DSPP) to encode rich multi-scale semantic features and a pyramid fusion Mamba (PFM) to reduce semantic redundancy in multi-scale feature fusion. Comprehensive ablation experiments illustrate the effectiveness and superiority of the proposed method in enhancing multi-scale feature representation as well as the great potential for real-time semantic segmentation. Moreover, our PyramidMamba yields state-of-the-art performance on three publicly available datasets, i.e. the OpenEarthMap (70.8% mIoU), ISPRS Vaihingen (84.8% mIoU) and Potsdam (88.0% mIoU) datasets. The code will be available at https://github.com/WangLibo1995/GeoSeg.\n\n**Published:** 2024-06-16\n\n\n\n#### 3. Linear Complexity Randomized Self-attention Mechanism\n\n*From Search Query: \"linear attention mechanisms\"*\n\n*Lingpeng Kong, Chong Wang, Lin Zheng*\n\n**Abstract:** Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.\n\n**Published:** 2022-04-10\n\n\n\n#### 4. Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation\n\n*From Search Query: \"linear attention mechanisms\"*\n\n*Jianlin Su, Rui Li, Shunyi Zheng, Chenxi Duan*\n\n**Abstract:** In this paper, to remedy this deficiency, we propose a Linear Attention Mechanism which is approximate to dot-product attention with much less memory and computational costs. The efficient design makes the incorporation between attention mechanisms and neural networks more flexible and versatile. Experiments conducted on semantic segmentation demonstrated the effectiveness of linear attention mechanism. Code is available at https://github.com/lironui/Linear-Attention-Mechanism.\n\n**Published:** 2020-07-29\n\n\n\n#### 5. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using selective state space models, linear attention mechanisms, and hardware-efficient implementations, here are some key findings and insights from the provided sources:\n\n## Integration of State Space Models with Attention Mechanisms\n\n### Selective State Updates and Memory Compression Techniques\n- The Mamba model integrates selective state spaces within a simplified neural network architecture, allowing for selective propagation or forgetting of information based on the current token. This approach improves content-based reasoning and reduces computational complexity.\n- The paper on \"Quamba: A Post-Training Quantization Recipe for Selective State Space Models\" discusses quantization techniques for SSMs, which include suppressing maximum values of input activations and quantizing output activations in an outlier-free space. This method enhances memory efficiency and maintains accuracy, making it suitable for deployment on cloud and edge platforms.\n\n### Hardware-Efficient Implementations\n- Mamba incorporates a hardware-aware parallel algorithm, which enables fast inference with 5 times higher throughput than Transformers and linear scaling in sequence length. This is crucial for hardware-efficient implementations.\n\n## Linear Attention Optimizations\n\n### Parallel Processing Strategies and Memory Access Patterns\n- The Mamba model uses a hardware-aware parallel algorithm in recurrent mode, which is designed to optimize parallel processing and memory access patterns. This approach allows for efficient handling of long sequences without the need for attention or MLP blocks.\n- Theoretical foundations of deep selective state-space models highlight the importance of parallel processing and efficient memory access. These models leverage multiplicative interactions between inputs and hidden states, which can surpass attention-powered models in efficiency and accuracy.\n\n### Cache Efficiency Improvements\n- While the sources do not explicitly discuss cache efficiency improvements, the focus on hardware-aware parallel algorithms and efficient memory access patterns implies that cache efficiency is a consideration in the design of these models.\n\n## Content-Based Selection Mechanisms\n\n### Dynamic Pruning Techniques and Adaptive Computation\n- The selective state mechanism in Mamba allows for dynamic selection of information to propagate or forget, which can be seen as a form of dynamic pruning. This mechanism is crucial for maintaining model expressiveness and efficiency.\n- Theoretical analysis of generalized selective SSMs provides a framework for understanding the expressive power of these models, including the role of cross-channel interactions and gating mechanisms. These elements are key to adaptive computation and dynamic pruning.\n\n### Memory Management Strategies\n- State compression techniques are discussed in the context of quantization for SSMs. The \"Quamba\" paper introduces a method to quantize SSMs, which includes suppressing maximum values and quantizing output activations, thereby reducing memory usage without significant loss in accuracy.\n- The SC-MAMBA2 model, designed for single-cell data, employs a bidirectional architecture and modifies the Mamba model to efficiently learn relationships among genes. This model demonstrates effective memory management through its ability to handle extensive gene context lengths and scalability.\n\n## Theoretical Foundations and Performance Comparisons\n\n### Theoretical Foundations\n- The paper on \"Theoretical Foundations of Deep Selective State-Space Models\" provides a theoretical grounding for the selectivity mechanism in SSMs using tools from Rough Path Theory. This analysis fully characterizes the expressive power of these models and identifies the gating mechanism as a crucial architectural choice.\n\n### Performance Metrics\n- Mamba models have been shown to outperform similarly sized Transformers and match larger Transformers in pretraining and downstream evaluation tasks. They achieve fast inference and linear scaling in sequence length, which is beneficial for efficiency and scalability.\n- SC-MAMBA2 demonstrates superior performance in single-cell analysis tasks, such as cell annotation, multi-batch data integration, and perturbation prediction, highlighting the model\u2019s capability to capture complex dependencies between genes.\n\n## Implementation Details and Hardware Optimization Techniques\n\n### Practical Challenges in Hardware Optimization\n- The \"Quamba\" paper addresses the practical challenges in hardware optimization by proposing a quantization method that leverages hardware acceleration features. This approach results in lower generation latency and minimal loss in accuracy.\n\n### Training Stability with Dynamic Compression\n- While the sources do not delve deeply into training stability with dynamic compression, the success of Mamba and SC-MAMBA2 models suggests that these architectures can be trained stably with appropriate techniques. Further research on the specific challenges and solutions in training stability would be beneficial.\n\n### Trade-offs Between Compression and Accuracy\n- The \"Quamba\" paper highlights that the proposed quantization method achieves a 1.72x lower generation latency with only a 0.9% drop in average accuracy on zero-shot tasks. This indicates that there are manageable trade-offs between compression and model quality.\n\nIn summary, the integration of selective state space models with attention mechanisms, optimized linear attention, and content-based selection mechanisms offers significant improvements in efficiency, scalability, and performance for autoregressive language models. Theoretical foundations, implementation details, and hardware optimization techniques are well-supported by the existing literature, particularly through the Mamba and SC-MAMBA2 models. These models provide a robust framework for further research and development in this area.\n",
            "analysis": "Initial Analysis of SelectiveStateAttention (SSA) Proposal:\n\n1. Key Components Requiring Investigation:\n\na) Selective State Mechanism:\n- Integration of state-based memory with dynamic content selection\n- Relationship to existing state space models (e.g., Mamba)\n- Impact on memory efficiency and computational complexity\n- Novelty compared to existing approaches\n\nb) Hardware-Efficient Linear Attention:\n- Implementation of parallel processing optimizations\n- Memory access patterns and cache efficiency\n- Comparison with existing linear attention methods\n- Real-world performance implications\n\nc) Memory Management:\n- State compression techniques\n- Dynamic memory allocation strategies\n- Trade-offs between compression and model quality\n- Scalability for long sequences\n\n2. Comparison with Previous Designs:\n\nThe proposal shows similarities with:\n- HierarchicalLinearAttention (HLA) in terms of memory management\n- AdaptiveHierarchicalLinearAttention (AHLA) regarding dynamic pruning\n- DynamicGatedConv in terms of content-based selection\n\nHowever, SSA introduces unique elements:\n- Integration of selective state space mechanisms\n- Hardware-aware parallel processing optimizations\n- Dynamic memory management based on content importance\n\n3. Research Questions:\n\na) Theoretical Foundations:\n- How does the selective state mechanism compare to traditional attention?\n- What are the theoretical bounds on memory efficiency?\n- How does the approach maintain model expressiveness?\n\nb) Implementation Considerations:\n- What are the practical challenges in hardware optimization?\n- How to ensure training stability with dynamic compression?\n- What are the trade-offs between compression and accuracy?\n\nc) Performance Metrics:\n- Impact on perplexity and downstream task performance\n- Memory usage and computational efficiency\n- Scalability with sequence length\n\n4. Investigation Plan:\n\nPhase 1: Theoretical Foundation\n- Research state space models and their applications\n- Analyze linear attention mechanisms\n- Study hardware optimization techniques\n\nPhase 2: Implementation Analysis\n- Examine parallel processing strategies\n- Investigate memory management techniques\n- Review hardware-specific optimizations\n\nPhase 3: Performance Evaluation\n- Compare with existing approaches\n- Assess scalability and efficiency\n- Evaluate potential trade-offs\n\nInitial findings suggest this proposal combines elements from state space models and linear attention in a novel way, with potential benefits for efficiency and scalability. Further investigation needed to validate uniqueness and practical benefits."
        },
        {
            "ready": false,
            "query": "\"memory compression techniques\", \"training stability selective state models\", \"cache optimization parallel processing\"",
            "detail": "Search for papers and implementations discussing:\n\n1. Memory management and compression:\n- State compression techniques\n- Dynamic memory allocation\n- Cache-efficient implementations\n\n2. Training stability:\n- Convergence analysis\n- Gradient flow optimization\n- Initialization strategies\n\n3. Hardware optimization:\n- Parallel processing implementations\n- Memory access patterns\n- Cache utilization techniques\n\nFocus on:\n- Implementation details\n- Empirical results\n- Theoretical analysis\n- Hardware-specific optimizations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations discussing:\n\n1. Memory management and compression:\n- State compression techniques\n- Dynamic memory allocation\n- Cache-efficient implementations\n\n2. Training stability:\n- Convergence analysis\n- Gradient flow optimization\n- Initialization strategies\n\n3. Hardware optimization:\n- Parallel processing implementations\n- Memory access patterns\n- Cache utilization techniques\n\nFocus on:\n- Implementation details\n- Empirical results\n- Theoretical analysis\n- Hardware-specific optimizations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.06)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 6/40 (Score: 0.06)*\n\n```\non pp. 1, 2, 9, 16). [3] Colin White et al. \"Neural architecture search: Insights from 1000 papers\". In: arXiv preprint arXiv:2301.08727 (2023) (cit.\n```\n\n#### 2. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.01)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n#### 3. Linear Attention Sequence Parallelism (Avg. Score: 0.01)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 21/24 (Score: 0.01)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open pre-trained transformer language models, 2022. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch FSDP: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, B., Liu, J., Sun, W., Chen, R., Tomlin, C. J., and Yuan, Y. pbSGD: Powered stochastic gradient descent methods for accelerated non-convex optimization. In IJCAI, pp. $3258-3266,2020$. ## A. Appendix\n\n## A.1. Hardware and Software\n\nHardware. Our experimental configuration involves a maximum of 16 DGX-A100 servers, each equipped with 8 A100 GPUs, these GPUs are interconnected through NVSwitch, ensuring an inter-GPU bandwidth of 600GBps. For inter-node communication, we employ RoCE (RDMA over Converged Ethernet) technology, utilizing 8 RoCE RDMA adapters in each server.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.01)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.01)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 5. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.01)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.01)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"memory compression techniques\", \"training stability selective state models\", \"cache optimization parallel processing\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models\n\n*From Search Query: \"memory compression techniques\"*\n\n*Krithika Ramesh, Arnav Chavan, Shrey Pandit, Sunayana Sitaram*\n\n**TL;DR:** This analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification and indicates that compression strategies can have an adverse effect on fairness measures.\n\n**Abstract:** Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 0*)\n\n#### 2. The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models\n\n*From Search Query: \"memory compression techniques\"*\n\n*Satya Sai, Srinath Namburi, Makesh Narsimhan Sreedhar, Srinath Srinivasan, Frederic Sala*\n\n**TL;DR:** A comprehensive analysis across multiple model families using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance, with a particular focus on tradeoffs involving parametric knowledge.\n\n**Abstract:** Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families (ENCODER, ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. We release our codebase1 to enable further research.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 3. Hardware-Aware Compression with Random Operation Access Specific Tile (ROAST) Hashing\n\n*From Search Query: \"memory compression techniques\"*\n\n*Aditya Desai, K. Zhou, Anshumali Shrivastava*\n\n**TL;DR:** With ROAST, the authors can efficiently train and deploy the model using a much smaller memory footprint in text and image classification tasks, and introduce global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest.\n\n**Abstract:** Advancements in deep learning are often associated with increasing model sizes. Training and deploying large models require sophisticated hardware and incur significantly higher costs. Thus, model compression is a widely explored approach to solving the problem. However, SOTA techniques fall short in one or more desirable aspects of compression - for instance, pruning does not reduce memory for training, quantization can only provide up to 32 \u00d7 compression, Hashed-Net is cache-inefficient, etc. This paper proposes a model-agnostic, cache-friendly, and hardware-aware model compression approach: Random Operation Access Specific Tile (ROAST) hashing. ROAST collapses the parameters by club-bing them through a lightweight mapping. While clubbing these parameters, ROAST utilizes cache hierarchies by aligning the memory access pattern with the parameter access pattern. ROAST is up to \u223c 25 \u00d7 faster to train and \u223c 50 \u00d7 faster to infer than the popular parameter sharing method HashedNet. Additionally, ROAST introduces global weight sharing, which is empirically and theoretically superior to local weight sharing in HashedNet, and can be of independent interest. With ROAST, we can efficiently train and deploy the model using a much smaller memory footprint ( \u223c 10 \u2212 100 \u00d7 lesser) in text and image classification tasks. ROAST-MM kernel implementation is open-source 1\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training\n\n*From Search Query: \"training stability selective state models\"*\n\n*Jaehyung Kim, Yuning Mao, Rui Hou, Hanchao Yu, Davis Liang, Pascale Fung, Qifan Wang, Fuli Feng, Lifu Huang, Madian Khabsa*\n\n**TL;DR:** Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, the effectiveness of RoAST is demonstrated compared to state-of-the-art fine- tuning methods on six different types of LMs, which indicates its usefulness in practice.\n\n**Abstract:** Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. RoAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, RoAST introduces adversarial perturbation during fine-tuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning methods on six different types of LMs, which indicates its usefulness in practice.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Just Fine-tune Twice: Selective Differential Privacy for Large Language Models\n\n*From Search Query: \"training stability selective state models\"*\n\n*Weiyan Shi, Si Chen, Chiyuan Zhang, R. Jia, Zhou Yu*\n\n**TL;DR:** A novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models and studies the scenario of imperfect implementation of policy functions that misses sensitive tokens and develops systematic methods to handle it.\n\n**Abstract:** Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying *differential privacy* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss. Utilizing the fact that sensitive information in language data tends to be sparse, Shi et al. (2021) formalized a DP notion extension called *Selective Differential Privacy* (SDP) to protect only the sensitive tokens defined by a policy function. However, their algorithm only works for RNN-based models. In this paper, we develop a novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models. Our method is easy to implement: it first fine-tunes the model with *redacted* in-domain data, and then fine-tunes it again with the *original* in-domain data using a private training mechanism. Furthermore, we study the scenario of imperfect implementation of policy functions that misses sensitive tokens and develop systematic methods to handle it. Experiments show that our method achieves strong utility compared to previous baselines. We also analyze the SDP privacy guarantee empirically with the canary insertion attack.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 24  (*Influential: 1*)\n\n#### 6. On the Stability of Iterative Retraining of Generative Models on their own Data\n\n*From Search Query: \"training stability selective state models\"*\n\n*Quentin Bertrand, A. Bose, Alexandre Duplessis, Marco Jiralerspong, Gauthier Gidel*\n\n**TL;DR:** This paper proves the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion of clean training data is large enough and the proportion of clean data from past models is large enough.\n\n**Abstract:** Deep generative models have made tremendous progress in modeling complex data, often exhibiting generation quality that surpasses a typical human's ability to discern the authenticity of samples. Undeniably, a key driver of this success is enabled by the massive amounts of web-scale data consumed by these models. Due to these models' striking performance and ease of availability, the web will inevitably be increasingly populated with synthetic content. Such a fact directly implies that future iterations of generative models will be trained on both clean and artificially generated data from past models. In this paper, we develop a framework to rigorously study the impact of training generative models on mixed datasets -- from classical training on real data to self-consuming generative models trained on purely synthetic data. We first prove the stability of iterative training under the condition that the initial generative models approximate the data distribution well enough and the proportion of clean training data (w.r.t. synthetic data) is large enough. We empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows and state-of-the-art diffusion models on CIFAR10 and FFHQ.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 26  (*Influential: 6*)\n\n#### 7. Cyclades: Conflict-free Asynchronous Machine Learning\n\n*From Search Query: \"cache optimization parallel processing\"*\n\n*Xinghao Pan, Maximilian Lam, Stephen Tu, Dimitris Papailiopoulos, Ce Zhang, Michael I. Jordan, K. Ramchandran, C. R\u00e9, B. Recht*\n\n**TL;DR:** This work presents CYCLADES, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting that consistently outperforms HOGWILD!-type algorithms on sufficiently sparse datasets, and offers a black-box analysis for provable speedups across a large family of algorithms.\n\n**Abstract:** We present CYCLADES, a general framework for parallelizing stochastic optimization algorithms in a shared memory setting. CYCLADES is asynchronous during shared model updates, and requires no memory locking mechanisms, similar to HOGWILD!-type algorithms. Unlike HOGWILD!, CYCLADES introduces no conflicts during the parallel execution, and offers a black-box analysis for provable speedups across a large family of algorithms. Due to its inherent conflict-free nature and cache locality, our multi-core implementation of CYCLADES consistently outperforms HOGWILD!-type algorithms on sufficiently sparse datasets, leading to up to 40% speedup gains compared to the HOGWILD! implementation of SGD, and up to 5x gains over asynchronous implementations of variance reduction algorithms.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 60  (*Influential: 3*)\n\n#### 8. KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation\n\n*From Search Query: \"cache optimization parallel processing\"*\n\n*Minsik Cho, Mohammad Rastegari, Devang Naik*\n\n**TL;DR:** This work proposes an efficient parallelization scheme, KV-Runahead to accelerate the prompt phase of LLM inference, and proposes context-level load-balancing to handle uneven KV-cache generation (due to the causal attention) and to optimize TTFT.\n\n**Abstract:** Large Language Model or LLM inference has two phases, the prompt (or prefill) phase to output the first token and the extension (or decoding) phase to the generate subsequent tokens. In this work, we propose an efficient parallelization scheme, KV-Runahead to accelerate the prompt phase. The key observation is that the extension phase generates tokens faster than the prompt phase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes the prompt phase by orchestrating multiple processes to populate the KV-cache and minimizes the time-to-first-token (TTFT). Dual-purposing the KV-cache scheme has two main benefits. First, since KV-cache is designed to leverage the causal attention map, we minimize computation and computation automatically. Second, since it already exists for the extension phase, KV-Runahead is easy to implement. We further propose context-level load-balancing to handle uneven KV-cache generation (due to the causal attention) and to optimize TTFT. Compared with an existing parallelization scheme such as tensor or sequential parallelization where keys and values are locally generated and exchanged via all-gather collectives, our experimental results demonstrate that KV-Runahead can offer over 1.4x and 1.6x speedups for Llama 7B and Falcon 7B respectively.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 9. Graph Oracle Models, Lower Bounds, and Gaps for Parallel Stochastic Optimization\n\n*From Search Query: \"cache optimization parallel processing\"*\n\n*Blake E. Woodworth, Jialei Wang, H. B. McMahan, N. Srebro*\n\n**TL;DR:** A general oracle-based framework is suggested that captures parallel stochastic optimization in different parallelization settings described by a dependency graph, and generic lower bounds are derived in terms of this graph.\n\n**Abstract:** We suggest a general oracle-based framework that captures parallel stochastic optimization in different parallelization settings described by a dependency graph, and derive generic lower bounds in terms of this graph. We then use the framework and derive lower bounds to study several specific parallel optimization settings, including delayed updates and parallel processing with intermittent communication. We highlight gaps between lower and upper bounds on the oracle complexity, and cases where the ``natural'' algorithms are not known to be optimal.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 120  (*Influential: 8*)\n\n### 6 related papers from ArXiv\n\n#### 1. CompAct: Compressed Activations for Memory-Efficient LLM Training\n\n*From Search Query: \"memory compression techniques\"*\n\n*Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*\n\n**Abstract:** We introduce CompAct, a technique that reduces peak memory utilization on GPU\nby 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory\nis a major limiting factor in training LLMs, with various recent works aiming\nto reduce model memory. However most works don't target the largest component\nof allocated memory during training: the model's compute graph, which is stored\nfor the backward pass. By storing low-rank, compressed activations to be used\nin the backward pass we greatly reduce the required memory, unlike previous\nmethods which only reduce optimizer overheads or the number of trained\nparameters. Our compression uses random projection matrices, thus avoiding\nadditional memory overheads. Comparisons with previous techniques for either\npretraining or fine-tuning show that CompAct substantially improves existing\ncompute-performance tradeoffs. We expect CompAct's savings to scale even higher\nfor larger models.\n\n**Published:** 2024-10-20T10:24:38Z  (*Updated: 2024-10-20T10:24:38Z*)\n\n\n\n#### 2. Learning to Summarize Long Texts with Memory Compression and Transfer\n\n*From Search Query: \"memory compression techniques\"*\n\n*Jaehong Park, Jonathan Pilault, Christopher Pal*\n\n**Abstract:** We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent\nneural network based encoder decoder architectures and we explore its use for\nabstractive document summarization. Mem2Mem transfers \"memories\" via\nreadable/writable external memory modules that augment both the encoder and\ndecoder. Our memory regularization compresses an encoded input article into a\nmore compact set of sentence representations. Most importantly, the memory\ncompression step performs implicit extraction without labels, sidestepping\nissues with suboptimal ground-truth data and exposure bias of hybrid\nextractive-abstractive summarization techniques. By allowing the decoder to\nread/write over the encoded input memory, the model learns to read salient\ninformation about the input article while keeping track of what has been\ngenerated. Our Mem2Mem approach yields results that are competitive with state\nof the art transformer based summarization methods, but with 16 times fewer\nparameters\n\n**Published:** 2020-10-21T21:45:44Z  (*Updated: 2020-10-21T21:45:44Z*)\n\n\n\n#### 3. MGR: Multi-generator Based Rationalization\n\n*From Search Query: \"training stability selective state models\"*\n\n*Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, Yuankai Zhang, Yang Qiu*\n\n**Abstract:** Rationalization is to employ a generator and a predictor to construct a\nself-explaining NLP model in which the generator selects a subset of\nhuman-intelligible pieces of the input text to the following predictor.\nHowever, rationalization suffers from two key challenges, i.e., spurious\ncorrelation and degeneration, where the predictor overfits the spurious or\nmeaningless pieces solely selected by the not-yet well-trained generator and in\nturn deteriorates the generator. Although many studies have been proposed to\naddress the two challenges, they are usually designed separately and do not\ntake both of them into account. In this paper, we propose a simple yet\neffective method named MGR to simultaneously solve the two problems. The key\nidea of MGR is to employ multiple generators such that the occurrence stability\nof real pieces is improved and more meaningful pieces are delivered to the\npredictor. Empirically, we show that MGR improves the F1 score by up to 20.9%\nas compared to state-of-the-art methods. Codes are available at\nhttps://github.com/jugechengzi/Rationalization-MGR .\n\n**Published:** 2023-05-08T06:36:46Z  (*Updated: 2023-07-23T08:54:43Z*)\n\n\n\n#### 4. GISTEmbed: Guided In-sample Selection of Training Negatives for Text\n  Embedding Fine-tuning\n\n*From Search Query: \"training stability selective state models\"*\n\n*Aivin V. Solatorio*\n\n**Abstract:** Embedding models are integral to AI applications like semantic search,\npersonalized recommendations, and retrieval augmented generation for LLMs,\nnecessitating high-quality training data. However, the limited scalability of\nmanual data curation prompts the need for automated methods to ensure data\nintegrity. Traditional unsupervised triplet mining automates training data\ngeneration, crucial for embedding model training, yet inadvertently injects\nbiases and noise, thereby degrading model performance. Addressing this, we\nintroduce GISTEmbed, a novel strategy that enhances in-batch negative selection\nduring contrastive training through a guide model. This approach departs from\nreliance on random sampling and equal utility assumption of batch negatives,\nsignificantly reducing noise from data quality issues and improving model\nfine-tuning. Benchmarked against the Massive Text Embedding Benchmark (MTEB),\nGISTEmbed showcases consistent performance improvements across various model\nsizes and achieves state-of-the-art results in select categories. This\nframework enables significant enhancements for smaller models by leveraging the\ncapabilities of powerful yet resource-intensive large models. GISTEmbed can\npotentially revolutionize the creation of highly efficient, smaller models,\ndemocratizing access to advanced AI technologies. Making these technologies\nmore accessible and cost-effective, especially for applications constrained by\nresources, significantly expands the impact and accessibility of\nstate-of-the-art AI solutions across diverse sectors.\n\n**Published:** 2024-02-26T18:55:15Z  (*Updated: 2024-02-26T18:55:15Z*)\n\n\n\n#### 5. FastSeq: Make Sequence Generation Faster\n\n*From Search Query: \"cache optimization parallel processing\"*\n\n*Yu Yan, Fei Hu, Jiusheng Chen, Nikhil Bhendawade, Ting Ye, Yeyun Gong, Nan Duan, Desheng Cui, Bingyu Chi, Ruofei Zhang*\n\n**Abstract:** Transformer-based models have made tremendous impacts in natural language\ngeneration. However the inference speed is a bottleneck due to large model size\nand intensive computing involved in auto-regressive decoding process. We\ndevelop FastSeq framework to accelerate sequence generation without accuracy\nloss. The proposed optimization techniques include an attention cache\noptimization, an efficient algorithm for detecting repeated n-grams, and an\nasynchronous generation pipeline with parallel I/O. These optimizations are\ngeneral enough to be applicable to Transformer-based models (e.g., T5, GPT2,\nand UniLM). Our benchmark results on a set of widely used and diverse models\ndemonstrate 4-9x inference speed gain. Additionally, FastSeq is easy to use\nwith a simple one-line code change. The source code is available at\nhttps://github.com/microsoft/fastseq.\n\n**Published:** 2021-06-08T22:25:28Z  (*Updated: 2021-07-12T22:24:37Z*)\n\n\n\n#### 6. RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation\n\n*From Search Query: \"cache optimization parallel processing\"*\n\n*Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, Xin Jin*\n\n**Abstract:** Retrieval-Augmented Generation (RAG) has shown significant improvements in\nvarious natural language processing tasks by integrating the strengths of large\nlanguage models (LLMs) and external knowledge databases. However, RAG\nintroduces long sequence generation and leads to high computation and memory\ncosts. We propose RAGCache, a novel multilevel dynamic caching system tailored\nfor RAG. Our analysis benchmarks current RAG systems, pinpointing the\nperformance bottleneck (i.e., long sequence due to knowledge injection) and\noptimization opportunities (i.e., caching knowledge's intermediate states).\nBased on these insights, we design RAGCache, which organizes the intermediate\nstates of retrieved knowledge in a knowledge tree and caches them in the GPU\nand host memory hierarchy. RAGCache proposes a replacement policy that is aware\nof LLM inference characteristics and RAG retrieval patterns. It also\ndynamically overlaps the retrieval and inference steps to minimize the\nend-to-end latency. We implement RAGCache and evaluate it on vLLM, a\nstate-of-the-art LLM inference system and Faiss, a state-of-the-art vector\ndatabase. The experimental results show that RAGCache reduces the time to first\ntoken (TTFT) by up to 4x and improves the throughput by up to 2.1x compared to\nvLLM integrated with Faiss.\n\n**Published:** 2024-04-18T18:32:30Z  (*Updated: 2024-04-25T06:47:57Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. XGBoost: Scalable GPU Accelerated Learning\n\n*From Search Query: \"memory compression techniques\"*\n\n*Rory Mitchell, Eibe Frank, Thejaswi Rao, Andrey Adinets*\n\n**Abstract:** We describe the multi-GPU gradient boosting algorithm implemented in the\nXGBoost library (https://github.com/dmlc/xgboost). Our algorithm allows fast,\nscalable training on multi-GPU systems with all of the features of the XGBoost\nlibrary. We employ data compression techniques to minimise the usage of scarce\nGPU memory while still allowing highly efficient implementation. Using our\nalgorithm we show that it is possible to process 115 million training instances\nin under three minutes on a publicly available cloud computing instance. The\nalgorithm is implemented using end-to-end GPU parallelism, with prediction,\ngradient calculation, feature quantisation, decision tree construction and\nevaluation phases all computed on device.\n\n**Published:** 2018-06-29\n\n\n\n#### 2. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n\n*From Search Query: \"memory compression techniques\"*\n\n*Song Han, Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer*\n\n**Abstract:** Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet\n\n**Published:** 2016-02-24\n\n\n\n#### 3. Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face Recognition\n\n*From Search Query: \"training stability selective state models\"*\n\n*Cong Liu, Simin Chen, Shouhong Ding, Juefeng Guo, Taiping Yao, Bangjie Yin, Zexin Li*\n\n**Abstract:** A hard challenge in developing practical face recognition (FR) attacks is due to the black-box nature of the target FR model, i.e., inaccessible gradient and parameter information to attackers. While recent research took an important step towards attacking black-box FR models through leveraging transferability, their performance is still limited, especially against online commercial FR systems that can be pessimistic (e.g., a less than 50% ASR--attack success rate on average). Motivated by this, we present Sibling-Attack, a new FR attack technique for the first time explores a novel multi-task perspective (i.e., leveraging extra information from multi-correlated tasks to boost attacking transferability). Intuitively, Sibling-Attack selects a set of tasks correlated with FR and picks the Attribute Recognition (AR) task as the task used in Sibling-Attack based on theoretical and quantitative analysis. Sibling-Attack then develops an optimization framework that fuses adversarial gradient information through (1) constraining the cross-task features to be under the same space, (2) a joint-task meta optimization framework that enhances the gradient compatibility among tasks, and (3) a cross-task gradient stabilization method which mitigates the oscillation effect during attacking. Extensive experiments demonstrate that Sibling-Attack outperforms state-of-the-art FR attack techniques by a non-trivial margin, boosting ASR by 12.61% and 55.77% on average on state-of-the-art pre-trained FR models and two well-known, widely used commercial FR systems.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-03-22\n\n\n\n#### 4. Time Evidence Fusion Network: Multi-source View in Long-Term Time Series Forecasting\n\n*From Search Query: \"training stability selective state models\"*\n\n*Qingsong Wen, Wenjie Du, Zhen Li, Yong Deng, Yuanpeng He, Tianxiang Zhan*\n\n**Abstract:** In practical scenarios, time series forecasting necessitates not only accuracy but also efficiency. Consequently, the exploration of model architectures remains a perennially trending topic in research. To address these challenges, we propose a novel backbone architecture named Time Evidence Fusion Network (TEFN) from the perspective of information fusion. Specifically, we introduce the Basic Probability Assignment (BPA) Module based on evidence theory to capture the uncertainty of multivariate time series data from both channel and time dimensions. Additionally, we develop a novel multi-source information fusion method to effectively integrate the two distinct dimensions from BPA output, leading to improved forecasting accuracy. Lastly, we conduct extensive experiments to demonstrate that TEFN achieves performance comparable to state-of-the-art methods while maintaining significantly lower complexity and reduced training time. Also, our experiments show that TEFN exhibits high robustness, with minimal error fluctuations during hyperparameter selection. Furthermore, due to the fact that BPA is derived from fuzzy theory, TEFN offers a high degree of interpretability. Therefore, the proposed TEFN balances accuracy, efficiency, stability, and interpretability, making it a desirable solution for time series forecasting.\n\n**Published:** 2024-05-10\n\n\n\n#### 5. GE-SpMM: General-purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks\n\n*From Search Query: \"cache optimization parallel processing\"*\n\n*Huazhong Yang, Guohao Dai, Yu Wang, Guyue Huang*\n\n**Abstract:** Graph Neural Networks (GNNs) have achieved significant improvements in various domains. Sparse Matrix-Matrix multiplication (SpMM) is a fundamental operator in GNNs, which performs a multiplication between a sparse matrix and a dense matrix. Accelerating SpMM on parallel hardware like GPUs can face the following challenges: From the GNN application perspective, the compatibility needs to be considered. General GNN algorithms require SpMM-like operations (e.g., pooling) between matrices, which are not supported in current high-performance GPU libraries (e.g., Nvidia cuSPARSE). Moreover, the sophisticated preprocessing in previous implementations will lead to heavy data format conversion overheads in GNN frameworks. From the GPU hardware perspective, optimizations in SpMV (Sparse Matrix-Vector) designs on GPUs do not apply well to SpMM. SpMM exposes the column-wise parallelism in the dense output matrix, but straightforward generalization from SpMV leads to inefficient, uncoalesced access to sparse matrix in global memory. The sparse row data can be reused among GPU threads, which is neither possible in SpMM designs inherited from SpMV. To tackle these challenges, we propose GE-SpMM. GE-SpMM performs SpMM-like operation on sparse matrices represented in the most common Compressed Sparse Row (CSR) format, so it can be embedded in GNN frameworks with no preprocessing overheads and support general GNN algorithms. We introduce the Coalesced Row Caching method to process columns in parallel and ensure coalesced access to sparse matrix data. We also present the Coarse-grained Warp Merging to reduce redundant data loading among GPU warps. Experiments on a real-world graph dataset show that GE-SpMM achieves up to 1.41X speedup over Nvidia cuSPARSE and up to 1.81X over GraphBLAST. We also embed GE-SpMM in GNN frameworks and get up to 3.67X speedup over popular GNN models like GCN and GraphSAGE.\n\n**Published:** 2020-07-07\n\n\n\n#### 6. Accurate, Efficient and Scalable Training of Graph Neural Networks\n\n*From Search Query: \"cache optimization parallel processing\"*\n\n*Viktor Prasanna, Rajgopal Kannan, Ajitesh Srivastava, Hongkuan Zhou, Hanqing Zeng*\n\n**Abstract:** Graph Neural Networks (GNNs) are powerful deep learning models to generate node embeddings on graphs. When applying deep GNNs on large graphs, it is still challenging to perform training in an efficient and scalable way. We propose a novel parallel training framework. Through sampling small subgraphs as minibatches, we reduce training workload by orders of magnitude compared with state-of-the-art minibatch methods. We then parallelize the key computation steps on tightly-coupled shared memory systems. For graph sampling, we exploit parallelism within and across sampler instances, and propose an efficient data structure supporting concurrent accesses from samplers. The parallel sampler theoretically achieves near-linear speedup with respect to number of processing units. For feature propagation within subgraphs, we improve cache utilization and reduce DRAM traffic by data partitioning. Our partitioning is a 2-approximation strategy for minimizing the communication cost compared to the optimal. We further develop a runtime scheduler to reorder the training operations and adjust the minibatch subgraphs to improve parallel performance. Finally, we generalize the above parallelization strategies to support multiple types of GNN models and graph samplers. The proposed training outperforms the state-of-the-art in scalability, efficiency and accuracy simultaneously. On a 40-core Xeon platform, we achieve 60x speedup (with AVX) in the sampling step and 20x speedup in the feature propagation step, compared to the serial implementation. Our algorithm enables fast training of deeper GNNs, as demonstrated by orders of magnitude speedup compared to the Tensorflow implementation. We open-source our code at https://github.com/GraphSAINT/GraphSAINT.\n\n**Published:** 2020-10-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on the aspects of memory management, training stability, and hardware optimization, here are some key points and references that can be useful:\n\n### Memory Management and Compression\n\n#### State Compression Techniques\n- **Run-Length Encoding (RLE)** and other lossless compression techniques can be adapted for compressing state spaces in autoregressive models. RLE is effective for data with repeated patterns, which can be common in certain state representations.\n- **Columnar Compression**: Techniques like Apache Parquet and Apache ORC, which are columnar storage formats with built-in compression, can be beneficial for structured data in models. These formats are particularly useful when working with large-scale data processing frameworks.\n- **GPU Memory Compression**: Utilizing GPU-specific compression techniques such as delta color compression (DCC) or adaptive scalable texture compression (ASTC) can be adapted for compressing model states, especially when dealing with large and complex models. These techniques can improve memory bandwidth efficiency and reduce power consumption.\n\n#### Dynamic Memory Allocation\n- **Tiling and Hierarchy Strategies**: Implementing tiling and hierarchy strategies, as seen in CHELA, can help in efficient memory management. This involves dividing the data into smaller chunks and processing them in a hierarchical manner to reduce memory usage and improve cache efficiency[Analysis Summary].\n\n#### Cache-Efficient Implementations\n- **Cache Utilization Techniques**: Ensuring that the model's memory access patterns are optimized for cache utilization is crucial. Techniques such as blocking and tiling can help in reducing cache misses and improving overall performance[Analysis Summary].\n- **Streaming Compression**: Using streaming compression algorithms like zlib or LZ4 can help in compressing and decompressing data on-the-fly, which is particularly useful for large datasets that don't fit in memory.\n\n### Training Stability\n\n#### Convergence Analysis\n- **Hybrid Models**: Analyzing the convergence properties of hybrid models that integrate selective state mechanisms with linear attention is essential. Theoretical connections between SSMs and attention variants, as discussed in \"Transformers are SSMs,\" can provide a rich framework for understanding convergence[Analysis Summary].\n\n#### Gradient Flow Optimization\n- **Gradient Flow in Selective Mechanisms**: Understanding how gradient flow operates in selective state mechanisms is critical. Papers like \"Mamba\" and \"Gated Linear Attention Transformers\" provide insights into optimizing gradient flow in such models[Analysis Summary][References].\n\n#### Initialization Strategies\n- **Initialization Methods**: Proper initialization strategies can significantly impact the training stability of autoregressive models. Techniques such as Xavier initialization or other adaptive initialization methods can be explored to ensure stable training[References].\n\n### Hardware Optimization\n\n#### Parallel Processing Implementations\n- **Parallel Algorithm Implementations**: Implementing parallel processing algorithms that balance state management and parallelization is key. Papers like \"CHELA\" and \"Gated Linear Attention Transformers\" offer insights into hardware-efficient training methods that leverage parallel processing[Analysis Summary][References].\n- **Memory Access Patterns**: Optimizing memory access patterns to reduce memory bandwidth usage is crucial. Techniques such as data tiling and hierarchical processing can help in this regard[Analysis Summary].\n\n#### Cache Utilization Techniques\n- **Cache Efficiency**: Ensuring cache efficiency through techniques like blocking, tiling, and optimizing memory access patterns is vital for practical performance. This involves careful design of the interaction between the selective state mechanism and linear attention to minimize cache misses[Analysis Summary].\n\n#### Hardware-Specific Optimizations\n- **GPU-Specific Optimizations**: Leveraging GPU-specific technologies such as memory compression (e.g., DCC, ASTC) and optimizing for GPU architectures (e.g., NVIDIA's Maxwell, Pascal, Turing, and Ampere; AMD's GCN, RDNA, and RDNA 2) can significantly improve performance and efficiency.\n\n### Additional Insights\n\n- **Theoretical Foundations**: Utilizing theoretical foundations from papers like \"Transformers are SSMs\" can help in designing a robust framework that integrates selective state mechanisms with linear attention. This integration can be optimized for both memory efficiency and training stability[Analysis Summary][References].\n- **Empirical Results**: Benchmarking different compression techniques, parallel processing implementations, and cache optimization strategies on specific datasets can provide empirical results to guide the design. This involves evaluating the trade-off between compression ratio, processing speed, and overall performance.\n\nBy focusing on these areas and leveraging the insights from the referenced papers, the researcher can develop an autoregressive language model that is optimized for memory management, training stability, and hardware efficiency.\n",
            "analysis": "### Analysis Summary\n\nThe search results reveal several key aspects about the proposed SelectiveStateAttention (SSA) design and its relationship to existing approaches:\n\n1. **State Space Models & Attention Integration**\n- Mamba's selective state space mechanism provides a foundation for efficient sequence processing with linear complexity\n- Recent work (Transformers are SSMs) establishes theoretical connections between SSMs and attention variants\n- CHELA demonstrates successful integration of linear attention with convolution-based approaches\n\n2. **Hardware Efficiency & Implementation**\n- Hardware-aware implementations are crucial for practical efficiency\n- Tiling and hierarchy strategies (from CHELA) offer promising approaches for memory management\n- Trade-offs between parallelization and memory access need careful consideration\n\n3. **Novel Elements in SSA Proposal**\n- Integration of selective state mechanism with linear attention is relatively unexplored\n- Dynamic memory management based on content importance appears novel\n- Hardware-aware parallel processing optimization combines insights from multiple approaches\n\n### Key References with Excerpts\n\n1. From Mamba paper (Gu & Dao, 2023):\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n\n2. From CHELA paper (Liu et al., 2024):\n\"leverage the thought of tiling and hierarchy to propose CHELA... implements linear attention in a divide-and-conquer manner\"\n\n3. From Transformers are SSMs (Dao & Gu, 2024):\n\"develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions\"\n\n### Key Insights for Design\n\n1. **Architectural Integration**\n- Need to carefully design the interaction between selective state mechanism and linear attention\n- Consider hierarchical processing for efficient memory management\n- Leverage theoretical foundations from SSM-attention duality\n\n2. **Hardware Considerations**\n- Tiling strategies from CHELA could be adapted for memory efficiency\n- Parallel processing optimizations need to balance with state management\n- Cache efficiency crucial for practical performance\n\n3. **Open Questions**\n- Optimal balance between state selection and attention mechanisms\n- Impact of memory compression on model quality\n- Practical implementation challenges of parallel processing\n\n### Future Search Plan\n\nNeed additional information on:\n\n1. Memory management techniques:\n- Compression strategies for state spaces\n- Dynamic memory allocation methods\n- Cache optimization approaches\n\n2. Training stability:\n- Convergence properties of hybrid models\n- Gradient flow in selective mechanisms\n- Initialization strategies\n\n3. Hardware optimization:\n- Parallel algorithm implementations\n- Memory access patterns\n- Cache utilization strategies\n\n### References\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n2. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n3. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. International Conference on Machine Learning.\n\n4. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint arXiv:2312.06635.\n\n5. Kong, L., Wang, C., & Zheng, L. (2022). Linear Complexity Randomized Self-attention Mechanism. arXiv preprint.\n\nFurther search needed to gather more information on implementation details and optimization strategies."
        },
        {
            "ready": false,
            "query": "\"state space models attention\", \"linear attention optimization\", \"hybrid model architecture\"",
            "detail": "Search for papers and implementations discussing:\n\n1. State Space Models and Attention:\n- Integration methods\n- Efficient implementations\n- Theoretical foundations\n- Performance comparisons\n\n2. Linear Attention Optimization:\n- Memory-efficient variants\n- Hardware-specific implementations\n- Scaling properties\n- Theoretical analysis\n\n3. Hybrid Model Architectures:\n- Combining state space and attention\n- Architecture design principles\n- Performance characteristics\n- Theoretical guarantees\n\nFocus on:\n- Implementation details\n- Theoretical foundations\n- Performance analysis\n- Hardware considerations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and implementations discussing:\n\n1. State Space Models and Attention:\n- Integration methods\n- Efficient implementations\n- Theoretical foundations\n- Performance comparisons\n\n2. Linear Attention Optimization:\n- Memory-efficient variants\n- Hardware-specific implementations\n- Scaling properties\n- Theoretical analysis\n\n3. Hybrid Model Architectures:\n- Combining state space and attention\n- Architecture design principles\n- Performance characteristics\n- Theoretical guarantees\n\nFocus on:\n- Implementation details\n- Theoretical foundations\n- Performance analysis\n- Hardware considerations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.87)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 0.87)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 2. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.76)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.76)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.69)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.69)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.56)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.56)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.47)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.47)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"state space models attention\", \"linear attention optimization\", \"hybrid model architecture\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"state space models attention\"*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 270  (*Influential: 21*)\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: \"state space models attention\"*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: \"state space models attention\"*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Linear attention is (maybe) all you need (to understand transformer optimization)\n\n*From Search Query: \"linear attention optimization\"*\n\n*Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, A. Jadbabaie, S. Sra*\n\n**TL;DR:** The results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.\n\n**Abstract:** Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 4*)\n\n#### 5. Linear Last-iterate Convergence in Constrained Saddle-point Optimization\n\n*From Search Query: \"linear attention optimization\"*\n\n*Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, Haipeng Luo*\n\n**TL;DR:** This work significantly expands the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting and introduces a sufficient condition under which OGDA exhibits concrete last- iterate convergence rates with a constant learning rate, which holds for strongly-convex-strongly-concave functions.\n\n**Abstract:** Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization have received growing attention due to their favorable last-iterate convergence. However, their behaviors for simple bilinear games over the probability simplex are still not fully understood -- previous analysis lacks explicit convergence rates, only applies to an exponentially small learning rate, or requires additional assumptions such as the uniqueness of the optimal solution. \nIn this work, we significantly expand the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting. Specifically, for OMWU in bilinear games over the simplex, we show that when the equilibrium is unique, linear last-iterate convergence is achievable with a constant learning rate, which improves the result of (Daskalakis & Panageas, 2019) under the same assumption. We then significantly extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate. We show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption. Our condition also holds for strongly-convex-strongly-concave functions, recovering the result of (Hsieh et al., 2019). Finally, we provide experimental results to further support our theory.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 106  (*Influential: 17*)\n\n#### 6. Achieving Linear Speedup in Non-IID Federated Bilevel Learning\n\n*From Search Query: \"linear attention optimization\"*\n\n*Minhui Huang, Dewei Zhang, Kaiyi Ji*\n\n**TL;DR:** This paper proposes a new federated bilevel algorithm named FedMBO with a novel client sampling scheme in the federated hypergradient estimation that achieves a convergence rate of $\\mathcal{O}\\big(\\sqrt{nK}}+\\frac{1}{K}+\\sqrt {n}}{K^{3/2}}\\big)$ on non-i.i.d.~datasets.\n\n**Abstract:** Federated bilevel optimization has received increasing attention in various emerging machine learning and communication applications. Recently, several Hessian-vector-based algorithms have been proposed to solve the federated bilevel optimization problem. However, several important properties in federated learning such as the partial client participation and the linear speedup for convergence (i.e., the convergence rate and complexity are improved linearly with respect to the number of sampled clients) in the presence of non-i.i.d.~datasets, still remain open. In this paper, we fill these gaps by proposing a new federated bilevel algorithm named FedMBO with a novel client sampling scheme in the federated hypergradient estimation. We show that FedMBO achieves a convergence rate of $\\mathcal{O}\\big(\\frac{1}{\\sqrt{nK}}+\\frac{1}{K}+\\frac{\\sqrt{n}}{K^{3/2}}\\big)$ on non-i.i.d.~datasets, where $n$ is the number of participating clients in each round, and $K$ is the total number of iteration. This is the first theoretical linear speedup result for non-i.i.d.~federated bilevel optimization. Extensive experiments validate our theoretical results and demonstrate the effectiveness of our proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 4*)\n\n#### 7. Learning a Deep Hybrid Model for Semi-Supervised Text Classification\n\n*From Search Query: \"hybrid model architecture\"*\n\n*Alexander Ororbia, C. Lee Giles, D. Reitter*\n\n**TL;DR:** A novel fine-tuning algorithm in a deep hybrid architecture for semisupervised text classification that serves as a top-down mechanism for pseudo-jointly modifying model parameters following a bottom-up generative learning pass is presented.\n\n**Abstract:** We present a novel fine-tuning algorithm in a deep hybrid architecture for semisupervised text classification. During each increment of the online learning process, the fine-tuning algorithm serves as a top-down mechanism for pseudo-jointly modifying model parameters following a bottom-up generative learning pass. The resulting model, trained under what we call the Bottom-Up-Top-Down learning algorithm, is shown to outperform a variety of competitive models and baselines trained across a wide range of splits between supervised and unsupervised training data.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2015\n\n**Citations:** 14  (*Influential: 0*)\n\n#### 8. Model-Based Episodic Memory Induces Dynamic Hybrid Controls\n\n*From Search Query: \"hybrid model architecture\"*\n\n*Hung Le, T. G. Karimpanal, Majid Abdolshah, T. Tran, S. Venkatesh*\n\n**TL;DR:** A new model-based episodic memory of trajectories addressing current limitations of episodic control is proposed and a complementary learning model via a dynamic hybrid control is constructed unifying model- based, episodic and habitual learning into a single architecture.\n\n**Abstract:** Episodic control enables sample efficiency in reinforcement learning by recalling past experiences from an episodic memory. We propose a new model-based episodic memory of trajectories addressing current limitations of episodic control. Our memory estimates trajectory values, guiding the agent towards good policies. Built upon the memory, we construct a complementary learning model via a dynamic hybrid control unifying model-based, episodic and habitual learning into a single architecture. Experiments demonstrate that our model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments including stochastic and non-Markovian settings.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 9. NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training\n\n*From Search Query: \"hybrid model architecture\"*\n\n*Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, Qiang Liu, Vikas Chandra*\n\n**TL;DR:** A discovered hybrid ViT model family, dubbed NASViT, achieves top-1 accuracy from 78.2% to 81.8% on ImageNet from 200M to 800M FLOPs, and outperforms all the prior art CNNs and ViTs, including AlphaNet and LeViT.\n\n**Abstract:** Designing accurate and ef\ufb01cient vision transformers (ViTs) is an important but challenging task. Supernet-based one-shot neural architecture search (NAS) enables fast architecture optimization and has achieved state-of-the-art results on convolutional neural networks (CNNs). However, directly applying the supernet-based NAS to optimize ViTs leads to poor performance - even worse compared to training single ViTs. In this work, we observe that the poor performance is due to a gradient con\ufb02ict issue: the gradients of different sub-networks con\ufb02ict with that of the supernet more severely in ViTs than CNNs, which leads to early saturation in training and inferior convergence. To alleviate this issue, we propose a series of techniques, including a gradient projection algorithm, a switchable layer scaling design, and a simpli\ufb01ed data augmentation and regularization training recipe. The proposed techniques signi\ufb01cantly improve the convergence and the performance of all sub-networks. Our discovered hybrid ViT model family, dubbed NASViT, achieves top-1 accuracy from 78.2% to 81.8% on ImageNet from 200M to 800M FLOPs, and outperforms all the prior art CNNs and ViTs, including AlphaNet and LeViT. When transferred to semantic segmentation tasks, NASViTs also out-perform previous backbones on both Cityscape and ADE20K datasets, achieving 73.2% and\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 78  (*Influential: 14*)\n\n### 6 related papers from ArXiv\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling\n\n*From Search Query: \"state space models attention\"*\n\n*Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11T17:50:51Z  (*Updated: 2024-06-11T17:50:51Z*)\n\n\n\n#### 2. MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\n\n*From Search Query: \"state space models attention\"*\n\n*Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, Liangchen Luo*\n\n**Abstract:** In sequence to sequence learning, the self-attention mechanism proves to be\nhighly effective, and achieves significant improvements in many tasks. However,\nthe self-attention mechanism is not without its own flaws. Although\nself-attention can model extremely long dependencies, the attention in deep\nlayers tends to overconcentrate on a single token, leading to insufficient use\nof local information and difficultly in representing long sequences. In this\nwork, we explore parallel multi-scale representation learning on sequence data,\nstriving to capture both long-range and short-range language structures. To\nthis end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple.\nMUSE-simple contains the basic idea of parallel multi-scale sequence\nrepresentation learning, and it encodes the sequence in parallel, in terms of\ndifferent scales with the help from self-attention, and pointwise\ntransformation. MUSE builds on MUSE-simple and explores combining convolution\nand self-attention for learning sequence representations from more different\nscales. We focus on machine translation and the proposed approach achieves\nsubstantial performance improvements over Transformer, especially on long\nsequences. More importantly, we find that although conceptually simple, its\nsuccess in practice requires intricate considerations, and the multi-scale\nattention must build on unified semantic space. Under common setting, the\nproposed model achieves substantial performance and outperforms all previous\nmodels on three main machine translation tasks. In addition, MUSE has potential\nfor accelerating inference due to its parallelism. Code will be available at\nhttps://github.com/lancopku/MUSE\n\n**Published:** 2019-11-17T09:36:07Z  (*Updated: 2019-11-17T09:36:07Z*)\n\n\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"linear attention optimization\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 4. Treeformer: Dense Gradient Trees for Efficient Attention Computation\n\n*From Search Query: \"linear attention optimization\"*\n\n*Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, Prateek Jain*\n\n**Abstract:** Standard inference and training with transformer based architectures scale\nquadratically with input sequence length. This is prohibitively large for a\nvariety of applications especially in web-page translation, query-answering\netc. Consequently, several approaches have been developed recently to speedup\nattention computation by enforcing different attention structures such as\nsparsity, low-rank, approximating attention using kernels. In this work, we\nview attention computation as that of nearest neighbor retrieval, and use\ndecision tree based hierarchical navigation to reduce the retrieval cost per\nquery token from linear in sequence length to nearly logarithmic. Based on such\nhierarchical navigation, we design Treeformer which can use one of two\nefficient attention layers -- TF-Attention and TC-Attention. TF-Attention\ncomputes the attention in a fine-grained style, while TC-Attention is a coarse\nattention layer which also ensures that the gradients are \"dense\". To optimize\nsuch challenging discrete layers, we propose a two-level bootstrapped training\nmethod. Using extensive experiments on standard NLP benchmarks, especially for\nlong-sequences, we demonstrate that our Treeformer architecture can be almost\nas accurate as baseline Transformer while using 30x lesser FLOPs in the\nattention layer. Compared to Linformer, the accuracy can be as much as 12%\nhigher while using similar FLOPs in the attention layer.\n\n**Published:** 2022-08-18T18:31:40Z  (*Updated: 2023-03-17T14:25:43Z*)\n\n\n\n#### 5. BERT-Based Combination of Convolutional and Recurrent Neural Network for\n  Indonesian Sentiment Analysis\n\n*From Search Query: \"hybrid model architecture\"*\n\n*Hendri Murfi, Syamsyuriani, Theresia Gowandi, Gianinna Ardaneswari, Siti Nurrohmah*\n\n**Abstract:** Sentiment analysis is the computational study of opinions and emotions\nex-pressed in text. Deep learning is a model that is currently producing\nstate-of-the-art in various application domains, including sentiment analysis.\nMany researchers are using a hybrid approach that combines different deep\nlearning models and has been shown to improve model performance. In sentiment\nanalysis, input in text data is first converted into a numerical\nrepresentation. The standard method used to obtain a text representation is the\nfine-tuned embedding method. However, this method does not pay attention to\neach word's context in the sentence. Therefore, the Bidirectional Encoder\nRepresentation from Transformer (BERT) model is used to obtain text\nrepresentations based on the context and position of words in sentences. This\nresearch extends the previous hybrid deep learning using BERT representation\nfor Indonesian sentiment analysis. Our simulation shows that the BERT\nrepresentation improves the accuracies of all hybrid architectures. The\nBERT-based LSTM-CNN also reaches slightly better accuracies than other\nBERT-based hybrid architectures.\n\n**Published:** 2022-11-10T00:32:40Z  (*Updated: 2022-11-10T00:32:40Z*)\n\n\n\n#### 6. General Cross-Architecture Distillation of Pretrained Language Models\n  into Matrix Embeddings\n\n*From Search Query: \"hybrid model architecture\"*\n\n*Lukas Galke, Isabelle Cuber, Christoph Meyer, Henrik Ferdinand N\u00f6lscher, Angelina Sonderecker, Ansgar Scherp*\n\n**Abstract:** Large pretrained language models (PreLMs) are revolutionizing natural\nlanguage processing across all benchmarks. However, their sheer size is\nprohibitive for small laboratories or for deployment on mobile devices.\nApproaches like pruning and distillation reduce the model size but typically\nretain the same model architecture. In contrast, we explore distilling PreLMs\ninto a different, more efficient architecture, Continual Multiplication of\nWords (CMOW), which embeds each word as a matrix and uses matrix multiplication\nto encode sequences. We extend the CMOW architecture and its CMOW/CBOW-Hybrid\nvariant with a bidirectional component for more expressive power, per-token\nrepresentations for a general (task-agnostic) distillation during pretraining,\nand a two-sequence encoding scheme that facilitates downstream tasks on\nsentence pairs, such as sentence similarity and natural language inference. Our\nmatrix-based bidirectional CMOW/CBOW-Hybrid model is competitive to DistilBERT\non question similarity and recognizing textual entailment, but uses only half\nof the number of parameters and is three times faster in terms of inference\nspeed. We match or exceed the scores of ELMo for all tasks of the GLUE\nbenchmark except for the sentiment analysis task SST-2 and the linguistic\nacceptability task CoLA. However, compared to previous cross-architecture\ndistillation approaches, we demonstrate a doubling of the scores on detecting\nlinguistic acceptability. This shows that matrix-based embeddings can be used\nto distill large PreLM into competitive models and motivates further research\nin this direction.\n\n**Published:** 2021-09-17T10:15:06Z  (*Updated: 2022-07-28T10:44:50Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Attentive State-Space Modeling of Disease Progression\n\n*From Search Query: \"state space models attention\"*\n\n*Mihaela van der Schaar, Ahmed M. Alaa*\n\n**Abstract:** Models of disease progression are instrumental for predicting patient outcomes and understanding disease dynamics. Existing models provide the patient with pragmatic (supervised) predictions of risk, but do not provide the clinician with intelligible (unsupervised) representations of disease pathophysiology. In this paper, we develop the attentive state-space model, a deep probabilistic model that learns accurate and interpretable structured representations for disease trajectories. Unlike Markovian state-space models, in which the dynamics are memoryless, our model uses an attention mechanism to create \"memoryful\" dynamics, whereby attention weights determine the dependence of future disease states on past medical history. To learn the model parameters from medical records, we develop an infer ence algorithm that simultaneously learns a compiled inference network and the model parameters, leveraging the attentive state-space representation to construct a  \"Rao-Blackwellized\" variational approximation of the posterior state distribution. Experiments on data from the UK Cystic Fibrosis registry show that our model demonstrates superior predictive accuracy and provides insights into the progression of chronic disease.\n\n**Proceeding:** neurips-2019-12\n\n**Published:** 2019-12-01\n\n\n\n#### 2. Cross-attention Inspired Selective State Space Models for Target Sound Extraction\n\n*From Search Query: \"state space models attention\"*\n\n*Tianshu Qu, Xihong Wu, Yiwen Wang, Donghang Wu*\n\n**Abstract:** The Transformer model, particularly its cross-attention module, is widely used for feature fusion in target sound extraction which extracts the signal of interest based on given clues. Despite its effectiveness, this approach suffers from low computational efficiency. Recent advancements in state space models, notably the latest work Mamba, have shown comparable performance to Transformer-based methods while significantly reducing computational complexity in various tasks. However, Mamba's applicability in target sound extraction is limited due to its inability to capture dependencies between different sequences as the cross-attention does. In this paper, we propose CrossMamba for target sound extraction, which leverages the hidden attention mechanism of Mamba to compute dependencies between the given clues and the audio mixture. The calculation of Mamba can be divided to the query, key and value. We utilize the clue to generate the query and the audio mixture to derive the key and value, adhering to the principle of the cross-attention mechanism in Transformers. Experimental results from two representative target sound extraction methods validate the efficacy of the proposed CrossMamba.\n\n**Published:** 2024-09-07\n\n\n\n#### 3. Rethinking Attention with Performers\n\n*From Search Query: \"linear attention optimization\"*\n\n*Lukasz Kaiser, Andreea Gane, Valerii Likhosherstov, Lucy Colwell, Afroz Mohiuddin, Xingyou Song, Krzysztof Choromanski, Adrian Weller, Jared Davis, David Dohan, David Belanger, Tamas Sarlos, Peter Hawkins*\n\n**Abstract:** We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\n**Proceeding:** iclr-2021-1\n\n**Published:** 2020-09-30\n\n\n\n#### 4. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n\n*From Search Query: \"linear attention optimization\"*\n\n*Tri Dao*\n\n**Abstract:** Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).\n\n**Published:** 2023-07-17\n\n\n\n#### 5. xDeepInt: a hybrid architecture for modeling the vector-wise and bit-wise feature interactions\n\n*From Search Query: \"hybrid model architecture\"*\n\n*Liubo Li, YaChen Yan*\n\n**Abstract:** Learning feature interactions is the key to success for the large-scale CTR prediction and recommendation. In practice, handcrafted feature engineering usually requires exhaustive searching. In order to reduce the high cost of human efforts in feature engineering, researchers propose several deep neural networks (DNN)-based approaches to learn the feature interactions in an end-to-end fashion. However, existing methods either do not learn both vector-wise interactions and bit-wise interactions simultaneously, or fail to combine them in a controllable manner. In this paper, we propose a new model, xDeepInt, based on a novel network architecture called polynomial interaction network (PIN) which learns higher-order vector-wise interactions recursively. By integrating subspace-crossing mechanism, we enable xDeepInt to balance the mixture of vector-wise and bit-wise feature interactions at a bounded order. Based on the network architecture, we customize a combined optimization strategy to conduct feature selection and interaction selection. We implement the proposed model and evaluate the model performance on three real-world datasets. Our experiment results demonstrate the efficacy and effectiveness of xDeepInt over state-of-the-art models. We open-source the TensorFlow implementation of xDeepInt: https://github.com/yanyachen/xDeepInt.\n\n**Published:** 2023-01-03\n\n\n\n#### 6. Modelling Context with User Embeddings for Sarcasm Detection in Social Media\n\n*From Search Query: \"hybrid model architecture\"*\n\n*Paula Carvalho M\u00e1rio J. Silva, Byron C. Wallace, Silvio Amir, Hao Lyu*\n\n**Abstract:** We introduce a deep neural network for automated sarcasm detection. Recent\nwork has emphasized the need for models to capitalize on contextual features,\nbeyond lexical and syntactic cues present in utterances. For example, different\nspeakers will tend to employ sarcasm regarding different subjects and, thus,\nsarcasm detection models ought to encode such speaker information. Current\nmethods have achieved this by way of laborious feature engineering. By\ncontrast, we propose to automatically learn and then exploit user embeddings,\nto be used in concert with lexical signals to recognize sarcasm. Our approach\ndoes not require elaborate feature engineering (and concomitant data scraping);\nfitting user embeddings requires only the text from their previous posts. The\nexperimental results show that our model outperforms a state-of-the-art\napproach leveraging an extensive set of carefully crafted features.\n\n**Conference:** modelling-context-with-user-embeddings-for-1\n\n**Published:** 2016-07-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using state space models, linear attention optimization, and hybrid model architectures, here are some key findings and insights from the provided sources:\n\n## State Space Models and Attention\n\n### Integration Methods\n- The MAMBA and MAMBA-2 models integrate state-space models (SSMs) with attention mechanisms. MAMBA-2, in particular, uses structured state-space models to generalize linear attention, allowing for efficient and scalable modeling of long sequences. This integration enables the capture of temporal dependencies and content-based interactions similar to attention.\n\n### Efficient Implementations\n- SC-MAMBA2, a model designed for single-cell transcriptomics, leverages SSMs to achieve efficient and scalable modeling of large gene sequences. This model modifies the MAMBA framework to incorporate bidirectional contextual relationships, enhancing its capacity for handling extensive gene sequences efficiently.\n\n### Theoretical Foundations\n- SSMs traditionally describe the dynamics of continuous systems by transforming an input sequence into a latent state representation. The discretization process and the use of state-space duality (SSD) simplify the matrix formulations, making them more suitable for deep learning architectures. This involves transforming continuous matrices into discrete counterparts and using techniques like the zero-order hold (ZOH) for discretization.\n\n### Performance Comparisons\n- SC-MAMBA2 has demonstrated superior performance over benchmark methods in various tasks such as cell annotation, multi-batch data integration, and perturbation prediction. This suggests that integrating SSMs with attention-like mechanisms can outperform traditional Transformer-based architectures in certain domains.\n\n## Linear Attention Optimization\n\n### Memory-Efficient Variants\n- MAMBA-2 optimizes linear attention by using a selective data-dependent mechanism and state-space duality, which reduces the computational complexity and memory requirements. This model can handle long sequences efficiently while maintaining a sparser attention weight map, enhancing computational efficiency.\n\n### Hardware-Specific Implementations\n- While the sources do not specifically discuss hardware implementations for linear attention, the general approach of using structured state-space models and linear attention can be optimized for hardware by leveraging efficient matrix multiplications and sparse attention mechanisms. This aligns with the broader theme of optimizing models for hardware efficiency, as seen in other optimization techniques like KV-Runahead and FastSeq.\n\n### Scaling Properties\n- MAMBA-2 and SC-MAMBA2 models exhibit near-linear scalability concerning sequence length, which is a significant improvement over traditional Transformer architectures. This scalability is crucial for handling large-scale data efficiently.\n\n### Theoretical Analysis\n- The theoretical analysis of MAMBA-2 shows that it generalizes linear attention and can be interpreted as a matrix multiplication, encoding temporal dependencies and content-based interactions. However, it shares limitations with linear attention, such as reduced performance in tasks requiring precise memory recall.\n\n## Hybrid Model Architectures\n\n### Combining State Space and Attention\n- MAMBA-2 is a prime example of a hybrid architecture that combines state-space models with attention mechanisms. This integration allows for efficient processing of long sequences while capturing important long-range dependencies, addressing a key limitation of existing efficient attention mechanisms.\n\n### Architecture Design Principles\n- The design of SC-MAMBA2 and MAMBA-2 involves modifying traditional state-space models to incorporate bidirectional contextual relationships and using state-space duality to simplify matrix formulations. These modifications enable the models to handle extensive sequence lengths and capture intricate dependencies efficiently.\n\n### Performance Characteristics\n- Hybrid models like SC-MAMBA2 and MAMBA-2 have shown superior performance in various tasks, including cell annotation, multi-batch data integration, and perturbation prediction. They offer a balance between computational efficiency and the ability to capture long-range dependencies.\n\n### Theoretical Guarantees\n- The theoretical guarantees of these hybrid models lie in their ability to generalize linear attention and capture temporal dependencies through state-space formulations. The use of state-space duality and selective data-dependent mechanisms provides a solid theoretical foundation for their efficiency and scalability.\n\n## Implementation Details and Hardware Considerations\n\n- **Efficient State Updates**: The use of structured state-space models and linear attention in MAMBA-2 allows for efficient state updates, which can be implemented in a convolutional manner, enhancing computational efficiency.\n- **Cache-Aware Implementations**: The sparser attention weight map in MAMBA-2 and the efficient matrix multiplications involved can be optimized for cache-friendly implementations, similar to the strategies discussed in other optimization techniques like FastSeq and KV-Runahead.\n- **Asynchronous Processing**: While not explicitly discussed in the sources, the efficient and scalable nature of these hybrid models suggests they could be optimized further with asynchronous processing pipelines, similar to those in FastSeq and Cyclades.\n\nBy integrating these insights, researchers can design autoregressive language models that leverage state space models and linear attention optimization, ensuring efficient, scalable, and high-performance architectures.\n",
            "analysis": "### Analysis Summary of Search Results (Round 2)\n\nThe second round of searches has revealed important insights about memory management, training stability, and hardware optimization techniques that can inform our SelectiveStateAttention (SSA) design:\n\n1. **Memory Management Innovations**\n- CompAct's approach to compressing activations shows promise for reducing memory usage by 25-30% during training\n- RAGCache's multilevel dynamic caching system offers insights for efficient knowledge management\n- Hardware-aware compression techniques like ROAST demonstrate the importance of cache-friendly implementations\n\n2. **Training Stability Mechanisms**\n- RoAST's approach to robustifying language models through adversarial perturbation with selective training\n- GISTEmbed's guided in-sample selection strategy for improving training stability\n- The importance of proper initialization and gradient flow management\n\n3. **Hardware Optimization Techniques**\n- KV-Runahead's efficient parallelization scheme for accelerating prompt phase\n- FastSeq's attention cache optimization and asynchronous generation pipeline\n- Cyclades' conflict-free asynchronous approach for parallel optimization\n\n### Key References with Excerpts\n\n1. From CompAct (Shamshoum et al., 2024):\n\"By storing low-rank, compressed activations to be used in the backward pass we greatly reduce the required memory, unlike previous methods which only reduce optimizer overheads or the number of trained parameters.\"\n\n2. From RAGCache (Jin et al., 2024):\n\"RAGCache proposes a replacement policy that is aware of LLM inference characteristics and RAG retrieval patterns. It also dynamically overlaps the retrieval and inference steps to minimize the end-to-end latency.\"\n\n3. From KV-Runahead (Cho et al., 2024):\n\"The key observation is that the extension phase generates tokens faster than the prompt phase because of key-value cache (KV-cache). Hence, KV-Runahead parallelizes the prompt phase by orchestrating multiple processes to populate the KV-cache.\"\n\n### Key Insights for Design\n\n1. **Memory Optimization Strategies**\n- Need to incorporate low-rank compression for activations\n- Consider multilevel caching system for state management\n- Implement cache-aware memory access patterns\n\n2. **Training Stability Improvements**\n- Include selective training mechanisms with adversarial perturbation\n- Implement guided selection for training samples\n- Design proper initialization and gradient flow strategies\n\n3. **Hardware Efficiency**\n- Develop efficient parallelization schemes for state updates\n- Implement asynchronous processing pipeline\n- Consider conflict-free parallel optimization approaches\n\n### Future Search Needed\n\nNeed additional information on:\n\n1. State Space Models:\n- Integration with attention mechanisms\n- Efficient state update strategies\n- Theoretical foundations\n\n2. Attention Optimization:\n- Linear attention variants\n- Memory-efficient attention implementations\n- Hardware-specific optimizations\n\n3. Model Architecture:\n- Hybrid architectures combining state space and attention\n- Scaling properties\n- Theoretical analysis\n\n### References\n\n1. Shamshoum, Y., Hodos, N., Sieradzki, Y., & Schuster, A. (2024). CompAct: Compressed Activations for Memory-Efficient LLM Training. arXiv preprint.\n\n2. Jin, C., Zhang, Z., Jiang, X., Liu, F., Liu, X., Liu, X., & Jin, X. (2024). RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation. arXiv preprint.\n\n3. Cho, M., Rastegari, M., & Naik, D. (2024). KV-Runahead: Scalable Causal LLM Inference by Parallel Key-Value Cache Generation. ICML 2024.\n\n4. Kim, J., Mao, Y., Hou, R., Yu, H., Liang, D., Fung, P., Wang, Q., Feng, F., Huang, L., & Khabsa, M. (2023). RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training. EMNLP 2023.\n\n5. Yan, Y., Hu, F., Chen, J., Bhendawade, N., Ye, T., Gong, Y., Duan, N., Cui, D., Chi, B., & Zhang, R. (2021). FastSeq: Make Sequence Generation Faster. arXiv preprint."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.920364,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.036764,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "AdaptiveLinearAttention",
    "proposal": "## Title: SelectiveStateAttention: Enhancing AdaptiveLinearAttention with Selective State Space Mechanisms\n\n### Motivation\n\nCurrent language models face significant challenges in efficiently processing long sequences while maintaining expressiveness. While AdaptiveLinearAttention addresses some efficiency concerns through linear attention mechanisms, there remain opportunities for improvement in:\n\n1. Content-based selection and reasoning\n2. Memory efficiency for long sequences\n3. Hardware utilization and parallel processing\n4. Adaptive computation based on input complexity\n\nThe motivation behind SelectiveStateAttention is to enhance AdaptiveLinearAttention by incorporating selective state space mechanisms and efficient parallel processing, enabling better handling of long sequences while maintaining model quality.\n\n### Problem Analysis\n\n#### Current Limitations\n\n1. **Content Processing**:\n   - Limited content-based reasoning capabilities\n   - Fixed processing patterns regardless of input complexity\n   - Inefficient handling of varying information density\n\n2. **Memory Management**:\n   - Suboptimal memory utilization for long sequences\n   - Fixed memory allocation regardless of content importance\n   - Inefficient state representation\n\n3. **Computational Efficiency**:\n   - Underutilized hardware capabilities\n   - Suboptimal parallel processing\n   - Inefficient memory access patterns\n\n### Core Idea and Philosophy\n\nThe core idea is to enhance AdaptiveLinearAttention by introducing:\n1. Selective state mechanisms for content-based processing\n2. Hardware-efficient parallel algorithms\n3. Dynamic memory management based on content importance\n\nThis approach aims to:\n- Improve content-based reasoning capabilities\n- Enhance memory efficiency through selective processing\n- Optimize hardware utilization\n- Maintain linear computational complexity\n\n### Design Plan\n\n#### Selected GAU to Modify\nAdaptiveLinearAttention\n\n#### Modification Overview\n\n1. **Selective State Integration**:\n   - Add state-based memory mechanism\n   - Implement content-dependent selection\n   - Optimize for hardware efficiency\n\n2. **Memory Management**:\n   - Dynamic state compression\n   - Content-based memory allocation\n   - Efficient parallel access\n\n3. **Hardware-Aware Processing**:\n   - Optimize parallel computation\n   - Efficient memory access patterns\n   - Hardware-specific optimizations\n\n### Detailed Modifications\n\n#### 1. Selective State Mechanism\n\n```python\nclass SelectiveStateModule(nn.Module):\n    def __init__(self, dim, state_dim=None, compression_factor=4):\n        super().__init__()\n        self.state_dim = state_dim or dim\n        self.state_proj = nn.Linear(dim, 2 * self.state_dim)\n        self.content_proj = nn.Linear(dim, self.state_dim)\n        self.compression_factor = compression_factor\n        \n    def forward(self, x, state=None):\n        # Project input for state update\n        state_gate, state_update = self.state_proj(x).chunk(2, dim=-1)\n        state_gate = torch.sigmoid(state_gate)\n        \n        # Content-based selection\n        content_score = self.content_proj(x)\n        \n        # Update state selectively\n        if state is None:\n            state = torch.zeros_like(state_update)\n        new_state = state * (1 - state_gate) + state_update * state_gate\n        \n        # Compress state based on content importance\n        importance = torch.norm(content_score, dim=-1, keepdim=True)\n        mask = torch.topk(importance, k=x.size(1)//self.compression_factor, dim=1)[1]\n        compressed_state = torch.gather(new_state, 1, mask.expand(-1, -1, new_state.size(-1)))\n        \n        return compressed_state, new_state\n```\n\n#### 2. Hardware-Efficient Linear Attention\n\n```python\nclass HardwareEfficientAttention(nn.Module):\n    def __init__(self, dim, num_heads=8, head_dim=None, dropout=0.1):\n        super().__init__()\n        self.num_heads = num_heads\n        self.head_dim = head_dim or dim // num_heads\n        \n        self.qkv = nn.Linear(dim, 3 * self.num_heads * self.head_dim)\n        self.proj = nn.Linear(self.num_heads * self.head_dim, dim)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, state=None):\n        B, L, D = x.shape\n        \n        # Efficient QKV projection\n        qkv = self.qkv(x)\n        q, k, v = qkv.chunk(3, dim=-1)\n        \n        # Reshape for parallel processing\n        q = q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        k = k.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        v = v.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        \n        # Linear attention with parallel processing\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        \n        # Efficient cumulative computation\n        kv = torch.einsum('bhld,bhlm->bhdm', k, v)\n        qkv = torch.einsum('bhld,bhdm->bhlm', q, kv)\n        \n        # Output projection\n        out = qkv.transpose(1, 2).contiguous().view(B, L, -1)\n        out = self.proj(out)\n        out = self.dropout(out)\n        \n        return out\n```\n\n#### 3. Integration Layer\n\n```python\nclass SelectiveStateAttention(nn.Module):\n    def __init__(self, dim, state_dim=None, num_heads=8, dropout=0.1):\n        super().__init__()\n        self.state_module = SelectiveStateModule(dim, state_dim)\n        self.attention = HardwareEfficientAttention(dim, num_heads, dropout=dropout)\n        self.norm1 = nn.LayerNorm(dim)\n        self.norm2 = nn.LayerNorm(dim)\n        \n    def forward(self, x, state=None):\n        # State processing\n        compressed_state, new_state = self.state_module(self.norm1(x), state)\n        \n        # Attention computation\n        attn_out = self.attention(self.norm2(x), compressed_state)\n        \n        # Combine outputs\n        out = x + attn_out\n        \n        return out, new_state\n```\n\n### Implementation Guidelines\n\n1. **State Management**:\n   ```python\n   # Initialize state\n   state = None\n   \n   # Process sequence\n   for chunk in sequence_chunks:\n       output, state = model(chunk, state)\n   ```\n\n2. **Memory Optimization**:\n   ```python\n   # Configure compression\n   compression_config = {\n       'factor': 4,  # Compression ratio\n       'min_tokens': 256  # Minimum tokens to maintain\n   }\n   \n   # Initialize model\n   model = SelectiveStateAttention(\n       dim=512,\n       state_dim=128,\n       compression_config=compression_config\n   )\n   ```\n\n3. **Hardware Utilization**:\n   ```python\n   # Enable tensor parallelism\n   model = torch.nn.parallel.DistributedDataParallel(model)\n   \n   # Configure memory access\n   torch.cuda.set_memory_allocator('caching')\n   ```\n\n### Theoretical Analysis\n\n1. **Computational Complexity**:\n   - Linear complexity O(n) for sequence length n\n   - State compression reduces memory requirements\n   - Parallel processing optimizes hardware utilization\n\n2. **Memory Efficiency**:\n   - Dynamic compression reduces memory footprint\n   - Selective state updates minimize redundant storage\n   - Efficient parallel access patterns\n\n3. **Model Capacity**:\n   - Enhanced content-based reasoning through selective states\n   - Improved long-range dependency modeling\n   - Adaptive computation based on input complexity\n\n### Expected Benefits\n\n1. **Performance**:\n   - Improved content-based reasoning\n   - Better handling of long sequences\n   - Enhanced memory efficiency\n\n2. **Efficiency**:\n   - Reduced memory requirements\n   - Optimized hardware utilization\n   - Faster training and inference\n\n3. **Scalability**:\n   - Linear complexity with sequence length\n   - Efficient parallel processing\n   - Hardware-aware optimizations\n\n### References\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n2. Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n3. Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n4. He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n5. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "This proposal introduces SelectiveStateAttention (SSA), an enhancement to AdaptiveLinearAttention that combines selective state space mechanisms with efficient linear attention computation. By integrating state-based memory with dynamic content selection and hardware-efficient parallel processing, SSA achieves improved efficiency and expressiveness while maintaining linear complexity. The design leverages insights from Mamba and CHELA architectures to create a more robust and adaptable attention mechanism.",
    "ideation": null,
    "modelname": "ssa_gpt",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory-computation trade-offs\n- Include concrete examples of state compression mechanisms\n- Add benchmarking guidelines for different components\n\n2. Training Stability:\n- Develop guidelines for parameter initialization\n- Include stability analysis for different sequence lengths\n- Provide more detailed training recommendations\n\n3. State Management:\n- Elaborate on state compression strategies\n- Include failure mode analysis\n- Add guidelines for handling very long sequences\n\n4. Hardware Optimization:\n- Provide more specific hardware-aware implementation details\n- Include cache optimization strategies\n- Add guidelines for different hardware architectures\n\n5. Empirical Validation:\n- Define specific metrics for evaluating efficiency gains\n- Include guidelines for comparing with baseline models\n- Add ablation study recommendations",
    "user_input": ""
}