{
    "variantname": "HierarchicalRMSNorm",
    "review": "The HierarchicalRMSNorm proposal presents an innovative approach to enhancing RMSNorm through multi-scale processing and adaptive normalization. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Novelty:\n- The combination of multi-scale processing with RMSNorm represents a novel approach not previously explored in language models\n- The adaptive initialization strategy based on scale-specific statistics is innovative\n- The hierarchical integration mechanism offers a unique solution to feature normalization\n\n2. Technical Design:\n- Well-structured mathematical formulation with clear derivations\n- Thoughtful consideration of memory hierarchy and cache utilization\n- Careful attention to backward compatibility with existing architectures\n\n3. Efficiency Considerations:\n- Memory-efficient design through hierarchical processing\n- Cache-friendly implementation strategies\n- Potential for reduced memory footprint through adaptive compression\n\n4. Theoretical Foundation:\n- Strong grounding in normalization theory\n- Clear connection to existing research in multi-scale processing\n- Well-reasoned approach to parameter initialization\n\nCONCERNS:\n\n1. Implementation Complexity:\n- The multi-scale processing adds significant complexity to the basic RMSNorm\n- Additional hyperparameters (scale factors, compression ratios) require careful tuning\n- Integration with existing architectures may be challenging\n\n2. Computational Overhead:\n- Multiple scale paths could introduce significant computational overhead\n- Memory savings might be offset by increased computation time\n- Scale selection and adaptation add runtime complexity\n\n3. Training Stability:\n- Adaptive initialization might affect training stability\n- Multiple normalization paths could complicate gradient flow\n- Scale-specific parameter updates need careful management\n\n4. Scalability Concerns:\n- Memory benefits might diminish with very large models\n- Computational overhead could become significant at scale\n- Complex interaction with model parallelism strategies\n\nSUGGESTIONS FOR IMPROVEMENT:\n\n1. Implementation Details:\n- Provide more detailed analysis of computational complexity\n- Include concrete examples of scale selection strategies\n- Add guidelines for hyperparameter selection\n\n2. Memory Analysis:\n- Include quantitative analysis of memory savings\n- Provide detailed cache utilization strategies\n- Address potential memory bottlenecks\n\n3. Training Considerations:\n- Add specific guidance for initialization strategies\n- Include stability analysis across different scales\n- Provide recommendations for gradient handling\n\n4. Scalability:\n- Address interaction with model parallelism\n- Include analysis of large-scale deployment\n- Provide benchmarking guidelines",
    "search_stack": [
        {
            "ready": false,
            "query": "language model block architecture, attention mechanism, efficiency",
            "detail": "Analyze recent innovations in language model block architectures, focusing on attention mechanisms, efficiency improvements, and novel ways to handle context and memory. Include specific details about block structures, mathematical formulations, and performance metrics.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent innovations in language model block architectures, focusing on attention mechanisms, efficiency improvements, and novel ways to handle context and memory. Include specific details about block structures, mathematical formulations, and performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.97)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.97)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.93)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 2/25 (Score: 0.93)*\n\n```\nCurrent innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential InContext Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset. ## 1 Introduction\n\nLarge Language Models (LLMs) are revolutionizing the field of natural language processing and establishing new benchmarks across various tasks (Touvron et al., 2023; Jiang et al., 2023). Nevertheless, despite their triumphs, most of these models are built on Transformer frameworks that employ attention mechanisms. These mechanisms scale poorly with long text sequences, leading to impractical computational complexity for extending contextual processing (Vaswani et al., 2017; Tay et al., 2021). To address this constraint, several alternatives to Transformers were proposed. Katharopoulos et al. (2020) suggested replacing the exponential function in the attention mechanism with the kernel function to change the order of computations and\n\n[^0]thus move away from quadratic complexity of the sequence length.\n```\n\n#### 3. Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention (Avg. Score: 0.86)\n\n*Kaiqiang Song, Xiaoyang Wang, Sangwoo Cho, Xiaoman Pan, Dong Yu*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** This paper proposes a new model architecture, referred to as Zebra, that efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers.\n\n**Abstract:** This paper introduces a novel approach to enhance the capabilities of Large Language Models (LLMs) in processing and understanding extensive text sequences, a critical aspect in applications requiring deep comprehension and synthesis of large volumes of information. Recognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency.\n\n##### *Relevant Chunk: No. 2/44 (Score: 0.86)*\n\n```\nRecognizing the inherent challenges in extending the context window for LLMs, primarily built on Transformer architecture, we propose a new model architecture, referred to as Zebra. This architecture efficiently manages the quadratic time and memory complexity issues associated with full attention in the Transformer by employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption. Comprehensive experiments, including pretraining from scratch, continuation of long context adaptation training, and long instruction tuning, are conducted to evaluate the Zebra's performance. The results show that Zebra achieves comparable or superior performance on both short and long sequence benchmarks, while also enhancing training and inference efficiency. ## 1 Introduction\n\nTo effectively leverage the power of Long Context in Large Language Models (LLMs), it is essential to develop and refine techniques that enable these models to process and interpret extensive text sequences accurately. This capability is particularly significant in applications that demand deep understanding and synthesis of large volumes of information, such as summarization (Huang et al., 2021; Hu et al., 2023; Song et al., 2022; Kry\u015bci\u0144ski et al., 2021), reading comprehension (Nguyen et al., 2016; Fan et al., 2019; Zhong et al., 2021; Yang et al., 2023), long-form generation (Guan et al., 2021; Deng et al., 2022; Roziere et al., 2023), and\n\n[^0]complex reasoning (Wei et al., 2022; Yao et al., 2023; Chen et al., 2023a). However, it is challenging to extend the context window from different viewpoints: First, the predominant LLM model uses Transformer architecture (Vaswani et al., 2017). Such models like BERT (Devlin et al., 2018), GPT (OpenAI, 2023), and T5 (Raffel et al., 2020) employ full attention in each layer which inherently incurs quadratic time and memory complexity. This may potentially diminish the efficiency of both the training and inference processes. Second, attention computation over an extremely long sequence might lead to an almost even distribution, potentially causing the omission of vital information (Han et al., 2023). This may further lead to the issue of being \"lost in the middle\" (Liu et al., 2023). Finally, the distribution of training signals for long and short sequences is imbalanced. It is evident that longer sequences are infrequent in both plain text and instructiontuning data. Consequently, this rarity poses a challenge in effectively capturing long-term dependencies during the training process. To tackle the above issues, we propose to group local-global attention layers into blocks during the training and inference phases. This strategy enhances efficiency while yielding results comparable to those of a global attention Transformer. Notably, it attains equivalent performance levels with merely half the computational effort required for training. Additionally, this approach significantly reduces memory consumption during inference by maintaining a local Key-Value (K-V) cache specifically for the local attention layers. In Section 2.1, we list the two critical components essential for a long-context model as well as the potential alternatives for consideration. These encompass diverse attention mechanisms and methodologies for positional embedding. Subsequently, in Section 2.2, we conduct a comparative analysis of these alternatives, presenting their em-\npirical outcomes for a comprehensive evaluation. Integrating these insights, we name our model Zebra, drawing an analogy to the alternating black and white stripes of a zebra, which resemble the grouped local and global layers in our model's architecture. To validate the proposed model at large scales, Section 3 details the continuation of training the Llama-2-7B model (Touvron et al., 2023) using long-context adaptation training through Zebra. This approach not only exhibits comparable performance on short-sequence benchmarks but also achieves superior perplexity results for longer sequences. Additionally, in Section 4, we conduct fine-tuning of Zebra using a combination of both short and long instruction-tuning datasets. This is followed by a systematic evaluation of the model's performance across a range of benchmark datasets. It demonstrates generally better performance on both long and short benchmarks compared to Llama-2-7b-chat. To Conclude, our contribution is 3 -fold:\n\n- We develop a novel architecture, referred to as Zebra, which incorporates grouped localglobal attention layers and rotary positional embedding. - We conduct comprehensive experiments and detailed analyses of the Zebra framework across various settings, including pretraining from scratch, continuation of training, and extensive instruction tuning. The findings from these results demonstrate the advantage of $\\mathbf{Z e}$ bra model architecture. - Additionally, we analyze the training and inference efficiency for Zebra and provide the pseudocode for implementation. ## 2 Zebra\n\n### 2.1 Model Architecture Design\n\nTo extend the context window for Transformer models, two critical elements must be addressed: First, the Attention mechanism that allows the model to efficiently focus on and process relevant parts of long sequences.\n```\n\n#### 4. XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference (Avg. Score: 0.53)\n\n*Shengnan Wang, Youhui Bai, Lin Zhang, Pingyi Zhou, Shixiong Zhao, Gong Zhang, Sen Wang, Renhai Chen, Hua Xu, Hongwei Sun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** An efficient training free framework, named XL3M (it means extra-long large language model), is proposed, which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning.\n\n**Abstract:** Length generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs. To address this problem, the existing methods either require substantial costs or introduce precision loss. In this paper, we empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty. Based on this, we propose an efficient training free framework, named XL3M (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning. Under the XL3M framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common ``question'' which is a few tokens from the end of the original context. Then XL3M gives a method to measure the relevance between each segment and the ``question'', and constructs a concise key context by splicing all the relevant segments in chronological order. The key context is further used instead of the original context to complete the inference task. Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card.\n\n##### *Relevant Chunk: No. 18/19 (Score: 0.53)*\n\n```\narXiv preprint arXiv:2302.13971, 2023. Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wei, C., Wang, Y.-C., Wang, B., and Kuo, C.-C. J. An overview on language models: Recent developments and outlook. arXiv preprint arXiv:2303.05759, 2023. Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with trainingfree memory.\n```\n\n#### 5. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.40)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 28/38 (Score: 0.40)*\n\n```\nArXiv, abs/2307.03170, 2023. [61] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. ArXiv, abs/2302.00294, 2023. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. ArXiv, abs/2312.03863, 2023. [64] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: language model block architecture, attention mechanism, efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models\n\n*From Search Query: language model block architecture*\n\n*Mojan Javaheripi, Gustavo de Rosa, Subhabrata Mukherjee, S. Shah, T. L. Religa, C. C. T. Mendes, S\u00e9bastien Bubeck, F. Koushanfar, Debadeepta Dey*\n\n**TL;DR:** The search phase of this training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs and effectively removes the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.\n\n**Abstract:** The Transformer architecture is ubiquitously used as the building block of large-scale autoregressive language models. However, finding architectures with the optimal trade-off between task performance (perplexity) and hardware constraints like peak memory utilization and latency is non-trivial. This is exacerbated by the proliferation of various hardware. We leverage the somewhat surprising empirical observation that the number of decoder parameters in autoregressive Transformers has a high rank correlation with task performance, irrespective of the architecture topology. This observation organically induces a simple Neural Architecture Search (NAS) algorithm that uses decoder parameters as a proxy for perplexity without need for any model training. The search phase of our training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of perplexity versus any hardware performance cost. We evaluate LTS on diverse devices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer backbones: GPT-2 and Transformer-XL. Results show that the perplexity of 16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster runtime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero and one-shot settings, LTS Pareto-frontier models achieve higher average accuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x lower latency. LTS extracts the Pareto-frontier in under 3 hours while running on a commodity laptop. We effectively remove the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 2. BADGE: Speeding Up BERT Inference after Deployment via Block-wise Bypasses and Divergence-based Early Exiting\n\n*From Search Query: language model block architecture*\n\n*Wei Zhu, Peifeng Wang, Yuan Ni, G. Xie, Xiaoling Wang*\n\n**TL;DR:** A novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs\u2019 early exiting, and proposes the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model.\n\n**Abstract:** Early exiting can reduce the average latency of pre-trained language models (PLMs) via its adaptive inference mechanism and work with other inference speed-up methods like model pruning, thus drawing much attention from the industry. In this work, we propose a novel framework, BADGE, which consists of two off-the-shelf methods for improving PLMs\u2019 early exiting. We first address the issues of training a multi-exit PLM, the backbone model for early exiting. We propose the novel architecture of block-wise bypasses, which can alleviate the conflicts in jointly training multiple intermediate classifiers and thus improve the overall performances of multi-exit PLM while introducing negligible additional flops to the model. Second, we propose a novel divergence-based early exiting (DGE) mechanism, which obtains early exiting signals by comparing the predicted distributions of two adjacent layers\u2019 exits. Extensive experiments on three proprietary datasets and three GLUE benchmark tasks demonstrate that our method can obtain a better speedup-performance trade-off than the existing baseline methods.\\footnote{Code will be made publicly available to the research community upon acceptance.}\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 3. MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases\n\n*From Search Query: language model block architecture*\n\n*Zechun Liu, Changsheng Zhao, Forrest N. Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra*\n\n**TL;DR:** The proposed MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.\n\n**Abstract:** This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 28  (*Influential: 3*)\n\n#### 4. Max-Margin Token Selection in Attention Mechanism\n\n*From Search Query: attention mechanism*\n\n*Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak*\n\n**TL;DR:** This work explores the seminal softmax-attention model and proves that running gradient descent on $\\boldsymbol{p}$ converges in direction to a max-margin solution that separates $\\textit{locally-optimal}$ tokens from non-optical ones, which clearly formalizes attention as an optimal token selection mechanism.\n\n**Abstract:** Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\\boldsymbol{X})=\\langle \\boldsymbol{Xv}, \\texttt{softmax}(\\boldsymbol{XWp})\\rangle$, where $\\boldsymbol{X}$ is the token sequence and $(\\boldsymbol{v},\\boldsymbol{W},\\boldsymbol{p})$ are trainable parameters. We prove that running gradient descent on $\\boldsymbol{p}$, or equivalently $\\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism. Remarkably, our results are applicable to general data and precisely characterize $\\textit{optimality}$ of tokens in terms of the value embeddings $\\boldsymbol{Xv}$ and problem geometry. We also provide a broader regularization path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$ and $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions under which the regularization paths directionally converge to their respective hard-margin SVM solutions where $\\boldsymbol{v}$ separates the input features based on their labels. Interestingly, the SVM formulation of $\\boldsymbol{p}$ is influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, we verify our theoretical findings via numerical experiments and provide insights.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 31  (*Influential: 2*)\n\n#### 5. Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism\n\n*From Search Query: attention mechanism*\n\n*Siqi Miao, Miaoyuan Liu, Pan Li*\n\n**TL;DR:** Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stoChasticity-reduced attention to select task-relevant subgraphs for interpretation.\n\n**Abstract:** Interpretable graph learning is in need as many scientific applications depend on learning models to collect insights from graph-structured data. Previous works mostly focused on using post-hoc approaches to interpret pre-trained models (graph neural networks in particular). They argue against inherently interpretable models because the good interpretability of these models is often at the cost of their prediction accuracy. However, those post-hoc methods often fail to provide stable interpretation and may extract features that are spuriously correlated with the task. In this work, we address these issues by proposing Graph Stochastic Attention (GSAT). Derived from the information bottleneck principle, GSAT injects stochasticity to the attention weights to block the information from task-irrelevant graph components while learning stochasticity-reduced attention to select task-relevant subgraphs for interpretation. The selected subgraphs provably do not contain patterns that are spuriously correlated with the task under some assumptions. Extensive experiments on eight datasets show that GSAT outperforms the state-of-the-art methods by up to 20%$\\uparrow$ in interpretation AUC and 5%$\\uparrow$ in prediction accuracy. Our code is available at https://github.com/Graph-COM/GSAT.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 145  (*Influential: 37*)\n\n#### 6. GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers\n\n*From Search Query: attention mechanism*\n\n*Takeru Miyato, Bernhard Jaeger, Max Welling, Andreas Geiger*\n\n**TL;DR:** This work argues that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure, and proposes a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs.\n\n**Abstract:** As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-based NVS models without any additional learned parameters and only minor computational overhead.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 7. Bigger, Better, Faster: Human-level Atari with human-level efficiency\n\n*From Search Query: efficiency*\n\n*Max Schwarzer, Johan Obando-Ceron, Aaron C. Courville, Marc G. Bellemare, Rishabh Agarwal, P. S. Castro*\n\n**TL;DR:** A value-based RL agent that achieves super-human performance in the Atari 100K benchmark is introduced, which relies on scaling the neural networks used for value estimation as well as a number of other design choices that enable this scaling in a sample-efficient manner.\n\n**Abstract:** We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 60  (*Influential: 9*)\n\n#### 8. Compressing Context to Enhance Inference Efficiency of Large Language Models\n\n*From Search Query: efficiency*\n\n*Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin*\n\n**TL;DR:** Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used, indicating that it strikes a good balance between efficiency and performance.\n\n**Abstract:** Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction in context cost, resulting in a 36\\% reduction in inference memory usage and a 32\\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 5*)\n\n#### 9. Learning MLPs on Graphs: A Unified View of Effectiveness, Robustness, and Efficiency\n\n*From Search Query: efficiency*\n\n*Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, N. Chawla*\n\n**Abstract:** and\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 36  (*Influential: 6*)\n\n### 4 related papers from ArXiv\n\n#### 1. Generalized Probabilistic Attention Mechanism in Transformers\n\n*From Search Query: attention mechanism*\n\n*DongNyeong Heo, Heeyoul Choi*\n\n**Abstract:** The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.\n\n**Published:** 2024-10-21T01:55:52Z  (*Updated: 2024-10-21T01:55:52Z*)\n\n\n\n#### 2. Generalized Attention Mechanism and Relative Position for Transformer\n\n*From Search Query: attention mechanism*\n\n*R. V. R. Pandya*\n\n**Abstract:** In this paper, we propose generalized attention mechanism (GAM) by first\nsuggesting a new interpretation for self-attention mechanism of Vaswani et al.\n. Following the interpretation, we provide description for different variants\nof attention mechanism which together form GAM. Further, we propose a new\nrelative position representation within the framework of GAM. This\nrepresentation can be easily utilized for cases in which elements next to each\nother in input sequence can be at random locations in actual dataset/corpus.\n\n**Published:** 2022-07-24T00:57:06Z  (*Updated: 2022-07-24T00:57:06Z*)\n\n\n\n#### 3. Building an Efficiency Pipeline: Commutativity and Cumulativeness of\n  Efficiency Operators for Transformers\n\n*From Search Query: efficiency*\n\n*Ji Xin, Raphael Tang, Zhiying Jiang, Yaoliang Yu, Jimmy Lin*\n\n**Abstract:** There exists a wide variety of efficiency methods for natural language\nprocessing (NLP) tasks, such as pruning, distillation, dynamic inference,\nquantization, etc. We can consider an efficiency method as an operator applied\non a model. Naturally, we may construct a pipeline of multiple efficiency\nmethods, i.e., to apply multiple operators on the model sequentially. In this\npaper, we study the plausibility of this idea, and more importantly, the\ncommutativity and cumulativeness of efficiency operators. We make two\ninteresting observations: (1) Efficiency operators are commutative -- the order\nof efficiency methods within the pipeline has little impact on the final\nresults; (2) Efficiency operators are also cumulative -- the final results of\ncombining several efficiency methods can be estimated by combining the results\nof individual methods. These observations deepen our understanding of\nefficiency operators and provide useful guidelines for their real-world\napplications.\n\n**Published:** 2022-07-31T18:01:06Z  (*Updated: 2022-07-31T18:01:06Z*)\n\n\n\n#### 4. MicroNet for Efficient Language Modeling\n\n*From Search Query: efficiency*\n\n*Zhongxia Yan, Hanrui Wang, Demi Guo, Song Han*\n\n**Abstract:** It is important to design compact language models for efficient deployment.\nWe improve upon recent advances in both the language modeling domain and the\nmodel-compression domain to construct parameter and computation efficient\nlanguage models. We use an efficient transformer-based architecture with\nadaptive embedding and softmax, differentiable non-parametric cache, Hebbian\nsoftmax, knowledge distillation, network pruning, and low-bit quantization. In\nthis paper, we provide the winning solution to the NeurIPS 2019 MicroNet\nChallenge in the language modeling track. Compared to the baseline language\nmodel provided by the MicroNet Challenge, our model is 90 times more\nparameter-efficient and 36 times more computation-efficient while achieving the\nrequired test perplexity of 35 on the Wikitext-103 dataset. We hope that this\nwork will aid future research into efficient language models, and we have\nreleased our full source code at\nhttps://github.com/mit-han-lab/neurips-micronet.\n\n**Published:** 2020-05-16T05:42:57Z  (*Updated: 2020-05-16T05:42:57Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding\n\n*From Search Query: language model block architecture*\n\n*Lidong Zhou, Min Zhang, Wanxiang Che, Cha Zhang, Dinei Florencio, Yijuan Lu, Guoxin Wang, Furu Wei, Lei Cui, Tengchao Lv, Yiheng Xu, Yang Xu*\n\n**Abstract:** Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$ 0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520), RVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our model and code publicly available at \\url{https://aka.ms/layoutlmv2}.\n\n**Proceeding:** acl-2021-5\n\n**Published:** 2020-12-29\n\n\n\n#### 2. FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision\n\n*From Search Query: language model block architecture*\n\n*Tri Dao, Pradeep Ramani, Vijay Thakkar, Ying Zhang, Ganesh Bikshandi, Jay Shah*\n\n**Abstract:** Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0$\\times$ with FP16 reaching up to 740 TFLOPs/s (75% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6$\\times$ lower numerical error than a baseline FP8 attention.\n\n**Published:** 2024-07-11\n\n\n\n#### 3. On the Contribution of Per-ICD Attention Mechanisms to Classify Health Records in Languages with Fewer Resources than English\n\n*From Search Query: attention mechanism*\n\n*Arantza Casillas, Hercules Dalianis, Alicia P\u00e9rez, Sonja Remmer, Alberto Blanco*\n\n**Abstract:** We introduce a multi-label text classifier with per-label attention for the classification of Electronic Health Records according to the International Classification of Diseases. We apply the model on two Electronic Health Records datasets with Discharge Summaries in two languages with fewer resources than English, Spanish and Swedish. Our model leverages the BERT Multilingual model (specifically the Wikipedia, as the model have been trained with 104 languages, including Spanish and Swedish, with the largest Wikipedia dumps) to share the language modelling capabilities across the languages. With the per-label attention, the model can compute the relevance of each word from the EHR towards the prediction of each label. For the experimental framework, we apply 157 labels from Chapter XI \u2013 Diseases of the Digestive System of the ICD, which makes the attention especially important as the model has to discriminate between similar diseases. 1 https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages\n\n**Proceeding:** ranlp-2021-9\n\n**Published:** None\n\n\n\n#### 4. An Empirical Study of Spatial Attention Mechanisms in Deep Networks\n\n*From Search Query: attention mechanism*\n\n*Jifeng Dai, Xizhou Zhu, Stephen Lin, Dazhi Cheng, Zheng Zhang*\n\n**Abstract:** Attention mechanisms have become a popular component in deep neural networks,\nyet there has been little examination of how different influencing factors and\nmethods for computing attention from these factors affect performance. Toward a\nbetter general understanding of attention mechanisms, we present an empirical\nstudy that ablates various spatial attention elements within a generalized\nattention formulation, encompassing the dominant Transformer attention as well\nas the prevalent deformable convolution and dynamic convolution modules.\nConducted on a variety of applications, the study yields significant findings\nabout spatial attention in deep networks, some of which run counter to\nconventional understanding. For example, we find that the query and key content\ncomparison in Transformer attention is negligible for self-attention, but vital\nfor encoder-decoder attention. A proper combination of deformable convolution\nwith key content only saliency achieves the best accuracy-efficiency tradeoff\nin self-attention. Our results suggest that there exists much room for\nimprovement in the design of attention mechanisms.\n\n**Conference:** an-empirical-study-of-spatial-attention-1\n\n**Published:** 2019-04-11\n\n\n\n#### 5. Reformer: The Efficient Transformer\n\n*From Search Query: efficiency*\n\n*\u0141ukasz Kaiser, Anselm Levskaya, Nikita Kitaev*\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2020-01-13\n\n\n\n#### 6. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\n\n*From Search Query: efficiency*\n\n*Menglong Zhu, Marco Andreetto, Weijun Wang, Dmitry Kalenichenko, Bo Chen, Tobias Weyand, Hartwig Adam, Andrew G. Howard*\n\n**Abstract:** We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.\n\n**Published:** 2017-04-17\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on attention mechanisms, efficiency improvements, and novel ways to handle context and memory, here are some key innovations and strategies from recent research:\n\n## Attention Mechanisms\n\n### Self-Attention and Variants\nThe self-attention mechanism in transformers is a cornerstone of modern language models. It allows the model to dynamically assess the importance of different parts of the input data, capturing long-range dependencies and context more effectively than previous architectures.\n\n- **Grouped Local-Global Attention**: Models like Zebra employ grouped local-global attention layers, balancing local and global attention to reduce computational requirements and memory consumption. This approach alternates between local and global attention layers, providing a more efficient and balanced information flow.\n\n- **Sparse Attention Mechanisms**: Techniques such as sparse attention, where the model only attends to a subset of the input at each layer, can significantly improve efficiency. Models like GPT-3 and the Sparse Transformer use dense and sparse attention mechanisms to handle larger batch sizes and longer sequences efficiently.\n\n### Hybrid Architectures\nHybrid models that combine different types of layers can offer improved efficiency and effectiveness.\n\n- **Jamba Model**: This model interleaves blocks of Transformer and Mamba layers, leveraging the benefits of both model families. It also incorporates Mixture of Experts (MoE) to increase model capacity while managing active parameter usage.\n\n## Efficiency Improvements\n\n### Parallel and Hierarchical Architectures\n- **ParallelGPT**: This architecture splits the decoder blocks into parallel blocks, allowing for faster training and inference by distributing the computation across multiple nodes. It also enables dropping some blocks during inference to trade off output quality for speed.\n\n- **LinearGPT**: This model reduces the dimension of the decoder blocks as the embedding of a token passes through the architecture, inspired by the downsampling in CNNs. This approach helps maintain performance while reducing the number of parameters.\n\n### Memory Efficiency\n- **Liquid Foundation Models (LFMs)**: These models have a reduced memory footprint compared to traditional transformer architectures, especially for long inputs. LFMs efficiently compress inputs, allowing them to process longer sequences on the same hardware.\n\n- **Chunked Processing**: Segmenting text into multiple chunks and using special tokens can facilitate more efficient memory usage. This approach, as seen in \"Taking a Deep Breath,\" helps LLMs interpret information from both individual tokens and aggregated chunk semantic information.\n\n## Context and Memory Handling\n\n### Relative Positional Encoding\nTechniques like relative positional encoding enhance the model\u2019s ability to handle longer sequences by understanding relationships based on relative positions rather than absolute positions. This approach helps transformers generalize better to sequences of varying lengths.\n\n### Hierarchical and Multi-Modal Approaches\n- **Swin Transformer**: Although primarily used in computer vision, the hierarchical structure of the Swin Transformer can be an inspiration for handling high-dimensional data in language models. It allows for varying levels of granularity in feature extraction, which could be adapted for text processing.\n\n- **Multi-Modal Transformers**: Research into multi-modal transformers is gaining momentum, aiming to integrate and process different types of data within a single architecture. This could lead to advancements in handling diverse contexts and improving overall model robustness.\n\n## Mathematical Formulations and Performance Metrics\n\n### Mathematical Representations\n- **ParallelGPT and LinearGPT**: These models are mathematically formulated to reduce dimensions and distribute computations. For example, LinearGPT reduces the dimension of the decoder blocks through dense or linear layers after every set of decoder blocks, ensuring the embedding dimension does not become too small.\n\n### Performance Metrics\n- **RULER Benchmark**: Models like LFMs are optimized to deliver best-in-class performance on benchmarks like RULER, which measures the effectiveness of models at different context lengths. LFMs have shown superior performance with minimal memory footprint, enabling long-context tasks on edge devices.\n\n## Future Directions\n\n### Adaptive Information Aggregation\nImplementing adaptive information aggregation strategies, such as using special tokens to aggregate chunk semantic information, can enhance context handling and memory efficiency.\n\n### Selective Attention Mechanisms\nDeveloping selective attention mechanisms, like grouped query attention or multi-head latent attention, can optimize resource usage and improve performance. For instance, DeepSeek-v2 uses multi-head latent attention to compress the Key-Value cache, achieving faster inference throughput.\n\nBy integrating these innovations\u2014hybrid attention mechanisms, parallel and hierarchical architectures, efficient memory handling, and adaptive information aggregation\u2014the researcher can significantly improve the autoregressive language model design, addressing key areas such as attention efficiency, memory utilization, and context handling.\n",
            "analysis": "## Analysis Note\n\n### 1. Summary of Analysis\n\nThe initial analysis of the seed design and provided references reveals several key themes and opportunities for innovation:\n\n1. **Current Architecture Overview**\n- The seed design uses a traditional GPT2-style architecture with MHA, GatedMLP, and RMSNorm components\n- Uses rotary positional embeddings for position encoding\n- Shows reasonable but not exceptional performance metrics\n\n2. **Key Innovation Areas**\n- Attention mechanism efficiency\n- Memory utilization\n- Context handling\n- Computational complexity\n\n3. **Emerging Trends**\nFrom the references, several promising directions emerge:\n- Hybrid architectures (Jamba combining Transformer-Mamba)\n- Selective attention patterns (Zebra's grouped local-global attention)\n- Information chunking strategies (Taking a Deep Breath paper)\n- Prefix-based optimizations (Just Read Twice approach)\n\n### 2. Useful References with Excerpts\n\n#### From Zebra Paper\nKey excerpt: \"employing grouped local-global attention layers. Our model, akin to a zebra's alternating stripes, balances local and global attention layers, significantly reducing computational requirements and memory consumption.\"\n\nThis suggests potential for:\n- Alternating attention patterns\n- Grouped processing\n- Balanced local-global information flow\n\n#### From Jamba Paper\nKey excerpt: \"interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable.\"\n\nHighlights:\n- Hybrid architecture benefits\n- Efficient capacity scaling\n- Resource management strategies\n\n#### From \"Taking a Deep Breath\" Paper\nKey excerpt: \"segment the text into multiple chunks and insert special token at the end of each chunk... facilitates LLMs to interpret information not only from historical individual tokens but also from the token, aggregating the chunk's semantic information.\"\n\nInsights:\n- Chunked processing benefits\n- Information aggregation strategies\n- Special token utilization\n\n### 3. Key Insights and Analysis\n\n1. **Architectural Patterns**\n- Hybrid approaches show promise for balancing efficiency and effectiveness\n- Alternating different types of processing can provide complementary benefits\n- Special tokens can serve as information aggregation points\n\n2. **Efficiency Considerations**\n- Local-global attention patterns can reduce computational overhead\n- Chunked processing can improve memory efficiency\n- Selective attention mechanisms can optimize resource usage\n\n3. **Innovation Opportunities**\n- Combining chunked processing with hybrid attention mechanisms\n- Implementing adaptive information aggregation\n- Developing new ways to balance local and global context\n\n### 4. Future Search Plan\n\nNext search rounds should focus on:\n1. Specific implementations of hybrid attention mechanisms\n2. Mathematical formulations for information aggregation\n3. Performance comparisons of different chunking strategies\n\n### 5. References\n\n[1] Song, K., Wang, X., Cho, S., Pan, X., Yu, D. (2024). \"Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention.\" arXiv:2402.00064.\n\n[2] Lieber, O., et al. (2024). \"Jamba: A Hybrid Transformer-Mamba Language Model.\" arXiv:2402.05900.\n\n[3] Luo, W., et al. (2024). \"Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens.\" arXiv:2401.12522.\n\n[4] Arora, S., et al. (2024). \"Just read twice: closing the recall gap for recurrent language models.\" arXiv:2401.08602."
        },
        {
            "ready": false,
            "query": "dense connectivity attention, selective attention mechanisms, state space models",
            "detail": "Analyze implementations of dense connectivity in attention mechanisms, focusing on mathematical formulations and performance metrics. Include specific details about selective attention patterns and their integration with state space models.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations of dense connectivity in attention mechanisms, focusing on mathematical formulations and performance metrics. Include specific details about selective attention patterns and their integration with state space models.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.27)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.27)*\n\n```\nFinally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code ${ }^{1}$ and pre-trained model checkpoints ${ }^{2}$ are publicly available. ## 2 Background\n\n### 2.1 Attention Mechanism\n\nSuppose the input to the layer is $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, where $L$ is the sequence length and $d$ is the embedding dimension, then the attention mechanism outputs\n\n$$\n\\operatorname{Attn}(\\mathbf{X})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}$. Here $\\mathbf{W}_{q}, \\mathbf{W}_{k}, \\mathbf{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix $\\mathbf{A}=$ $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\in \\mathbb{R}^{L \\times L}$. Then, $\\mathbf{A}_{i j}$ captures the alignment between the $i$-th and the $j$-th input tokens. ### 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a 1-dimensional input signal $u(t)$ to a $d_{s}$-dimensional latent state $x(t)$, after which $x(t)$ is mapped to a 1-dimensional output signal $y(t)$. Concretely,\n\n$$\nx^{\\prime}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nHere, $\\mathbf{A} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{B} \\in \\mathbb{R}^{d_{s}}$ and $\\mathbf{C} \\in \\mathbb{R}^{d_{s}}$. Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$\n\n[^1]cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, highorder polynomial projection operators) are proposed to initialize A. The HiPPO matrices are designed such that the state $x(t)$ at time $t$ can memorize the history of the input $u(t)$ up to time $t$. Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, where $L$ is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size $\\Delta$, such that\n\n$$\n\\begin{aligned}\n& x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k} \\\\\n& \\text { where } \\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1}(\\mathbf{I}+\\Delta / 2 \\cdot \\mathbf{A}) \\\\\n& \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1} \\Delta \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n\\end{aligned}\n$$\n\nWe unroll the above recurrent representation, after which we have\n\n$$\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\cdots+\\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C B}} u_{k}\n$$\n\nThis can be written as a convolutional representation $y=\\overline{\\mathbf{K}} * u$, where the convolution kernel\n\n$$\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\cdots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nHere, \" $*$ \" is the discrete convolution operator, $u$ represents the input sequence $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, and $y$ represents the corresponding output sequence $\\left(y_{0}, y_{1}, \\cdots, y_{L}\\right)$.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.26)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.26)*\n\n```\nWhile state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5\\% accuracy improvement on public benchmarks. code is avalaible at: https://github.com/ WailordHe/DenseSSM. ## 1. Introduction\n\nSince the release of ChatGPT (OpenAI, 2023), large language models have entered a new epoch, showcasing outstanding abilities in language comprehension, dialogue, and logical reasoning. Over the past year, the industry has witnessed the emergence of numerous large language models, such as LLaMA (Touvron et al., 2023) and ChatGLM (Zeng et al., 2023). These large language models have given rise to a plethora of practical applications, including conversational bots, code assistants, and AI agents. The foundation of large language models lies in the Transformer network\n\n[^0]structure (Vaswani et al., 2017), primarily utilizing a multihead self-attention module for modeling relationships between tokens and a Feed-forward network for non-linear feature transformations. The scaling law (Kaplan et al., 2020) based on the Transformer structure has propelled the continuous development and expansion of large language models. In the Transformer network, multi-head self-attention (MHSA) plays a crucial role, but it comes with significant computational demands and memory requirements during inference. In terms of computational complexity, for an input sentence of length $N$, the calculation of selfattention has a complexity of $O\\left(N^{2}\\right)$ during training and inference. Regarding memory usage, previously encountered keys and values are stored, leading to a memory occupation of $O(N D)$. As a result, recent efforts on network architectures have focused on simplifying Transformer by reducing its computation and space complexity. This includes various approaches, notably convolutional language models (Poli et al., 2023), recurrent unit (Lei, 2021), long context models (Ding et al., 2023), and state space models (SSMs) (Gu et al., 2021; Gu \\& Dao, 2023). These new models have provided strong alternatives to Transformer for building efficient LLMs. SSMs propose modeling sequences by introducing an appropriate design of hidden states for handling long-range dependencies with both training parallelizability and inference efficiency. Starting from the continuous mapping system, SSMs are discretized to process discrete inputs in deep learning such as language sequence. The discretized SSMs can be computed in both linear recurrence and global convolution modes. Commonly, convolution mode is used during training to achieve parallel acceleration, while recurrence mode is used during autoregressive inference because it has lower computational complexity. The core distinction of SSMs from other neural networks, such as fully-connected neural networks, lies in the design of hidden states. Hidden states enable information to be propagated along the temporal dimension, while avoiding the computation complexity of accessing historical tokens at each step. Through state transition parameters $A$, hidden states transfer the hidden information from the previous time\nsteps to the current time step, allowing for autoregressive prediction of the next token. Hidden states play a crucial role in SSMs, but have not received sufficient investigation in the past. Weights and hidden features in different layers contain information at various levels from fine-grained to coarsegrained (Gu et al., 2021). However, in previous versions of SSMs, hidden states only flowed within the current layer and could not transmit more information to deeper layers, thus failing to capture more hierarchical information. In this paper, we propose DenseSSM to facilitate a more comprehensive flow of hidden information between layers in state space models. We first analyze the hidden state degradation in conventional SSMs which will prevent hidden information flow from low levels to high levels. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information that is useful for the final output. The proposed method is applicable to different types of SSMs, such as RetNet (Sun et al., 2023) and Mamba (Gu \\& Dao, 2023). Our approach maintains the training parallelizability and inference efficiency of SSMs, while achieving a significant improvement with only a slight increase in the number of parameters. For instance, our DenseRetNet model outperforms traditional RetNet with up to 5\\% accuracy improvement on public benchmarks.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.26)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.38)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n##### *Relevant Chunk: No. 3/29 (Score: 0.14)*\n\n```\nWe use superscripts, e.g. . ${ }^{d}$, to denote the elements or block-elements of a matrix and a block-matrix. We use subscripts, e.g. ${ }_{i}$, to denote the time index (or input dependency). Specifically, $v_{i}$ represents the value of vector $v$ at time $i$. We use bold notation to indicate sequences, i.e., $\\mathbf{v}_{i}=\\left[v_{1}, \\ldots, v_{i}\\right]$. We use $\\sigma(\\cdot)$ to denote is the sigmoid function. The products $\\odot$ and $\\otimes$ denote the Hadamard (element-wise) product and the Kronecker (block-wise) product, respectively. $\\mathbb{I}_{n}$ denotes the identity matrix of size $\\mathbb{R}^{n \\times n}$. Generally, we omit stating the bias term for weight matrices unless stating the bias term helps with clarity. ## 2 Preliminaries\n\nIn this section, we introduce the key architectural components studied in this work: attention, SSMs, and RNNs. We remark that these components are often the central block - considered to be the backbone - within a complex architecture composed of other blocks and skip connections (see for instance Touvron et al., 2023]). In what follows, we review exclusively the backbone block, which\nwe denote as $f(\\cdot)$ in $\\mathbf{y}=f(\\mathbf{u})$, where $\\mathbf{u} \\in \\mathbb{R}^{L \\times d}$ and $\\mathbf{y} \\in \\mathbb{R}^{L \\times d}$ are the input and output sequences, respectively. ### 2.1 Attention\n\nThe standard self-attention block [Vaswani et al. 2017] consists of three matrices: $W_{Q}, W_{K}$, and $W_{V}$, which are the learnt parameters of the model. These matrices, when multiplied with the input $\\mathbf{u}$, yield the queries $\\mathbf{q} \\in \\mathbb{R}^{d_{k}}$, keys $\\mathbf{k} \\in \\mathbb{R}^{d_{k}}$, and values $\\mathbf{v} \\in \\mathbb{R}^{d_{v}}$, respectively:\n\n$$\n\\mathbf{q}=\\mathbf{u} W_{Q}, \\quad \\mathbf{k}=\\mathbf{u} W_{K}, \\quad \\mathbf{v}=\\mathbf{u} W_{V}\n$$\n\nKeys, queries, and values are then combined in the attention block to produce the output\n\n$$\n\\mathbf{y}=\\zeta\\left(\\frac{\\mathbf{q} \\mathbf{k}^{\\top}}{\\sqrt{d_{k}}}\\right) \\mathbf{v}\n$$\n\nwhere $\\zeta(\\cdot)$ is a map $\\mathbb{R}^{L} \\rightarrow \\mathbb{R}^{L}$ and is applied row-wise. In the standard version of attention - softmax attention $-\\zeta(\\cdot):=\\operatorname{softmax}(\\cdot)$. However, given the limitations of the softmax function, alternative formulations have been proposed. In this work, we consider two formulations of attention: softmax attention (2) and linear attention [Katharopoulos et al. 2020]. We focus on the masked attention formulations, i.e., the attention matrix $\\zeta\\left(\\mathbf{q k}^{\\top}\\right)$ has a lower-triangular structure. Furthermore, in order to simplify the derivations, we drop the scaling factor $\\sqrt{d_{k}}$. ### 2.2 State Space Models\n\nArchitectures based on a state space parametrization compute the output y through a dynamic recurrence of input signals at each time step $i$,\n\n$$\n\\begin{aligned}\nh_{i} & =A_{i} h_{i-1}+B_{i} u_{i} \\\\\ny_{i} & =C_{i} h_{i}+D_{i} u_{i}\n\\end{aligned}\n$$\n\nwhere $h_{i}$ is the hidden state of the system, and the dynamic matrices of appropriate dimensions $A_{i}, B_{i}, C_{i}, D_{i}$ are the learnt parameters of the model. Different time-varying and time-invariant parameterizations for $A_{i}, B_{i}, C_{i}, D_{i}$ have been proposed in the literature (an overview is given in (Amo Alonso et al., 2024]).\n```\n\n#### 4. Generating Long Sequences with Sparse Transformers (Avg. Score: 0.23)\n\n*R. Child, S. Gray, Alec Radford, I. Sutskever*\n\n**Published in:** arXiv.org (2019)\t**Cited by** 1488  (*Influential: 137*)\n\n**TL;DR:** This paper introduces sparse factorizations of the attention matrix which reduce this to $O(n)$, and generates unconditional samples that demonstrate global coherence and great diversity, and shows it is possible in principle to use self-attention to model sequences of length one million or more.\n\n**Abstract:** Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.23)*\n\n```\nWe tested whether such factorizations could match in performance the rich connectivity patterns of Figure 2. ### 4.1. Qualitative assessment of learned attention patterns\n\nWe visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices. In this paper, we restricted our investigation to a class of sparse attention patterns that have connectivity between all positions over several steps of attention. These methods can be more efficient than full attention while still providing global context to any given position. We aimed to empirically validate the performance of these factorized patterns on a range of tasks, given that they are unable to learn the exact same mappings as those in Figure 2. We present the formulation of factorized attention below. ### 4.2. Factorized self-attention\n\nA self-attention layer maps a matrix of input embeddings $X$ to an output matrix and is parameterized by a connectivity pattern $S=\\left\\{S_{1}, \\ldots, S_{n}\\right\\}$, where $S_{i}$ denotes the set of indices of the input vectors to which the $i$ th output vector attends. The output vector is a weighted sum of transformations of the input vectors:\n\n$$\n\\begin{gathered}\n\\operatorname{Attend}(X, S)=\\left(a\\left(\\mathbf{x}_{i}, S_{i}\\right)\\right)_{i \\in\\{1, \\ldots, n\\}} \\\\\na\\left(\\mathbf{x}_{i}, S_{i}\\right)=\\operatorname{softmax}\\left(\\frac{\\left(W_{q} \\mathbf{x}_{i}\\right) K_{S_{i}}^{T}}{\\sqrt{d}}\\right) V_{S_{i}} \\\\\nK_{S_{i}}=\\left(W_{k} \\mathbf{x}_{j}\\right)_{j \\in S_{i}} \\quad V_{S_{i}}=\\left(W_{v} \\mathbf{x}_{j}\\right)_{j \\in S_{i}}\n\\end{gathered}\n$$\n\nHere $W_{q}, W_{k}$, and $W_{v}$ represent the weight matrices which transform a given $\\mathbf{x}_{i}$ into a query, key, or value, and $d$ is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries. Full self-attention for autoregressive models defines $S_{i}=$ $\\{j: j \\leq i\\}$, allowing every element to attend to all previous positions and its own position. Factorized self-attention instead has $p$ separate attention heads, where the $m$ th head defines a subset of the indices $A_{i}^{(m)} \\subset\\{j: j \\leq i\\}$ and lets $S_{i}=A_{i}^{(m)}$. We are chiefly interested in efficient choices for the subset $A$, where $\\left|A_{i}^{(m)}\\right| \\propto \\sqrt[p]{n}$. Additionally, for the time being we consider valid choices of $A$, where all input positions are connected to all future output positions across the $p$ steps of attention. For every $j \\leq i$ pair, we set every $A$ such that $i$ can attend to $j$ through a path of locations with maximum length $p+1$. Specifically, if $(j, a, b, c, \\ldots, i)$ is the path of indices, then $j \\in A_{a}^{(1)}, a \\in A_{b}^{(2)}, b \\in A_{c}^{(3)}$, and so forth. These two criteria allow us keep the ability of Transformers to propagate signals from arbitrary input positions to arbitrary output positions in a constant number of steps, while reducing the total effective computation to $O(n \\sqrt[p]{n})$. We also note that softening the validity criterion (for instance, having a series of only locally connected layers) may be a useful inductive bias for certain domains. In this work, we explore two factorizations for $p=2$, which we describe in the following section, though we note that the same techniques can be easily extended to higher dimensions. ### 4.3. Two-dimensional factorized attention\n\nA natural approach to defining a factorized attention pattern in two dimensions is to have one head attend to the previous $l$ locations, and the other head attend to every $l$ th location, where $l$ is the stride and chosen to be close to $\\sqrt{n}$, a method we call strided attention.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dense connectivity attention, selective attention mechanisms, state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Densely connected normalizing flows\n\n*From Search Query: dense connectivity attention*\n\n*Matej Grcic, I. Grubisic, Sinisa Segvic*\n\n**TL;DR:** This work incrementally padding intermediate representations with noise to increase the model expressivity by fusing a densely connected block with Nystrom self-attention and refers to the architecture as DenseFlow since both cross-unit and intra-module couplings rely on dense connectivity.\n\n**Abstract:** Normalizing flows are bijective mappings between inputs and latent representations with a fully factorized distribution. They are very attractive due to exact likelihood valuation and efficient sampling. However, their effective capacity is often insufficient since the bijectivity constraint limits the model width. We address this issue by incrementally padding intermediate representations with noise. We precondition the noise in accordance with previous invertible units, which we describe as cross-unit coupling. Our invertible glow-like modules increase the model expressivity by fusing a densely connected block with Nystrom self-attention. We refer to our architecture as DenseFlow since both cross-unit and intra-module couplings rely on dense connectivity. Experiments show significant improvements due to the proposed contributions and reveal state-of-the-art density estimation under moderate computing budgets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 52  (*Influential: 3*)\n\n#### 2. Treeformer: Dense Gradient Trees for Efficient Attention Computation\n\n*From Search Query: dense connectivity attention*\n\n*Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, Prateek Jain*\n\n**TL;DR:** This work views attention computation as that of nearest neighbor retrieval, and uses decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic.\n\n**Abstract:** Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are\"dense\". To optimize such challenging discrete layers, we propose a two-level bootstrapped training method. Using extensive experiments on standard NLP benchmarks, especially for long-sequences, we demonstrate that our Treeformer architecture can be almost as accurate as baseline Transformer while using 30x lesser FLOPs in the attention layer. Compared to Linformer, the accuracy can be as much as 12% higher while using similar FLOPs in the attention layer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 3. What Dense Graph Do You Need for Self-Attention?\n\n*From Search Query: dense connectivity attention*\n\n*Yuxing Wang, Chu-Tak Lee, Qipeng Guo, Zhangyue Yin, Yunhua Zhou, Xuanjing Huang, Xipeng Qiu*\n\n**TL;DR:** Hypercube Transformer is presented, a sparse Transformer that models token interactions in a hypercube and shows comparable or even better results with vanilla Transformer while yielding O(N\\log N)$ complexity with sequence length $N$.\n\n**Abstract:** Transformers have made progress in miscellaneous tasks, but suffer from quadratic computational and memory complexities. Recent works propose sparse Transformers with attention on sparse graphs to reduce complexity and remain strong performance. While effective, the crucial parts of how dense a graph needs to be to perform well are not fully explored. In this paper, we propose Normalized Information Payload (NIP), a graph scoring function measuring information transfer on graph, which provides an analysis tool for trade-offs between performance and complexity. Guided by this theoretical analysis, we present Hypercube Transformer, a sparse Transformer that models token interactions in a hypercube and shows comparable or even better results with vanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length $N$. Experiments on tasks requiring various sequence lengths lay validation for our graph function well.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. IIANet: An Intra- and Inter-Modality Attention Network for Audio-Visual Speech Separation\n\n*From Search Query: selective attention mechanisms*\n\n*Kai Li, Run Yang, Fuchun Sun, Xiaolin Hu*\n\n**TL;DR:** The fast version of IIANet (IIANet-fast) has only 7% of CTCNet's MACs and is 40% faster than CTCNet on CPUs while achieving better separation quality, showing the great potential of attention mechanism for efficient and effective multimodal fusion.\n\n**Abstract:** Recent research has made significant progress in designing fusion modules for audio-visual speech separation. However, they predominantly focus on multi-modal fusion at a single temporal scale of auditory and visual features without employing selective attention mechanisms, which is in sharp contrast with the brain. To address this issue, We propose a novel model called Intra- and Inter-Attention Network (IIANet), which leverages the attention mechanism for efficient audio-visual feature fusion. IIANet consists of two types of attention blocks: intra-attention (IntraA) and inter-attention (InterA) blocks, where the InterA blocks are distributed at the top, middle and bottom of IIANet. Heavily inspired by the way how human brain selectively focuses on relevant content at various temporal scales, these blocks maintain the ability to learn modality-specific features and enable the extraction of different semantics from audio-visual features. Comprehensive experiments on three standard audio-visual separation benchmarks (LRS2, LRS3, and VoxCeleb2) demonstrate the effectiveness of IIANet, outperforming previous state-of-the-art methods while maintaining comparable inference time. In particular, the fast version of IIANet (IIANet-fast) has only 7% of CTCNet's MACs and is 40% faster than CTCNet on CPUs while achieving better separation quality, showing the great potential of attention mechanism for efficient and effective multimodal fusion.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. SPECTRA: Sparse Structured Text Rationalization\n\n*From Search Query: selective attention mechanisms*\n\n*Nuno M. Guerreiro, Andr\u00e9 F. T. Martins*\n\n**TL;DR:** This paper presents a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer and provides a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks.\n\n**Abstract:** Selective rationalization aims to produce decisions along with rationales (e.g., text highlights or word alignments between two sentences). Commonly, rationales are modeled as stochastic binary masks, requiring sampling-based gradient estimators, which complicates training and requires careful hyperparameter tuning. Sparse attention mechanisms are a deterministic alternative, but they lack a way to regularize the rationale extraction (e.g., to control the sparsity of a text highlight or the number of alignments). In this paper, we present a unified framework for deterministic extraction of structured explanations via constrained inference on a factor graph, forming a differentiable layer. Our approach greatly eases training and rationale regularization, generally outperforming previous work on what comes to performance and plausibility of the extracted rationales. We further provide a comparative study of stochastic and deterministic methods for rationale extraction for classification and natural language inference tasks, jointly assessing their predictive power, quality of the explanations, and model variability.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 24  (*Influential: 4*)\n\n#### 6. Closed-Book Training to Improve Summarization Encoder Memory\n\n*From Search Query: selective attention mechanisms*\n\n*Yichen Jiang, Mohit Bansal*\n\n**TL;DR:** This paper presents a memory ability test, two saliency metrics, as well as several sanity-check ablations to prove that the encoder of the 2-decoder model does in fact learn stronger memory representations than the baseline encoder.\n\n**Abstract:** A good neural sequence-to-sequence summarization model should have a strong encoder that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the encoder\u2019s memory. In this paper, we aim to improve the memorization capabilities of the encoder of a pointer-generator model by adding an additional \u2018closed-book\u2019 decoder without attention and pointer mechanisms. Such a decoder forces the encoder to be more selective in the information encoded in its memory state because the decoder can\u2019t rely on the extra information provided by the attention and possibly copy modules, and hence improves the entire model. On the CNN/Daily Mail dataset, our 2-decoder model outperforms the baseline significantly in terms of ROUGE and METEOR metrics, for both cross-entropy and reinforced setups (and on human evaluation). Moreover, our model also achieves higher scores in a test-only DUC-2002 generalizability setup. We further present a memory ability test, two saliency metrics, as well as several sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to prove that the encoder of our 2-decoder model does in fact learn stronger memory representations than the baseline encoder.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 27  (*Influential: 0*)\n\n#### 7. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 61  (*Influential: 7*)\n\n#### 8. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: state space models*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 9. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: state space models*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 209  (*Influential: 37*)\n\n### 5 related papers from ArXiv\n\n#### 1. Densely Connected Attention Propagation for Reading Comprehension\n\n*From Search Query: dense connectivity attention*\n\n*Yi Tay, Luu Anh Tuan, Siu Cheung Hui, Jian Su*\n\n**Abstract:** We propose DecaProp (Densely Connected Attention Propagation), a new densely\nconnected neural architecture for reading comprehension (RC). There are two\ndistinct characteristics of our model. Firstly, our model densely connects all\npairwise layers of the network, modeling relationships between passage and\nquery across all hierarchical levels. Secondly, the dense connectors in our\nnetwork are learned via attention instead of standard residual skip-connectors.\nTo this end, we propose novel Bidirectional Attention Connectors (BAC) for\nefficiently forging connections throughout the network. We conduct extensive\nexperiments on four challenging RC benchmarks. Our proposed approach achieves\nstate-of-the-art results on all four, outperforming existing baselines by up to\n$2.6\\%-14.2\\%$ in absolute F1 score.\n\n**Published:** 2018-11-10T07:54:13Z  (*Updated: 2019-04-02T11:19:54Z*)\n\n\n\n#### 2. Cross-relation Cross-bag Attention for Distantly-supervised Relation\n  Extraction\n\n*From Search Query: selective attention mechanisms*\n\n*Yujin Yuan, Liyuan Liu, Siliang Tang, Zhongfei Zhang, Yueting Zhuang, Shiliang Pu, Fei Wu, Xiang Ren*\n\n**Abstract:** Distant supervision leverages knowledge bases to automatically label\ninstances, thus allowing us to train relation extractor without human\nannotations. However, the generated training data typically contain massive\nnoise, and may result in poor performances with the vanilla supervised\nlearning. In this paper, we propose to conduct multi-instance learning with a\nnovel Cross-relation Cross-bag Selective Attention (C$^2$SA), which leads to\nnoise-robust training for distant supervised relation extractor. Specifically,\nwe employ the sentence-level selective attention to reduce the effect of noisy\nor mismatched sentences, while the correlation among relations were captured to\nimprove the quality of attention weights. Moreover, instead of treating all\nentity-pairs equally, we try to pay more attention to entity-pairs with a\nhigher quality. Similarly, we adopt the selective attention mechanism to\nachieve this goal. Experiments with two types of relation extractor demonstrate\nthe superiority of the proposed approach over the state-of-the-art, while\nfurther ablation studies verify our intuitions and demonstrate the\neffectiveness of our proposed two techniques.\n\n**Published:** 2018-12-27T03:03:51Z  (*Updated: 2018-12-27T03:03:51Z*)\n\n\n\n#### 3. Selective Attention Encoders by Syntactic Graph Convolutional Networks\n  for Document Summarization\n\n*From Search Query: selective attention mechanisms*\n\n*Haiyang Xu, Yun Wang, Kun Han, Baochang Ma, Junwen Chen, Xiangang Li*\n\n**Abstract:** Abstractive text summarization is a challenging task, and one need to design\na mechanism to effectively extract salient information from the source text and\nthen generate a summary. A parsing process of the source text contains critical\nsyntactic or semantic structures, which is useful to generate more accurate\nsummary. However, modeling a parsing tree for text summarization is not trivial\ndue to its non-linear structure and it is harder to deal with a document that\nincludes multiple sentences and their parsing trees. In this paper, we propose\nto use a graph to connect the parsing trees from the sentences in a document\nand utilize the stacked graph convolutional networks (GCNs) to learn the\nsyntactic representation for a document. The selective attention mechanism is\nused to extract salient information in semantic and structural aspect and\ngenerate an abstractive summary. We evaluate our approach on the CNN/Daily Mail\ntext summarization dataset. The experimental results show that the proposed\nGCNs based selective attention approach outperforms the baselines and achieves\nthe state-of-the-art performance on the dataset.\n\n**Published:** 2020-03-18T01:30:02Z  (*Updated: 2020-03-18T01:30:02Z*)\n\n\n\n#### 4. LOCOST: State-Space Models for Long Document Abstractive Summarization\n\n*From Search Query: state space models*\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n\n**Published:** 2024-01-31T15:33:37Z  (*Updated: 2024-03-25T12:52:42Z*)\n\n\n\n#### 5. Mimetic Initialization Helps State Space Models Learn to Recall\n\n*From Search Query: state space models*\n\n*Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, Srinadh Bhojanapalli*\n\n**Abstract:** Recent work has shown that state space models such as Mamba are significantly\nworse than Transformers on recall-based tasks due to the fact that their state\nsize is constant with respect to their input sequence length. But in practice,\nstate space models have fairly large state sizes, and we conjecture that they\nshould be able to perform much better at these tasks than previously reported.\nWe investigate whether their poor copying and recall performance could be due\nin part to training difficulties rather than fundamental capacity constraints.\nBased on observations of their \"attention\" maps, we propose a structured\ninitialization technique that allows state space layers to more readily mimic\nattention. Across a variety of architecture settings, our initialization makes\nit substantially easier for Mamba to learn to copy and do associative recall\nfrom scratch.\n\n**Published:** 2024-10-14T23:17:46Z  (*Updated: 2024-10-14T23:17:46Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\n*From Search Query: dense connectivity attention*\n\n*Baining Guo, Stephen Lin, Zheng Zhang, Yixuan Wei, Han Hu, Yue Cao, Yutong Lin, Ze Liu*\n\n**Abstract:** This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-25\n\n\n\n#### 2. AFS: An Attention-based mechanism for Supervised Feature Selection\n\n*From Search Query: selective attention mechanisms*\n\n*Danni Ge, Ning Gui, Ziyin Hu*\n\n**Abstract:** As an effective data preprocessing step, feature selection has shown its\neffectiveness to prepare high-dimensional data for many machine learning tasks.\nThe proliferation of high di-mension and huge volume big data, however, has\nbrought major challenges, e.g. computation complexity and stability on noisy\ndata, upon existing feature-selection techniques. This paper introduces a novel\nneural network-based feature selection architecture, dubbed Attention-based\nFeature Selec-tion (AFS). AFS consists of two detachable modules: an at-tention\nmodule for feature weight generation and a learning module for the problem\nmodeling. The attention module for-mulates correlation problem among features\nand supervision target into a binary classification problem, supported by a\nshallow attention net for each feature. Feature weights are generated based on\nthe distribution of respective feature se-lection patterns adjusted by\nbackpropagation during the train-ing process. The detachable structure allows\nexisting off-the-shelf models to be directly reused, which allows for much less\ntraining time, demands for the training data and requirements for expertise. A\nhybrid initialization method is also intro-duced to boost the selection\naccuracy for datasets without enough samples for feature weight generation.\nExperimental results show that AFS achieves the best accuracy and stability in\ncomparison to several state-of-art feature selection algo-rithms upon both\nMNIST, noisy MNIST and several datasets with small samples.\n\n**Published:** 2019-02-28\n\n\n\n#### 3. How Does Selective Mechanism Improve Self-Attention Networks?\n\n*From Search Query: selective attention mechanisms*\n\n*Long-Yue Wang, Zhaopeng Tu, Xing Wang, Xinwei Geng, Ting Liu, Bing Qin*\n\n**Abstract:** Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words. However, the underlying reasons for their strong performance have not been well explained. In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax. Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs. Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN.\n\n**Conference:** how-does-selective-mechanism-improve-self-1\n\n**Published:** 2020-05-03\n\n\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 5. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space models*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model (LM) design using dense connectivity, attention mechanisms, and state space models, here are some key insights and specific details based on the provided keywords and analysis:\n\n### Dense Connectivity in Attention Mechanisms\n\n1. **Dense Block Structure**:\n   Dense connectivity, as seen in DenseNet architectures, involves each layer receiving inputs from all preceding layers. This approach can be integrated into attention mechanisms to enhance information flow. For example, in the context of image classification, adding attention blocks after dense blocks helps the network focus on important features dynamically.\n\n2. **Mathematical Formulations**:\n   - In dense connectivity, each layer's output is concatenated with the outputs of all previous layers. Mathematically, this can be represented as:\n     \\[\n     x_l = [x_0, x_1, \\ldots, x_{l-1}]\n     \\]\n     where \\(x_l\\) is the output of the \\(l\\)-th layer, and \\([x_0, x_1, \\ldots, x_{l-1}]\\) represents the concatenation of all previous layer outputs.\n\n3. **Selective Attention Patterns**:\n   - Selective attention mechanisms can be implemented using global average pooling followed by fully connected layers and sigmoid activation. This allows the network to calculate attention weights for each channel of the input feature maps, emphasizing important features while minimizing less relevant ones.\n   - Sparse attention mechanisms can also be used to focus on the most significant areas of the input, reducing computational demands while preserving accuracy. This is particularly useful in models that need to handle high-dimensional data and diverse patterns.\n\n### Integration with State Space Models\n\n1. **State Space Modeling**:\n   - State Space Models (SSMs) can be integrated with dense connectivity to maintain fine-grained information crucial for the final output. For instance, DenseSSM selectively integrates shallow-layer hidden states into deeper layers, retaining training parallelizability and inference efficiency[Analysis Note].\n\n2. **Multi-Timescale Processing**:\n   - SSMs can handle multi-timescale processing, which is beneficial for language models that need to capture both short-term and long-term dependencies. This integration can be achieved by using hierarchical processing structures that balance local and global context processing[Analysis Note].\n\n3. **Adaptive Information Routing**:\n   - Adaptive context compression and information routing can be implemented using selective attention mechanisms within SSMs. This allows the model to dynamically adjust the focus on different parts of the input sequence based on relevance, improving efficiency and performance[Analysis Note].\n\n### Performance Metrics and Benchmarks\n\n1. **Efficiency and Performance**:\n   - Models incorporating dense connectivity and selective attention show improved performance metrics such as accuracy, precision, and recall. For example, the enhanced DenseNet model with attention blocks outperformed other state-of-the-art models in image classification tasks.\n\n2. **Benchmarks**:\n   - Benchmarks for these models often include metrics like perplexity for language modeling tasks, as well as efficiency metrics such as training time, inference latency, and memory usage. The integration of dense connectivity and selective attention should be evaluated against these benchmarks to ensure balanced performance and efficiency[Analysis Note].\n\n### Specific Implementations and Strategies\n\n1. **Hybrid Attention Mechanisms**:\n   - Combining different attention types, such as local-global attention balancing, can be beneficial. For instance, the Zebra model extends the context window using layerwise grouped local-global attention, which can be adapted for language models[Analysis Note].\n\n2. **Weight Sharing and Grouped-Query Attention**:\n   - Strategies like weight sharing and grouped-query attention mechanisms can optimize resource usage without increasing model size or latency. These approaches can be integrated into dense connectivity frameworks to enhance efficiency[Analysis Note].\n\nBy focusing on these aspects, researchers can develop autoregressive LM blocks that leverage dense connectivity, selective attention mechanisms, and state space models to achieve better performance, efficiency, and scalability.\n",
            "analysis": "## Analysis Note\n\n### 1. Summary of Analysis\n\nThe initial search results reveal several promising directions for improving LM block design:\n\n1. **Attention Mechanism Innovations**\n- Hybrid approaches combining different attention types\n- Sparse and selective attention patterns\n- Local-global attention balancing\n- Hierarchical processing structures\n\n2. **Efficiency Optimization Approaches**\n- Memory-efficient architectures\n- Parallel processing strategies\n- Chunked and hierarchical processing\n- Dense hidden connections\n\n3. **Context Handling Improvements**\n- Multi-timescale processing\n- Information aggregation techniques\n- Adaptive context compression\n- State space modeling integration\n\n### 2. Useful References with Excerpts\n\n#### From DenseMamba Paper\nKey excerpt: \"DenseSSM retains fine-grained information crucial for the final output. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM maintains the training parallelizability and inference efficiency.\"\n\nThis suggests potential for:\n- Dense connectivity patterns\n- Selective information integration\n- Efficient parallel processing\n\n#### From Linear Transformers Paper\nKey excerpt: \"Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities.\"\n\nHighlights:\n- Trade-offs between efficiency and capability\n- Importance of in-context learning\n- Need for balanced architectures\n\n#### From MobileLLM Paper\nKey excerpt: \"Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms... immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead.\"\n\nInsights:\n- Efficient architecture design principles\n- Weight sharing strategies\n- Grouped attention mechanisms\n\n### 3. Key Insights and Analysis\n\n1. **Architectural Patterns**\n- Dense connectivity can improve information flow\n- Hybrid approaches show promise for balancing efficiency and effectiveness\n- Block-wise processing and weight sharing offer efficiency gains\n\n2. **Attention Mechanisms**\n- Need to balance between global context and local processing\n- Selective attention can improve efficiency without sacrificing performance\n- Grouped-query mechanisms show promise for resource optimization\n\n3. **Innovation Opportunities**\n- Combining dense connectivity with selective attention\n- Integrating multi-timescale processing\n- Implementing adaptive information routing\n\n### 4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of dense connectivity in attention mechanisms\n2. Mathematical formulations for selective attention\n3. Performance metrics and benchmarks for different attention patterns\n4. Integration strategies for state space models with attention\n\n### 5. References\n\n[1] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models.\" arXiv:2402.08608.\n\n[2] Aksenov, Y., et al. (2024). \"Linear Transformers with Learnable Kernel Functions are Better In-Context Models.\" arXiv:2402.17764.\n\n[3] Liu, Z., et al. (2024). \"MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases.\" ICML 2024.\n\n[4] Tarzanagh, D.A., et al. (2023). \"Max-Margin Token Selection in Attention Mechanism.\" NeurIPS 2023.\n\n[5] Song, K., et al. (2023). \"Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention.\" arXiv:2402.00064."
        },
        {
            "ready": false,
            "query": "hybrid SSM attention architectures, memory efficient connectivity, initialization strategies",
            "detail": "Analyze specific implementations of hybrid architectures combining SSMs with attention mechanisms, focusing on initialization strategies and memory-efficient connectivity patterns. Include mathematical formulations and performance comparisons.",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze specific implementations of hybrid architectures combining SSMs with attention mechanisms, focusing on initialization strategies and memory-efficient connectivity patterns. Include mathematical formulations and performance comparisons.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.97)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.98)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.96)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 57/74 (Score: 0.96)*\n\n```\n2019. [113] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. \"An Attention Free Transformer\". In: arXiv preprint arXiv:2105.14103 (2021). [114] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \"Effectively Modeling Time Series with Simple Discrete State Spaces\". In: The International Conference on Learning Representations (ICLR). 2023. [115] Lin Zheng, Chong Wang, and Lingpeng Kong. \"Linear complexity randomized self-attention mechanism\". In: International Conference on Machine Learning. PMLR. 2022, pp. 27011-27041. [116] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \"Efficient Long Sequence Modeling via State Space Augmented Transformer\". In: arXiv preprint arXiv:2212.08136 (2022). ## A Discussion: Selection Mechanism\n\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It can also be viewed as related to \"fast weights\" (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs with the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept that is worth clarifying. Gating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal through time and causes inputs to interact along the sequence length dimension. However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function). For example, elementwise multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta et al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of $R N N$ gating versus the popular usage of multiplicative gating actually have a very different semantic meaning. Hypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose recurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber 1992). Data-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data (Poli et al. 2023). Example: GLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear layer $y=D x$, where $D$ is a diagonal weight parameter. Now suppose that $D$ is itself generated from a linear transformation of $x$, with an optional nonlinearity: $D=\\sigma(\\boldsymbol{W} x)$. Since it is diagonal, the multiplication becomes an elementwise product: $y=\\sigma(W x) \\circ x$. This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative \"branch\"), hypernetworks (since the parameter $\\boldsymbol{D}$ is generated by another layer), and data-dependent (since $\\boldsymbol{D}$ depends on the data $x$ ). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer. Selection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructions-essentially anything with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as well-and we find it uninformative to think of them as such. Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of $\\Delta$ (Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term \"gating\" in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention. ## B Related Work\n\nWe overview several prior works related to our methods. We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV. ## B. 1 S4 Variants and Derivatives\n\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method. - S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020). - DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022). - S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the first S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO dimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism. Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where $\\bar{A}$ is manually set to 0 , instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries. - Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show that real-valued SSMs are empirically effective in certain settings or when combined with different architectural components. - Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI. - SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of S4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do fast autoregressive inference directly. Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant). ## B. 2 SSM Architectures\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer. - GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most importantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1. - Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient attention approximation. - H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures. - Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the input. While sharing the \"selection\" name, we consider this an architectural modification that is closer to architectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\nCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0 ).\n```\n\n#### 2. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.95)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.95)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.95)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.95)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hybrid SSM attention architectures, memory efficient connectivity, initialization strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Block-State Transformers\n\n*From Search Query: hybrid SSM attention architectures*\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid SSM attention architectures*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. CoAtNet: Marrying Convolution and Attention for All Data Sizes\n\n*From Search Query: hybrid SSM attention architectures*\n\n*Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan*\n\n**TL;DR:** This work presents CoAtNets, a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention and (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency.\n\n**Abstract:** Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced\"coat\"nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 991  (*Influential: 105*)\n\n#### 4. Efficient Dialogue Complementary Policy Learning via Deep Q-network Policy and Episodic Memory Policy\n\n*From Search Query: memory efficient connectivity*\n\n*Yangyang Zhao, Zhenyu Wang, Changxi Zhu, Shihan Wang*\n\n**TL;DR:** A novel complementary policy learning (CPL) framework is proposed, which exploits the complementary advantages of the episodic memory (EM) policy and the deep Q-network (DQN) policy to achieve fast and effective dialogue policy learning.\n\n**Abstract:** Deep reinforcement learning has shown great potential in training dialogue policies. However, its favorable performance comes at the cost of many rounds of interaction. Most of the existing dialogue policy methods rely on a single learning system, while the human brain has two specialized learning and memory systems, supporting to find good solutions without requiring copious examples. Inspired by the human brain, this paper proposes a novel complementary policy learning (CPL) framework, which exploits the complementary advantages of the episodic memory (EM) policy and the deep Q-network (DQN) policy to achieve fast and effective dialogue policy learning. In order to coordinate between the two policies, we proposed a confidence controller to control the complementary time according to their relative efficacy at different stages. Furthermore, memory connectivity and time pruning are proposed to guarantee the flexible and adaptive generalization of the EM policy in dialog tasks. Experimental results on three dialogue datasets show that our method significantly outperforms existing methods relying on a single learning system.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 5. Multigrid Neural Memory\n\n*From Search Query: memory efficient connectivity*\n\n*T. Huynh, M. Maire, Matthew R. Walter*\n\n**TL;DR:** A novel approach to endowing neural networks with emergent, long-term, large-scale memory, which functions as a truly generic memory and yields excellent results on tasks decoupled from any notion of spatial geometry.\n\n**Abstract:** We introduce a novel approach to endowing neural networks with emergent, long-term, large-scale memory. Distinct from strategies that connect neural networks to external memory banks via intricately crafted controllers and hand-designed attentional mechanisms, our memory is internal, distributed, co-located alongside computation, and implicitly addressed, while being drastically simpler than prior efforts. Architecting networks with multigrid structure and connectivity, while distributing memory cells alongside computation throughout this topology, we observe the emergence of coherent memory subsystems. Our hierarchical spatial organization, parameterized convolutionally, permits efficient instantiation of large-capacity memories, while multigrid topology provides short internal routing pathways, allowing convolutional networks to efficiently approximate the behavior of fully connected networks. Such networks have an implicit capacity for internal attention; augmented with memory, they learn to read and write specific memory locations in a dynamic data-dependent manner. We demonstrate these capabilities on exploration and mapping tasks, where our network is able to self-organize and retain long-term memory for trajectories of thousands of time steps. On tasks decoupled from any notion of spatial geometry: sorting, associative recall, and question answering, our design functions as a truly generic memory and yields excellent results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 6. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n*From Search Query: memory efficient connectivity*\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 1345  (*Influential: 135*)\n\n#### 7. Expected Gradients of Maxout Networks and Consequences to Parameter Initialization\n\n*From Search Query: initialization strategies*\n\n*Hanna Tseran, Guido Mont\u00fafar*\n\n**TL;DR:** Based on the moments of the gradients of a maxout network, parameter initialization strategies that avoid vanishing and exploding gradients in wide networks are formulated.\n\n**Abstract:** We study the gradients of a maxout network with respect to inputs and parameters and obtain bounds for the moments depending on the architecture and the parameter distribution. We observe that the distribution of the input-output Jacobian depends on the input, which complicates a stable parameter initialization. Based on the moments of the gradients, we formulate parameter initialization strategies that avoid vanishing and exploding gradients in wide networks. Experiments with deep fully-connected and convolutional networks show that this strategy improves SGD and Adam training of deep maxout networks. In addition, we obtain refined bounds on the expected number of linear regions, results on the expected curve length distortion, and results on the NTK.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 8. PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization\n\n*From Search Query: initialization strategies*\n\n*Benyuan Sun, Hongxing Huo, Yi Yang, Bo Bai*\n\n**TL;DR:** A novel idea, PartialFed, which loads a subset of the global model\u2019s parameters rather than loading the entire model used in most previous works is proposed, which improves the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively and develops PartialFed-Adaptive, which automatically selects personalized loading strategy for each client.\n\n**Abstract:** The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data con\ufb01dentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a signi\ufb01cant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain characteristic, such as traf\ufb01c, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed , which loads a subset of the global model\u2019s parameters rather than loading the entire model used in most previous works. We \ufb01rst validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix . Then we develop PartialFed-Adaptive , which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classi\ufb01cation and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs signi\ufb01cantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 81  (*Influential: 6*)\n\n#### 9. Sparse tree-based initialization for neural networks\n\n*From Search Query: initialization strategies*\n\n*P. Lutz, Ludovic Arnould, Claire Boyer, Erwan Scornet*\n\n**TL;DR:** A new sparse initialization technique for (potentially deep) multilayer perceptrons (MLP) is proposed, which first train a tree-based procedure to detect feature interactions and use the resulting information to initialize the network, which is subsequently trained via standard stochastic gradient strategies.\n\n**Abstract:** Dedicated neural network (NN) architectures have been designed to handle specific data types (such as CNN for images or RNN for text), which ranks them among state-of-the-art methods for dealing with these data. Unfortunately, no architecture has been found for dealing with tabular data yet, for which tree ensemble methods (tree boosting, random forests) usually show the best predictive performances. In this work, we propose a new sparse initialization technique for (potentially deep) multilayer perceptrons (MLP): we first train a tree-based procedure to detect feature interactions and use the resulting information to initialize the network, which is subsequently trained via standard stochastic gradient strategies. Numerical experiments on several tabular data sets show that this new, simple and easy-to-use method is a solid concurrent, both in terms of generalization capacity and computation time, to default MLP initialization and even to existing complex deep learning solutions. In fact, this wise MLP initialization raises the resulting NN methods to the level of a valid competitor to gradient boosting when dealing with tabular data. Besides, such initializations are able to preserve the sparsity of weights introduced in the first layers of the network through training. This fact suggests that this new initializer operates an implicit regularization during the NN training, and emphasizes that the first layers act as a sparse feature extractor (as for convolutional layers in CNN).\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 2  (*Influential: 0*)\n\n### 2 related papers from ArXiv\n\n#### 1. Evaluating Lottery Tickets Under Distributional Shifts\n\n*From Search Query: initialization strategies*\n\n*Shrey Desai, Hongyuan Zhan, Ahmed Aly*\n\n**Abstract:** The Lottery Ticket Hypothesis suggests large, over-parameterized neural\nnetworks consist of small, sparse subnetworks that can be trained in isolation\nto reach a similar (or better) test accuracy. However, the initialization and\ngeneralizability of the obtained sparse subnetworks have been recently called\ninto question. Our work focuses on evaluating the initialization of sparse\nsubnetworks under distributional shifts. Specifically, we investigate the\nextent to which a sparse subnetwork obtained in a source domain can be\nre-trained in isolation in a dissimilar, target domain. In addition, we examine\nthe effects of different initialization strategies at transfer-time. Our\nexperiments show that sparse subnetworks obtained through lottery ticket\ntraining do not simply overfit to particular domains, but rather reflect an\ninductive bias of deep neural networks that can be exploited in multiple\ndomains.\n\n**Published:** 2019-10-28T14:29:28Z  (*Updated: 2019-10-28T14:29:28Z*)\n\n\n\n#### 2. OpenPrompt: An Open-source Framework for Prompt-learning\n\n*From Search Query: initialization strategies*\n\n*Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun*\n\n**Abstract:** Prompt-learning has become a new paradigm in modern natural language\nprocessing, which directly adapts pre-trained language models (PLMs) to\n$cloze$-style prediction, autoregressive modeling, or sequence to sequence\ngeneration, resulting in promising performances on various tasks. However, no\nstandard implementation framework of prompt-learning is proposed yet, and most\nexisting prompt-learning codebases, often unregulated, only provide limited\nimplementations for specific scenarios. Since there are many details such as\ntemplating strategy, initializing strategy, and verbalizing strategy, etc. need\nto be considered in prompt-learning, practitioners face impediments to quickly\nadapting the desired prompt learning methods to their applications. In this\npaper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct\nprompt-learning over PLMs. OpenPrompt is a research-friendly framework that is\nequipped with efficiency, modularity, and extendibility, and its combinability\nallows the freedom to combine different PLMs, task formats, and prompting\nmodules in a unified paradigm. Users could expediently deploy prompt-learning\nframeworks and evaluate the generalization of them on different NLP tasks\nwithout constraints. OpenPrompt is publicly released at {\\url{\nhttps://github.com/thunlp/OpenPrompt}}.\n\n**Published:** 2021-11-03T03:31:14Z  (*Updated: 2021-11-03T03:31:14Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: hybrid SSM attention architectures*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: hybrid SSM attention architectures*\n\n*ChengXiang Zhai, Chenguang Zhu, Yichong Xu, Shuohang Wang, Yang Liu, Liliang Ren*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.\n\n**Conference:** sparse-modular-activation-for-efficient\n\n**Published:** 2023-06-19\n\n\n\n#### 3. PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search\n\n*From Search Query: memory efficient connectivity*\n\n*Guo-Jun Qi, Xiaopeng Zhang, Qi Tian, Hongkai Xiong, Yuhui Xu, Lingxi Xie, Xin Chen*\n\n**Abstract:** Differentiable architecture search (DARTS) provided a fast solution in finding effective network architectures, but suffered from large memory and computing overheads in jointly training a super-network and searching for an optimal architecture. In this paper, we present a novel approach, namely, Partially-Connected DARTS, by sampling a small part of super-network to reduce the redundancy in exploring the network space, thereby performing a more efficient search without comprising the performance. In particular, we perform operation search in a subset of channels while bypassing the held out part in a shortcut. This strategy may suffer from an undesired inconsistency on selecting the edges of super-net caused by sampling different channels. We alleviate it using edge normalization, which adds a new set of edge-level parameters to reduce uncertainty in search. Thanks to the reduced memory cost, PC-DARTS can be trained with a larger batch size and, consequently, enjoys both faster speed and higher training stability. Experimental results demonstrate the effectiveness of the proposed method. Specifically, we achieve an error rate of 2.57% on CIFAR10 with merely 0.1 GPU-days for architecture search, and a state-of-the-art top-1 error rate of 24.2% on ImageNet (under the mobile setting) using 3.8 GPU-days for search. Our code has been made available at: https://github.com/yuhuixu1993/PC-DARTS.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-07-12\n\n\n\n#### 4. On improving deep learning generalization with adaptive sparse connectivity\n\n*From Search Query: memory efficient connectivity*\n\n*Mykola Pechenizkiy, Shiwei Liu, Decebal Constantin Mocanu*\n\n**Abstract:** Large neural networks are very successful in various tasks. However, with limited data, the generalization capabilities of deep neural networks are also very limited. In this paper, we empirically start showing that intrinsically sparse neural networks with adaptive sparse connectivity, which by design have a strict parameter budget during the training phase, have better generalization capabilities than their fully-connected counterparts. Besides this, we propose a new technique to train these sparse models by combining the Sparse Evolutionary Training (SET) procedure with neurons pruning. Operated on MultiLayer Perceptron (MLP) and tested on 15 datasets, our proposed technique zeros out around 50% of the hidden neurons during training, while having a linear number of parameters to optimize with respect to the number of neurons. The results show a competitive classification and generalization performance.\n\n**Published:** 2019-06-27\n\n\n\n#### 5. Physics-informed Neural Networks-based Model Predictive Control for Multi-link Manipulators\n\n*From Search Query: initialization strategies*\n\n*Benjamin Unger, J\u00f6rg Fehr, Jonas Kneifl, Jonas Nicodemus*\n\n**Abstract:** We discuss nonlinear model predictive control (NMPC) for multi-body dynamics via physics-informed machine learning methods. Physics-informed neural networks (PINNs) are a promising tool to approximate (partial) differential equations. PINNs are not suited for control tasks in their original form since they are not designed to handle variable control actions or variable initial values. We thus present the idea of enhancing PINNs by adding control actions and initial conditions as additional network inputs. The high-dimensional input space is subsequently reduced via a sampling strategy and a zero-hold assumption. This strategy enables the controller design based on a PINN as an approximation of the underlying system dynamics. The additional benefit is that the sensitivities are easily computed via automatic differentiation, thus leading to efficient gradient-based algorithms. Finally, we present our results using our PINN-based MPC to solve a tracking problem for a complex mechanical system, a multi-link manipulator.\n\n**Published:** 2021-09-22\n\n\n\n#### 6. A Data-driven Approach to Neural Architecture Search Initialization\n\n*From Search Query: initialization strategies*\n\n*Xiao Xiang Zhu, Andr\u00e9s Camero, Kalifou Ren\u00e9 Traor\u00e9*\n\n**Abstract:** Algorithmic design in neural architecture search (NAS) has received a lot of attention, aiming to improve performance and reduce computational cost. Despite the great advances made, few authors have proposed to tailor initialization techniques for NAS. However, literature shows that a good initial set of solutions facilitate finding the optima. Therefore, in this study, we propose a data-driven technique to initialize a population-based NAS algorithm. Particularly, we proposed a two-step methodology. First, we perform a calibrated clustering analysis of the search space, and second, we extract the centroids and use them to initialize a NAS algorithm. We benchmark our proposed approach against random and Latin hypercube sampling initialization using three population-based algorithms, namely a genetic algorithm, evolutionary algorithm, and aging evolution, on CIFAR-10. More specifically, we use NAS-Bench-101 to leverage the availability of NAS benchmarks. The results show that compared to random and Latin hypercube sampling, the proposed initialization technique enables achieving significant long-term improvements for two of the search baselines, and sometimes in various search scenarios (various training budgets). Moreover, we analyze the distributions of solutions obtained and find that that the population provided by the data-driven initialization technique enables retrieving local optima (maxima) of high fitness and similar configurations.\n\n**Published:** 2021-11-05\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by integrating State Space Models (SSMs) with attention mechanisms, focusing on initialization strategies and memory-efficient connectivity patterns, here are some key insights and specific implementations from the provided sources:\n\n## Hybrid SSM-Attention Architectures\n\n### Interleaved SSM and Attention Layers\nHybrid models like Zamba2-7B and Falcon Mamba demonstrate the effectiveness of combining SSMs with attention layers. These models interleave Mamba layers (SSMs) with shared attention layers, which helps in leveraging the strengths of both architectures. For example, Zamba2-7B uses a backbone of Mamba layers alternated with one or more shared attention layers, improving performance by maintaining information across depth and reducing parameter costs.\n\n### Mathematical Formulations\nThe integration of SSMs and attention can be theoretically unified through structured semiseparable matrices, which connect SSMs and variants of attention. This framework suggests that SSMs can be enhanced with attention-like capabilities through proper decompositions and initializations.\n\n### Performance Comparisons\nHybrid models like Zamba2-7B outperform several strong baselines, including Meta's Llama3 series and Google's Gemma series, in terms of inference efficiency, latency, and memory usage. Zamba2-7B achieves state-of-the-art performance among models with \u22648B parameters, highlighting the benefits of combining SSMs and attention.\n\n## Memory-Efficient Connectivity Patterns\n\n### Hierarchical Information Flow\nModels like Zamba2-7B utilize hierarchical information flow by concatenating original model embeddings to the input of the attention block, which improves performance by better maintaining information across depth. This approach ensures that key features are propagated effectively across layers.\n\n### Cross-Layer Connections and Selective Feature Propagation\nThe use of LoRA (Low-Rank Adaptation) projection matrices in Zamba2-7B allows each shared block to specialize slightly to its own unique position, enhancing expressivity while keeping the additional parameter overhead small. This selective feature propagation helps in efficient information flow and reduces memory requirements.\n\n## Initialization Strategies\n\n### Mimetic Initialization\nMimetic initialization techniques allow state space layers to mimic attention mechanisms more effectively. This approach makes it easier for Mamba models to learn tasks such as copying and associative recall from scratch, enhancing their memory capabilities and overall performance[Analysis Note].\n\n### Structured Initialization\nStructured initialization techniques, such as those proposed for diagonal state space models, can significantly improve the learning process of SSMs. These techniques ensure that the SSM layers are initialized in a way that aligns with the capabilities of attention mechanisms, facilitating better integration and performance.\n\n## Implementation Considerations\n\n### Layer-Wise Nonlinearity\nStacking state-space models with layer-wise nonlinear activation is crucial for approximating any continuous sequence-to-sequence relationship. This design principle ensures that the hybrid model is expressive enough to handle complex tasks while maintaining efficiency[Analysis Note].\n\n### Selective State Transitions\nImplementing selective state transitions in SSMs can improve memory efficiency. For instance, the Sequential Prefill method in Falcon Mamba allows SSMs to process arbitrarily long prompts by processing tokens sequentially, mitigating memory scaling issues.\n\nIn summary, the key to improving autoregressive language model design lies in:\n\n- **Hybrid Architectures**: Interleaving SSMs with attention layers to leverage their complementary strengths.\n- **Memory-Efficient Connectivity**: Using hierarchical information flow, cross-layer connections, and selective feature propagation to maintain information across depth efficiently.\n- **Initialization Strategies**: Employing mimetic and structured initialization techniques to align SSMs with attention mechanisms.\n- **Layer-Wise Nonlinearity**: Ensuring expressiveness through layer-wise nonlinear activations.\n- **Selective State Transitions**: Implementing methods like Sequential Prefill to handle long sequences efficiently.\n\nThese approaches can help in designing models that are not only highly performant but also efficient in terms of memory and computational resources.\n",
            "analysis": "## Analysis Note\n\n### 1. Summary of Analysis\n\nThe search results reveal several promising directions for improving LM block design through the integration of:\n\n1. **State Space Models (SSMs)**\n- Novel parameterizations and initializations for better performance\n- Integration with attention mechanisms\n- Selective state transitions for improved memory capabilities\n\n2. **Dense Connectivity Patterns**\n- Hierarchical information flow\n- Cross-layer connections\n- Selective feature propagation\n\n3. **Hybrid Architectures**\n- Combination of different processing mechanisms\n- Multi-scale temporal modeling\n- Adaptive computation paths\n\n### 2. Useful References with Excerpts\n\n#### From \"Transformers are SSMs\"\nKey excerpt: \"develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices.\"\n\nThis suggests:\n- Theoretical unification of SSMs and attention\n- Potential for hybrid architectures\n- Optimization opportunities\n\n#### From \"Mimetic Initialization\"\nKey excerpt: \"propose a structured initialization technique that allows state space layers to more readily mimic attention [...] makes it substantially easier for Mamba to learn to copy and do associative recall from scratch.\"\n\nHighlights:\n- Importance of initialization\n- Bridging SSM and attention capabilities\n- Memory enhancement techniques\n\n#### From \"State-space Models with Layer-wise Nonlinearity\"\nKey excerpt: \"stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\"\n\nInsights:\n- Layer-wise nonlinearity importance\n- Universal approximation capabilities\n- Architectural design principles\n\n### 3. Key Insights and Analysis\n\n1. **Architectural Integration**\n- SSMs can be enhanced with attention-like capabilities through proper initialization\n- Dense connectivity can improve information flow across layers\n- Layer-wise nonlinearity is crucial for expressiveness\n\n2. **Memory and Processing**\n- SSMs with proper design can handle long-range dependencies\n- Selective mechanisms can improve memory efficiency\n- Hybrid approaches can combine strengths of different architectures\n\n3. **Implementation Considerations**\n- Initialization strategies are critical for performance\n- Layer-wise design choices affect overall capabilities\n- Balance needed between expressiveness and efficiency\n\n### 4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of hybrid SSM-attention architectures\n2. Memory-efficient dense connectivity patterns\n3. Novel initialization strategies for improved performance\n4. Integration methods for different processing mechanisms\n\n### 5. References\n\n[1] Gu, A., & Dao, T. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" arXiv:2402.18391.\n\n[2] Trockman, A., et al. (2024). \"Mimetic Initialization Helps State Space Models Learn to Recall.\" arXiv:2402.05402.\n\n[3] Wang, S., & Xue, B. (2023). \"State-space Models with Layer-wise Nonlinearity are Universal Approximators.\" NeurIPS 2023.\n\n[4] Lu, C.X., et al. (2023). \"Structured State Space Models for In-Context Reinforcement Learning.\" NeurIPS 2023.\n\n[5] Gu, A., et al. (2022). \"On the Parameterization and Initialization of Diagonal State Space Models.\" NeurIPS 2022."
        },
        {
            "ready": false,
            "query": "multi-scale processing, memory hierarchy optimization, adaptive initialization",
            "detail": "Analyze specific implementations of multi-scale processing in language model blocks, focusing on memory hierarchy optimization and adaptive initialization strategies. Include mathematical formulations and performance metrics.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze specific implementations of multi-scale processing in language model blocks, focusing on memory hierarchy optimization and adaptive initialization strategies. Include mathematical formulations and performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.02)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 24/46 (Score: 0.02)*\n\n```\nMultiMedia LLC. 2009. Large text compression benchmark. G\u00e1bor Melis, Charles Blundell, Tom\u00e1\u0161 Ko\u010disk\u1ef3, Karl Moritz Hermann, Chris Dyer, and Phil Blunsom. 2018. Pushing the bounds of dropout. arXiv preprint arXiv:1805.09208. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843. Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc'Aurelio Ranzato. 2014. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753. Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Jan \u010cernock\u1ef3, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association. Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. SLT, 12(234-239):8.\n```\n\n#### 2. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.01)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.01)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 3. Scaling Laws of RoPE-based Extrapolation (Avg. Score: 0.00)\n\n*Xiaoran Liu, Hang Yan, Shuo Zhang, Chen An, Xipeng Qiu, Dahua Lin*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 36  (*Influential: 4*)\n\n**TL;DR:** This work proposes a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length, and achieves extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n**Abstract:** The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.00)*\n\n```\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/ P16-1144. URL https://doi.org/10.18653/v1/p16-1144. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. doi: 10.48550/ARXIV.2309. 00071. URLhttps://doi.org/10.48550/arXiv.2309.00071. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Shawn Presser. Books3, 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, pp. 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https: //doi.org/10.1109/SC41405.2020.00024. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve in-context learning of large language models.\n```\n\n#### 4. Scalable MatMul-free Language Modeling (Avg. Score: 0.00)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.00)*\n\n```\nEshraghian ${ }^{1 *}$<br>${ }^{1}$ University of California, Santa Cruz ${ }^{2}$ Soochow University<br>${ }^{3}$ University of California, Davis ${ }^{4}$ LuxiTech\n\n\n#### Abstract\n\nMatrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to $61 \\%$ over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than $10 \\times$ compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13 W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github. com/ridgerchu/matmulfreellm. ## 1 Introduction\n\nMatrix Multiplication (MatMul) is the dominant operation in most neural networks, where dense layers involve vector-matrix multiplication (VMM), convolutions can be implemented as block-sparse VMMs with shared weights, and self-attention relies on matrix-matrix multiplication (MMM). The prevalence of MatMul is primarily due to Graphics Processing Units (GPUs) being optimized for MatMul operations. By leveraging Compute Unified Device Architecture (CUDA) and highly optimized linear algebra libraries such as cuBLAS, the MatMul operation can be efficiently parallelized and accelerated. This optimization was a key factor in the victory of AlexNet in the ILSVRC2012 competition and a historic marker for the rise of deep learning [1]. AlexNet notably utilized GPUs to boost training speed beyond CPU capabilities, and as such, deep learning won the 'hardware lottery' [2]. It also helped that both training and inference rely on MatMul. Despite its prevalence in deep learning, MatMul operations account for the dominant portion of computational expense, often consuming the majority of the execution time and memory access during\n\n[^0]both training and inference phases. Several works have replaced MatMul with simpler operations through two main strategies. The first strategy involves substituting MatMul with elementary operations, e.g., AdderNet replaces multiplication with signed addition in convolutional neural networks (CNNs) [3]. Given the focus on convolutions, AdderNet is intended for use in computer vision over language modeling. The second approach employs binary or ternary quantization, simplifying MatMul to operations where values are either flipped or zeroed out before accumulation. Quantization can be applied to either activations or weights: spiking neural networks (SNNs) use binarized activations [4, 5, 6], while binary and ternary neural networks (BNNs and TNNs) use quantized weights [7]. Both methods can also be combined $[8,9]$. Recent advances in language modeling, like BitNet [10, 11], demonstrate quantization's scalability, replacing all dense layer weights with binary/ternary values to support up to 3 billion parameters. Despite replacing VMMs with accumulations in all dense layers, BitNet retains the self-attention mechanism which relies on an expensive MMM. Dynamically computed matrices $Q$ (query) and $K$ (key) are multiplied to form the attention map. Since both $Q$ and $K$ matrices are dynamically computed from pre-activation values, achieving optimal hardware efficiency on GPUs requires custom optimizations, such as specialized kernels and advanced memory access patterns. Despite these efforts, such MatMul operations remain resource-intensive on GPUs, as they involve extensive data movement and synchronization which can significantly hinder computational throughput and efficiency [12]. In our experiments, ternary quantization of the attention matrices in BitNet causes a significant drop in performance and failure to reach model convergence (see Fig. 1). This raises the question: is it possible to completely eliminate MatMul from LLMs? In this work, we develop the first scalable MatMul-free language model (Matmul-free LM) by using additive operations in dense layers and element-wise Hadamard products for self-attention-like functions. Specifically, ternary weights eliminate MatMul in dense layers, similar to BNNs. To remove MatMul from self-attention, we optimize the Gated Recurrent Unit (GRU) [13] to rely solely on element-wise products and show that this model competes with state-of-the-art Transformers while eliminating all MatMul operations. To quantify the hardware benefits of lightweight models, we provide an optimized GPU implementation in addition to a custom FPGA accelerator. By using fused kernels in the GPU implementation of the ternary dense layers, training is accelerated by $25.6 \\%$ and memory consumption is reduced by up to $61.0 \\%$ over an unoptimized baseline on GPU. Furthermore, by employing lower-bit optimized CUDA kernels, inference speed is increased by 4.57 times, and memory usage is reduced by a factor of 10 when the model is scaled up to 13B parameters. This work goes beyond software-only implementations of lightweight models and shows how scalable, yet lightweight, language models can both reduce computational demands and energy use in the real-world. ## 2 Related Works\n\nBinary, Ternary, and Low-Precision Quantization for Language Models: The effort to quantize language models began with reducing a ternary BERT into a binarized model [14], achieving $41 \\%$ average accuracy on the GLUE benchmarks with subsequent fine-tuning. Ref. [15] distilled the intermediate outputs from a full precision BERT to a quantized version. Recently, Ref. [16] introduced an incremental quantization approach, progressively quantizing a model from 32-bit to 4-bit, 2-bit, and finally to binary model parameters. Following the quantization of BERT, low-precision language generation models have gained momentum. Ref. [17] used Quantization-Aware Training (QAT) to successfully train a model with 2-bit weights. BitNet pushed this to 3-billion-parameter binary and ternary models while maintaining competitive performance with Llama-like language models [10, 11]. MatMul-free Transformers: The use of MatMul-free Transformers has been largely concentrated in the domain of SNNs. Spikformer led the first integration of the Transformer architecture with SNNs [18, 19], with later work developing alternative Spike-driven Transformers [20, 21]. These techniques demonstrated success in vision tasks. In the language understanding domain, SpikingBERT [22] and SpikeBERT [23] applied SNNs to BERT utilizing knowledge distillation techniques to perform sentiment analysis. In language generation, SpikeGPT trained a 216 M -parameter generative model using a spiking RWKV architecture. However, these models remain constrained in size, with\n\nSpikeGPT being the largest, reflecting the challenges of scaling with binarized activations. In addition to SNNs, BNNs have also made significant progress in this area. BinaryViT [24] and BiViT [25] successfully applied Binary Vision Transformers to visual tasks. Beyond these approaches, Kosson et al. [26] achieve multiplication-free training by replacing multiplications, divisions, and non-linearities with piecewise affine approximations while maintaining performance. ## 3 Method\n\nIn this section, we break down the components of the proposed MatMul-free LM. We first describe the MatMul-free dense layers (BitLinear layers) that use ternary weights. By constraining the weights to the set $\\{-1,0,+1\\}$ and applying additional quantization techniques, MatMul operations are replaced with addition and negation operations. This reduces computational cost and memory utilization, while preserving the expressiveness of the network. We then provide further detail of our MatMul-free LM architecture, which includes a token mixer for capturing sequential dependencies and a channel mixer for integrating information across embedding dimensions. The Method section is structured as follows. First, in Sec. 3.1, we provide a comprehensive description of the MatMul-free dense layers with ternary weights, which form the foundation of our approach. Next, Sec. 3.2 introduces our hardware-efficient fused BitLinear layer, designed to optimize the implementation of BitLinear layers. Building upon these components, Sec. 3.3 delves into the details of our MatMul-free LM architecture. We present the MatMul-free token mixer, where we propose the MatMul-free Linear Gated Recurrent Unit (MLGRU), and the MatMul-free channel mixer, which employs the Gated Linear Unit (GLU) with BitLinear layers. By combining the MLGRU token mixer and the GLU channel mixer with ternary weights, our proposed architecture relies solely on addition and element-wise products.\n```\n\n#### 5. Sequence Modeling with Multiresolution Convolutional Memory  (Avg. Score: 0.00)\n\n*Jiaxin Shi, Ke Alexander Wang, E. Fox*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 6  (*Influential: 1*)\n\n**TL;DR:** The key component of the model is the multiresolution convolution, capturing multiscale trends in the input sequence, and can be implemented with shared filters across a dilated causal convolution tree, garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions.\n\n**Abstract:** Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -- poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. Our MultiresLayer is straightforward to implement, requires significantly fewer parameters, and maintains at most a $\\mathcal{O}(N\\log N)$ memory footprint for a length $N$ sequence. Yet, by stacking such layers, our model yields state-of-the-art performance on a number of sequence classification and autoregressive density estimation tasks using CIFAR-10, ListOps, and PTB-XL datasets.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.00)*\n\n```\nWe do not observe statistically significant differences between the approaches. This demonstrates the advantage of our model over S4-related methods that require careful initialization. Memory mechanism. We do not notice a statistically significant difference between uniform and resolution fading, though resolution fading provides a simpler implementation. Importance of receptive fields. Finally, we show that we can significantly improve the performance of this model by increasing either the filter size (Experiment 2 vs. 6) or the depth of the MultiresConv (Experiment 3 vs. 4). We believe this is because both changes increase the receptive field size of the MultiresConV operation, which is particularly important for reasoning tasks like ListOps. ## 6. Conclusion\n\nWe presented MultiresLayer for robust and efficient memorization of long-term patterns in sequential data sources. It takes inspiration from the multiresolution analysis (MRA) literature, building on wavelet decompositions, to memorize patterns occurring at multiple timescales. In particular, our memory is generated by multiresolution convolutions, implemented as dilated causal convolutions with learned filters shared between tree levels that are connected via purely linear operations. To create the memory, all multiresolution values may be maintained, or more emphasis can be placed on more recent time points by leveraging the time-localized nature of wavelet transforms. The resulting MUltiReSNET garners the computational advantages of convolutional networks while being defined by dramatically fewer parameters than competitor models, all while achieving state-of-the-art performance in a number of benchmark sequence modeling tasks. These experiments demonstrate the portability of our multiresolution memory structure to a number of tasks, even in cases where a given task may not intuitively be viewed in a multiscale fashion (e.g., syntax tree parsing in ListOps). By taking inspiration from the wavelet literature, we built an effective convolutional layer with dramatically fewer parameters without taking a performance hit. The principled underpinnings of the MultiresCONV ensure it possesses a configuration with strong reconstruction capabilities (e.g., when our filters equal the wavelet filters); however, as we showed, predictive performance can be improved by learning the filters. Another potential benefit of starting from the wavelet framework is the ability to leverage that vast literature in that\ndomain for future modeling advances. In particular, we plan to explore the utility of MUltiresCONV in representation learning and long-term forecasting. For representation learning, we can consider the structure of shift-invariant wavelet transforms (Kingsbury, 1998; Selesnick et al., 2005) to target representations that are invariant to shifts of the input signals. For example, we may want to cluster individuals with similar ECG signals even if the key signatures are shifted relative to one another. Wavelets may also be extended to image analysis, enabling video analysis in our sequential setting. ## Acknowledgements\n\nThis work was supported in part by AFOSR Grant FA955021-1-0397, ONR Grant N00014-22-1-2110, the National Science Foundation under grant 2205084, and the Stanford Institute for Human-Centered Artificial Intelligence (HAI). EBF is a Chan Zuckerberg Biohub - San Francisco Investigator. KAW was partially supported by Stanford Data Science as a Stanford Data Science Scholar. ## References\n\nAkansu, A. N., Haddad, R. A., and Haddad, P. A. Multiresolution signal decomposition: transforms, subbands, and wavelets. Academic press, 2001. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Bai, S., Kolter, J. Z., and Koltun, V. Trellis networks for sequence modeling. In International Conference on Learning Representations, 2019. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Chen, X., Mishra, N., Rohaninejad, M., and Abbeel, P. PixelSNAIL: An improved autoregressive generative model. In International Conference on Machine Learning, pp. 864-872. PMLR, 2018. Cheng, L., Khalitov, R., Yu, T., Zhang, J., and Yang, Z. Classification of long sequential data using circular dilated convolutional neural networks. Neurocomputing, 518:50-59, 2023. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. In International Conference on Learning Representations, 2020. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(ARTICLE):2493-2537, 2011. Daubechies, I. Orthonormal bases of compactly supported wavelets. Communications on Pure and Applied Mathematics, 41(7):909-996, 1988. Dauphin, Y. N., Fan, A., Auli, M., and Grangier, D. Language modeling with gated convolutional networks. In International Conference on Machine Learning, pp. 933941. PMLR, 2017. Erichson, N.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: multi-scale processing, memory hierarchy optimization, adaptive initialization\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Weakly-Supervised Concealed Object Segmentation with SAM-based Pseudo Labeling and Multi-scale Feature Grouping\n\n*From Search Query: multi-scale processing*\n\n*Chunming He, Kai Li, Yachao Zhang, Guoxia Xu, Longxiang Tang, Yulun Zhang, Z. Guo, Xiu Li*\n\n**TL;DR:** A multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images.\n\n**Abstract:** Weakly-Supervised Concealed Object Segmentation (WSCOS) aims to segment objects well blended with surrounding environments using sparsely-annotated data for model training. It remains a challenging task since (1) it is hard to distinguish concealed objects from the background due to the intrinsic similarity and (2) the sparsely-annotated training data only provide weak supervision for model learning. In this paper, we propose a new WSCOS method to address these two challenges. To tackle the intrinsic similarity challenge, we design a multi-scale feature grouping module that first groups features at different granularities and then aggregates these grouping results. By grouping similar features together, it encourages segmentation coherence, helping obtain complete segmentation results for both single and multiple-object images. For the weak supervision challenge, we utilize the recently-proposed vision foundation model, Segment Anything Model (SAM), and use the provided sparse annotations as prompts to generate segmentation masks, which are used to train the model. To alleviate the impact of low-quality segmentation masks, we further propose a series of strategies, including multi-augmentation result ensemble, entropy-based pixel-level weighting, and entropy-based image-level selection. These strategies help provide more reliable supervision to train the segmentation model. We verify the effectiveness of our method on various WSCOS tasks, and experiments demonstrate that our method achieves state-of-the-art performance on these tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 67  (*Influential: 2*)\n\n#### 2. Depth Map Prediction from a Single Image using a Multi-Scale Deep Network\n\n*From Search Query: multi-scale processing*\n\n*D. Eigen, Christian Puhrsch, R. Fergus*\n\n**TL;DR:** This paper employs two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally, and applies a scale-invariant error to help measure depth relations rather than scale.\n\n**Abstract:** Predicting depth is an essential component in understanding the 3D geometry of a scene. While for stereo images local correspondence suffices for estimation, finding depth relations from a single image is less straightforward, requiring integration of both global and local information from various cues. Moreover, the task is inherently ambiguous, with a large source of uncertainty coming from the overall scale. In this paper, we present a new method that addresses this task by employing two deep network stacks: one that makes a coarse global prediction based on the entire image, and another that refines this prediction locally. We also apply a scale-invariant error to help measure depth relations rather than scale. By leveraging the raw datasets as large sources of training data, our method achieves state-of-the-art results on both NYU Depth and KITTI, and matches detailed depth boundaries without the need for superpixelation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2014\n\n**Citations:** 3716  (*Influential: 673*)\n\n#### 3. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: multi-scale processing*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 189  (*Influential: 27*)\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: memory hierarchy optimization*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**TL;DR:** The low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter, is introduced and achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 3*)\n\n#### 5. CAME: Confidence-guided Adaptive Memory Efficient Optimization\n\n*From Search Query: memory hierarchy optimization*\n\n*Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, Yang You*\n\n**TL;DR:** This paper proposes CAME, a confidence-guided strategy to reduce the instability of existing memory efficient optimizers and proposes it to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods.\n\n**Abstract:** Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 6. Memory-Constrained Algorithms for Convex Optimization via Recursive Cutting-Planes\n\n*From Search Query: memory hierarchy optimization*\n\n*Moise Blanchard, Junhui Zhang, P. Jaillet*\n\n**TL;DR:** This is the first class of algorithms that provides a positive trade-off between gradient descent and cutting-plane methods in any regime with $\\epsilon\\leq 1/\\sqrt d$ and achieves the information-theoretic optimal memory usage and improves the oracle-complexity of gradient descent.\n\n**Abstract:** We propose a family of recursive cutting-plane algorithms to solve feasibility problems with constrained memory, which can also be used for first-order convex optimization. Precisely, in order to find a point within a ball of radius $\\epsilon$ with a separation oracle in dimension $d$ -- or to minimize $1$-Lipschitz convex functions to accuracy $\\epsilon$ over the unit ball -- our algorithms use $\\mathcal O(\\frac{d^2}{p}\\ln \\frac{1}{\\epsilon})$ bits of memory, and make $\\mathcal O((C\\frac{d}{p}\\ln \\frac{1}{\\epsilon})^p)$ oracle calls, for some universal constant $C \\geq 1$. The family is parametrized by $p\\in[d]$ and provides an oracle-complexity/memory trade-off in the sub-polynomial regime $\\ln\\frac{1}{\\epsilon}\\gg\\ln d$. While several works gave lower-bound trade-offs (impossibility results) -- we explicit here their dependence with $\\ln\\frac{1}{\\epsilon}$, showing that these also hold in any sub-polynomial regime -- to the best of our knowledge this is the first class of algorithms that provides a positive trade-off between gradient descent and cutting-plane methods in any regime with $\\epsilon\\leq 1/\\sqrt d$. The algorithms divide the $d$ variables into $p$ blocks and optimize over blocks sequentially, with approximate separation vectors constructed using a variant of Vaidya's method. In the regime $\\epsilon \\leq d^{-\\Omega(d)}$, our algorithm with $p=d$ achieves the information-theoretic optimal memory usage and improves the oracle-complexity of gradient descent.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 7. Adaptive Universal Generalized PageRank Graph Neural Network\n\n*From Search Query: adaptive initialization*\n\n*Eli Chien, Jianhao Peng, Pan Li, O. Milenkovic*\n\n**TL;DR:** This work introduces a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic.\n\n**Abstract:** In many important graph data processing applications the acquired information includes both node features and observations of the graph topology. Graph neural networks (GNNs) are designed to exploit both sources of evidence but they do not optimally trade-off their utility and integrate them in a manner that is also universal. Here, universality refers to independence on homophily or heterophily graph assumptions. We address these issues by introducing a new Generalized PageRank (GPR) GNN architecture that adaptively learns the GPR weights so as to jointly optimize node feature and topological information extraction, regardless of the extent to which the node labels are homophilic or heterophilic. Learned GPR weights automatically adjust to the node label pattern, irrelevant on the type of initialization, and thereby guarantee excellent learning performance for label patterns that are usually hard to handle. Furthermore, they allow one to avoid feature over-smoothing, a process which renders feature information nondiscriminative, without requiring the network to be shallow. Our accompanying theoretical analysis of the GPR-GNN method is facilitated by novel synthetic benchmark datasets generated by the so-called contextual stochastic block model. We also compare the performance of our GNN architecture with that of several state-of-the-art GNNs on the problem of node-classification, using well-known benchmark homophilic and heterophilic datasets. The results demonstrate that GPR-GNN offers significant performance improvement compared to existing techniques on both synthetic and benchmark data.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 590  (*Influential: 180*)\n\n#### 8. PartialFed: Cross-Domain Personalized Federated Learning via Partial Initialization\n\n*From Search Query: adaptive initialization*\n\n*Benyuan Sun, Hongxing Huo, Yi Yang, Bo Bai*\n\n**TL;DR:** A novel idea, PartialFed, which loads a subset of the global model\u2019s parameters rather than loading the entire model used in most previous works is proposed, which improves the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively and develops PartialFed-Adaptive, which automatically selects personalized loading strategy for each client.\n\n**Abstract:** The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data con\ufb01dentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a signi\ufb01cant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain characteristic, such as traf\ufb01c, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed , which loads a subset of the global model\u2019s parameters rather than loading the entire model used in most previous works. We \ufb01rst validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix . Then we develop PartialFed-Adaptive , which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classi\ufb01cation and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs signi\ufb01cantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 81  (*Influential: 6*)\n\n#### 9. Task-adaptive Label Dependency Transfer for Few-shot Named Entity Recognition\n\n*From Search Query: adaptive initialization*\n\n*Shan Zhang, Bin Cao, Tianming Zhang, Yuqi Liu, Jing Fan*\n\n**TL;DR:** The T ask-adaptive TLDT method is proposed to make label dependency transferable and effectively adapt to new tasks by a few samples and improves the existing optimization-based meta-learning methods by learning general initialization and individual parameter up-date rule for label dependency.\n\n**Abstract:** Named Entity Recognition (NER), as a crucial subtask in natural language processing (NLP), suffers from limited labeled samples (a.k.a. few-shot). Meta-learning methods are widely used for few-shot NER, but these existing methods overlook the importance of label dependency for NER, resulting in suboptimal performance. However, applying meta-learning methods to label dependency learning faces a special challenge, that is, due to the discrepancy of label sets in different domains, the label dependencies can not be transferred across domains. In this paper, we propose the T ask-adaptive L abel D ependency T ransfer (TLDT) method to make label dependency transferable and effectively adapt to new tasks by a few samples. TLDT improves the existing optimization-based meta-learning methods by learning general initialization and individual parameter up-date rule for label dependency. Extensive experiments show that TLDT achieves significant improvement over the state-of-the-art methods.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n### 1 related papers from ArXiv\n\n#### 1. Towards Robust and Cost-Efficient Knowledge Unlearning for Large\n  Language Models\n\n*From Search Query: adaptive initialization*\n\n*Sungmin Cha, Sungjun Cho, Dasol Hwang, Moontae Lee*\n\n**Abstract:** Large Language Models (LLMs) have demonstrated strong reasoning and\nmemorization capabilities via pretraining on massive textual corpora. However,\nthis poses risk of privacy and copyright violations, highlighting the need for\nefficient machine unlearning methods that remove sensitive data without\nretraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn\nby reducing the likelihood of generating unwanted content, it leads to unstable\noptimization and catastrophic forgetting of retrained knowledge. We also find\nthat combining GA with low-rank adaptation results in poor trade-offs between\ncomputational cost and generative performance. To address these challenges, we\npropose two novel techniques for robust and efficient unlearning for LLMs.\nFirst, we introduce Inverted Hinge loss, which suppresses unwanted tokens while\nmaintaining fluency by boosting the probability of the next most likely token.\nSecond, we develop a data-adaptive initialization for LoRA adapters via\nlow-rank approximation weighted with relative Fisher information, thereby\nfocusing updates on parameters critical for removing targeted knowledge.\nExperiments on the Training Data Extraction Challenge dataset using GPT-Neo\nmodels as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models\ndemonstrate that our approach effectively removes sensitive information while\nmaintaining reasoning and generative capabilities with minimal impact.\n\n**Published:** 2024-08-13T04:18:32Z  (*Updated: 2024-10-13T19:03:38Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Multi-fidelity classification using Gaussian processes: accelerating the prediction of large-scale computational models\n\n*From Search Query: multi-scale processing*\n\n*Daniel E. Hurtado, Paris Perdikaris, Francisco Sahli Costabal, Ellen Kuhl*\n\n**Abstract:** Machine learning techniques typically rely on large datasets to create accurate classifiers. However, there are situations when data is scarce and expensive to acquire. This is the case of studies that rely on state-of-the-art computational models which typically take days to run, thus hindering the potential of machine learning tools. In this work, we present a novel classifier that takes advantage of lower fidelity models and inexpensive approximations to predict the binary output of expensive computer simulations. We postulate an autoregressive model between the different levels of fidelity with Gaussian process priors. We adopt a fully Bayesian treatment for the hyper-parameters and use Markov Chain Mont Carlo samplers. We take advantage of the probabilistic nature of the classifier to implement active learning strategies. We also introduce a sparse approximation to enhance the ability of themulti-fidelity classifier to handle large datasets. We test these multi-fidelity classifiers against their single-fidelity counterpart with synthetic data, showing a median computational cost reduction of 23% for a target accuracy of 90%. In an application to cardiac electrophysiology, the multi-fidelity classifier achieves an F1 score, the harmonic mean of precision and recall, of 99.6% compared to 74.1% of a single-fidelity classifier when both are trained with 50 samples. In general, our results show that the multi-fidelity classifiers outperform their single-fidelity counterpart in terms of accuracy in all cases. We envision that this new tool will enable researchers to study classification problems that would otherwise be prohibitively expensive. Source code is available at https://github.com/fsahli/MFclass.\n\n**Published:** 2019-05-09\n\n\n\n#### 2. Rain Removal from Light Field Images with 4D Convolution and Multi-scale Gaussian Process\n\n*From Search Query: multi-scale processing*\n\n*Rynson W. H. Lau, Yang Yang, Bin Li, Mingyue Li, Tao Yan*\n\n**Abstract:** Existing deraining methods focus mainly on a single input image. However, with just a single input image, it is extremely difficult to accurately detect and remove rain streaks, in order to restore a rain-free image. In contrast, a light field image (LFI) embeds abundant 3D structure and texture information of the target scene by recording the direction and position of each incident ray via a plenoptic camera. LFIs are becoming popular in the computer vision and graphics communities. However, making full use of the abundant information available from LFIs, such as 2D array of sub-views and the disparity map of each sub-view, for effective rain removal is still a challenging problem. In this paper, we propose a novel method, 4D-MGP-SRRNet, for rain streak removal from LFIs. Our method takes as input all sub-views of a rainy LFI. To make full use of the LFI, it adopts 4D convolutional layers to simultaneously process all sub-views of the LFI. In the pipeline, the rain detection network, MGPDNet, with a novel Multi-scale Self-guided Gaussian Process (MSGP) module is proposed to detect high-resolution rain streaks from all sub-views of the input LFI at multi-scales. Semi-supervised learning is introduced for MSGP to accurately detect rain streaks by training on both virtual-world rainy LFIs and real-world rainy LFIs at multi-scales via computing pseudo ground truths for real-world rain streaks. We then feed all sub-views subtracting the predicted rain streaks into a 4D convolution-based Depth Estimation Residual Network (DERNet) to estimate the depth maps, which are later converted into fog maps. Finally, all sub-views concatenated with the corresponding rain streaks and fog maps are fed into a powerful rainy LFI restoring model based on the adversarial recurrent neural network to progressively eliminate rain streaks and recover the rain-free LFI.\n\n**Published:** 2022-08-16\n\n\n\n#### 3. Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data\n\n*From Search Query: memory hierarchy optimization*\n\n*Peng Di, Jie Zhao*\n\n**Abstract:** Optimizing compilers exploit the memory hierarchy using loop tiling and fusion, but these two transformations usually interfere with each other due to the oversight of transformations on data in memories. We present a novel composition of loop tiling and fusion in this paper. Unlike existing tiling-after-fusion algorithms that only transform computation spaces, our approach first applies rectangular/parallelogram tiling to live-out computation spaces for fitting the memory hierarchy, followed by the computation of the memory footprints required by each tile. The upwards exposed data extracted from the memory footprints are used to determine the tile shapes of intermediate computation spaces, allowing the construction of arbitrary tile shapes. Finally, our technique implements a post-tiling fusion strategy for maximizing data locality without losing tilability or parallelism of live-out computation spaces, thereby enabling storage reduction and reuse, and optimizing the memory hierarchy. We demonstrate that our approach can achieve superior performance on both CPU and GPU architectures over the state of the art by experimenting on 11 benchmarks extracted from numerous domains including neural networks, image processing, sparse matrix computation and linear algebra. Also, the results of the ResNet-50 model on an AI accelerator show that our approach can obtain 16% performance improvement.\n\n**Proceeding:** 53rd-annual-ieee-acm-international-symposium\n\n**Published:** 2020-11-11\n\n\n\n#### 4. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n\n*From Search Query: memory hierarchy optimization*\n\n*Tri Dao*\n\n**Abstract:** Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).\n\n**Published:** 2023-07-17\n\n\n\n#### 5. An Initial Investigation of Language Adaptation for TTS Systems under Low-resource Scenarios\n\n*From Search Query: adaptive initialization*\n\n*Junichi Yamagishi, Korin Richmond, Aidan Pine, Marc Tessier, Jianwu Dang, Longbiao Wang, Dan Wells, Mengzhe Geng, Chunyu Qiang, Xin Wang, Erica Cooper, Cheng Gong*\n\n**Abstract:** Self-supervised learning (SSL) representations from massively multilingual models offer a promising solution for low-resource language speech tasks. Despite advancements, language adaptation in TTS systems remains an open problem. This paper explores the language adaptation capability of ZMM-TTS, a recent SSL-based multilingual TTS system proposed in our previous work. We conducted experiments on 12 languages using limited data with various fine-tuning configurations. We demonstrate that the similarity in phonetics between the pre-training and target languages, as well as the language category, affects the target language's adaptation performance. Additionally, we find that the fine-tuning dataset size and number of speakers influence adaptability. Surprisingly, we also observed that using paired data for fine-tuning is not always optimal compared to audio-only data. Beyond speech intelligibility, our analysis covers speaker similarity, language identification, and predicted MOS.\n\n**Published:** 2024-06-13\n\n\n\n#### 6. I2I: Initializing Adapters with Improvised Knowledge\n\n*From Search Query: adaptive initialization*\n\n*Jesse Thomason, Mohammad Rostami, Furong Jia, Tejas Srinivasan*\n\n**Abstract:** Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.\n\n**Published:** 2023-04-04\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model (LM) design, focusing on multi-scale processing, memory hierarchy optimization, and adaptive initialization strategies, here are some key insights and references that can be useful:\n\n## Multi-Scale Processing\n\n### Hybrid Architectures and Hierarchical Processing\n- The concept of integrating different processing scales, such as combining Short Sequence Memory (SSM) sublayers with attention mechanisms, is highlighted in the \"Block-State Transformers\" paper. This hybrid architecture allows for both long-range contextualization and short-term representation of sequences, demonstrating significant efficiency improvements.\n- Multi-scale processing can be achieved through hierarchical architectures, where different layers or sublayers handle different scales of information. For example, the \"Mamba\" paper discusses selective SSMs that simplify the end-to-end neural network architecture, allowing for fast inference and linear scaling in sequence length.\n\n### Mathematical Formulations\n- In multi-scale processing, the mathematical formulation often involves combining different types of attention mechanisms and state space models. For instance, the \"Block-State Transformers\" paper formulates the combination of SSM and Block Transformer sublayers using:\n  \\[\n  \\mathbf{h}_{t} = \\text{SSM}(\\mathbf{h}_{t-1}, \\mathbf{x}_t) + \\text{BlockTransformer}(\\mathbf{h}_{t-1}, \\mathbf{x}_t)\n  \\]\n  where \\(\\mathbf{h}_{t}\\) is the hidden state at time \\(t\\), \\(\\mathbf{x}_t\\) is the input at time \\(t\\), and \\(\\text{SSM}\\) and \\(\\text{BlockTransformer}\\) represent the respective sublayers.\n\n## Memory Hierarchy Optimization\n\n### IO-Aware Algorithms\n- The \"FlashAttention\" paper introduces an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. This approach significantly optimizes memory hierarchy usage and reduces IO complexity.\n- Hierarchical information flow and selective feature propagation are also crucial. For example, the \"Mamba\" paper discusses how selective SSMs can reduce memory requirements by only processing relevant state information.\n\n### Tiling Strategies\n- Tiling strategies, as mentioned in \"FlashAttention,\" can be mathematically formulated as:\n  \\[\n  \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{TileAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}, \\text{tile_size})\n  \\]\n  where \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) are the query, key, and value matrices, and \\(\\text{tile_size}\\) determines the size of the tiles used for memory optimization.\n\n## Adaptive Initialization Strategies\n\n### Structured Parameter Initialization\n- Adaptive initialization strategies, such as mimetic initialization for SSMs and structured parameter initialization, can significantly impact performance. The \"Transformers are SSMs\" paper discusses how structured state space duality can be used to initialize parameters in a way that leverages the strengths of both transformers and SSMs.\n- Layer-wise nonlinearity importance and adaptive training strategies can also be integrated. For example, initializing parameters based on the importance of nonlinearity at each layer can help in balancing expressiveness and efficiency.\n\n### Performance Metrics\n- Performance metrics for these improvements include:\n  - **Speed and Efficiency**: Measured by the reduction in computational time and memory usage, such as the tenfold increase in speed at the layer level mentioned in \"Block-State Transformers\".\n  - **Accuracy and Effectiveness**: Evaluated through metrics like perplexity, accuracy in downstream tasks, and robustness to variant inputs.\n  - **Scalability**: Assessed by the model's ability to perform better with more data and larger model sizes, such as the linear scaling in sequence length discussed in \"Mamba\".\n\nIn summary, to achieve the goal of improving autoregressive LM design, researchers should focus on:\n\n- **Hybrid architectures** that combine different processing scales and mechanisms.\n- **Memory hierarchy optimization** using IO-aware algorithms and tiling strategies.\n- **Adaptive initialization** strategies that leverage structured parameter initialization and layer-wise nonlinearity importance.\n\nThese approaches can be mathematically formulated and evaluated using various performance metrics to ensure both efficiency and effectiveness.\n",
            "analysis": "## Analysis Note\n\n### 1. Summary of Analysis\n\nAfter three rounds of searches, several key themes and opportunities have emerged for improving LM block design:\n\n1. **Hybrid Architectures**\n- Integration of SSMs with attention mechanisms\n- Hierarchical and multi-scale processing\n- Dense connectivity patterns\n- Selective state transitions\n\n2. **Memory Efficiency**\n- IO-aware algorithms\n- Hierarchical information flow\n- Selective feature propagation\n- Adaptive compression techniques\n\n3. **Initialization and Training**\n- Mimetic initialization for SSMs\n- Structured parameter initialization\n- Layer-wise nonlinearity importance\n- Adaptive training strategies\n\n### 2. Useful References with Excerpts\n\n#### From Block-State Transformers\nKey excerpt: \"combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences [...] demonstrates more than tenfold increase in speed at the layer level\"\n\nHighlights:\n- Hybrid architecture benefits\n- Complementary processing scales\n- Efficiency improvements\n\n#### From FlashAttention\nKey excerpt: \"IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\"\n\nInsights:\n- Memory hierarchy optimization\n- Tiling strategies\n- IO complexity reduction\n\n#### From Mamba Paper\nKey excerpt: \"selective SSMs into a simplified end-to-end neural network architecture [...] enjoys fast inference and linear scaling in sequence length\"\n\nImplications:\n- Selective state processing\n- Simplified architectures\n- Linear complexity scaling\n\n### 3. Key Insights and Analysis\n\n1. **Architectural Integration**\n- SSMs and attention mechanisms can be effectively combined through careful design\n- Multi-scale processing improves both efficiency and effectiveness\n- Dense connectivity patterns enhance information flow\n\n2. **Memory Optimization**\n- IO-aware design is crucial for practical efficiency\n- Hierarchical processing can reduce memory requirements\n- Selective mechanisms improve memory utilization\n\n3. **Implementation Considerations**\n- Initialization strategies significantly impact performance\n- Layer-wise design choices affect overall capabilities\n- Balance needed between expressiveness and efficiency\n\n### 4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of multi-scale processing in LM blocks\n2. Memory-efficient dense connectivity patterns\n3. Adaptive initialization and training strategies\n4. Integration methods for different processing mechanisms\n\n### 5. References\n\n[1] Fathi, M., et al. (2023). \"Block-State Transformers.\" NeurIPS 2023.\n\n[2] Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\" NeurIPS 2022.\n\n[3] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv:2312.00752.\n\n[4] Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" arXiv:2402.18391.\n\n[5] Sieber, J., et al. (2024). \"Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks.\" arXiv:2402.05881.\n\n[6] Chen, W., et al. (2024). \"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling.\" arXiv:2402.11791."
        },
        {
            "ready": false,
            "query": "hierarchical feature processing neural networks",
            "detail": "Find papers discussing:\n1. Hierarchical feature processing in neural networks\n2. Multi-scale architectures for sequence modeling\n3. Memory-efficient hierarchical implementations\n4. Integration of hierarchical processing with attention\n5. Gradient flow in multi-scale architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical feature processing in neural networks\n2. Multi-scale architectures for sequence modeling\n3. Memory-efficient hierarchical implementations\n4. Integration of hierarchical processing with attention\n5. Gradient flow in multi-scale architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.98)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 7/30 (Score: 0.98)*\n\n```\nV. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 77-87, 2017. [6] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [8] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [9] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [11] Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052, 2022. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding.\n```\n\n#### 2. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.95)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 50/97 (Score: 0.95)*\n\n```\nArXiv, 2404.05892, 2024. M. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In Proceedings of the 40th International Conference on Machine Learning (ICML). JMLR.org, 2023. doi: 10.5555/3618408.3619572. M. Poli, A. W. Thomas, E. Nguyen, P. Ponnusamy, B. Deiseroth, K. Kersting, T. Suzuki, B. Hie, S. Ermon, C. R\u00e9, C. Zhang, and S. Massaroli. Mechanistic design and scaling of hybrid architectures. ArXiv, $2403.17844,2024$. Z. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence modeling. In Advances in Neural Information Processing Systems (NeurIPS), volume 37, 2023. URL https://openreview.net/forum?id=P1TCHxJwLB. Z. Qin, S. Yang, W. Sun, X. Shen, D. Li, W.\n```\n\n#### 3. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.95)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n#### 4. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.94)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 42/46 (Score: 0.94)*\n\n```\n[N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n## Appendix\n\n## 6 Related Work\n\nHierarchical or Multiscale Recurrent neural networks. This work takes inspiration from a wide array of work on introducing multiple scales of processing into recurrent neural networks (Chung et al. 2016; Hihi \\& Bengio, 1995; Mozer, 1991, Schmidhuber, 1991, Koutn\u00edk et al., 2014). These works divide the processing into multiple streams each operating at a different temporal granularity. While these works mainly focus on recurrent neural networks and their application is mainly on natural language tasks, we focus on introducing multiple streams of processing and a hierarchical structure into Transformers while also focusing on a broader range of domains beyond natural language. Transformers. Some of the components we describe in the proposed model have been used previously in various Transformer models. Transformer XL (Dai et al., 2019) also divides the input into segments. Each segment considers the tokens from the current segment and the previous segment for attention without passing gradients into the previous segments. A number of previous works (Zhang et al., 2021; Liu et al., 2021b, Wu et al., 2021, Yuan et al., 2021, Wang et al., 2021; Yang et al., 2021) have worked on introducing a hierarchical structure in Transformers mainly in the domain of vision. The main goal of these works has been to introduce convolution-like hierarchies into Vision Transformers (Dosovitskiy et al. 2020). While these works progressively reduce the spatial resolution of the inputs in order to introduce hierarchies, we introduce hierarchies by adding another slow stream of information processing and without reducing the spatial resolution of the inputs. We also provision for the higher level of the hierarchy (i.e. the slow stream) to provide information to the lower levels as top-down conditioning which is not possible in any of the previous works. Top-Down Conditioning. Top-down information is information propagated from higher to lower levels of the network. It represents the models beliefs of the world and provides context for interpreting perceptual information.\n```\n\n#### 5. Universal Transformers  (Avg. Score: 0.77)\n\n*Mostafa Dehghani, Stephan Gouws, O. Vinyals, Jakob Uszkoreit, Lukasz Kaiser*\n\n**Published in:** International Conference on Learning Representations (2018)\t**Cited by** 673  (*Influential: 65*)\n\n**TL;DR:** The Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses issues of parallelizability and global receptive field, is proposed.\n\n**Abstract:** Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.\n\n##### *Relevant Chunk: No. 21/32 (Score: 0.77)*\n\n```\narXiv preprint arXiv:1610.02357, 2016. Zewei Chu, Hai Wang, Kevin Gimpel, and David McAllester. Broad context language modeling as reading comprehension. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, volume 2, pp. 52-57, 2017. Bhuwan Dhingra, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Linguistic knowledge as memory for recurrent neural networks. arXiv preprint arXiv:1703.02620, 2017. Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for reasoning over multiple mentions using coreference. arXiv preprint arXiv:1804.05922, 2018. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. CoRR, abs/1705.03122, 2017. URL/http://arxiv.org/abs/1705.03122\n\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016. Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850\n\nAlex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401\n\nCaglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786, 2018. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2003. A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in Neural Information Processing Systems, (NIPS), 2015. \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1511.08228\n\n\u0141ukasz Kaiser, Aidan N. Gomez, and Francois Chollet. Depthwise separable convolutions for neural machine translation. CoRR, abs/1706.03059, 2017. URLhttp://arxiv.org/abs/1706.03059. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378-1387, 2016. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of 1stms to learn syntax-sensitive dependencies.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical feature processing neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*C. Qi, L. Yi, Hao Su, L. Guibas*\n\n**TL;DR:** A hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set and proposes novel set learning layers to adaptively combine features from multiple scales to learn deep point set features efficiently and robustly.\n\n**Abstract:** Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 9542  (*Influential: 1894*)\n\n#### 2. Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*Eshaan Nichani, Alexandru Damian, Jason D. Lee*\n\n**TL;DR:** This work analyzes the features learned by a three-layer network trained with layer-wise gradient descent, and presents a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure.\n\n**Abstract:** One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic features -- and show that in the latter setting three-layer networks obtain a sample complexity improvement over all existing guarantees for two-layer networks. Crucially, this sample complexity improvement relies on the ability of three-layer networks to efficiently learn nonlinear features. We then establish a concrete optimization-based depth separation by constructing a function which is efficiently learnable via gradient descent on a three-layer network, yet cannot be learned efficiently by a two-layer network. Our work makes progress towards understanding the provable benefit of three-layer neural networks over two-layer networks in the feature learning regime.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Hyperbolic Graph Convolutional Neural Networks\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*Ines Chami, Rex Ying, Christopher R\u00e9, J. Leskovec*\n\n**TL;DR:** This work proposes Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs andhyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs.\n\n**Abstract:** Graph convolutional neural networks (GCNs) embed nodes in a graph into Euclidean space, which has been shown to incur a large distortion when embedding real-world graphs with scale-free or hierarchical structure. Hyperbolic geometry offers an exciting alternative, as it enables embeddings with much smaller distortion. However, extending GCNs to hyperbolic geometry presents several unique challenges because it is not clear how to define neural network operations, such as feature transformation and aggregation, in hyperbolic space. Furthermore, since input features are often Euclidean, it is unclear how to transform the features into hyperbolic embeddings with the right amount of curvature. Here we propose Hyperbolic Graph Convolutional Neural Network (HGCN), the first inductive hyperbolic GCN that leverages both the expressiveness of GCNs and hyperbolic geometry to learn inductive node representations for hierarchical and scale-free graphs. We derive GCNs operations in the hyperboloid model of hyperbolic space and map Euclidean input features to embeddings in hyperbolic spaces with different trainable curvature at each layer. Experiments demonstrate that HGCN learns embeddings that preserve hierarchical structure, and leads to improved performance when compared to Euclidean analogs, even with very low dimensional embeddings: compared to state-of-the-art GCNs, HGCN achieves an error reduction of up to 63.1% in ROC AUC for link prediction and of up to 47.5% in F1 score for node classification, also improving state-of-the art on the Pubmed dataset.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 572  (*Influential: 138*)\n\n#### 4. Discourse Parsing with Attention-based Hierarchical Neural Networks\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*Qi Li, Tianshi Li, Baobao Chang*\n\n**TL;DR:** Experimental results show that the proposed attention-based hierarchical neural network model for discourse parsing obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.\n\n**Abstract:** RST-style document-level discourse parsing remains a dif\ufb01cult task and ef\ufb01cient deep learning models on this task have rarely been presented. In this paper, we propose an attention-based hierarchical neural network model for discourse parsing. We also incorporate tensor-based transformation function to model complicated feature interactions. Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2016\n\n**Citations:** 98  (*Influential: 14*)\n\n#### 5. Post-training Iterative Hierarchical Data Augmentation for Deep Networks\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*Adil Hamid Khan, Khadija Fraz*\n\n**TL;DR:** IHDA provided a state-of-the-art performance on CIFAR-10, CIFar-100, and ImageNet for several DNs, and beat the performance of existing state- of theart DA approaches for the same networks on these datasets.\n\n**Abstract:** In this paper, we propose a new iterative hierarchical data augmentation (IHDA) method to \ufb01ne-tune trained deep neural networks to improve their generalization performance. The IHDA is motivated by three key insights: (1) Deep networks (DNs) are good at learning multi-level representations from data. (2) Performing data augmentation (DA) in the learned feature spaces of DNs can signi\ufb01cantly improve their performance. (3) Implementing DA in hard-to-learn regions of a feature space can effectively augment the dataset to improve generalization. Accordingly, the IHDA performs DA in a deep feature space, at level l , by transforming it into a distribution space and synthesizing new samples using the learned distributions for data points that lie in hard-to-classify regions, which is estimated by analyzing the neighborhood characteristics of each data point. The synthesized samples are used to \ufb01ne-tune the parameters of the subsequent layers. The same procedure is then repeated for the feature space at level l + 1 . To avoid over\ufb01tting, the concept of dropout probability is employed, which is gradually relaxed as the IHDA works towards high-level feature spaces. IHDA provided a state-of-the-art performance on CIFAR-10, CIFAR-100, and ImageNet for several DNs, and beat the performance of existing state-of-the-art DA approaches for the same networks on these datasets. Finally, to demonstrate its domain-agnostic properties, we show the signi\ufb01cant improvements that IHDA provided for a deep neural network on a non-image wearable sensor-based activity recognition benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 13  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Human level control through deep reinforcement learning\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*Shane Legg1 & Demis Hassabis, Daan Wierstra, Dharshan Kumaran, Helen King, Ioannis Antonoglou, Amir Sadik, Charles Beattie, Stig Petersen, Georg Ostrovski, Andreas K. Fidjeland, Martin Riedmiller, Alex Graves, Marc G. Bellemare, Joel Veness, Andrei A. Rusu, David Silver, Koray Kavukcuoglu, Volodymyr Mnih*\n\n**Abstract:** The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.\n\n**Published:** 2015-02-25\n\n\n\n#### 2. H-DenseUNet: Hybrid Densely Connected UNet for Liver and Tumor Segmentation from CT Volumes\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*Chi-Wing Fu, Xiaojuan Qi, Qi Dou, Hao Chen, Pheng Ann Heng, Xiaomeng Li*\n\n**Abstract:** Liver cancer is one of the leading causes of cancer death. To assist doctors\nin hepatocellular carcinoma diagnosis and treatment planning, an accurate and\nautomatic liver and tumor segmentation method is highly demanded in the\nclinical practice. Recently, fully convolutional neural networks (FCNs),\nincluding 2D and 3D FCNs, serve as the back-bone in many volumetric image\nsegmentation. However, 2D convolutions can not fully leverage the spatial\ninformation along the third dimension while 3D convolutions suffer from high\ncomputational cost and GPU memory consumption. To address these issues, we\npropose a novel hybrid densely connected UNet (H-DenseUNet), which consists of\na 2D DenseUNet for efficiently extracting intra-slice features and a 3D\ncounterpart for hierarchically aggregating volumetric contexts under the spirit\nof the auto-context algorithm for liver and tumor segmentation. We formulate\nthe learning process of H-DenseUNet in an end-to-end manner, where the\nintra-slice representations and inter-slice features can be jointly optimized\nthrough a hybrid feature fusion (HFF) layer. We extensively evaluated our\nmethod on the dataset of MICCAI 2017 Liver Tumor Segmentation (LiTS) Challenge\nand 3DIRCADb Dataset. Our method outperformed other state-of-the-arts on the\nsegmentation results of tumors and achieved very competitive performance for\nliver segmentation even with a single model.\n\n**Published:** 2017-09-21\n\n\n\n#### 3. Multi-scale Residual Network for Image Super-Resolution\n\n*From Search Query: hierarchical feature processing neural networks*\n\n*Guixu Zhang, Juncheng Li, Faming Fang, Kangfu Mei*\n\n**Abstract:** Recent studies have shown that deep neural networks can significantly improve the quality of single-image super-resolution. Current researches tend to use deeper convolutional neural networks to enhance performance. However, blindly increasing the depth of the network cannot ameliorate the network effectively. Worse still, with the depth of the network increases, more problems occurred in the training process and more training tricks are needed. In this paper, we propose a novel multi-scale residual network (MSRN) to fully exploit the image features, which outperform most of the state-of-the-art methods. Based on the residual block, we introduce convolution kernels of different sizes to adaptively detect the image features in different scales. Meanwhile, we let these features interact with each other to get the most efficacious image information, we call this structure Multi-scale Residual Block (MSRB). Furthermore, the outputs of each MSRB are used as the hierarchical features for global feature fusion. Finally, all these features are sent to the reconstruction module for recovering the high-quality image. \n\n**Proceeding:** eccv-2018-9\n\n**Published:** 2018-09-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical feature processing, multi-scale architectures, memory-efficient implementations, integration with attention mechanisms, and gradient flow, here are some key insights and relevant information from the provided sources and additional context:\n\n## Hierarchical Feature Processing in Neural Networks\n\n- The concept of hierarchical feature extraction is well-documented in the context of deep learning. For instance, Convolutional Neural Networks (CNNs) are known for their hierarchical feature extraction capabilities, where each layer captures progressively more complex features from the input data.\n- In the context of graph neural networks, the hierarchical Graph Neural Network (h-GNN) model demonstrates how successive hierarchies of graphs can be created through clustering, preserving both local and global information within nodes and across the entire graph. This approach is analogous to the stacking of multiple layers in other GNN-based feature matchers and can be adapted for sequence modeling by considering hierarchical representations of sequences.\n\n## Multi-scale Architectures for Sequence Modeling\n\n- Multi-scale architectures can be achieved by incorporating hierarchical processing similar to the h-GNN model. This involves creating multiple levels of representation, where each level captures features at different scales. For sequence modeling, this could mean using a combination of short-term and long-term dependencies, similar to how recurrent and feedforward depths are increased to enhance performance in tasks with long-term dependencies.\n- The use of hierarchical structures in neural networks, as characterized by the Ladderpath approach, suggests that networks with richer hierarchical structures (indicated by an order-rate \u03b7 around 0.5) perform better. This principle can be applied to sequence modeling by designing architectures that naturally adjust their hierarchical structure during training.\n\n## Memory-Efficient Hierarchical Implementations\n\n- Memory efficiency is crucial for scalability, and techniques like the Layer-Condensed KV Cache can significantly reduce memory consumption while maintaining competitive performance. This approach can be integrated into hierarchical architectures to optimize memory usage[Analysis Note].\n- The AI Query Optimizer model, although specific to query optimization, demonstrates that even small, well-designed neural networks can be powerful and memory-efficient. This suggests that careful design and feature selection can lead to memory-efficient hierarchical implementations without sacrificing performance.\n\n## Integration of Hierarchical Processing with Attention Mechanisms\n\n- Attention mechanisms can be integrated with hierarchical processing by ensuring that the attention weights are computed in a way that respects the hierarchical structure of the data. For example, using self-attention networks where the eigenspectrum of query-key parameter matrices is concentrated can lead to better localization and expressivity of the model[Analysis Note: \"Self-attention Networks Localize When QK-eigenspectrum Concentrates\"].\n- Normalization techniques, such as those proposed in \"The Devil in Linear Transformer,\" can stabilize gradients in attention computations, which is essential for hierarchical designs that involve multi-scale processing[Analysis Note: \"The Devil in Linear Transformer\"].\n\n## Gradient Flow in Multi-scale Architectures\n\n- Gradient stability is critical for the convergence of multi-scale architectures. Techniques like replacing the scaling operation with normalization in attention computations can help stabilize gradients and ensure smooth training[Analysis Note: \"The Devil in Linear Transformer\"].\n- The hierarchical structure characterized by the Ladderpath approach also suggests that networks tend to adjust their hierarchical structure during training, which can impact gradient flow. Ensuring that the network's structure naturally moves towards a balanced hierarchical order (\u03b7 around 0.5) can improve gradient flow and overall performance.\n\n### Additional Considerations\n\n- **Hardware-aware Design**: Ensuring that the hierarchical architecture is designed with hardware considerations in mind can significantly impact performance. Cache-friendly implementations and efficient memory access patterns are crucial for practical efficiency[Analysis Note].\n- **Adaptive Normalization**: Dynamic parameter adjustment and scale selection strategies can further enhance the performance of hierarchical models integrated with attention mechanisms. This involves adapting normalization techniques to the specific needs of the hierarchical structure and attention dynamics[Analysis Note].\n\nBy integrating these insights, researchers can design autoregressive language models that leverage hierarchical feature processing, multi-scale architectures, and memory-efficient implementations, while ensuring stable gradient flow and effective integration with attention mechanisms.\n",
            "analysis": "Analysis Note on HierarchicalRMSNorm Proposal - Round 3\n\n1. Summary of Analysis:\nThe search results provide valuable insights into normalization techniques, memory efficiency, and their integration with attention mechanisms. The HierarchicalRMSNorm proposal shows promise but requires further investigation into specific technical aspects.\n\n2. Useful References with Key Excerpts:\n\nFrom \"Self-attention Networks Localize When QK-eigenspectrum Concentrates\":\n\"The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\"\n- Important for understanding how normalization affects attention dynamics\n\nFrom \"The Devil in Linear Transformer\":\n\"unbounded gradients in the attention computation adversely impact the convergence of linear transformer models... we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients.\"\n- Relevant for gradient stability considerations in hierarchical designs\n\nFrom \"Layer-Condensed KV Cache\":\n\"Huge memory consumption has been a major bottleneck... our method achieves up to 26\u00d7 higher throughput than standard transformers and competitive performance\"\n- Demonstrates importance of memory efficiency\n\n3. Key Insights:\n\na) Memory Efficiency:\n- Memory optimization is crucial for scalability\n- Trade-offs between feature representation and memory usage need careful consideration\n- Cache-friendly implementations significantly impact performance\n\nb) Normalization Impact:\n- Gradient stability is critical for convergence\n- Multi-scale processing must balance computational overhead\n- Attention localization affects model expressivity\n\nc) Implementation Considerations:\n- Hardware-aware design is essential for practical efficiency\n- Cache utilization crucial for performance\n- Memory access patterns significantly impact throughput\n\n4. Future Search Plan:\n\nNeed additional investigation in:\na) Hierarchical feature processing\n- Specific focus on multi-scale architectures\n- Impact on gradient flow\n- Memory-computation trade-offs\n\nb) Adaptive normalization techniques\n- Dynamic parameter adjustment\n- Scale selection strategies\n- Integration with attention mechanisms\n\n5. References:\n\n[1] Bao, H., Hataya, R., & Karakida, R. (2024). Self-attention Networks Localize When QK-eigenspectrum Concentrates. arXiv preprint.\n\n[2] Qin, Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N., & Zhong, Y. (2022). The Devil in Linear Transformer. Conference on Empirical Methods in Natural Language Processing.\n\n[3] Wu, H., & Tu, K. (2024). Layer-Condensed KV Cache for Efficient Inference of Large Language Models. Annual Meeting of the Association for Computational Linguistics.\n\n[4] Legresley, P., et al. (2019). Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv preprint.\n\n[5] Lu, J., et al. (2021). SOFT: Softmax-free Transformer with Linear Complexity. Neural Information Processing Systems.\n\nNeed to continue search to gather more information about:\n1. Hierarchical feature processing techniques\n2. Memory-efficient multi-scale architectures\n3. Integration with modern attention mechanisms"
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "hierarchical normalization language models",
            "detail": "Find papers discussing:\n1. Multi-scale or hierarchical normalization techniques in neural networks\n2. Memory-efficient implementations of normalization layers\n3. Adaptive parameter initialization methods\n4. Applications of hierarchical processing in language models\n5. Theoretical analysis of normalization techniques",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Multi-scale or hierarchical normalization techniques in neural networks\n2. Memory-efficient implementations of normalization layers\n3. Adaptive parameter initialization methods\n4. Applications of hierarchical processing in language models\n5. Theoretical analysis of normalization techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.87)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.87)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n#### 2. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.80)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.80)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 3. Normalized Attention Without Probability Cage (Avg. Score: 0.77)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.77)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 4. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.74)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.74)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.55)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 27/74 (Score: 0.55)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: The International Conference on Learning Representations (ICLR). 2015. [6] David Balduzzi and Muhammad Ghifary. \"Strongly-typed Recurrent Neural Networks\". In: International Conference on Machine Learning. PMLR. 2016, pp. 1292-1300. [7] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. \"Pythia: A Suite for Analyzing Large Language Models across Training and Scaling\".\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical normalization language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Generalizing over Long Tail Concepts for Medical Term Normalization\n\n*From Search Query: hierarchical normalization language models*\n\n*Beatrice Portelli, Simone Scaboro, Enrico Santus, Hooman Sedghamiz, Emmanuele Chersoni, Giuseppe Serra*\n\n**TL;DR:** The evaluation shows that the proposed strategy produces state-of-the-art performance on seen concepts and consistent improvements on unseen ones, allowing also for efficient zero-shot knowledge transfer across text typologies and datasets.\n\n**Abstract:** Medical term normalization consists in mapping a piece of text to a large number of output classes.Given the small size of the annotated datasets and the extremely long tail distribution of the concepts, it is of utmost importance to develop models that are capable to generalize to scarce or unseen concepts.An important attribute of most target ontologies is their hierarchical structure. In this paper we introduce a simple and effective learning strategy that leverages such information to enhance the generalizability of both discriminative and generative models.The evaluation shows that the proposed strategy produces state-of-the-art performance on seen concepts and consistent improvements on unseen ones, allowing also for efficient zero-shot knowledge transfer across text typologies and datasets.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 2. Revisiting Over-smoothing in BERT from the Perspective of Graph\n\n*From Search Query: hierarchical normalization language models*\n\n*Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, J. Kwok*\n\n**TL;DR:** This work makes the attempt to analyze the over-smoothing problem of Transformer-based models from the perspective of graph, where such problem was first discovered and explored and finds that layer normalization plays a key role.\n\n**Abstract:** Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 56  (*Influential: 8*)\n\n#### 3. Variational Bayes on Monte Carlo Steroids\n\n*From Search Query: hierarchical normalization language models*\n\n*Aditya Grover, Stefano Ermon*\n\n**TL;DR:** A new class of bounds on the marginal log-likelihood of directed latent variable models is proposed, which relies on random projections to simplify the posterior, and empirical improvements on benchmark datasets in vision and language for sigmoid belief networks are demonstrated.\n\n**Abstract:** Variational approaches are often used to approximate intractable posteriors or normalization constants in hierarchical latent variable models. While often effective in practice, it is known that the approximation error can be arbitrarily large. We propose a new class of bounds on the marginal log-likelihood of directed latent variable models. Our approach relies on random projections to simplify the posterior. In contrast to standard variational methods, our bounds are guaranteed to be tight with high probability. We provide a new approach for learning latent variable models based on optimizing our new bounds on the log-likelihood. We demonstrate empirical improvements on benchmark datasets in vision and language for sigmoid belief networks, where a neural network is used to approximate the posterior.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 4. Test-Time Distribution Normalization for Contrastively Learned Visual-language Models\n\n*From Search Query: hierarchical normalization language models*\n\n*Yi Zhou, Juntao Ren, Fengyu Li, Ramin Zabih, S. Lim*\n\n**TL;DR:** Distribution Normalization (DN) is proposed, where the mean representation of a batch of test samples is approximate and used to represent what would be analogous to negative samples in the InfoNCE loss, which requires no retraining or fine-tuning and can be effortlessly applied during inference.\n\n**Abstract:** Advances in the field of vision-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations. One of the most representative approaches proposed recently known as CLIP has garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference in a computationally efficient way. To this end, we propose Distribution Normalization (DN), where we approximate the mean representation of a batch of test samples and use such a mean to represent what would be analogous to negative samples in the InfoNCE loss. DN requires no retraining or fine-tuning and can be effortlessly applied during inference. Extensive experiments on a wide variety of downstream tasks exhibit a clear advantage of DN over the dot product on top of other existing test-time augmentation methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Strategies for Training Large Vocabulary Neural Language Models\n\n*From Search Query: hierarchical normalization language models*\n\n*Welin Chen, Michael Auli, David Grangier*\n\n**Abstract:** Training neural network language models over large vocabularies is still\ncomputationally very costly compared to count-based models such as Kneser-Ney.\nAt the same time, neural language models are gaining popularity for many\napplications such as speech recognition and machine translation whose success\ndepends on scalability. We present a systematic comparison of strategies to\nrepresent and train large vocabularies, including softmax, hierarchical\nsoftmax, target sampling, noise contrastive estimation and self normalization.\nWe further extend self normalization to be a proper estimator of likelihood and\nintroduce an efficient variant of softmax. We evaluate each method on three\npopular benchmarks, examining performance on rare words, the speed/accuracy\ntrade-off and complementarity to Kneser-Ney.\n\n**Conference:** strategies-for-training-large-vocabulary-1\n\n**Published:** 2015-12-15\n\n\n\n#### 2. Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models\n\n*From Search Query: hierarchical normalization language models*\n\n*Zhenzhong Lan, Hongliang He, Anqi Li, Shuai Zhang, Huachuan Qiu*\n\n**Abstract:** Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation framework. We present a systematic analysis of the safety and robustness of LLMs regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). Our results demonstrate that current LLMs not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.\n\n**Published:** 2023-07-17\n\n\n\n#### 3. SOWA: Adapting Hierarchical Frozen Window Self-Attention to Visual-Language Models for Better Anomaly Detection\n\n*From Search Query: hierarchical normalization language models*\n\n*Zhaosheng Zhang, Zongxiang Hu*\n\n**Abstract:** Visual anomaly detection is critical in industrial manufacturing, but traditional methods often rely on extensive normal datasets and custom models, limiting scalability. Recent advancements in large-scale visual-language models have significantly improved zero/few-shot anomaly detection. However, these approaches may not fully utilize hierarchical features, potentially missing nuanced details. We introduce a window self-attention mechanism based on the CLIP model, combined with learnable prompts to process multi-level features within a Soldier-Offier Window self-Attention (SOWA) framework. Our method has been tested on five benchmark datasets, demonstrating superior performance by leading in 18 out of 20 metrics compared to existing state-of-the-art techniques.\n\n**Published:** 2024-07-04\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models using hierarchical normalization techniques, here are some key points and references that align with the specified details:\n\n## Multi-scale or Hierarchical Normalization Techniques in Neural Networks\n\n- While the provided sources do not directly discuss hierarchical normalization in language models, the concept of layer normalization and its variants can be extended to hierarchical settings. Layer normalization, as discussed in, stabilizes the hidden state dynamics in recurrent networks. Extending this to a hierarchical structure could involve applying normalization at multiple scales, similar to how hierarchical processing is used in other domains.\n\n## Memory-Efficient Implementations of Normalization Layers\n\n- Batch normalization, as mentioned in, is a technique that stabilizes and accelerates the training process by normalizing the inputs to each layer. However, for memory efficiency, techniques like those discussed in \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\" could be adapted. This paper provides insights into cache-friendly implementations, which could be crucial for designing memory-efficient hierarchical normalization layers.\n\n## Adaptive Parameter Initialization Methods\n\n- Adaptive parameter initialization is crucial for training stability. While the sources provided do not specifically address adaptive initialization in the context of hierarchical normalization, the general principles of data-dependent parameter initialization can be applied. For instance, understanding how scale-specific initialization impacts training stability can be inferred from studies on weight initialization in general neural networks. The importance of proper weight initialization is highlighted in, where it is noted that inadequate weight initialization can disadvantage the model in the optimization landscape.\n\n## Applications of Hierarchical Processing in Language Models\n\n- Hierarchical processing is more commonly discussed in the context of vision models and graph neural networks, such as the hierarchical GNN (h-GNN) described in. However, the concept can be adapted to language models by considering multi-scale feature decomposition. For example, hierarchical attention mechanisms or multi-scale transformer layers could be explored, where different scales of features are processed and normalized hierarchically.\n\n## Theoretical Analysis of Normalization Techniques\n\n- Theoretical analysis of normalization techniques, such as layer normalization and batch normalization, is well-established. For instance, provides empirical evidence and theoretical foundations for layer normalization. Extending this analysis to hierarchical normalization would involve studying how multi-scale normalization affects the stability and performance of the model. This could include analyzing the trade-offs between computational overhead and improved feature representation, as well as the impact on training stability and model robustness.\n\n### Additional Considerations\n\n- **Multi-scale Processing**: To compare multi-scale feature decomposition with existing normalization techniques, researchers could explore how hierarchical processing affects the representation of features at different scales. This might involve analyzing the computational overhead and the trade-offs in terms of feature representation quality.\n  \n- **Memory Efficiency**: Evaluating the memory overhead of maintaining multiple scale paths and investigating cache-friendly implementations, as suggested by the \"FlashAttention\" paper, would be crucial. This could involve comparing the computational complexity of hierarchical normalization with standard normalization techniques.\n\n- **Adaptive Initialization**: Studying data-dependent initialization techniques and their impact on training stability, as well as comparing them with other adaptive normalization methods, would be essential. This could involve analyzing how scale-specific initialization affects the model's performance and stability.\n\nBy focusing on these areas, researchers can develop a more comprehensive understanding of how to integrate hierarchical normalization techniques into autoregressive language models, addressing concerns about memory efficiency, adaptive initialization, and theoretical foundations.\n",
            "analysis": "Initial Analysis of HierarchicalRMSNorm Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) Multi-scale Processing:\n- How does multi-scale feature decomposition compare to existing normalization techniques?\n- What are the trade-offs between computational overhead and improved feature representation?\n- Are there existing implementations of hierarchical normalization in language models?\n\nb) Adaptive Parameter Initialization:\n- How does scale-specific initialization impact training stability?\n- What are the best practices for data-dependent parameter initialization?\n- Are there similar approaches in current research?\n\nc) Memory Efficiency:\n- What is the actual memory overhead of maintaining multiple scale paths?\n- How does the hierarchical structure affect cache utilization?\n- Are there existing techniques for efficient multi-scale computation?\n\n2. Investigation Plan:\n\nPhase 1: Examine Existing Multi-scale Approaches\n- Search for papers on hierarchical/multi-scale normalization\n- Analyze implementations in vision and language models\n- Identify key differences from proposed approach\n\nPhase 2: Evaluate Memory Efficiency Claims\n- Research memory-efficient normalization techniques\n- Compare computational complexity with standard RMSNorm\n- Investigate cache-friendly implementations\n\nPhase 3: Study Adaptive Initialization\n- Research data-dependent initialization techniques\n- Analyze impact on training stability\n- Compare with other adaptive normalization methods\n\nInitial Observations:\n\n1. Novelty Assessment:\n- The combination of multi-scale processing with RMSNorm appears novel\n- Adaptive initialization based on scale-specific statistics is interesting\n- Hierarchical integration mechanism seems unique\n\n2. Potential Concerns:\n- Additional computational overhead from multi-scale processing\n- Complexity in parameter initialization\n- Memory impact of maintaining multiple scale paths\n\n3. Promising Aspects:\n- Potential for improved feature normalization\n- Scale-specific processing could enhance model robustness\n- Memory-efficient design through hierarchical structure\n\n4. Research Context:\n- Builds on RMSNorm foundation\n- Incorporates ideas from multi-scale processing\n- Addresses known limitations in normalization techniques\n\nReferences found so far:\n\n[1] Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization.\"\n- Establishes baseline RMSNorm performance\n- Provides theoretical foundation\n\n[2] Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\"\n- Demonstrates importance of memory efficiency\n- Provides insights into cache-friendly implementations\n\n[3] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\"\n- Shows recent trends in efficient architectures\n- Relevant for memory optimization strategies\n\nNext Steps:\n1. Conduct detailed search on multi-scale normalization techniques\n2. Investigate memory-efficient implementations\n3. Research adaptive initialization methods"
        },
        {
            "ready": false,
            "query": "memory efficient multi scale processing neural networks",
            "detail": "Find papers discussing:\n1. Memory-efficient implementations of multi-scale or hierarchical processing in neural networks\n2. Cache optimization techniques for deep learning\n3. Memory-bandwidth trade-offs in neural network architectures\n4. Efficient implementations of feature hierarchies\n5. Memory optimization strategies for language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Memory-efficient implementations of multi-scale or hierarchical processing in neural networks\n2. Cache optimization techniques for deep learning\n3. Memory-bandwidth trade-offs in neural network architectures\n4. Efficient implementations of feature hierarchies\n5. Memory optimization strategies for language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.97)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.97)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 2. N-grammer: Augmenting Transformers with latent n-grams (Avg. Score: 0.92)\n\n*Aurko Roy, Rohan Anil, Guangda Lai, Benjamin Lee, Jeffrey Zhao, Shuyuan Zhang, Shibo Wang, Ye Zhang, Shen Wu, Rigel Swavely, Tao Yu, Phuong Dao, Christopher Fifty, Z. Chen, Yonghui Wu*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** The N-Grammer is proposed, a simple yet effective modification to the Transformer architecture inspired by the literature in statistical language modeling, by augmenting the model with n-grams that are constructed from a discrete latent representation of the text sequence.\n\n**Abstract:** Transformer models have recently emerged as one of the foundational models in natural language processing, and as a byproduct, there is significant recent interest and investment in scaling these models. However, the training and inference costs of these large Transformer language models are prohibitive, thus necessitating more research in identifying more efficient variants. In this work, we propose a simple yet effective modification to the Transformer architecture inspired by the literature in statistical language modeling, by augmenting the model with n-grams that are constructed from a discrete latent representation of the text sequence. We evaluate our model, the N-Grammer on language modeling on the C4 data-set as well as text classification on the SuperGLUE data-set, and find that it outperforms several strong baselines such as the Transformer and the Primer. We open-source our model for reproducibility purposes in Jax.\n\n##### *Relevant Chunk: No. 15/21 (Score: 0.92)*\n\n```\narXiv preprint arXiv:1911.00172. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In 1995 international conference on acoustics, speech, and signal processing, volume 1, pages 181-184. IEEE. Philipp Koehn. 2009. Statistical machine translation. Cambridge University Press. Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021. Hurdles to progress in long-form question answering. arXiv preprint arXiv:2103.06332. Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, and Herv\u00e9 J\u00e9gou. 2019. Large memory layers with product keys. arXiv preprint arXiv:1907.05242. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Rina Panigrahy, Xin Wang, and Manzil Zaheer. 2021 Sketch based memory for neural networks.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.88)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.88)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.83)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.83)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 5. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.80)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 23/43 (Score: 0.80)*\n\n```\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5484-5495, 2021. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. Alan L Hodgkin and Andrew F Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The J. of Physiol., 117(4):500-544, 1952. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Mark Horowitz. 1.1 computing's energy problem (and what we can do about it). In 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC), pp. 10-14. IEEE, 2014. Jean Kaddour. The minipile challenge for data-efficient language models. arXiv preprint arXiv:2304.08442, 2023 . Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: memory efficient multi scale processing neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n*Shuai Zhang, M. Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miaoyuan Liu*\n\n**TL;DR:** This paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error and proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexityand improve convergence without compromising the test accuracy.\n\n**Abstract:** Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include \\textit{graph sparsification} that samples a subgraph to reduce the amount of data aggregation and \\textit{model sparsification} that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 4*)\n\n#### 2. Efficient Learning of Mesh-Based Physical Simulation with Bi-Stride Multi-Scale Graph Neural Network\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n*Yadi Cao, Menglei Chai, Minchen Li, Chenfanfu Jiang*\n\n**TL;DR:** Inspired by the bipartite graph determination, a novel pooling strategy, bi-stride, is proposed, which significantly outperforms existing methods in terms of both accuracy and computational efficiency in representative physical simulations.\n\n**Abstract:** Learning the physical simulation on large-scale meshes with flat Graph Neural Networks (GNNs) and stacking Message Passings (MPs) is challenging due to the scaling complexity w.r.t. the number of nodes and over-smoothing. There has been growing interest in the community to introduce \\textit{multi-scale} structures to GNNs for physical simulation. However, current state-of-the-art methods are limited by their reliance on the labor-intensive drawing of coarser meshes or building coarser levels based on spatial proximity, which can introduce wrong edges across geometry boundaries. Inspired by the bipartite graph determination, we propose a novel pooling strategy, \\textit{bi-stride} to tackle the aforementioned limitations. Bi-stride pools nodes on every other frontier of the breadth-first search (BFS), without the need for the manual drawing of coarser meshes and avoiding the wrong edges by spatial proximity. Additionally, it enables a one-MP scheme per level and non-parametrized pooling and unpooling by interpolations, resembling U-Nets, which significantly reduces computational costs. Experiments show that the proposed framework, \\textit{BSMS-GNN}, significantly outperforms existing methods in terms of both accuracy and computational efficiency in representative physical simulations.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 3. Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n*Zhiqi Bu, J. Mao, Shiyun Xu*\n\n**TL;DR:** An efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, is proposed that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy.\n\n**Abstract:** Large convolutional neural networks (CNN) can be difficult to train in the differentially private (DP) regime, since the optimization algorithms require a computationally expensive operation, known as the per-sample gradient clipping. We propose an efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy. The improvement in efficiency is rigorously studied through the first complexity analysis for the mixed ghost clipping and existing DP training algorithms. Extensive experiments on vision classification tasks, with large ResNet, VGG, and Vision Transformers, demonstrate that DP training with mixed ghost clipping adds $1\\sim 10\\%$ memory overhead and $<2\\times$ slowdown to the standard non-private training. Specifically, when training VGG19 on CIFAR10, the mixed ghost clipping is $3\\times$ faster than state-of-the-art Opacus library with $18\\times$ larger maximum batch size. To emphasize the significance of efficient DP training on convolutional layers, we achieve 96.7\\% accuracy on CIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous best results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy engine (\\url{https://github.com/woodyx218/private_vision}) that implements DP training of CNN with a few lines of code.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 36  (*Influential: 3*)\n\n#### 4. SNIPER: Efficient Multi-Scale Training\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n*Bharat Singh, Mahyar Najibi, L. Davis*\n\n**TL;DR:** SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct.\n\n**Abstract:** We present SNIPER, an algorithm for performing efficient multi-scale training in instance level visual recognition tasks. Instead of processing every pixel in an image pyramid, SNIPER processes context regions around ground-truth instances (referred to as chips) at the appropriate scale. For background sampling, these context-regions are generated using proposals extracted from a region proposal network trained with a short learning schedule. Hence, the number of chips generated per image during training adaptively changes based on the scene complexity. SNIPER only processes 30% more pixels compared to the commonly used single scale training at 800x1333 pixels on the COCO dataset. But, it also observes samples from extreme resolutions of the image pyramid, like 1400x2000 pixels. As SNIPER operates on resampled low resolution chips (512x512 pixels), it can have a batch size as large as 20 on a single GPU even with a ResNet-101 backbone. Therefore it can benefit from batch-normalization during training without the need for synchronizing batch-normalization statistics across GPUs. SNIPER brings training of instance level recognition tasks like object detection closer to the protocol for image classification and suggests that the commonly accepted guideline that it is important to train on high resolution images for instance level visual recognition tasks might not be correct. Our implementation based on Faster-RCNN with a ResNet-101 backbone obtains an mAP of 47.6% on the COCO dataset for bounding box detection and can process 5 images per second during inference with a single GPU. Code is available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 468  (*Influential: 35*)\n\n#### 5. Searching for Efficient Multi-Scale Architectures for Dense Image Prediction\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n*Liang-Chieh Chen, Maxwell D. Collins, Yukun Zhu, G. Papandreou, Barret Zoph, Florian Schroff, Hartwig Adam, Jonathon Shlens*\n\n**TL;DR:** This work constructs a recursive search space for meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation and demonstrates that even with efficient random search, this architecture can outperform human-invented architectures.\n\n**Abstract:** The design of neural network architectures is an important component for achieving state-of-the-art performance with machine learning systems across a broad array of tasks. Much work has endeavored to design and build architectures automatically through clever construction of a search space paired with simple learning algorithms. Recent progress has demonstrated that such meta-learning methods may exceed scalable human-invented architectures on image classification tasks. An open question is the degree to which such methods may generalize to new domains. In this work we explore the construction of meta-learning techniques for dense image prediction focused on the tasks of scene parsing, person-part segmentation, and semantic image segmentation. Constructing viable search spaces in this domain is challenging because of the multi-scale representation of visual information and the necessity to operate on high resolution imagery. Based on a survey of techniques in dense image prediction, we construct a recursive search space and demonstrate that even with efficient random search, we can identify architectures that outperform human-invented architectures and achieve state-of-the-art performance on three dense prediction tasks including 82.7% on Cityscapes (street scene parsing), 71.3% on PASCAL-Person-Part (person-part segmentation), and 87.9% on PASCAL VOC 2012 (semantic image segmentation). Additionally, the resulting architecture is more computationally efficient, requiring half the parameters and half the computational cost as previous state of the art systems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 394  (*Influential: 28*)\n\n### 3 related papers from Papers with Code\n\n#### 1. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n*Rameswar Panda, Quanfu Fan, Chun-Fu Chen*\n\n**Abstract:** The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at \\url{https://github.com/IBM/CrossViT}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-27\n\n\n\n#### 2. 3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n*J. Dolz, I. Ben Ayed, C. Desrosiers*\n\n**Abstract:** This study investigates a 3D and fully convolutional neural network (CNN) for\nsubcortical brain structure segmentation in MRI. 3D CNN architectures have been\ngenerally avoided due to their computational and memory requirements during\ninference. We address the problem via small kernels, allowing deeper\narchitectures. We further model both local and global context by embedding\nintermediate-layer outputs in the final prediction, which encourages\nconsistency between features extracted at different scales and embeds\nfine-grained information directly in the segmentation process. Our model is\nefficiently trained end-to-end on a graphics processing unit (GPU), in a single\nstage, exploiting the dense inference capabilities of fully CNNs.\n  We performed comprehensive experiments over two publicly available datasets.\nFirst, we demonstrate a state-of-the-art performance on the ISBR dataset. Then,\nwe report a {\\em large-scale} multi-site evaluation over 1112 unregistered\nsubject datasets acquired from 17 different sites (ABIDE dataset), with ages\nranging from 7 to 64 years, showing that our method is robust to various\nacquisition protocols, demographics and clinical factors. Our method yielded\nsegmentations that are highly consistent with a standard atlas-based approach,\nwhile running in a fraction of the time needed by atlas-based methods and\navoiding registration/normalization steps. This makes it convenient for massive\nmulti-site neuroanatomical imaging studies. To the best of our knowledge, our\nwork is the first to study subcortical structure segmentation on such\nlarge-scale and heterogeneous data.\n\n**Published:** 2016-12-12\n\n\n\n#### 3. Self-Guided Network for Fast Image Denoising\n\n*From Search Query: memory efficient multi scale processing neural networks*\n\n* Radu Timofte,  Luc Van Gool,  Yawei Li, Shuhang Gu*\n\n**Abstract:** During the past years, tremendous advances in image restoration tasks have been achieved using highly complex neural networks. Despite their good restoration performance, the heavy computational burden hinders the deployment of these networks on constrained devices, e.g. smart phones and consumer electronic products. To tackle this problem, we propose a self-guided network (SGN), which adopts a top-down self-guidance architecture to better exploit image multi-scale information. SGN directly generates multi-resolution inputs with the shuffling operation. Large-scale contextual information extracted at low resolution is gradually propagated into the higher resolution sub-networks to guide the feature extraction processes at these scales. Such a self-guidance strategy enables SGN to efficiently incorporate multi-scale information and extract good local features to recover noisy images. We validate the effectiveness of SGN through extensive experiments. The experimental results demonstrate that SGN greatly improves the memory and runtime efficiency over state-of-the-art efficient methods, without trading off PSNR accuracy. \r\n\n**Proceeding:** iccv-2019-10\n\n**Published:** 2019-10-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on memory efficiency, multi-scale processing, and other optimization strategies, here are some key findings and references that align with your requirements:\n\n## Memory-Efficient Implementations of Multi-Scale or Hierarchical Processing\n\n- The paper on \"Accelerating GaLore for memory-efficient LLM Training and Fine-tuning\" introduces the Natural GaLore algorithm, which significantly reduces memory usage during the training and fine-tuning of large language models (LLMs). It leverages the low-rank structure of gradients and incorporates second-order information, making it a memory-efficient approach for multi-scale processing in LLMs.\n\n## Cache Optimization Techniques for Deep Learning\n\n- The same paper on Natural GaLore discusses various distributed computing techniques, including Distributed Data-Parallel (DDP) and model parallelism, which help in optimizing cache utilization and reducing memory bottlenecks. Techniques like gradient checkpointing and memory offloading strategies (e.g., ZeRO-Offload) are also highlighted as effective methods for managing memory and cache efficiency.\n\n## Memory-Bandwidth Trade-Offs in Neural Network Architectures\n\n- The work on \"Adaptive higher order reversible integrators for memory efficient computations\" addresses the memory-bandwidth trade-offs by introducing reversible integration methods that allow for variable time-steps. This approach ensures that the memory requirement for training is independent of network depth, which is crucial for deep neural networks and can help in optimizing memory-bandwidth trade-offs.\n\n## Efficient Implementations of Feature Hierarchies\n\n- The MSCL-Attention model, which combines multi-scale convolutional neural networks, long short-term memory networks, and multi-head self-attention mechanisms, is an example of efficient feature hierarchy implementation. This model enhances prediction accuracy and robustness by capturing cross-scale information and managing temporal dependencies effectively.\n\n## Memory Optimization Strategies for Language Models\n\n- The Natural GaLore algorithm, as mentioned earlier, is a significant contribution to memory optimization for language models. It reduces memory usage by up to 65.5% in optimizer states without compromising performance. Additionally, techniques like Fully Sharded Data Parallel and LoRA (Low-Rank Adaptation) are discussed, which help in reducing the memory footprint during training and fine-tuning of large models.\n\n## Additional Insights\n\n- For hierarchical feature processing, the integration of multi-scale feature extraction with self-attention mechanisms, as seen in the MSCL-Attention model, can be beneficial. This approach allows for the capture of salient features at different scales, enhancing predictive accuracy.\n\n- The use of reversible integrators and neural ODEs, as discussed in the paper on adaptive higher order reversible integrators, can help in managing the memory requirements and computational costs associated with deep neural networks, which is crucial for autoregressive language models.\n\nBy leveraging these strategies and techniques, researchers can develop more memory-efficient, scalable, and performant autoregressive language models that incorporate multi-scale processing and hierarchical feature extraction effectively.\n",
            "analysis": "Analysis Note on HierarchicalRMSNorm Proposal\n\n1. Summary of Analysis:\n\nThe initial search reveals several important aspects about the proposed HierarchicalRMSNorm design:\n\na) Novelty Assessment:\n- The combination of multi-scale processing with RMSNorm appears to be relatively novel\n- No direct precedent found for hierarchical normalization in language models\n- Some related work in vision and graph domains, but different approaches\n\nb) Technical Feasibility:\n- Memory efficiency claims need further investigation\n- Multi-scale processing overhead requires careful analysis\n- Integration with existing LM architectures needs more research\n\n2. Useful References with Excerpts:\n\nFrom \"Revisiting Over-smoothing in BERT\":\n\"layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace\"\n\nFrom \"Position Coupling\":\n\"Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training\"\n\nFrom \"Mamba\":\n\"Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency\"\n\n3. Key Insights:\n\na) Memory Efficiency:\n- Need to investigate actual memory overhead of multi-scale paths\n- Cache utilization crucial for performance\n- Trade-off between feature representation and memory usage\n\nb) Normalization Impact:\n- Layer normalization affects model convergence\n- Over-smoothing is a potential concern\n- Scale-specific normalization needs theoretical justification\n\nc) Integration Challenges:\n- Compatibility with existing architectures\n- Impact on training stability\n- Computational overhead management\n\n4. Future Search Plan:\n\nNeed additional searches focusing on:\n\na) Query 1: Memory-efficient implementations of multi-scale processing\n- Focus on practical implementations\n- Memory optimization techniques\n- Cache-friendly algorithms\n\nb) Query 2: Theoretical analysis of hierarchical feature processing\n- Mathematical foundations\n- Convergence properties\n- Stability analysis\n\nc) Query 3: Integration strategies with transformer architectures\n- Architectural modifications\n- Training strategies\n- Performance optimization\n\n5. References:\n\n[1] Shi, H., Gao, J., Xu, H., Liang, X., Li, Z., Kong, L., Lee, S.M.S., & Kwok, J. (2022). Revisiting Over-smoothing in BERT from the Perspective of Graph. International Conference on Learning Representations.\n\n[2] Cho, H., Cha, J., Awasthi, P., Bhojanapalli, S., Gupta, A., & Yun, C. (2024). Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers. arXiv.org.\n\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.org.\n\n[4] Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). Efficient Content-Based Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics.\n\n[5] Zhou, Y., Ren, J., Li, F., Zabih, R., & Lim, S. (2023). Test-Time Distribution Normalization for Contrastively Learned Visual-language Models. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "transformer normalization techniques memory efficiency",
            "detail": "Find papers discussing:\n1. Memory-efficient normalization techniques in transformers\n2. Impact of normalization on model performance\n3. Integration of normalization with attention mechanisms\n4. Hardware-aware normalization implementations\n5. Adaptive normalization strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Memory-efficient normalization techniques in transformers\n2. Impact of normalization on model performance\n3. Integration of normalization with attention mechanisms\n4. Hardware-aware normalization implementations\n5. Adaptive normalization strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.99)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.99)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.97)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 50/56 (Score: 0.97)*\n\n```\narXiv preprint arXiv:1609.08144, 2016. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.\n```\n\n#### 3. Transformer Quality in Linear Time (Avg. Score: 0.90)\n\n*Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le*\n\n**Published in:** International Conference on Machine Learning (2022)\t**Cited by** 152  (*Influential: 36*)\n\n**TL;DR:** This work revisit the design choices in Transformers, and proposes a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss, and a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality.\n\n**Abstract:** We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and 12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on C4 for masked language modeling.\n\n##### *Relevant Chunk: No. 17/29 (Score: 0.90)*\n\n```\nIn International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Kitaev, N., Kaiser, \u0141., and Levskaya, A. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451, 2020. Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.X., and Yan, X. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in Neural Information Processing Systems, 32:5243-5253, 2019. Liu, H., Dai, Z., So, D. R., and Le, Q. V. Pay attention to mlps. NeurIPS, 2021. Narang, S., Chung, H. W., Tay, Y., Fedus, W., Fevry, T., Matena, M., Malkan, K., Fiedel, N., Shazeer, N., Lan, Z., et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021. Nguyen, T. Q. and Salazar, J. Transformers without tears: Improving the normalization of self-attention.\n```\n\n#### 4. The Devil in Linear Transformer  (Avg. Score: 0.86)\n\n*Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, Yiran Zhong*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2022)\t**Cited by** 39  (*Influential: 3*)\n\n**TL;DR:** This paper proposes a new linear attention that replaces the scaling of attention matrices with a normalization to stabilize gradients, and proposes a diagonal attention to confine attention to only neighbouring tokens in early layers.\n\n**Abstract:** Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .\n\n##### *Relevant Chunk: No. 2/25 (Score: 0.86)*\n\n```\nHowever, they usually suffer from degraded performances on various tasks and corpora. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, TransNORMER, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at TRANSNORMER. ## 1 Introduction\n\nTransformer models show great performance on a wide range of natural language processing and computer vision tasks (Qin et al., 2022; Sun et al., 2022b; Cheng et al., 2022a,b; Zhou et al., 2022). One issue of the vanilla transformer model lies in\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_1e87ac89fa2bec7d4d6fg-01.jpg?height=441&width=780&top_left_y=750&top_left_x=1049)\n\nFigure 1: TransNORMER has smaller memory footprints (circle sizes) and produces clearly favorable speed ( $x$-axis) and overall scores ( $y$-axis), when evaluated on the challenging Long-Range Arena benchmark than the vanilla transformer and other competing methods. its quadratic space-time complexity with respect to the input length. Various prior works attempt to alleviate this inefficiency (Zaheer et al., 2020; Beltagy et al., 2020; Tay et al., 2020a; Kitaev et al., 2020; Child et al., 2019; Liu et al., 2022; Sun et al., 2022b). In this work, we focus on a particular subset of these methods, known as kernel-based linear transformers (Choromanski et al., 2020; Wang et al., 2020; Katharopoulos et al., 2020; Peng et al., 2020; Qin et al., 2022) considering their desirable linear space-time complexity. Despite their space-time efficiency, linear transformers are not always in favor for practical adoption, largely due to the degraded performance than the vanilla model. To address this issue, we take a close look at existing kernel-based linear transformers and identify $\\boldsymbol{t w o}$ deficiencies that lead to such a performance gap. Unbounded gradients. Most existing linear transformers inherit attention formulation from the vanilla transformer, which scales attention scores to ensure they are bounded within $[0,1]$. However, we theoretically show that such a scaling strategy\nrenders unbounded gradients for linear transformer models. As a result, the unbounded gradients empirically lead to unstable convergence as our preliminary experiments suggest. Attention dilution. Previous works (Titsias, 2016; Jang et al., 2016; Gao and Pavel, 2017; Qin et al., 2022; Sun et al., 2022b,a) suggest that in vanilla transformer, softmax attention maps tend to be local. In contrast, as shown in Fig 2, we observe that linear transformers often trivially distribute attention scores over the entire sequence even in early layers. Due to this issue, which we refer as attention dilution, important local information is less well preserved in linear models, resulting in inferior performance. This negative impact of attention dilution is also evidenced by the performance drop in our controlled experiments if partly replacing vanilla attention in transformer layers with linear attention ones. To mitigate these issues, we propose a linear transformer model, called TrANSNORMER, which shows better performance than vanilla transformer on a wide range of task while being significantly faster during runtime, as shown in Fig. 1. To avoid the unbounded gradients, we introduce NORMATTENTION, which gets rid of scaling over attention matrices while appending an additional normalization only after the attention layer. The choice of the normalization operator is unrestricted, for example, LayerNorm (Ba et al., 2016) or RMSNorm (Zhang and Sennrich, 2019) both serve the purpose. We show empirical results demonstrating that with Normattention, the gradients are more stable during training, which in turn leads to more consistent convergence. To alleviate the attention dilution issue, we modify the vanilla attention and allow each token to only attend to its neighbouring tokens, resulting in a diagonal attention. To mimic the behaviors on local semantics of the vanilla transformer, we employ the diagonal attention on early layers while using NormAttention for later ones. In this way, we encourage the model to capture both local and global language context. Note that our diagonal attention can be efficiently computed such that the overall linear space-time complexity of TRANSNORMER is preserved. We perform extensive experiments on standard tasks, where TransNORmER demonstrates lower language modeling perplexities on WikiText-103 and overall higher text classification accuracy on\nGLUE than vanilla model and other competing methods. In addition, on the challenging LongRange Arena benchmark, TransNormer also shows favorable results while being faster and more scalable with longer inputs during both training and inference time. ## 2 Background and related work\n\nWe first briefly review vanilla transformer (Vaswani et al., 2017) and its efficient variants. The key component of transformers is the self-attention, which operates on query $\\mathbf{Q}$, key $\\mathbf{K}$ and value $\\mathbf{V}$ matrices; each of them is the image of a linear projection taking $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ as input:\n\n$$\n\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{Q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{K}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{V} \\in \\mathbb{R}^{n \\times d}\n$$\n\nwith $n$ the input length, $d$ the hidden dimension. The output $\\mathbf{O} \\in \\mathbb{R}^{n \\times d}$ is formulated as:\n\n$$\n\\mathbf{O}=\\operatorname{Softmax}\\left(\\mathbf{Q K}^{\\boldsymbol{\\top}} / \\sqrt{d}\\right) \\mathbf{V}\n$$\n\nwhere the $\\operatorname{Softmax}(\\cdot)$ step renders quadratic spacetime complexity with respect to the input length, making it prohibitive for vanilla transformer to scale to long input sequences. To address this issue, numerous efficient transformers have been explored in the literature. These methods can be generally categorized into two families, i.e., pattern based methods and kernel based methods. Pattern based methods (Zaheer et al., 2020; Beltagy et al., 2020; Tay et al., 2020a; Kitaev et al., 2020; Child et al., 2019) sparsify the attention calculation with handcrafted or learnable masking patterns. Kernel-based methods adopt kernel functions to decompose softmax attention, which reduces the theoretical space-time complexity to linear. In this paper, we refer the kernel-based variants as linear transformers for simplicity. In the kernel-based methods (Choromanski et al., 2020; Katharopoulos et al., 2020; Peng et al., 2020; Qin et al., 2022; Zheng et al., 2022; Wang et al., 2020), a kernel function $\\phi(\\cdot)$ maps queries and keys to their hidden representations. Then the output of the linear attention can be rewritten as:\n\n$$\n\\begin{aligned}\n\\mathbf{O} & =\\boldsymbol{\\Delta}^{-1} \\phi(\\mathbf{Q})\\left[\\phi(\\mathbf{K})^{\\top} \\mathbf{V}\\right] \\\\\n\\boldsymbol{\\Delta} & =\\operatorname{diag}\\left(\\phi(\\mathbf{Q})\\left[\\phi(\\mathbf{K})^{\\top} \\mathbf{1}_{n}\\right]\\right)\n\\end{aligned}\n$$\n\nwhere the product of keys and values are computed to avoid the quadratic $n \\times n$ matrix.\n```\n\n#### 5. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.80)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.80)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: transformer normalization techniques memory efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Trainable Transformer in Transformer\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*A. Panigrahi, Sadhika Malladi, Mengzhou Xia, Sanjeev Arora*\n\n**TL;DR:** This work proposes an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models), and introduces innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulateand fine-Tune a 125 million parameter transformer model within a single forward pass.\n\n**Abstract:** Recent works attribute the capability of in-context learning (ICL) in large pre-trained language models to implicitly simulating and fine-tuning an internal model (e.g., linear or 2-layer MLP) during inference. However, such constructions require large memory overhead, which makes simulation of more sophisticated internal models intractable. In this work, we propose an efficient construction, Transformer in Transformer (in short, TinT), that allows a transformer to simulate and fine-tune complex models internally during inference (e.g., pre-trained language models). In particular, we introduce innovative approximation techniques that allow a TinT model with less than 2 billion parameters to simulate and fine-tune a 125 million parameter transformer model within a single forward pass. TinT accommodates many common transformer variants and its design ideas also improve the efficiency of past instantiations of simple models inside transformers. We conduct end-to-end experiments to validate the internal fine-tuning procedure of TinT on various language modeling and downstream tasks. For example, even with a limited one-step budget, we observe TinT for a OPT-125M model improves performance by 4-16% absolute on average compared to OPT-125M. These findings suggest that large pre-trained language models are capable of performing intricate subroutines. To facilitate further work, a modular and extensible codebase for TinT is included.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 2. Reformer: The Efficient Transformer\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 2020  (*Influential: 227*)\n\n#### 3. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**TL;DR:** The low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter, is introduced and achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 3*)\n\n#### 4. SOFT: Softmax-free Transformer with Linear Complexity\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, T. Xiang, Li Zhang*\n\n**TL;DR:** For the first time, a softmax-free transformer or SOFT is proposed, which significantly improves the computational efficiency of existing ViT variants and allows much longer token sequences to be permitted in SOFT, resulting in superior trade-off between accuracy and complexity.\n\n**Abstract:** Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 132  (*Influential: 9*)\n\n#### 5. Layer-Condensed KV Cache for Efficient Inference of Large Language Models\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*Haoyi Wu, Kewei Tu*\n\n**TL;DR:** A novel method is proposed that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput, and is orthogonal to existing transformer memory-saving techniques.\n\n**Abstract:** Huge memory consumption has been a major bottleneck for deploying high-throughput large language models in real-world applications. In addition to the large number of parameters, the key-value (KV) cache for the attention mechanism in the transformer architecture consumes a significant amount of memory, especially when the number of layers is large for deep language models. In this paper, we propose a novel method that only computes and caches the KVs of a small number of layers, thus significantly saving memory consumption and improving inference throughput. Our experiments on large language models show that our method achieves up to 26$\\times$ higher throughput than standard transformers and competitive performance in language modeling and downstream tasks. In addition, our method is orthogonal to existing transformer memory-saving techniques, so it is straightforward to integrate them with our model, achieving further improvement in inference efficiency. Our code is available at https://github.com/whyNLP/LCKV.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 13  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*Patrick Legresley, Mostofa Patwary, Jared Casper, Mohammad Shoeybi, Raul Puri, Bryan Catanzaro*\n\n**Abstract:** Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).\n\n**Published:** 2019-09-17\n\n\n\n#### 2. Softmax-free Linear Transformers\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*Tao Xiang, Junge Zhang, Jianfeng Feng, Jiachen Lu, Xiatian Zhu, Li Zhang*\n\n**Abstract:** Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax-based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under low-rank matrix decomposition. For computational robustness, we estimate the Moore-Penrose inverse using an iterative Newton-Raphson method in the forward process only, while calculating its theoretical gradients only once in the backward process. To further expand applicability (e.g., dense prediction tasks), an efficient symmetric normalization technique is introduced. Extensive experiments on ImageNet, COCO, and ADE20K show that our SOFT significantly improves the computational efficiency of existing ViT variants. With linear complexity, much longer token sequences are permitted by SOFT, resulting in superior trade-off between accuracy and complexity. Code and models are available at https://github.com/fudan-zvg/SOFT.\n\n**Published:** 2022-07-05\n\n\n\n#### 3. TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer\n\n*From Search Query: transformer normalization techniques memory efficiency*\n\n*Yu Qiao, Xiao Luo, Yiran Zhong, Baohong Lv, Yunshen Wei, Xiaodong Han, Xuyang Shen, Weixuan Sun, Weigao Sun, Dong Li, Zhen Qin*\n\n**Abstract:** We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanisms, tensor normalization, and inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism for smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over $20\\%$. Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., LLMs with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art LLMs with Transformer but are also significantly faster. Code is released at: https://github.com/OpenNLPLab/TransnormerLLM.\n\n**Published:** 2023-07-27\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on memory-efficient normalization techniques, impact on model performance, integration with attention mechanisms, hardware-aware implementations, and adaptive normalization strategies, here are some key findings and references:\n\n## Memory-Efficient Normalization Techniques\n\n- **Memory-Efficient Bidirectional Transformers**: These models utilize several techniques to enhance memory efficiency, such as parallel layer execution, omission of biases in QKV projections and LayerNorms, and sharding techniques to distribute model parameters and activations across multiple compute devices. These adjustments lead to significant reductions in memory usage and training time.\n\n- **Neural Attention Memory Models (NAMMs)**: This framework introduces a learned network for memory management that improves both the performance and efficiency of transformers. NAMMs condition exclusively on the values in the produced attention matrices, allowing for substantial reductions in the number of retained tokens in the KV cache, thus enhancing memory efficiency.\n\n## Impact of Normalization on Model Performance\n\n- **Normalization Techniques in Transformers**: Normalization techniques, such as layer normalization, are crucial for stabilizing the training process and improving model performance. For instance, recent normalization techniques proposed by Gilmer et al. have proven effective in stabilizing the training of memory-efficient bidirectional transformers.\n\n- **Normalized Transformer (nGPT)**: This architecture incorporates representation learning on a hypersphere, where all vectors involved in embeddings, MLPs, attention matrices, and hidden states are constrained to have a unit norm. This normalization approach contributes to improved model performance and stability.\n\n## Integration with Attention Mechanisms\n\n- **NAMMs and Attention Mechanisms**: NAMMs are integrated with the attention mechanisms of transformers by conditioning on the attention matrices. This allows individual layers and attention heads to focus on the most relevant information, enhancing the model's performance and efficiency across various tasks.\n\n- **Multi-Head Attention with Normalization**: Traditional transformer architectures use multi-head attention mechanisms, which can be enhanced with normalization techniques. For example, layer normalization after self-attention and feedforward layers helps in stabilizing the training and improving the model's expressiveness.\n\n## Hardware-Aware Normalization Implementations\n\n- **IO-Aware Attention Algorithms**: Implementations like FlashAttention emphasize the importance of making attention algorithms IO-aware, accounting for reads and writes between levels of GPU memory. This approach significantly improves performance by optimizing memory access patterns[Analysis Note, FlashAttention].\n\n- **Memristor-Based Accelerators**: Using memristor-based in-memory computing can accelerate transformer self-attention operations. This hardware-aware approach leverages the analog nature of memristors to optimize matrix multiplication, reducing power consumption and latency compared to digital implementations.\n\n## Adaptive Normalization Strategies\n\n- **Adaptive Memory Management with NAMMs**: NAMMs provide a framework for adaptive memory management by learning to focus on the most relevant information for each layer and attention head. This adaptability allows for better performance and efficiency across various tasks and input modalities.\n\n- **HierarchicalRMSNorm Proposal**: The proposal suggests investigating hierarchical normalization techniques that can adapt to different scales and contexts within the model. This adaptive approach could help in balancing computational overhead and feature representation, leading to more efficient and effective models[Analysis Note].\n\nBy integrating these insights, researchers can develop autoregressive language models that are not only more memory-efficient but also perform better, are more robust, and can scale more effectively with larger datasets and models.\n",
            "analysis": "Analysis Note on HierarchicalRMSNorm Proposal - Round 2\n\n1. Summary of Analysis:\nThe search results reveal important insights about memory-efficient architectures and multi-scale processing in neural networks. The proposed HierarchicalRMSNorm shows promise but requires deeper investigation into specific aspects.\n\n2. Useful References with Key Excerpts:\n\nFrom FlashFFTConv:\n\"A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in O(N logN) time in sequence length N but has poor hardware utilization.\"\n- Highlights importance of hardware-aware design\n- Shows trade-offs between algorithmic and practical efficiency\n\nFrom Loki:\n\"Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models.\"\n- Suggests potential for dimensional reduction in normalization\n- Indicates opportunity for memory optimization\n\nFrom FlashAttention:\n\"We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory.\"\n- Emphasizes importance of memory hierarchy awareness\n- Provides insights for efficient implementation\n\n3. Key Insights:\n\na) Memory Hierarchy Considerations:\n- Need to explicitly consider GPU memory hierarchy\n- Cache-friendly implementations crucial for performance\n- Memory access patterns significantly impact efficiency\n\nb) Multi-scale Processing:\n- Successful implementations exist in vision tasks\n- Trade-offs between computational overhead and feature representation\n- Need for careful balance in scale selection\n\nc) Implementation Efficiency:\n- IO-aware algorithms show significant performance gains\n- Hardware utilization crucial for practical efficiency\n- Memory bandwidth often bottlenecks performance\n\n4. Future Search Plan:\n\nNeed additional investigation in:\na) Normalization techniques in transformer architectures\n- Specific focus on memory-efficient implementations\n- Impact on model performance\n- Integration with existing architectures\n\nb) Scale selection strategies\n- Adaptive approaches\n- Impact on model performance\n- Computational overhead\n\nc) Hardware-specific optimizations\n- GPU memory hierarchy utilization\n- Cache-friendly implementations\n- Memory bandwidth optimization\n\n5. References:\n\n[1] Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[2] Singhania, P., Singh, S., He, S., Feizi, S., & Bhatele, A. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint.\n\n[3] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[4] Chen, L. C., Collins, M. D., Zhu, Y., Papandreou, G., Zoph, B., Schroff, F., Adam, H., & Shlens, J. (2018). Searching for Efficient Multi-Scale Architectures for Dense Image Prediction. Neural Information Processing Systems.\n\n[5] Panda, R., Fan, Q., & Chen, C. F. (2021). CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification. ICCV 2021."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.702489,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.64335,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "RMSNorm",
    "proposal": "## HierarchicalRMSNorm: Multi-scale Adaptive Normalization for Language Models\n\n### 1. Motivation\n\nCurrent language models face several challenges in processing sequences of varying lengths and complexities:\n- Standard normalization techniques treat all features uniformly\n- Fixed-scale processing limits the model's ability to capture hierarchical patterns\n- Memory inefficiency in handling long sequences\n- Suboptimal parameter initialization affecting training stability\n\n### 2. Problem Analysis\n\nThe original RMSNorm, while efficient, has limitations:\n1. Single-scale processing misses hierarchical feature relationships\n2. Uniform normalization may not be optimal for all feature scales\n3. Fixed parameter initialization doesn't adapt to data characteristics\n4. Limited ability to handle varying sequence lengths efficiently\n\n### 3. Core Idea and Philosophy\n\nHierarchicalRMSNorm introduces a multi-scale normalization approach that:\n- Processes features at multiple resolutions simultaneously\n- Adapts normalization parameters based on scale-specific statistics\n- Uses memory-efficient computation through hierarchical grouping\n- Employs adaptive initialization for improved training stability\n\n### 4. Design Plan\n\n#### 4.1 Architecture Overview\n\nThe HierarchicalRMSNorm consists of three main components:\n1. Multi-scale Feature Decomposition\n2. Scale-specific Normalization Paths\n3. Adaptive Parameter Initialization\n\n#### 4.2 Mathematical Formulation\n\n1. Multi-scale Decomposition:\n```python\nx_scales = {s: downsample(x, scale=s) for s in scales}\n```\n\n2. Scale-specific Normalization:\nFor each scale s:\n```\nrms_s(x) = sqrt(mean(x_s^2 + eps))\ny_s = x_s / rms_s(x) * gamma_s\n```\n\n3. Adaptive Weight Initialization:\n```python\ngamma_s = init_scale_weights(s, data_stats)\nwhere:\ninit_scale_weights(s, stats) = base_gamma * scale_factor(s, stats)\n```\n\n4. Hierarchical Integration:\n```python\ny = sum(upsample(y_s) * w_s for s, y_s in y_scales.items())\n```\n\n#### 4.3 Implementation Details\n\n```python\nclass HierarchicalRMSNorm(nn.Module):\n    def __init__(self, embed_dim, scales=[1, 2, 4], eps=1e-5):\n        super().__init__()\n        self.scales = scales\n        self.eps = eps\n        self.gammas = nn.ParameterDict({\n            f's{s}': nn.Parameter(torch.ones(embed_dim))\n            for s in scales\n        })\n        self.scale_weights = nn.Parameter(\n            torch.ones(len(scales))\n        )\n        \n    def forward(self, x):\n        # Multi-scale decomposition\n        x_scales = self._decompose_scales(x)\n        \n        # Scale-specific normalization\n        y_scales = {}\n        for s, x_s in x_scales.items():\n            rms_s = torch.sqrt(\n                x_s.pow(2).mean(-1, keepdim=True) + self.eps\n            )\n            y_scales[s] = x_s / rms_s * self.gammas[f's{s}']\n        \n        # Hierarchical integration\n        y = self._integrate_scales(y_scales)\n        return y\n        \n    def _decompose_scales(self, x):\n        return {s: self._downsample(x, s) for s in self.scales}\n        \n    def _integrate_scales(self, y_scales):\n        weights = F.softmax(self.scale_weights, dim=0)\n        return sum(\n            self._upsample(y_s) * w \n            for (s, y_s), w in zip(y_scales.items(), weights)\n        )\n```\n\n### 5. Implementation Guidelines\n\n1. Scale Selection:\n- Use power-of-2 scales (1, 2, 4) for efficient computation\n- Adjust scale range based on sequence length\n- Initialize scale weights uniformly\n\n2. Memory Optimization:\n- Implement in-place operations where possible\n- Use gradient checkpointing for long sequences\n- Cache intermediate results for repeated scales\n\n3. Initialization Strategy:\n- Use data statistics to initialize gamma parameters\n- Adapt scale weights based on sequence characteristics\n- Implement warm-up period for scale mixing\n\n### 6. Theoretical Analysis\n\n1. Computational Complexity:\n- Time: O(N log N) due to multi-scale processing\n- Memory: O(N) through efficient scale integration\n- Additional parameters: O(D * S) where D is dimension and S is number of scales\n\n2. Benefits:\n- Improved feature normalization across scales\n- Better handling of long-range dependencies\n- Reduced memory footprint through hierarchical processing\n- More stable training through adaptive initialization\n\n3. Limitations:\n- Slight increase in computation time\n- Additional hyperparameters to tune\n- Potential overhead in very short sequences\n\n### 7. Research Summary\n\nKey findings from research:\n1. Multi-scale processing improves model robustness\n2. Hierarchical structures enable efficient memory usage\n3. Adaptive initialization enhances training stability\n4. Scale-specific normalization captures better feature relationships\n\n### 8. Evolution of Design\n\nThe design evolved through several iterations:\n1. Initial focus on pure multi-scale processing\n2. Addition of adaptive initialization\n3. Integration of memory optimization techniques\n4. Refinement of scale integration mechanism\n\n### 9. References\n\n[1] Dao, T., et al. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\"\n\n[2] Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization.\"\n\n[3] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\"\n\n[4] Dai, Z., et al. (2019). \"Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.\"\n\n[5] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\"\n\n### 10. Conclusion\n\nHierarchicalRMSNorm represents a significant advancement in normalization techniques for language models. By combining multi-scale processing with adaptive initialization and memory-efficient computation, it addresses key limitations of existing approaches while maintaining computational efficiency. The design's flexibility and scalability make it particularly suitable for modern large language models dealing with varying sequence lengths and complex feature distributions.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "We propose HierarchicalRMSNorm, a novel variant of RMSNorm that incorporates multi-scale processing and adaptive normalization through a hierarchical structure. The design introduces scale-specific normalization paths and adaptive parameter initialization to better handle varying sequence lengths and feature distributions. By combining memory-efficient computation with multi-resolution feature processing, our approach achieves improved performance while maintaining the computational efficiency of the original RMSNorm.",
    "ideation": null,
    "modelname": "HierGPT",
    "suggestions": "1. Memory Efficiency:\n- Provide detailed analysis of memory-computation trade-offs\n- Include cache optimization strategies\n- Add benchmarking methodology for memory usage\n\n2. Implementation Guidelines:\n- Add concrete examples of scale selection\n- Include initialization best practices\n- Provide more detailed integration guidelines\n\n3. Training Stability:\n- Include gradient flow analysis\n- Add stability monitoring recommendations\n- Provide failure mode analysis\n\n4. Scalability Analysis:\n- Address model parallelism considerations\n- Include large-scale deployment guidelines\n- Add performance scaling analysis\n\n5. Documentation:\n- Provide more detailed API specifications\n- Include example configurations\n- Add debugging guidelines",
    "user_input": ""
}