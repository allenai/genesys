{
    "variantname": "FreqHAA",
    "review": "The FreqHAA-GPT proposal presents an innovative approach to enhancing hierarchical adaptive attention through frequency-domain operations. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel integration of frequency-domain operations with hierarchical attention\n- Well-grounded mathematical formulation of FFT-based computations\n- Clear connection to hardware optimization\n- Thoughtful consideration of memory efficiency\n\n2. Technical Design:\n- Detailed mathematical formulation of frequency transformations\n- Clear implementation guidelines with practical considerations\n- Careful attention to memory management and computational overhead\n- Integration of adaptive frequency filtering for improved expressiveness\n\n3. Efficiency Considerations:\n- Matrix decomposition for efficient FFT computation\n- Hardware-aware design with specific optimization strategies\n- Clear focus on memory bandwidth reduction\n- Efficient parallel processing capabilities\n\n4. Scalability:\n- Linear complexity maintained through careful design\n- Efficient memory management through frequency-domain compression\n- Clear strategy for handling long sequences\n- Consideration of cache hierarchy in design\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex coordination between frequency and spatial domain operations\n- Potential challenges in maintaining numerical stability\n- Need for careful tuning of frequency filtering parameters\n- Complex memory access patterns requiring optimization\n\n2. Training Stability:\n- Frequency-domain operations may affect gradient flow\n- Complex interaction between attention and frequency components\n- Need for careful initialization strategies\n- Possible instability in early training stages\n\n3. Memory Management:\n- Additional overhead from FFT computations\n- Complex state management in frequency domain\n- Need for efficient caching strategies\n- Trade-offs between compression and information preservation\n\n4. Hardware Utilization:\n- Complex memory access patterns may impact efficiency\n- Need for hardware-specific optimizations\n- Potential challenges in parallel implementation\n- Trade-offs between computation and memory access\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than FNet's pure Fourier approach\n2. Novel integration with hierarchical attention\n3. Unique approach to adaptive frequency filtering\n4. Thoughtful integration with hardware optimization\n\nHowever, it shares some concepts with:\n1. FlashFFTConv's matrix decomposition strategy\n2. H-Transformer-1D's hierarchical processing\n3. Block Transformer's multi-scale approach",
    "search_stack": [
        {
            "ready": false,
            "query": "adaptive state compression neural networks",
            "detail": "Find papers discussing techniques for adaptive state compression and memory management in neural networks, particularly focusing on methods that dynamically adjust compression based on input content.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing techniques for adaptive state compression and memory management in neural networks, particularly focusing on methods that dynamically adjust compression based on input content.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.95)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.95)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Human-like Episodic Memory for Infinite Context LLMs (Avg. Score: 0.89)\n\n*Z. Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** EM-LLM is introduced, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency and providing a computational framework for exploring human memory mechanisms.\n\n**Abstract:** Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs, enabling them to effectively handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an on-line fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information. Experiments on the LongBench dataset demonstrate EM-LLM's superior performance, outperforming the state-of-the-art InfLLM model with an overall relative improvement of 4.3% across various tasks, including a 33% improvement on the PassageRetrieval task. Furthermore, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart. This work not only advances LLM capabilities in processing extended contexts but also provides a computational framework for exploring human memory mechanisms, opening new avenues for interdisciplinary research in AI and cognitive science.\n\n##### *Relevant Chunk: No. 20/36 (Score: 0.89)*\n\n```\narXiv preprint arXiv:2311.09431, 2023. Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, and Edoardo M Ponti. Dynamic memory compression: Retrofitting llms for accelerated inference.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.77)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.77)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n#### 4. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.70)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 24/46 (Score: 0.70)*\n\n```\nMultiMedia LLC. 2009. Large text compression benchmark. G\u00e1bor Melis, Charles Blundell, Tom\u00e1\u0161 Ko\u010disk\u1ef3, Karl Moritz Hermann, Chris Dyer, and Phil Blunsom. 2018. Pushing the bounds of dropout. arXiv preprint arXiv:1805.09208. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843. Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc'Aurelio Ranzato. 2014. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753. Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Jan \u010cernock\u1ef3, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association. Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. SLT, 12(234-239):8.\n```\n\n#### 5. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.65)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.65)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive state compression neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. An Adaptive Empirical Bayesian Method for Sparse Deep Learning\n\n*From Search Query: adaptive state compression neural networks*\n\n*Wei Deng, Xiao Zhang, F. Liang, Guang Lin*\n\n**TL;DR:** A novel adaptive empirical Bayesian method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors, which leads to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks and the state of theart compression performance on CIFAR10 with Residual Networks.\n\n**Abstract:** We propose a novel adaptive empirical Bayesian (AEB) method for sparse deep learning, where the sparsity is ensured via a class of self-adaptive spike-and-slab priors. The proposed method works by alternatively sampling from an adaptive hierarchical posterior distribution using stochastic gradient Markov Chain Monte Carlo (MCMC) and smoothly optimizing the hyperparameters using stochastic approximation (SA). We further prove the convergence of the proposed method to the asymptotically correct distribution under mild conditions. Empirical applications of the proposed method lead to the state-of-the-art performance on MNIST and Fashion MNIST with shallow convolutional neural networks (CNN) and the state-of-the-art compression performance on CIFAR10 with Residual Networks. The proposed method also improves resistance to adversarial attacks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 40  (*Influential: 2*)\n\n#### 2. SLAMB: Accelerated Large Batch Training with Sparse Communication\n\n*From Search Query: adaptive state compression neural networks*\n\n*Hang Xu, Wenxuan Zhang, Jiawei Fei, Yuzhe Wu, Tingwen Xie, Jun Huang, Yuchen Xie, Mohamed Elhoseiny, Panos Kalnis*\n\n**TL;DR:** This paper combines sparsification-based gradient compression with the layer-wise adaptive moments optimizer for large batch training (LAMB) to propose SLAMB, a novel communication-efficient optimizer that supports large batch sizes and scales to thousands of GPUs.\n\n**Abstract:** Distributed training of large deep neural networks requires frequent exchange of massive data be-tween machines, thus communication efficiency is a major concern. Existing compressed communication methods are either not compatible with large batch optimization algorithms, or do not provide sufficient speedup in large scale. In this paper, we combine sparsification-based gradient compression with the layer-wise adaptive moments optimizer for large batch training (LAMB). We propose SLAMB, a novel communication-efficient optimizer that supports large batch sizes and scales to thousands of GPUs. SLAMB employs momentum masking, local error compensation, and element-wise adaptive rescaling to achieve accurate layer-wise weight updates, which translates to fast convergence for very large batches. Our empirical results show that, compared to the state-of-the-art, SLAMB transmits half the amount of data in large-batch BERT pre-training, without sacrificing accuracy. Moreover, SLAMB achieves excellent scalability in large computing infrastructures. For instance, SLAMB with 128 GPUs reduces the training time of Swin Transformer pre-training on ImageNet to 5.35 hours, which is 2 hours faster than the state-of-the-art. At the extreme, we trained BERT-XL (2.8B parameters) on 1,024 NVIDIA A100 GPUs, where SLAMB achieved 90% scaling efficiency.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 3. Adaptive Smoothing Gradient Learning for Spiking Neural Networks\n\n*From Search Query: adaptive state compression neural networks*\n\n*Ziming Wang, Runhao Jiang, S. Lian, Rui Yan, Huajin Tang*\n\n**TL;DR:** This work proposes a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise, in this way, the network learns adaptively the accurate gradients of loss landscape in SNNs.\n\n**Abstract:** Spiking neural networks (SNNs) with biologically inspired spatio-temporal dynamics demonstrate superior energy efficiency on neuromorphic architectures. Error backpropagation in SNNs is pro-hibited by the all-or-none nature of spikes. The existing solution circumvents this problem by a relaxation on the gradient calculation using a continuous function with a constant relaxation degree, so-called surrogate gradient learning. Nevertheless, such a solution introduces additional smoothing error on spike firing which leads to the gradients being estimated inaccurately. Thus, how to adaptively adjust the relaxation degree and eliminate smoothing error progressively is crucial. Here, we propose a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise. In this way, the network learns adaptively the accurate gradients of loss landscape in SNNs. The theoretical analysis further shows optimization on such a noisy network could be evolved into optimization on the embedded SNN with shared weights progressively. Moreover, The experiments on static images, dynamic event streams, speech, and instrumental sounds show the proposed method achieves state-of-the-art performance across all the datasets with remarkable robustness on different relaxation degrees.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 4. Adaptive Estimators Show Information Compression in Deep Neural Networks\n\n*From Search Query: adaptive state compression neural networks*\n\n*Ivan Chelombiev, Conor J. Houghton, Cian O\u2019Donnell*\n\n**TL;DR:** More robust mutual information estimation techniques are developed, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions, which explore compression in networks with a range of different activation functions.\n\n**Abstract:** To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 7*)\n\n#### 5. ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks\n\n*From Search Query: adaptive state compression neural networks*\n\n*Jungmin Kwon, Jeongseop Kim, Hyunseong Park, I. Choi*\n\n**TL;DR:** A novel learning method, adaptive sharpness-aware minimization (ASAM), is suggested utilizing the proposed generalization bound, and experimental results show that ASAM contributes to significant improvement of model generalization performance.\n\n**Abstract:** Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-of-the-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scale-invariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 238  (*Influential: 43*)\n\n### 3 related papers from Papers with Code\n\n#### 1. ContextNet: Exploring Context and Detail for Semantic Segmentation in Real-time\n\n*From Search Query: adaptive state compression neural networks*\n\n*Rudra P. K. Poudel, Christopher Zach, Ujwal Bonde, Stephan Liwicki*\n\n**Abstract:** Modern deep learning architectures produce highly accurate results on many\nchallenging semantic segmentation datasets. State-of-the-art methods are,\nhowever, not directly transferable to real-time applications or embedded\ndevices, since naive adaptation of such systems to reduce computational cost\n(speed, memory and energy) causes a significant drop in accuracy. We propose\nContextNet, a new deep neural network architecture which builds on factorized\nconvolution, network compression and pyramid representation to produce\ncompetitive semantic segmentation in real-time with low memory requirement.\nContextNet combines a deep network branch at low resolution that captures\nglobal context information efficiently with a shallow branch that focuses on\nhigh-resolution segmentation details. We analyse our network in a thorough\nablation study and present results on the Cityscapes dataset, achieving 66.1%\naccuracy at 18.3 frames per second at full (1024x2048) resolution (41.9 fps\nwith pipelined computations for streamed data).\n\n**Published:** 2018-05-11\n\n\n\n#### 2. How Well Do Sparse Imagenet Models Transfer?\n\n*From Search Query: adaptive state compression neural networks*\n\n*Dan Alistarh, Mark Kurtz, Alexandra Peste, Eugenia Iofinova*\n\n**Abstract:** Transfer learning is a classic paradigm by which models pretrained on large \"upstream\" datasets are adapted to yield good results on \"downstream\" specialized datasets. Generally, more accurate models on the \"upstream\" dataset tend to provide better transfer accuracy \"downstream\". In this work, we perform an in-depth investigation of this phenomenon in the context of convolutional neural networks (CNNs) trained on the ImageNet dataset, which have been pruned - that is, compressed by sparsifying their connections. We consider transfer using unstructured pruned models obtained by applying several state-of-the-art pruning methods, including magnitude-based, second-order, re-growth, lottery-ticket, and regularization approaches, in the context of twelve standard transfer tasks. In a nutshell, our study shows that sparse models can match or even outperform the transfer performance of dense models, even at high sparsities, and, while doing so, can lead to significant inference and even training speedups. At the same time, we observe and analyze significant differences in the behaviour of different pruning methods.\n\n**Proceeding:** cvpr-2022-1\n\n**Published:** 2021-11-26\n\n\n\n#### 3. A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness\n\n*From Search Query: adaptive state compression neural networks*\n\n*Bhavya Kailkhura, Jize Zhang, Shreya Chaganti, Brian R. Bartoldson, James Diffenderfer*\n\n**Abstract:** Successful adoption of deep learning (DL) in the wild requires models to be: (1) compact, (2) accurate, and (3) robust to distributional shifts. Unfortunately, efforts towards simultaneously meeting these requirements have mostly been unsuccessful. This raises an important question: Is the inability to create Compact, Accurate, and Robust Deep neural networks (CARDs) fundamental? To answer this question, we perform a large-scale analysis of popular model compression techniques which uncovers several intriguing patterns. Notably, in contrast to traditional pruning approaches (e.g., fine tuning and gradual magnitude pruning), we find that \"lottery ticket-style\" approaches can surprisingly be used to produce CARDs, including binary-weight CARDs. Specifically, we are able to create extremely compact CARDs that, compared to their larger counterparts, have similar test accuracy and matching (or better) robustness -- simply by pruning and (optionally) quantizing. Leveraging the compactness of CARDs, we develop a simple domain-adaptive test-time ensembling approach (CARD-Decks) that uses a gating module to dynamically select appropriate CARDs from the CARD-Deck based on their spectral-similarity with test samples. The proposed approach builds a \"winning hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on CIFAR-10-C accuracies (i.e., 96.8% standard and 92.75% robust) and CIFAR-100-C accuracies (80.6% standard and 71.3% robust) with better memory usage than non-compressed baselines (pretrained CARDs and CARD-Decks available at https://github.com/RobustBench/robustbench). Finally, we provide theoretical support for our empirical findings.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-16\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive state compression and memory management, here are some relevant techniques and papers that align with your analysis and goals:\n\n## Adaptive Pruning and Compression\n\nThe paper on \"Developmental Plasticity-inspired Adaptive Pruning for Deep Neural Networks and Spiking Neural Networks\" (SNNs) introduces a method that could be highly relevant for adaptive state compression and memory management in neural networks.\n\n- This work proposes a generalized developmental plasticity-inspired adaptive pruning (DPAP) method, which dynamically prunes inactive synapses and neurons based on their activity levels. This approach is inspired by the brain's developmental plasticity and the \"use it or lose it\" principle.\n- The DPAP method integrates multi-scale spatio-temporal developmental plasticity to measure the importance of neurons and synapses, allowing for the progressive elimination of redundancy and the evolution of compact, brain-inspired neural circuits.\n- This method can help in reducing memory and computational requirements while maintaining or improving the performance of the network, which is crucial for efficient state management in autoregressive language models.\n\n## Dynamic Memory Management\n\nThe concept of external dynamic memory, as discussed in the \"Memformer\" paper, is also pertinent. Memformer utilizes an external dynamic memory to encode and retrieve past information, achieving linear time complexity and constant memory space complexity.\n\n- This approach can be adapted to include adaptive compression mechanisms. By dynamically managing the memory based on the input content, the model can efficiently handle long sequences and reduce memory usage.\n- Combining such external memory architectures with hierarchical processing and adaptive pruning techniques could enhance the model's ability to manage long-range dependencies efficiently.\n\n## Input-Dependent Processing\n\nThe idea of making model components input-dependent, as suggested by the Mamba paper, is crucial for improving adaptivity and flexibility in autoregressive models.\n\n- By letting the model parameters be functions of the input, the model can adaptively adjust its state compression and memory management based on the input content. This can be achieved through techniques like dynamic layer normalization and hierarchical adaptive multi-head attention, as seen in the HieraNorm-AttnGPT design.\n\n## Efficient Parameter Generation and Compression\n\nRecent advances in parameter generation and compression, such as those discussed in the context of neural network pruning and quantization, can also be beneficial. For example:\n\n- Weight magnitude, weight gradient, and weight similarity-based pruning methods can be adapted to dynamically adjust the model's parameters based on the input content, ensuring efficient state compression and memory usage.\n\nBy integrating these techniques\u2014adaptive pruning, dynamic memory management, input-dependent processing, and efficient parameter generation\u2014the researchers can develop an autoregressive language model that is more efficient, scalable, and adaptable to varying input content.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis\n\nThe seed design HieraNorm-AttnGPT introduces hierarchical adaptive multi-head attention with dynamic layer normalization. From analyzing this and the references, I've identified several key areas for potential innovation:\n\na) Memory and State Management:\n- Current designs struggle with efficient memory utilization for long sequences\n- Need for better state compression and management techniques\n- Opportunity to improve information flow across different timescales\n\nb) Computational Efficiency:\n- Linear attention mechanisms show promise but often lack expressiveness\n- Trade-off between computational complexity and model capability\n- Hardware utilization could be improved\n\nc) Adaptivity and Flexibility:\n- Current models often use fixed architectures regardless of input\n- Opportunity for more dynamic, input-dependent processing\n- Need for better handling of varying sequence lengths\n\n2. Useful References with Excerpts\n\nFrom Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning [...] simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n\nThis suggests the importance of making model components input-dependent.\n\nFrom Memformer:\n\"utilizes an external dynamic memory to encode and retrieve past information [...] achieves linear time complexity and constant memory space complexity\"\n\nShows potential for external memory architectures.\n\nFrom PoSE:\n\"smartly simulates long inputs using a fixed context window [...] can potentially support infinite length, limited only by memory usage in inference\"\n\nDemonstrates innovative approaches to handling long sequences.\n\nFrom HOPE:\n\"view SSMs through the lens of Hankel operator theory [...] allows for random initializations of the LTI systems and helps to improve training stability\"\n\nProvides theoretical insights for stable training of state-based models.\n\n3. Key Insights and Analysis\n\na) State Space Models (SSMs):\n- SSMs offer efficient sequence modeling but need content-based adaptation\n- Combining SSMs with attention mechanisms could provide both efficiency and expressiveness\n- Need to address initialization and training stability challenges\n\nb) Memory Management:\n- External memory architectures can help manage long-range dependencies\n- Need for efficient compression and retrieval mechanisms\n- Opportunity to combine with hierarchical processing\n\nc) Position Encoding:\n- Position information crucial for sequence understanding\n- Current methods often limited by fixed context windows\n- Opportunity for more flexible position encoding schemes\n\nd) Potential Innovation Directions:\n- Combine SSMs with hierarchical processing for efficient long-range modeling\n- Integrate adaptive compression mechanisms for memory efficiency\n- Develop input-dependent parameter generation for flexibility\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in state compression techniques\n2. Methods for efficient parameter generation\n3. Hardware-efficient implementations of SSMs\n4. Adaptive computation mechanisms\n\n5. References\n\n[1] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". arXiv preprint arXiv:2305.10327.\n\n[2] Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). \"Memformer: A Memory-Augmented Transformer for Sequence Modeling\". AACL/IJCNLP.\n\n[3] Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., & Li, S. (2023). \"PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training\". arXiv preprint.\n\n[4] Yu, A., Mahoney, M. W., & Erichson, N. B. (2024). \"There is HOPE to Avoid HiPPOs for Long-memory State Space Models\". arXiv preprint."
        },
        {
            "ready": false,
            "query": "adaptive pruning neural networks dynamic compression",
            "detail": "Find papers discussing techniques for dynamic pruning and compression in neural networks, particularly focusing on methods that adapt pruning strategies based on input patterns or network state.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing techniques for dynamic pruning and compression in neural networks, particularly focusing on methods that adapt pruning strategies based on input patterns or network state.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (Avg. Score: 0.98)\n\n*Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 2*)\n\n**TL;DR:** A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n##### *Relevant Chunk: No. 16/67 (Score: 0.98)*\n\n```\nSIAM Journal on Scientific Computing, 36(3):A1207-A1220, 2014. [33] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.92)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.99)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n##### *Relevant Chunk: No. 33/46 (Score: 0.84)*\n\n```\narXiv preprint arXiv:2210.09298, 2022. [72] Ji Lin, Yongming Rao, Jiwen Lu, and Jie Zhou. Runtime neural pruning. Advances in neural information processing systems, 30, 2017. [73] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_2e48bf63902e7cf6031fg-17.jpg?height=47&width=378&top_left_y=291&top_left_x=326)\n[74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11976-11986, 2022. [75] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. Advances in Neural Information Processing Systems, 34:2441$2453,2021$. [76] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention.\n```\n\n#### 3. MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases (Avg. Score: 0.85)\n\n*Zechun Liu, Changsheng Zhao, Forrest N. Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, Liangzhen Lai, Vikas Chandra*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** The proposed MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.\n\n**Abstract:** This paper addresses the growing need for efficient large language models (LLMs) on mobile devices, driven by increasing cloud costs and latency concerns. We focus on designing top-quality LLMs with fewer than a billion parameters, a practical choice for mobile deployment. Contrary to prevailing belief emphasizing the pivotal role of data and parameter quantity in determining model quality, our investigation underscores the significance of model architecture for sub-billion scale LLMs. Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network denoted as MobileLLM, which attains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M state-of-the-art models. Additionally, we propose an immediate block-wise weight-sharing approach with no increase in model size and only marginal latency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a further accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover, MobileLLM model family shows significant improvements compared to previous sub-billion models on chat benchmarks, and demonstrates close correctness to LLaMA-v2 7B in API calling tasks, highlighting the capability of small models for common on-device use cases.\n\n##### *Relevant Chunk: No. 28/45 (Score: 0.85)*\n\n```\nIn International Conference on Machine Learning, pp. 31094-31116. PMLR, 2023. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. A simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Tan, M. and Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 6105-6114. PMLR, 2019. Taylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn, A., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022. Thawakar, O., Vayani, A., Khan, S., Cholakal, H., Anwer, R. M., Felsberg, M., Baldwin, T., Xing, E. P., and Khan, F. S. Mobillama: Towards accurate and lightweight fully transparent gpt.\n```\n\n#### 4. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.80)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.80)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive pruning neural networks dynamic compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Pruning Deep Neural Networks from a Sparsity Perspective\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*Enmao Diao, G. Wang, Jiawei Zhan, Yuhong Yang, Jie Ding, V. Tarokh*\n\n**TL;DR:** The proposed PQ Index (PQI) is proposed to measure the potential compressibility of deep neural networks and used to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm that is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.\n\n**Abstract:** In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 2. Pruning Neural Networks via Coresets and Convex Geometry: Towards No Assumptions\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*M. Tukan, Loay Mualem, Alaa Maalouf*\n\n**TL;DR:** This work suggests a novel and robust framework for computing coresets under mild assumptions on the model's weights and without any assumption on the training data, which outperforms existing coreset based neural pruning approaches across a wide range of networks and datasets.\n\n**Abstract:** Pruning is one of the predominant approaches for compressing deep neural networks (DNNs). Lately, coresets (provable data summarizations) were leveraged for pruning DNNs, adding the advantage of theoretical guarantees on the trade-off between the compression rate and the approximation error. However, coresets in this domain were either data-dependent or generated under restrictive assumptions on both the model's weights and inputs. In real-world scenarios, such assumptions are rarely satisfied, limiting the applicability of coresets. To this end, we suggest a novel and robust framework for computing such coresets under mild assumptions on the model's weights and without any assumption on the training data. The idea is to compute the importance of each neuron in each layer with respect to the output of the following layer. This is achieved by a combination of L\\\"{o}wner ellipsoid and Caratheodory theorem. Our method is simultaneously data-independent, applicable to various networks and datasets (due to the simplified assumptions), and theoretically supported. Experimental results show that our method outperforms existing coreset based neural pruning approaches across a wide range of networks and datasets. For example, our method achieved a $62\\%$ compression rate on ResNet50 on ImageNet with $1.09\\%$ drop in accuracy.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 3. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*Song Han, Huizi Mao, W. Dally*\n\n**TL;DR:** This work introduces \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy.\n\n**Abstract:** Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2015\n\n**Citations:** 8244  (*Influential: 819*)\n\n#### 4. RED : Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*Edouard Yvinec, Arnaud Dapogny, M. Cord, K\u00e9vin Bailly*\n\n**TL;DR:** Red is presented, a data-free structured, unified approach to tackle structured pruning ofDeep Neural Networks by proposing a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors.\n\n**Abstract:** Deep Neural Networks (DNNs) are ubiquitous in today's computer vision land-scape, despite involving considerable computational costs. The mainstream approaches for runtime acceleration consist in pruning connections (unstructured pruning) or, better, filters (structured pruning), both often requiring data to re-train the model. In this paper, we present RED, a data-free structured, unified approach to tackle structured pruning. First, we propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons represented by their weight vectors. Second, we prune the network by merging redundant neurons based on their relative similarities, as defined by their distance. Third, we propose a novel uneven depthwise separation technique to further prune convolutional layers. We demonstrate through a large variety of benchmarks that RED largely outperforms other data-free pruning methods, often reaching performance similar to unconstrained, data-driven methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 0*)\n\n#### 5. Frequency-Domain Dynamic Pruning for Convolutional Neural Networks\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*Z. Liu, Jizheng Xu, Xiulian Peng, Ruiqin Xiong*\n\n**TL;DR:** Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin and can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-110.\n\n**Abstract:** Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-110.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 130  (*Influential: 11*)\n\n### 3 related papers from Papers with Code\n\n#### 1. A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*Bhavya Kailkhura, Jize Zhang, Shreya Chaganti, Brian R. Bartoldson, James Diffenderfer*\n\n**Abstract:** Successful adoption of deep learning (DL) in the wild requires models to be: (1) compact, (2) accurate, and (3) robust to distributional shifts. Unfortunately, efforts towards simultaneously meeting these requirements have mostly been unsuccessful. This raises an important question: Is the inability to create Compact, Accurate, and Robust Deep neural networks (CARDs) fundamental? To answer this question, we perform a large-scale analysis of popular model compression techniques which uncovers several intriguing patterns. Notably, in contrast to traditional pruning approaches (e.g., fine tuning and gradual magnitude pruning), we find that \"lottery ticket-style\" approaches can surprisingly be used to produce CARDs, including binary-weight CARDs. Specifically, we are able to create extremely compact CARDs that, compared to their larger counterparts, have similar test accuracy and matching (or better) robustness -- simply by pruning and (optionally) quantizing. Leveraging the compactness of CARDs, we develop a simple domain-adaptive test-time ensembling approach (CARD-Decks) that uses a gating module to dynamically select appropriate CARDs from the CARD-Deck based on their spectral-similarity with test samples. The proposed approach builds a \"winning hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on CIFAR-10-C accuracies (i.e., 96.8% standard and 92.75% robust) and CIFAR-100-C accuracies (80.6% standard and 71.3% robust) with better memory usage than non-compressed baselines (pretrained CARDs and CARD-Decks available at https://github.com/RobustBench/robustbench). Finally, we provide theoretical support for our empirical findings.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-16\n\n\n\n#### 2. Adaptively Placed Multi-Grid Scene Representation Networks for Large-Scale Data Visualization\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*Tom Peterka, Hanqi Guo, Han-Wei Shen, Tianyu Xiong, Skylar Wolfgang Wurster*\n\n**Abstract:** Scene representation networks (SRNs) have been recently proposed for compression and visualization of scientific data. However, state-of-the-art SRNs do not adapt the allocation of available network parameters to the complex features found in scientific data, leading to a loss in reconstruction quality. We address this shortcoming with an adaptively placed multi-grid SRN (APMGSRN) and propose a domain decomposition training and inference technique for accelerated parallel training on multi-GPU systems. We also release an open-source neural volume rendering application that allows plug-and-play rendering with any PyTorch-based SRN. Our proposed APMGSRN architecture uses multiple spatially adaptive feature grids that learn where to be placed within the domain to dynamically allocate more neural network resources where error is high in the volume, improving state-of-the-art reconstruction accuracy of SRNs for scientific data without requiring expensive octree refining, pruning, and traversal like previous adaptive models. In our domain decomposition approach for representing large-scale data, we train an set of APMGSRNs in parallel on separate bricks of the volume to reduce training time while avoiding overhead necessary for an out-of-core solution for volumes too large to fit in GPU memory. After training, the lightweight SRNs are used for realtime neural volume rendering in our open-source renderer, where arbitrary view angles and transfer functions can be explored. A copy of this paper, all code, all models used in our experiments, and all supplemental materials and videos are available at https://github.com/skywolf829/APMGSRN.\n\n**Published:** 2023-07-16\n\n\n\n#### 3. A Brief Review of Hypernetworks in Deep Learning\n\n*From Search Query: adaptive pruning neural networks dynamic compression*\n\n*David A. Clifton, Soheila Molaei, Ping Lu, Jiandong Zhou, Vinod Kumar Chauhan*\n\n**Abstract:** Hypernetworks, or hypernets for short, are neural networks that generate weights for another neural network, known as the target network. They have emerged as a powerful deep learning technique that allows for greater flexibility, adaptability, dynamism, faster training, information sharing, and model compression. Hypernets have shown promising results in a variety of deep learning problems, including continual learning, causal inference, transfer learning, weight pruning, uncertainty quantification, zero-shot learning, natural language processing, and reinforcement learning. Despite their success across different problem settings, there is currently no comprehensive review available to inform researchers about the latest developments and to assist in utilizing hypernets. To fill this gap, we review the progress in hypernets. We present an illustrative example of training deep neural networks using hypernets and propose categorizing hypernets based on five design criteria: inputs, outputs, variability of inputs and outputs, and the architecture of hypernets. We also review applications of hypernets across different deep learning problem settings, followed by a discussion of general scenarios where hypernets can be effectively employed. Finally, we discuss the challenges and future directions that remain underexplored in the field of hypernets. We believe that hypernetworks have the potential to revolutionize the field of deep learning. They offer a new way to design and train neural networks, and they have the potential to improve the performance of deep learning models on a variety of tasks. Through this review, we aim to inspire further advancements in deep learning through hypernetworks.\n\n**Published:** 2023-06-12\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model (LM) design, particularly focusing on dynamic pruning and compression techniques that adapt to input patterns or network state, here are some relevant findings and references:\n\n## Adaptive Pruning Techniques\n\n1. **Developmental Plasticity-inspired Adaptive Pruning (DPAP)**\n   This method, described in the paper \"Developmental Plasticity-inspired Adaptive Pruning for Deep Neural Networks,\" incorporates dynamic pruning strategies inspired by the brain's developmental plasticity. It dynamically prunes redundant synapses and neurons based on their activity levels, which can be particularly effective for event-driven spiking neural networks (SNNs) and deep artificial neural networks (ANNs).\n\n2. **Dynamic Pruning Based on Activity Levels**\n   The DPAP method uses a \"use it or lose it\" principle, where synapses and neurons are pruned based on their activity levels. This approach is biologically plausible and can help in preventing overfitting and underfitting, making it suitable for complex deep networks.\n\n## Dynamic Compression Techniques\n\n1. **NeuZip: Memory-Efficient Training and Inference**\n   NeuZip introduces a dynamic compression method for neural networks, which compresses the model during both training and inference. This approach is designed to optimize memory usage on devices with limited memory, making it relevant for efficient LM block design.\n\n## Adaptive State Management and Memory Efficiency\n\n1. **IO-Aware Algorithms**\n   Techniques like FlashAttention, which use tiling to reduce memory reads/writes, are crucial for IO-aware design. These methods can be integrated with adaptive pruning and compression to optimize memory access patterns and reduce bandwidth requirements[Analysis Note].\n\n2. **Layer-Wise Adaptive Compression**\n   Methods such as SLAMB, which combine sparsification-based gradient compression with layer-wise adaptive moments optimizers, demonstrate the potential for layer-wise adaptation in compression. This can be applied to LM blocks to achieve efficient memory management and maintain accuracy[Analysis Note].\n\n## Hardware Optimization and Balancing Compression\n\n1. **Hardware-Aware Implementation Strategies**\n   Efficient parameter generation methods and hardware-specific optimizations for matrix operations are essential for balancing compression with computational overhead. Integrating these strategies with adaptive pruning and compression can lead to more efficient LM block designs[Analysis Note].\n\n## Potential Innovation Directions\n\n1. **Combining IO-Aware Design with Adaptive Compression**\n   Combining techniques like FlashAttention with dynamic pruning methods like DPAP could lead to highly efficient and adaptive LM blocks that optimize both memory access and model capacity[Analysis Note].\n\n2. **Integrating Lottery Ticket Approaches with Dynamic Pruning**\n   Using lottery ticket-style approaches, which can produce compact, accurate, and robust deep neural networks, in conjunction with dynamic pruning methods, could further enhance the efficiency and performance of LM blocks[Analysis Note].\n\nBy exploring these techniques and integrating them into the design of autoregressive LM blocks, researchers can achieve significant improvements in terms of memory efficiency, computational optimization, and overall performance.\n",
            "analysis": "Analysis Note: Second Round Research on LM Block Design\n\n1. Summary of Analysis\n\nThe second round of research revealed several promising directions for improving LM block design, particularly around adaptive compression and efficient memory management:\n\na) Adaptive State Management:\n- Dynamic pruning and compression based on input content\n- Multi-scale approaches to memory organization\n- Hardware-aware implementation strategies\n\nb) Memory Efficiency:\n- IO-aware algorithms for reduced memory access\n- Tiling and blocking strategies for better cache utilization\n- Adaptive compression techniques\n\nc) Computational Optimization:\n- Hardware-specific optimizations for matrix operations\n- Efficient parameter generation methods\n- Balanced trade-offs between compression and model capacity\n\n2. Useful References with Excerpts\n\nFrom FlashAttention:\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM [...] requiring fewer HBM accesses than standard attention\"\n\nThis suggests the importance of IO-aware design for memory efficiency.\n\nFrom SLAMB:\n\"combines sparsification-based gradient compression with the layer-wise adaptive moments optimizer [...] transmits half the amount of data in large-batch BERT pre-training, without sacrificing accuracy\"\n\nShows potential for adaptive compression techniques.\n\nFrom \"A Winning Hand\":\n\"lottery ticket-style approaches can surprisingly be used to produce Compact, Accurate, and Robust Deep neural networks (CARDs), including binary-weight CARDs\"\n\nDemonstrates viability of extreme compression while maintaining performance.\n\n3. Key Insights and Analysis\n\na) IO-Aware Design:\n- Memory access patterns crucial for performance\n- Tiling and blocking can significantly reduce memory bandwidth requirements\n- Need to balance compression with computational overhead\n\nb) Adaptive Compression:\n- Content-dependent compression ratios possible\n- Layer-wise adaptation shows promise\n- Trade-off between compression and model capacity needs careful management\n\nc) Hardware Optimization:\n- Matrix multiplication efficiency critical\n- Cache utilization important for performance\n- Need for hardware-specific implementation strategies\n\nd) Potential Innovation Directions:\n- Combine IO-aware design with adaptive compression\n- Integrate lottery ticket approaches with dynamic pruning\n- Develop hardware-aware adaptive compression strategies\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in adaptive pruning techniques\n2. Methods for dynamic parameter generation\n3. Hardware-efficient implementation strategies for compressed models\n4. Techniques for balancing compression and model capacity\n\n5. References\n\n[1] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\". Neural Information Processing Systems.\n\n[2] Xu, H., et al. (2023). \"SLAMB: Accelerated Large Batch Training with Sparse Communication\". International Conference on Machine Learning.\n\n[3] Kailkhura, B., et al. (2021). \"A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness\". NeurIPS.\n\n[4] Chelombiev, I., Houghton, C. J., & O'Donnell, C. (2019). \"Adaptive Estimators Show Information Compression in Deep Neural Networks\". International Conference on Learning Representations.\n\n[5] Kwon, J., et al. (2021). \"ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks\". International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "frequency domain neural networks efficient implementation",
            "detail": "Find papers discussing efficient implementations of frequency-domain operations in neural networks, particularly focusing on methods that maintain computational efficiency while enabling frequency-based pruning or compression.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing efficient implementations of frequency-domain operations in neural networks, particularly focusing on methods that maintain computational efficiency while enabling frequency-based pruning or compression.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.97)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.97)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n#### 2. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (Avg. Score: 0.90)\n\n*Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 2*)\n\n**TL;DR:** A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n##### *Relevant Chunk: No. 16/67 (Score: 0.90)*\n\n```\nSIAM Journal on Scientific Computing, 36(3):A1207-A1220, 2014. [33] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.\n```\n\n#### 3. Fnet: Mixing tokens with fourier transforms (Avg. Score: 0.73)\n\n*J. Lee-Thorp, J. Ainslie, Ilya Eckstein, Santiago Onta\u00f1\u00f3n*\n\n**Published in:** North American Chapter of the Association for Computational Linguistics (2021)\t**Cited by** 400  (*Influential: 56*)\n\n**TL;DR:** The FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\n\n**Abstract:** We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.\n\n##### *Relevant Chunk: No. 12/38 (Score: 0.73)*\n\n```\nHazem M El-Bakry and Qiangfu Zhao. 2004. Fast object/face detection using neural networks and fast fourier transform. International Journal of Signal Processing, 1(3):182-187. Matteo Frigo and Steven G Johnson. 2005. The design and implementation of fftw3. Proceedings of the IEEE, 93(2):216-231. Kfir Goldberg, Stav Shapiro, Elad Richardson, and Shai Avidan. 2020. Rethinking fun: Frequencydomain utilization networks. arXiv preprint arXiv:2012.03357.\n```\n\n#### 4. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.66)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 8/54 (Score: 0.70)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] George B Arfken and Hans J Weber. Mathematical methods for physicists. Elsevier Academic Press, 2005 . [3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In The International Conference on Machine Learning (ICML), pages 1120-1128, 2016. [4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [5] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019. [7] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2(1):24-47, 2020. [8] John P Boyd. Chebyshev and Fourier spectral methods.\n```\n\n##### *Relevant Chunk: No. 13/54 (Score: 0.63)*\n\n```\nIn Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2019. [19] Tri Dao, Christopher M De Sa, and Christopher R\u00e9. Gaussian quadrature for kernel features. In Advances in Neural Information Processing Systems (NeurIPS), pages 6107-6117, 2017. [20] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In The International Conference on Machine Learning (ICML), 2019. [21] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In The International Conference on Learning Representations (ICLR), 2020. [22] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. A two-pronged progress in structured dense matrix vector multiplication. In Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 1060-1079. SIAM, 2018. [23] Raymond A DeCarlo. Linear systems: A state variable approach with numerical implementation. Prentice-Hall, Inc., 1989. [24] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems (NeurIPS), pages $3844-3852,2016$. [25] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci. edu $/ \\mathrm{ml}$. [26] Krzysztof Duda. Accurate, guaranteed stable, sliding discrete Fourier transform [DSP tips \\& tricks]. IEEE Signal Processing Magazine, 27(6):124-127, 2010. [27] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural ODEs. In Advances in Neural Information Processing Systems, pages 3134-3144, 2019. [28] Behrouz Farhang-Boroujeny and Saeed Gazor. Generalized sliding FFT and its application to implementation of block LMS adaptive filters.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: frequency domain neural networks efficient implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Changwoo Lee, Hun-Seok Kim*\n\n**TL;DR:** This paper proposes a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent based on the Gaussian-Dirichlet kernel, and defines a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters.\n\n**Abstract:** This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. On the image and language tasks, our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 2. Rethinking and Improving Robustness of Convolutional Neural Networks: a Shapley Value-based Approach in Frequency Domain\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Yiting Chen, Qibing Ren, Junchi Yan*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 1*)\n\n#### 3. Transform Once: Efficient Operator Learning in Frequency Domain\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Michael Poli, Stefano Massaroli, Federico Berto, J. Park, Tri Dao, Christopher R\u00e9, Stefano Ermon*\n\n**TL;DR:** Deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time are studied, including incompressible Navier-Stokes, turbulent flows around airfoils and high-resolution video of smoke.\n\n**Abstract:** Spectral analysis provides one of the most effective paradigms for information-preserving dimensionality reduction, as simple descriptions of naturally occurring signals are often obtained via few terms of periodic basis functions. In this work, we study deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time: frequency-domain models (FDMs). Existing FDMs are based on complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: transform once (T1). To enable efficient, direct learning in the frequency domain we derive a variance-preserving weight initialization scheme and investigate methods for frequency selection in reduced-order FDMs. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of 3x to 10x that increase with data resolution and model size. We perform extensive experiments on learning the solution operator of spatio-temporal dynamics, including incompressible Navier-Stokes, turbulent flows around airfoils and high-resolution video of smoke. T1 models improve on the test performance of FDMs while requiring significantly less computation (5 hours instead of 32 for our large-scale experiment), with over 20% reduction in average predictive error across tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 2*)\n\n#### 4. Learning Frequency Domain Approximation for Binary Neural Networks\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, Yunhe Wang*\n\n**TL;DR:** This work proposes to estimate the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA), and embeds a noise adaptation module into the training phase to compensate the approximation error.\n\n**Abstract:** Binary neural networks (BNNs) represent original full-precision weights and activations into 1-bit with sign function. Since the gradient of the conventional sign function is almost zero everywhere which cannot be used for back-propagation, several attempts have been proposed to alleviate the optimization difficulty by using approximate gradient. However, those approximations corrupt the main direction of factual gradient. To this end, we propose to estimate the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. In addition, we embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using our method achieves the state-of-the-art accuracy. Code will be available at \\textit{https://gitee.com/mindspore/models/tree/master/research/cv/FDA-BNN}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 47  (*Influential: 7*)\n\n#### 5. Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Zhiqi Bu, J. Mao, Shiyun Xu*\n\n**TL;DR:** An efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, is proposed that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy.\n\n**Abstract:** Large convolutional neural networks (CNN) can be difficult to train in the differentially private (DP) regime, since the optimization algorithms require a computationally expensive operation, known as the per-sample gradient clipping. We propose an efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy. The improvement in efficiency is rigorously studied through the first complexity analysis for the mixed ghost clipping and existing DP training algorithms. Extensive experiments on vision classification tasks, with large ResNet, VGG, and Vision Transformers, demonstrate that DP training with mixed ghost clipping adds $1\\sim 10\\%$ memory overhead and $<2\\times$ slowdown to the standard non-private training. Specifically, when training VGG19 on CIFAR10, the mixed ghost clipping is $3\\times$ faster than state-of-the-art Opacus library with $18\\times$ larger maximum batch size. To emphasize the significance of efficient DP training on convolutional layers, we achieve 96.7\\% accuracy on CIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous best results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy engine (\\url{https://github.com/woodyx218/private_vision}) that implements DP training of CNN with a few lines of code.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 37  (*Influential: 4*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Balanced Resonate-and-Fire Neurons\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Sebastian Otte, Sander M. Bohte, Sebastian Kairat, Saya Higuchi*\n\n**Abstract:** The resonate-and-fire (RF) neuron, introduced over two decades ago, is a simple, efficient, yet biologically plausible spiking neuron model, which can extract frequency patterns within the time domain due to its resonating membrane dynamics. However, previous RF formulations suffer from intrinsic shortcomings that limit effective learning and prevent exploiting the principled advantage of RF neurons. Here, we introduce the balanced RF (BRF) neuron, which alleviates some of the intrinsic limitations of vanilla RF neurons and demonstrates its effectiveness within recurrent spiking neural networks (RSNNs) on various sequence learning tasks. We show that networks of BRF neurons achieve overall higher task performance, produce only a fraction of the spikes, and require significantly fewer parameters as compared to modern RSNNs. Moreover, BRF-RSNN consistently provide much faster and more stable training convergence, even when bridging many hundreds of time steps during backpropagation through time (BPTT). These results underscore that our BRF-RSNN is a strong candidate for future large-scale RSNN architectures, further lines of research in SNN methodology, and more efficient hardware implementations.\n\n**Published:** 2024-02-02\n\n\n\n#### 2. Benchmarking Spectral Graph Neural Networks: A Comprehensive Study on Effectiveness and Efficiency\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Laks V. S. Lakshmanan, Siqiang Luo, Zulun Zhu, Haoyu Liu, Ningyi Liao*\n\n**Abstract:** With the recent advancements in graph neural networks (GNNs), spectral GNNs have received increasing popularity by virtue of their specialty in capturing graph signals in the frequency domain, demonstrating promising capability in specific tasks. However, few systematic studies have been conducted on assessing their spectral characteristics. This emerging family of models also varies in terms of designs and settings, leading to difficulties in comparing their performance and deciding on the suitable model for specific scenarios, especially for large-scale tasks. In this work, we extensively benchmark spectral GNNs with a focus on the frequency perspective. We analyze and categorize over 30 GNNs with 27 corresponding filters. Then, we implement these spectral models under a unified framework with dedicated graph computations and efficient training schemes. Thorough experiments are conducted on the spectral models with inclusive metrics on effectiveness and efficiency, offering practical guidelines on evaluating and selecting spectral GNNs with desirable performance. Our implementation enables application on larger graphs with comparable performance and less overhead, which is available at: https://github.com/gdmnl/Spectral-GNN-Benchmark.\n\n**Published:** 2024-06-14\n\n\n\n#### 3. CFDP: Common Frequency Domain Pruning\n\n*From Search Query: frequency domain neural networks efficient implementation*\n\n*Weihan Luo, Samir Khaki*\n\n**Abstract:** As the saying goes, sometimes less is more -- and when it comes to neural networks, that couldn't be more true. Enter pruning, the art of selectively trimming away unnecessary parts of a network to create a more streamlined, efficient architecture. In this paper, we introduce a novel end-to-end pipeline for model pruning via the frequency domain. This work aims to shed light on the interoperability of intermediate model outputs and their significance beyond the spatial domain. Our method, dubbed Common Frequency Domain Pruning (CFDP) aims to extrapolate common frequency characteristics defined over the feature maps to rank the individual channels of a layer based on their level of importance in learning the representation. By harnessing the power of CFDP, we have achieved state-of-the-art results on CIFAR-10 with GoogLeNet reaching an accuracy of 95.25%, that is, +0.2% from the original model. We also outperform all benchmarks and match the original model's performance on ImageNet, using only 55% of the trainable parameters and 60% of the FLOPs. In addition to notable performances, models produced via CFDP exhibit robustness to a variety of configurations including pruning from untrained neural architectures, and resistance to adversarial attacks. The implementation code can be found at https://github.com/Skhaki18/CFDP.\n\n**Published:** 2023-06-07\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model (LM) design through efficient implementations of frequency-domain operations, several papers and concepts are relevant:\n\n## Frequency-Domain Inference and Pruning\n\n1. **Accelerating Inference of Networks in the Frequency Domain**:\n   This paper proposes a method to accelerate network inference by performing the entire inference process in the frequency domain. It uses the discrete cosine transform (DCT) and inverse discrete cosine transform (IDCT) only once at the beginning and end of the network, which significantly reduces the computational cost. This approach maintains high accuracy even with a high speedup ratio, making it suitable for efficient compression and pruning.\n\n## Frequency-Domain Pruning and Compression\n\n2. **Frequency-Domain Dynamic Pruning**:\n   This concept involves dynamically pruning frequency-domain coefficients in each iteration, with different frequency bands pruned discriminatively based on their importance to accuracy. This approach can be particularly effective in maintaining model performance while reducing the number of parameters, which is crucial for efficient LM block design (mentioned in the analysis note).\n\n## Efficient Frequency-Domain Operations\n\n3. **Frequency Domain Fusion and Transformation**:\n   While primarily focused on image fusion, the concept of transforming data into the frequency domain using techniques like Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (IFFT) can be adapted for neural networks. This method can help in capturing and integrating effective information from different modalities or layers, potentially enhancing the efficiency of frequency-domain operations in LMs.\n\n## Hardware-Aware and Cache-Efficient Implementations\n\n4. **Efficient Ensemble Adversarial Attack**:\n   Although this paper is about adversarial attacks, it discusses a frequency decomposition-based perturbation generator that decomposes images into different frequency bands. This approach can be insightful for developing hardware-aware and cache-efficient implementations of compressed models, as it involves optimizing weight updates within these frequency bands. Similar strategies could be applied to LM block design to ensure efficient computation and memory usage.\n\n## Key Insights for LM Block Design\n\n- **Frequency-Domain Pruning**: Using frequency-domain coefficients for pruning can offer better granularity and maintain model accuracy.\n- **Efficient Transformations**: Utilizing transforms like DCT and IDCT only at the beginning and end of the network can significantly reduce computational costs.\n- **Hardware-Aware Implementation**: Optimizing frequency-domain operations with respect to hardware capabilities, such as cache utilization, is crucial for maintaining efficiency.\n- **Dynamic Compression**: Combining frequency-domain pruning with adaptive compression strategies can help in maintaining model capacity during compression.\n\nBy integrating these insights, researchers can develop more efficient autoregressive LM blocks that leverage frequency-domain operations for pruning, compression, and inference acceleration, while ensuring hardware-aware and cache-efficient implementations.\n",
            "analysis": "Analysis Note: Third Round Research on LM Block Design\n\n1. Summary of Analysis\n\nAfter analyzing the search results, several promising directions have emerged for improving LM block design through adaptive pruning and compression:\n\na) Structured Pruning:\n- Data-free approaches show promise for efficient compression\n- Frequency-domain pruning offers better performance than spatial-domain methods\n- Adaptive hashing can identify redundant neurons effectively\n\nb) Dynamic Compression:\n- Lottery ticket approaches can maintain or improve robustness\n- Layer-wise adaptive compression shows strong results\n- Frequency-domain methods enable discriminative pruning\n\nc) Hardware Considerations:\n- IO-aware algorithms crucial for efficiency\n- Tiling and blocking strategies important for cache utilization\n- Need to balance compression with hardware capabilities\n\n2. Useful References with Excerpts\n\nFrom \"Pruning Deep Neural Networks from a Sparsity Perspective\":\n\"PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit [...] Subsequently, PQI decreases again when model collapse occurs\"\n\nThis suggests using compressibility metrics to guide pruning decisions.\n\nFrom \"RED: Looking for Redundancies\":\n\"propose a novel adaptive hashing of the scalar DNN weight distribution densities to increase the number of identical neurons [...] prune the network by merging redundant neurons based on their relative similarities\"\n\nShows potential for efficient redundancy elimination.\n\nFrom \"Frequency-Domain Dynamic Pruning\":\n\"frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy\"\n\nDemonstrates benefits of frequency-domain approaches.\n\n3. Key Insights and Analysis\n\na) Adaptive Pruning Strategies:\n- Compressibility metrics can guide pruning decisions\n- Frequency-domain pruning offers better granularity\n- Data-free methods reduce dependency on training data\n\nb) Redundancy Elimination:\n- Adaptive hashing helps identify similar neurons\n- Merging similar neurons maintains model capacity\n- Frequency-band discrimination improves pruning decisions\n\nc) Implementation Considerations:\n- Need to balance pruning granularity with computational cost\n- Hardware-aware implementation crucial for efficiency\n- Cache utilization important for performance\n\nd) Potential Innovation Directions:\n- Combine frequency-domain pruning with adaptive compression\n- Integrate compressibility metrics for dynamic pruning\n- Develop hardware-aware pruning strategies\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Methods for efficient frequency-domain operations in neural networks\n2. Techniques for hardware-aware pruning and compression\n3. Approaches for maintaining model capacity during compression\n4. Cache-efficient implementations of compressed models\n\n5. References\n\n[1] Diao, E., Wang, G., Zhan, J., Yang, Y., Ding, J., & Tarokh, V. (2023). \"Pruning Deep Neural Networks from a Sparsity Perspective\". International Conference on Learning Representations.\n\n[2] Yvinec, E., Dapogny, A., Cord, M., & Bailly, K. (2021). \"RED: Looking for Redundancies for Data-Free Structured Compression of Deep Neural Networks\". Neural Information Processing Systems.\n\n[3] Liu, Z., Xu, J., Peng, X., & Xiong, R. (2018). \"Frequency-Domain Dynamic Pruning for Convolutional Neural Networks\". Neural Information Processing Systems.\n\n[4] Kailkhura, B., et al. (2021). \"A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness\". NeurIPS.\n\n[5] Han, S., Mao, H., & Dally, W. J. (2015). \"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding\". International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "efficient attention frequency domain neural networks",
            "detail": "Find papers discussing the combination of attention mechanisms with frequency-domain operations in neural networks, particularly focusing on efficient implementations and hardware considerations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing the combination of attention mechanisms with frequency-domain operations in neural networks, particularly focusing on efficient implementations and hardware considerations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.99)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.99)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.97)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.97)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.96)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.96)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 4. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.95)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 14/40 (Score: 0.95)*\n\n```\non pp. 1-4, 12, 16, 19, 29, 30). [13] Songlin Yang et al. \"Gated Linear Attention Transformers with Hardware-Efficient Training\". In: arXiv preprint arXiv:2312.06635 (2023) (cit.\n```\n\n#### 5. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.95)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 33/41 (Score: 0.95)*\n\n```\nArXiv, abs/2009.06097, 2020. URL https://api.semanticscholar.org/CorpusID: 260424300. [75] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. ArXiv, abs/2006.04768, 2020. URL https://api.semanticscholar.org/CorpusID: 219530577 . [76] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [77] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient attention frequency domain neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Global Filter Networks for Image Classification\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, Jie Zhou*\n\n**TL;DR:** The Global Filter Network is presented, a conceptually simple yet computationally efficient architecture that learns long-term spatial dependencies in the frequency domain with log-linear complexity and can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness.\n\n**Abstract:** Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 356  (*Influential: 47*)\n\n#### 2. Solving High Frequency and Multi-Scale PDEs with Gaussian Processes\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Shikai Fang, Madison Cooley, Da Long, Shibo Li, R. Kirby, Shandian Zhe*\n\n**TL;DR:** The Gaussian process (GP) framework is used to solve high-frequency and multi-scale PDEs, using Kronecker product properties and multilinear algebra to promote computational efficiency and scalability, without low-rank approximations.\n\n**Abstract:** Machine learning based solvers have garnered much attention in physical simulation and scientific computing, with a prominent example, physics-informed neural networks (PINNs). However, PINNs often struggle to solve high-frequency and multi-scale PDEs, which can be due to spectral bias during neural network training. To address this problem, we resort to the Gaussian process (GP) framework. To flexibly capture the dominant frequencies, we model the power spectrum of the PDE solution with a student $t$ mixture or Gaussian mixture. We apply the inverse Fourier transform to obtain the covariance function (by Wiener-Khinchin theorem). The covariance derived from the Gaussian mixture spectrum corresponds to the known spectral mixture kernel. Next, we estimate the mixture weights in the log domain, which we show is equivalent to placing a Jeffreys prior. It automatically induces sparsity, prunes excessive frequencies, and adjusts the remaining toward the ground truth. Third, to enable efficient and scalable computation on massive collocation points, which are critical to capture high frequencies, we place the collocation points on a grid, and multiply our covariance function at each input dimension. We use the GP conditional mean to predict the solution and its derivatives so as to fit the boundary condition and the equation itself. As a result, we can derive a Kronecker product structure in the covariance matrix. We use Kronecker product properties and multilinear algebra to promote computational efficiency and scalability, without low-rank approximations. We show the advantage of our method in systematic experiments. The code is released at \\url{https://github.com/xuangu-fang/Gaussian-Process-Slover-for-High-Freq-PDE}.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Changwoo Lee, Hun-Seok Kim*\n\n**TL;DR:** This paper proposes a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent based on the Gaussian-Dirichlet kernel, and defines a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters.\n\n**Abstract:** This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. On the image and language tasks, our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 4. Rethinking and Improving Robustness of Convolutional Neural Networks: a Shapley Value-based Approach in Frequency Domain\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Yiting Chen, Qibing Ren, Junchi Yan*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 1*)\n\n#### 5. Transform Once: Efficient Operator Learning in Frequency Domain\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Michael Poli, Stefano Massaroli, Federico Berto, J. Park, Tri Dao, Christopher R\u00e9, Stefano Ermon*\n\n**TL;DR:** Deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time are studied, including incompressible Navier-Stokes, turbulent flows around airfoils and high-resolution video of smoke.\n\n**Abstract:** Spectral analysis provides one of the most effective paradigms for information-preserving dimensionality reduction, as simple descriptions of naturally occurring signals are often obtained via few terms of periodic basis functions. In this work, we study deep neural networks designed to harness the structure in frequency domain for efficient learning of long-range correlations in space or time: frequency-domain models (FDMs). Existing FDMs are based on complex-valued transforms i.e. Fourier Transforms (FT), and layers that perform computation on the spectrum and input data separately. This design introduces considerable computational overhead: for each layer, a forward and inverse FT. Instead, this work introduces a blueprint for frequency domain learning through a single transform: transform once (T1). To enable efficient, direct learning in the frequency domain we derive a variance-preserving weight initialization scheme and investigate methods for frequency selection in reduced-order FDMs. Our results noticeably streamline the design process of FDMs, pruning redundant transforms, and leading to speedups of 3x to 10x that increase with data resolution and model size. We perform extensive experiments on learning the solution operator of spatio-temporal dynamics, including incompressible Navier-Stokes, turbulent flows around airfoils and high-resolution video of smoke. T1 models improve on the test performance of FDMs while requiring significantly less computation (5 hours instead of 32 for our large-scale experiment), with over 20% reduction in average predictive error across tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Adaptive Frequency Filters As Efficient Global Token Mixers\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Baining Guo, Yan Lu, Zheng-Jun Zha, Cuiling Lan, Zhizheng Zhang, Zhipeng Huang*\n\n**Abstract:** Recent vision transformers, large-kernel CNNs and MLPs have attained remarkable successes in broad vision tasks thanks to their effective information fusion in the global scope. However, their efficient deployments, especially on mobile devices, still suffer from noteworthy challenges due to the heavy computational costs of self-attention mechanisms, large kernels, or fully connected layers. In this work, we apply conventional convolution theorem to deep learning for addressing this and reveal that adaptive frequency filters can serve as efficient global token mixers. With this insight, we propose Adaptive Frequency Filtering (AFF) token mixer. This neural operator transfers a latent representation to the frequency domain via a Fourier transform and performs semantic-adaptive frequency filtering via an elementwise multiplication, which mathematically equals to a token mixing operation in the original latent space with a dynamic convolution kernel as large as the spatial resolution of this latent representation. We take AFF token mixers as primary neural operators to build a lightweight neural network, dubbed AFFNet. Extensive experiments demonstrate the effectiveness of our proposed AFF token mixer and show that AFFNet achieve superior accuracy and efficiency trade-offs compared to other lightweight network designs on broad visual tasks, including visual recognition and dense prediction tasks.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-07-26\n\n\n\n#### 2. SuperNeurons: FFT-based Gradient Sparsification in the Distributed Training of Deep Neural Networks\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Anonymous*\n\n**Abstract:** The performance and efficiency of distributed training of Deep Neural\nNetworks highly depend on the performance of gradient averaging among all\nparticipating nodes, which is bounded by the communication between nodes. There\nare two major strategies to reduce communication overhead: one is to hide\ncommunication by overlapping it with computation, and the other is to reduce\nmessage sizes. The first solution works well for linear neural architectures,\nbut latest networks such as ResNet and Inception offer limited opportunity for\nthis overlapping. Therefore, researchers have paid more attention to minimizing\ncommunication. In this paper, we present a novel gradient compression framework\nderived from insights of real gradient distributions, and which strikes a\nbalance between compression ratio, accuracy, and computational overhead. Our\nframework has two major novel components: sparsification of gradients in the\nfrequency domain, and a range-based floating point representation to quantize\nand further compress gradients frequencies. Both components are dynamic, with\ntunable parameters that achieve different compression ratio based on the\naccuracy requirement and systems' platforms, and achieve very high throughput\non GPUs. We prove that our techniques guarantee the convergence with a\ndiminishing compression ratio. Our experiments show that the proposed\ncompression framework effectively improves the scalability of most popular\nneural networks on a 32 GPU cluster to the baseline of no compression, without\ncompromising the accuracy and convergence speed.\n\n**Published:** 2018-11-21\n\n\n\n#### 3. Anti-Oversmoothing in Deep Vision Transformers via the Fourier Domain Analysis: From Theory to Practice\n\n*From Search Query: efficient attention frequency domain neural networks*\n\n*Zhangyang Wang, Tianlong Chen, Wenqing Zheng, Peihao Wang*\n\n**Abstract:** Vision Transformer (ViT) has recently demonstrated promise in computer vision problems. However, unlike Convolutional Neural Networks (CNN), it is known that the performance of ViT saturates quickly with depth increasing, due to the observed attention collapse or patch uniformity. Despite a couple of empirical solutions, a rigorous framework studying on this scalability issue remains elusive. In this paper, we first establish a rigorous theory framework to analyze ViT features from the Fourier spectrum domain. We show that the self-attention mechanism inherently amounts to a low-pass filter, which indicates when ViT scales up its depth, excessive low-pass filtering will cause feature maps to only preserve their Direct-Current (DC) component. We then propose two straightforward yet effective techniques to mitigate the undesirable low-pass limitation. The first technique, termed AttnScale, decomposes a self-attention block into low-pass and high-pass components, then rescales and combines these two filters to produce an all-pass self-attention matrix. The second technique, termed FeatScale, re-weights feature maps on separate frequency bands to amplify the high-frequency signals. Both techniques are efficient and hyperparameter-free, while effectively overcoming relevant ViT training artifacts such as attention collapse and patch uniformity. By seamlessly plugging in our techniques to multiple ViT variants, we demonstrate that they consistently help ViTs benefit from deeper architectures, bringing up to 1.1% performance gains \"for free\" (e.g., with little parameter overhead). We publicly release our codes and pre-trained models at https://github.com/VITA-Group/ViT-Anti-Oversmoothing.\n\n**Published:** 2022-03-09\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model (LM) design by incorporating efficient attention mechanisms and frequency-domain operations, here are some key insights and relevant papers:\n\n## Combining Attention Mechanisms with Frequency-Domain Operations\n\n### Cross-Attention Mechanism in Frequency Domain\nThe papers on spectrum sensing and signal processing provide valuable insights into combining attention mechanisms with frequency-domain operations. For instance, the Time-Frequency Cross Fusion Network (TFCFN) described in and uses a cross-attention mechanism to fuse time-domain and frequency-domain features. This approach can be adapted to language models by treating different domains (e.g., time-domain sequence and frequency-domain representations) as separate modalities and using cross-attention to integrate them.\n\n### Efficient Frequency-Domain Operations\nThe analysis note highlights the efficiency gains from using Fourier transforms and matrix decomposition. For example, the FNet model mentioned in the analysis note replaces self-attention with a standard, unparameterized Fourier Transform, achieving significant speedups while maintaining high accuracy.\n\n## Efficient Implementations and Hardware Considerations\n\n### Hardware-Efficient FFT Operations\nThe FlashFFTConv method, as mentioned in the analysis note, uses matrix decomposition to compute FFTs efficiently, enabling kernel fusion and reducing I/O operations. This approach can be integrated into language models to speed up frequency-domain operations[Analysis Note].\n\n### Structured Matrices and Frequency-Domain Operations\nThe Monarch Mixer architecture, also mentioned, utilizes structured matrices that capture many linear transforms, including Fourier transforms. This can be beneficial for unified efficient operations across different dimensions in language models[Analysis Note].\n\n## Techniques for Efficient Gradient Computation and Adaptive Operations\n\n### Frequency-Domain Pruning and Compression\nDynamic pruning in the frequency domain, as suggested in the analysis note, can help balance compression and accuracy. This technique can be applied to reduce the computational cost of frequency-domain operations in language models while maintaining performance[Analysis Note].\n\n### Frequency Domain Forecasting (FreDF)\nThe FreDF method, which aligns predicted values and label sequences in the frequency domain, demonstrates how frequency-domain operations can reduce autocorrelation effects and improve model efficiency. This concept can be explored for autoregressive language models to enhance their forecasting capabilities and efficiency.\n\n## Conclusion\n\nTo improve autoregressive LM design, researchers can:\n\n- **Integrate Cross-Attention Mechanisms**: Use cross-attention to fuse time-domain and frequency-domain features, similar to TFCFN.\n- **Utilize Efficient FFT Operations**: Implement methods like FlashFFTConv to speed up frequency-domain operations using matrix decomposition and kernel fusion[Analysis Note].\n- **Employ Structured Matrices**: Leverage architectures like Monarch Mixer to perform efficient operations across multiple dimensions[Analysis Note].\n- **Implement Dynamic Pruning**: Apply dynamic pruning in the frequency domain to balance compression and accuracy[Analysis Note].\n- **Explore Frequency Domain Forecasting**: Adapt the FreDF method to reduce autocorrelation effects and enhance model efficiency in autoregressive language models.\n\nBy combining these approaches, researchers can develop more efficient, scalable, and accurate autoregressive language models.\n",
            "analysis": "Analysis Note: Fourth Round Research on LM Block Design\n\n1. Summary of Analysis\n\nThe fourth round of research revealed promising directions for efficient frequency-domain operations in neural networks, particularly around:\n\na) Efficient FFT Operations:\n- FlashFFTConv shows significant speedups for long sequences\n- Matrix decomposition enables better hardware utilization\n- Partial and frequency-sparse convolutions offer further optimization\n\nb) Hardware-Efficient Implementations:\n- IO-aware algorithms crucial for performance\n- Matrix multiply unit optimization important\n- Memory hierarchy considerations essential\n\nc) Frequency-Domain Pruning:\n- Dynamic pruning in frequency domain shows promise\n- Discriminative pruning across frequency bands\n- Balance between compression and accuracy\n\n2. Useful References with Excerpts\n\nFrom FlashFFTConv:\n\"FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O [...] speeds up exact FFT convolutions by up to 7.93\u00d7 over PyTorch\"\n\nThis suggests potential for significant efficiency gains through hardware-aware implementation.\n\nFrom Monarch Mixer:\n\"uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms\"\n\nShows potential for unified efficient operations across different dimensions.\n\nFrom FNet:\n\"replacing the self-attention sublayer with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts [...] trains 80% faster on GPUs\"\n\nDemonstrates viability of Fourier transforms as efficient alternatives to attention.\n\n3. Key Insights and Analysis\n\na) Hardware-Efficient FFT:\n- Matrix decomposition enables better use of specialized hardware\n- Kernel fusion reduces memory I/O\n- Partial computations can maintain accuracy while reducing cost\n\nb) Frequency-Domain Operations:\n- Unparameterized Fourier transforms can be surprisingly effective\n- Structured matrices offer efficiency across multiple dimensions\n- Balance between expressiveness and computation important\n\nc) Implementation Considerations:\n- Memory hierarchy optimization crucial\n- Matrix multiply unit utilization important\n- Trade-off between accuracy and efficiency needs careful management\n\nd) Potential Innovation Directions:\n- Combine efficient FFT with adaptive pruning\n- Integrate structured matrices with frequency-domain operations\n- Develop hardware-aware implementations of frequency-domain compression\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Methods for combining FFT with attention mechanisms\n2. Techniques for efficient gradient computation in frequency domain\n3. Approaches for adaptive frequency-domain operations\n4. Hardware-efficient implementations of structured matrices\n\n5. References\n\n[1] Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\". arXiv preprint.\n\n[2] Fu, D. Y., et al. (2023). \"Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\". Neural Information Processing Systems.\n\n[3] Lee-Thorp, J., Ainslie, J., Eckstein, I., & Onta\u00f1\u00f3n, S. (2021). \"Fnet: Mixing tokens with fourier transforms\". North American Chapter of the Association for Computational Linguistics.\n\n[4] Gu, A., et al. (2020). \"HiPPO: Recurrent Memory with Optimal Polynomial Projections\". Neural Information Processing Systems.\n\n[5] Lee, C., & Kim, H. S. (2023). \"Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\". International Conference on Learning Representations.\n\n[6] Poli, M., et al. (2022). \"Transform Once: Efficient Operator Learning in Frequency Domain\". Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "\"frequency domain hierarchical attention\", \"memory efficient FFT neural networks\", \"hybrid attention architecture stability\"",
            "detail": "Search for:\n1. Integration techniques for combining frequency-domain operations with hierarchical attention:\n   - Hybrid architectures\n   - Multi-scale processing\n   - Efficient implementations\n\n2. Memory optimization for FFT operations in neural networks:\n   - Matrix decomposition techniques\n   - Cache optimization strategies\n   - Bandwidth reduction methods\n\n3. Training stability analysis for hybrid architectures:\n   - Gradient flow analysis\n   - Convergence properties\n   - Stability techniques\n\n4. Hardware optimization for attention models:\n   - Parallel processing strategies\n   - Memory access patterns\n   - Computational efficiency",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for:\n1. Integration techniques for combining frequency-domain operations with hierarchical attention:\n   - Hybrid architectures\n   - Multi-scale processing\n   - Efficient implementations\n\n2. Memory optimization for FFT operations in neural networks:\n   - Matrix decomposition techniques\n   - Cache optimization strategies\n   - Bandwidth reduction methods\n\n3. Training stability analysis for hybrid architectures:\n   - Gradient flow analysis\n   - Convergence properties\n   - Stability techniques\n\n4. Hardware optimization for attention models:\n   - Parallel processing strategies\n   - Memory access patterns\n   - Computational efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.71)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.71)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 2. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (Avg. Score: 0.68)\n\n*Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 2*)\n\n**TL;DR:** A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n##### *Relevant Chunk: No. 4/67 (Score: 0.68)*\n\n```\n0}$ | $\\mathbf{9 . 6}$ | $\\mathbf{9 . 0}$ | Attention-Free, MLP-Free |\n\n## 6 Related Work\n\nLong Convolutions Recent work proposes to use long convolution layers as a replacement for the Transformer attention layers in sequence modeling [26,63, 66-68]. Many of these models rely on the FFT convolution theorem to compute the long convolutions. We build on the insights in many of these architectures in constructing our M2 architectures, and additionally replaces the FFT operations with Monarch matrices. Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev bases [79] or orthogonal polynomial bases [32]. These approaches have analogues in our multivariate analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be able to approximate some of these operations. An interesting question for future work would be to study how well our techniques and concerns about causality and hardware utilization translate to these alternative convolution bases. Optimization of deep learning primitives There is a rich history of the optimization of deep learning primitives, as accelerating their performance can yield substantial savings in compute and cost for large models. There are many approaches to speed up these operations, but they usually either reduce data movement or compute. Reducing data movement: In many applications, the major bottleneck is the storage and movement of large amounts of memory. One popular approach to reducing data movement is checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory $[44,76]$. Another approach is kernel fusion, wherein algorithms initially described as sequential steps can often be fused in ways that improve their properties. For example, it is generally faster to implement a dot-product through a multiply-accumulate rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [62] have added kernel fusion capabilities, although the very best performance usually still arises from\nhandwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load small blocks of memory, do intensive computation on them, and then write the results a tile at a time [80]. Finally, many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [53]. Efficient algorithms usually make use of a combination of these techniques. For example, FlashAttention [13] uses all four to dramatically decrease both the latency and memory consumption of multi-head attention. Though we have made a modest effort to implement Monarch Mixer efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques. Reducing flops: A first target for optimization is the multi-layer perceptron (MLP), owing to its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this work $[5,9,12,14,15,17,24,88]$. Attention is also a popular target for optimization. Recently, a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the fact that the attention matrix is extremely sparse at long sequence lengths [2, 21, 22, 40, 51]. Others use low-rank approximations of the attention matrix [11, 77, 88] or kernel methods instead [7, 39]. A subset use a combination of these techniques, such as $[6,71]$. Finally, a third category of methods $[25,63]$ aim to replace attention entirely, relying on state-space models [31]. ## 7 Discussion and Conclusion\n\nWe explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence length and model dimension and is hardware-efficient on modern accelerators. We motivate M2 from both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept investigation into performance on masked language modeling, image classification, and causal language modeling. While our initial results are promising, our work is only a first step in this direction. The M2 layer can likely be further optimized with systems optimization techniques such as kernel fusion. Our work has also not been optimized for inference like more well-established models such as Transformers, or even more recent models such as state space models. It also remains to be seen whether M2 layers can have as widespread applicability as Transformers. We hope that these can be fruitful directions for future work. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.\n```\n\n#### 3. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.21)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 4/32 (Score: 0.21)*\n\n```\nA primary line of work addresses the training efficiency problem. Given the attention to computation's quadratic complexity during training, developing a long-context LLM is both a computational and memory challenge. Solutions have ranged from system-focused optimizations like FlashAttention (Dao et al., 2022; Dao, 2023), which accelerates attention computation and reduces memory footprint, to approximate attention methods (Zaheer et al.\n```\n\n#### 4. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.17)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 25/29 (Score: 0.17)*\n\n```\nArXiv, abs/2405.05254, 2024b. URL https://api. semanticscholar org/CorpusID:269626143. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention in transformer models, 2021a. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview net/forum?id=qVyeW-grC2k\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers \\& distillation through attention. In International Conference on Machine Learning, volume 139, pp. 10347-10357, July 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Jos van der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. CoRR, abs/1804.04849, 2018. Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M. Rush. Pretraining without attention. CoRR, abs/2212.10544, 2022. Songlin Yang and Yu Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, January 2024. URL https://github.com/ sustcsonglin/flash-linear-attention\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. CoRR, abs/2312.06635, 2023. doi: 10.48550/ARXIV.2312.06635. URL https://doi.org/10.48550/arXiv.2312.06635. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length.\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.10)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.10)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"frequency domain hierarchical attention\", \"memory efficient FFT neural networks\", \"hybrid attention architecture stability\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System\n\n*From Search Query: \"frequency domain hierarchical attention\"*\n\n*Tong Zhang, Yong Liu, Boyang Albert Li, Zhiwei Zeng, Pengwei Wang, Yuan You, Chun Miao, Li-zhen Cui*\n\n**TL;DR:** Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models and human evaluation results support that HAHT generates more human-like, context-relevant and history-relevant responses than baseline models.\n\n**Abstract:** With the evolution of pre-trained language models, current open-domain dialogue systems have achieved great progress in conducting one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is under-investigated. In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for multi-session open-domain dialogue. HAHT maintains a long-term memory of history conversations and utilizes history information to understand current conversation context and generate well-informed and context-relevant responses. Specifically, HAHT first encodes history conversation sessions hierarchically into a history memory. Then, HAHT leverages historical information to facilitate the understanding of the current conversation context by encoding the history memory together with the current context with attention-based mechanisms. Finally, to explicitly utilize historical information, HAHT uses a history-aware response generator that switches between a generic vocabulary and a history-aware vocabulary. Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models. Human evaluation results support that HAHT generates more human-like, context-relevant and history-relevant responses than baseline models.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 1*)\n\n#### 2. Data Augmentation with Hierarchical SQL-to-Question Generation for Cross-domain Text-to-SQL Parsing\n\n*From Search Query: \"frequency domain hierarchical attention\"*\n\n*Ao Zhang, Kun Wu, Lijie Wang, Zhenghua Li, Xinyan Xiao, Hua Wu, Min Zhang, Haifeng Wang*\n\n**TL;DR:** Experiments on three cross-domain datasets show that the proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.\n\n**Abstract:** Data augmentation has attracted a lot of research attention in the deep learning era for its ability in alleviating data sparseness. The lack of labeled data for unseen evaluation databases is exactly the major challenge for cross-domain text-to-SQL parsing. Previous works either require human intervention to guarantee the quality of generated data, or fail to handle complex SQL queries. This paper presents a simple yet effective data augmentation framework. First, given a database, we automatically produce a large number of SQL queries based on an abstract syntax tree grammar. For better distribution matching, we require that at least 80% of SQL patterns in the training data are covered by generated queries. Second, we propose a hierarchical SQL-to-question generation model to obtain high-quality natural language questions, which is the major contribution of this work. Finally, we design a simple sampling strategy that can greatly improve training efficiency given large amounts of generated data. Experiments on three cross-domain datasets, i.e., WikiSQL and Spider in English, and DuSQL in Chinese, show that our proposed data augmentation framework can consistently improve performance over strong baselines, and the hierarchical generation component is the key for the improvement.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 25  (*Influential: 4*)\n\n#### 3. Hierarchical Neural Operator Transformer with Learnable Frequency-aware Loss Prior for Arbitrary-scale Super-resolution\n\n*From Search Query: \"frequency domain hierarchical attention\"*\n\n*Xihaier Luo, Xiaoning Qian, Byung-Jun Yoon*\n\n**Abstract:** In this work, we present an arbitrary-scale super-resolution (SR) method to enhance the resolution of scientific data, which often involves complex challenges such as continuity, multi-scale physics, and the intricacies of high-frequency signals. Grounded in operator learning, the proposed method is resolution-invariant. The core of our model is a hierarchical neural operator that leverages a Galerkin-type self-attention mechanism, enabling efficient learning of mappings between function spaces. Sinc filters are used to facilitate the information transfer across different levels in the hierarchy, thereby ensuring representation equivalence in the proposed neural operator. Additionally, we introduce a learnable prior structure that is derived from the spectral resizing of the input data. This loss prior is model-agnostic and is designed to dynamically adjust the weighting of pixel contributions, thereby balancing gradients effectively across the model. We conduct extensive experiments on diverse datasets from different domains and demonstrate consistent improvements compared to strong baselines, which consist of various state-of-the-art SR methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. MEC: Memory-efficient Convolution for Deep Neural Network\n\n*From Search Query: \"memory efficient FFT neural networks\"*\n\n*Minsik Cho, D. Brand*\n\n**TL;DR:** This work proposes a memory-efficient convolution or MEC with compact lowering, which reduces memory-overhead substantially and accelerates convolution process and reduces memory consumption significantly with good speedup on both mobile and server platforms.\n\n**Abstract:** Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2col-based convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memory-overhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory sub-system efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 80  (*Influential: 15*)\n\n#### 5. Joint Edge-Model Sparse Learning is Provably Efficient for Graph Neural Networks\n\n*From Search Query: \"memory efficient FFT neural networks\"*\n\n*Shuai Zhang, M. Wang, Pin-Yu Chen, Sijia Liu, Songtao Lu, Miaoyuan Liu*\n\n**TL;DR:** This paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error and proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexityand improve convergence without compromising the test accuracy.\n\n**Abstract:** Due to the significant computational challenge of training large-scale graph neural networks (GNNs), various sparse learning techniques have been exploited to reduce memory and storage costs. Examples include \\textit{graph sparsification} that samples a subgraph to reduce the amount of data aggregation and \\textit{model sparsification} that prunes the neural network to reduce the number of trainable weights. Despite the empirical successes in reducing the training cost while maintaining the test accuracy, the theoretical generalization analysis of sparse learning for GNNs remains elusive. To the best of our knowledge, this paper provides the first theoretical characterization of joint edge-model sparse learning from the perspective of sample complexity and convergence rate in achieving zero generalization error. It proves analytically that both sampling important nodes and pruning neurons with the lowest-magnitude can reduce the sample complexity and improve convergence without compromising the test accuracy. Although the analysis is centered on two-layer GNNs with structural constraints on data, the insights are applicable to more general setups and justified by both synthetic and practical citation datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 4*)\n\n#### 6. Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy\n\n*From Search Query: \"memory efficient FFT neural networks\"*\n\n*Zhiqi Bu, J. Mao, Shiyun Xu*\n\n**TL;DR:** An efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, is proposed that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy.\n\n**Abstract:** Large convolutional neural networks (CNN) can be difficult to train in the differentially private (DP) regime, since the optimization algorithms require a computationally expensive operation, known as the per-sample gradient clipping. We propose an efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy. The improvement in efficiency is rigorously studied through the first complexity analysis for the mixed ghost clipping and existing DP training algorithms. Extensive experiments on vision classification tasks, with large ResNet, VGG, and Vision Transformers, demonstrate that DP training with mixed ghost clipping adds $1\\sim 10\\%$ memory overhead and $<2\\times$ slowdown to the standard non-private training. Specifically, when training VGG19 on CIFAR10, the mixed ghost clipping is $3\\times$ faster than state-of-the-art Opacus library with $18\\times$ larger maximum batch size. To emphasize the significance of efficient DP training on convolutional layers, we achieve 96.7\\% accuracy on CIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous best results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy engine (\\url{https://github.com/woodyx218/private_vision}) that implements DP training of CNN with a few lines of code.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 37  (*Influential: 4*)\n\n#### 7. Transformers as Algorithms: Generalization and Stability in In-context Learning\n\n*From Search Query: \"hybrid attention architecture stability\"*\n\n*Yingcong Li, M. E. Ildiz, Dimitris Papailiopoulos, Samet Oymak*\n\n**TL;DR:** This work formalizes in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time and identifies an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner.\n\n**Abstract:** In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 121  (*Influential: 6*)\n\n#### 8. HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction\n\n*From Search Query: \"hybrid attention architecture stability\"*\n\n*Zalan Fabian, M. Soltanolkotabi*\n\n**TL;DR:** HUMUS-Net is proposed, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network that establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset.\n\n**Abstract:** In accelerated MRI reconstruction, the anatomy of a patient is recovered from a set of under-sampled and noisy measurements. Deep learning approaches have been proven to be successful in solving this ill-posed inverse problem and are capable of producing very high quality reconstructions. However, current architectures heavily rely on convolutions, that are content-independent and have difficulties modeling long-range dependencies in images. Recently, Transformers, the workhorse of contemporary natural language processing, have emerged as powerful building blocks for a multitude of vision tasks. These models split input images into non-overlapping patches, embed the patches into lower-dimensional tokens and utilize a self-attention mechanism that does not suffer from the aforementioned weaknesses of convolutional architectures. However, Transformers incur extremely high compute and memory cost when 1) the input image resolution is high and 2) when the image needs to be split into a large number of patches to preserve fine detail information, both of which are typical in low-level vision problems such as MRI reconstruction, having a compounding effect. To tackle these challenges, we propose HUMUS-Net, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network. HUMUS-Net extracts high-resolution features via convolutional blocks and refines low-resolution features via a novel Transformer-based multi-scale feature extractor. Features from both levels are then synthesized into a high-resolution output reconstruction. Our network establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset. We further demonstrate the performance of HUMUS-Net on two other popular MRI datasets and perform fine-grained ablation studies to validate our design.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 34  (*Influential: 3*)\n\n#### 9. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: \"hybrid attention architecture stability\"*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Relational Graph Representation Learning for Open-Domain Question\n  Answering\n\n*From Search Query: \"frequency domain hierarchical attention\"*\n\n*Salvatore Vivona, Kaveh Hassani*\n\n**Abstract:** We introduce a relational graph neural network with bi-directional attention\nmechanism and hierarchical representation learning for open-domain question\nanswering task. Our model can learn contextual representation by jointly\nlearning and updating the query, knowledge graph, and document representations.\nThe experiments suggest that our model achieves state-of-the-art on the\nWebQuestionsSP benchmark.\n\n**Published:** 2019-10-18T03:54:58Z  (*Updated: 2019-10-18T03:54:58Z*)\n\n\n\n#### 2. Which Kind Is Better in Open-domain Multi-turn Dialog,Hierarchical or\n  Non-hierarchical Models? An Empirical Study\n\n*From Search Query: \"frequency domain hierarchical attention\"*\n\n*Tian Lan, Xian-Ling Mao, Wei Wei, Heyan Huang*\n\n**Abstract:** Currently, open-domain generative dialog systems have attracted considerable\nattention in academia and industry. Despite the success of single-turn dialog\ngeneration, multi-turn dialog generation is still a big challenge. So far,\nthere are two kinds of models for open-domain multi-turn dialog generation:\nhierarchical and non-hierarchical models. Recently, some works have shown that\nthe hierarchical models are better than non-hierarchical models under their\nexperimental settings; meanwhile, some works also demonstrate the opposite\nconclusion. Due to the lack of adequate comparisons, it's not clear which kind\nof models are better in open-domain multi-turn dialog generation. Thus, in this\npaper, we will measure systematically nearly all representative hierarchical\nand non-hierarchical models over the same experimental settings to check which\nkind is better. Through extensive experiments, we have the following three\nimportant conclusions: (1) Nearly all hierarchical models are worse than\nnon-hierarchical models in open-domain multi-turn dialog generation, except for\nthe HRAN model. Through further analysis, the excellent performance of HRAN\nmainly depends on its word-level attention mechanism; (2) The performance of\nother hierarchical models will also obtain a great improvement if integrating\nthe word-level attention mechanism into these models. The modified hierarchical\nmodels even significantly outperform the non-hierarchical models; (3) The\nreason why the word-level attention mechanism is so powerful for hierarchical\nmodels is because it can leverage context information more effectively,\nespecially the fine-grained information. Besides, we have implemented all of\nthe models and already released the codes.\n\n**Published:** 2020-08-07T02:54:55Z  (*Updated: 2020-08-07T02:54:55Z*)\n\n\n\n#### 3. Improving Neural Language Models with a Continuous Cache\n\n*From Search Query: \"memory efficient FFT neural networks\"*\n\n*Edouard Grave, Armand Joulin, Nicolas Usunier*\n\n**Abstract:** We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.\n\n**Published:** 2016-12-13T23:09:49Z  (*Updated: 2016-12-13T23:09:49Z*)\n\n\n\n#### 4. Learning to Ignore: Long Document Coreference with Bounded Memory Neural\n  Networks\n\n*From Search Query: \"memory efficient FFT neural networks\"*\n\n*Shubham Toshniwal, Sam Wiseman, Allyson Ettinger, Karen Livescu, Kevin Gimpel*\n\n**Abstract:** Long document coreference resolution remains a challenging task due to the\nlarge memory and runtime requirements of current models. Recent work doing\nincremental coreference resolution using just the global representation of\nentities shows practical benefits but requires keeping all entities in memory,\nwhich can be impractical for long documents. We argue that keeping all entities\nin memory is unnecessary, and we propose a memory-augmented neural network that\ntracks only a small bounded number of entities at a time, thus guaranteeing a\nlinear runtime in length of document. We show that (a) the model remains\ncompetitive with models with high memory and computational requirements on\nOntoNotes and LitBank, and (b) the model learns an efficient memory management\nstrategy easily outperforming a rule-based strategy.\n\n**Published:** 2020-10-06T15:16:31Z  (*Updated: 2020-11-17T02:31:30Z*)\n\n\n\n#### 5. Zoology: Measuring and Improving Recall in Efficient Language Models\n\n*From Search Query: \"hybrid attention architecture stability\"*\n\n*Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** Attention-free language models that combine gating and convolutions are\ngrowing in popularity due to their efficiency and increasingly competitive\nperformance. To better understand these architectures, we pretrain a suite of\n17 attention and \"gated-convolution\" language models, finding that SoTA\ngated-convolution architectures still underperform attention by up to 2.1\nperplexity points on the Pile. In fine-grained analysis, we find 82% of the gap\nis explained by each model's ability to recall information that is previously\nmentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it\nmeans no\" $\\rightarrow$ \"??\". On this task, termed \"associative recall\", we\nfind that attention outperforms gated-convolutions by a large margin: a 70M\nparameter attention model outperforms a 1.4 billion parameter gated-convolution\nmodel on associative recall. This is surprising because prior work shows gated\nconvolutions can perfectly solve synthetic tests for AR capability. To close\nthe gap between synthetics and real language, we develop a new formalization of\nthe task called multi-query associative recall (MQAR) that better reflects\nactual language. We perform an empirical and theoretical study of MQAR that\nelucidates differences in the parameter-efficiency of attention and\ngated-convolution recall. Informed by our analysis, we evaluate simple\nconvolution-attention hybrids and show that hybrids with input-dependent sparse\nattention patterns can close 97.4% of the gap to attention, while maintaining\nsub-quadratic scaling. Our code is accessible at:\nhttps://github.com/HazyResearch/zoology.\n\n**Published:** 2023-12-08T09:44:25Z  (*Updated: 2023-12-08T09:44:25Z*)\n\n\n\n#### 6. BERT-Based Combination of Convolutional and Recurrent Neural Network for\n  Indonesian Sentiment Analysis\n\n*From Search Query: \"hybrid attention architecture stability\"*\n\n*Hendri Murfi, Syamsyuriani, Theresia Gowandi, Gianinna Ardaneswari, Siti Nurrohmah*\n\n**Abstract:** Sentiment analysis is the computational study of opinions and emotions\nex-pressed in text. Deep learning is a model that is currently producing\nstate-of-the-art in various application domains, including sentiment analysis.\nMany researchers are using a hybrid approach that combines different deep\nlearning models and has been shown to improve model performance. In sentiment\nanalysis, input in text data is first converted into a numerical\nrepresentation. The standard method used to obtain a text representation is the\nfine-tuned embedding method. However, this method does not pay attention to\neach word's context in the sentence. Therefore, the Bidirectional Encoder\nRepresentation from Transformer (BERT) model is used to obtain text\nrepresentations based on the context and position of words in sentences. This\nresearch extends the previous hybrid deep learning using BERT representation\nfor Indonesian sentiment analysis. Our simulation shows that the BERT\nrepresentation improves the accuracies of all hybrid architectures. The\nBERT-based LSTM-CNN also reaches slightly better accuracies than other\nBERT-based hybrid architectures.\n\n**Published:** 2022-11-10T00:32:40Z  (*Updated: 2022-11-10T00:32:40Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection\n\n*From Search Query: \"frequency domain hierarchical attention\"*\n\n*Arjan Kuijper, Florian Kirchbuchner, Naser Damer, Meiling Fang*\n\n**Abstract:** With the increased deployment of face recognition systems in our daily lives, face presentation attack detection (PAD) is attracting much attention and playing a key role in securing face recognition systems. Despite the great performance achieved by the hand-crafted and deep-learning-based methods in intra-dataset evaluations, the performance drops when dealing with unseen scenarios. In this work, we propose a dual-stream convolution neural networks (CNNs) framework. One stream adapts four learnable frequency filters to learn features in the frequency domain, which are less influenced by variations in sensors/illuminations. The other stream leverages the RGB images to complement the features of the frequency domain. Moreover, we propose a hierarchical attention module integration to join the information from the two streams at different stages by considering the nature of deep features in different layers of the CNN. The proposed method is evaluated in the intra-dataset and cross-dataset setups, and the results demonstrate that our proposed approach enhances the generalizability in most experimental setups in comparison to state-of-the-art, including the methods designed explicitly for domain adaption/shift problems. We successfully prove the design of our proposed PAD solution in a step-wise ablation study that involves our proposed learnable frequency decomposition, our hierarchical attention module design, and the used loss function. Training codes and pre-trained models are publicly released\n\n**Published:** 2021-09-16\n\n\n\n#### 2. TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis\n\n*From Search Query: \"frequency domain hierarchical attention\"*\n\n*Ming Jin, Zhixuan Chu, Shengtong Ju, Wenze Lin, Baichuan Mo, Zhou Ye, Xiaoming Shi, Jiawei Li, Shiyu Wang*\n\n**Abstract:** Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. This method achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis.\n\n**Published:** 2024-10-21\n\n\n\n#### 3. Ensemble learning for alcoholism classification using EEG signals\n\n*From Search Query: \"memory efficient FFT neural networks\"*\n\n*Lior Rokach, Ofir Arbili, Dan Presil, Or Katz, Seffi Cohen*\n\n**Abstract:** Excessive drinking is a major risk factor that leads to many health complications. The diagnosis of alcoholism is challenging, especially when the standard diagnostic tests rely on blood tests and questionnaires that are subjective to the patient and the examiner. The study\u2019s major goal is to find new electroencephalography (EEG) classification methods to improve past findings and construct a robust EEG classification algorithm to generate accurate predictions with explainable results. The EEG records were examined from two different perspectives and combined with an ensemble of classification models. The first approach was temporal data, and the second was images derived from the original signals. Using fast Fourier transform (FFT) and independent component analysis (ICA), we convert 64-channel temporal data into images along with applying the Symbolic Aggregate approXimation (SAX) technique. Our model combines input data in tabular, temporal, and image formats with an ensemble of linear neural networks, long short-term memory (LSTM), and efficient-net classification models. We have evaluated our method using a publicly available dataset for EEG classification of alcoholic and nonalcoholic subjects. Overall, our algorithm\u2019s highest cross-validation classification accuracy is 85.52% compared to the state-of-the-art EEG-NET\u2019s accuracy of 81.19%\n\n**Proceeding:** ieee-sensors-2023-6\n\n**Published:** 2023-06-17\n\n\n\n#### 4. AFS: An Attention-based mechanism for Supervised Feature Selection\n\n*From Search Query: \"hybrid attention architecture stability\"*\n\n*Danni Ge, Ning Gui, Ziyin Hu*\n\n**Abstract:** As an effective data preprocessing step, feature selection has shown its\neffectiveness to prepare high-dimensional data for many machine learning tasks.\nThe proliferation of high di-mension and huge volume big data, however, has\nbrought major challenges, e.g. computation complexity and stability on noisy\ndata, upon existing feature-selection techniques. This paper introduces a novel\nneural network-based feature selection architecture, dubbed Attention-based\nFeature Selec-tion (AFS). AFS consists of two detachable modules: an at-tention\nmodule for feature weight generation and a learning module for the problem\nmodeling. The attention module for-mulates correlation problem among features\nand supervision target into a binary classification problem, supported by a\nshallow attention net for each feature. Feature weights are generated based on\nthe distribution of respective feature se-lection patterns adjusted by\nbackpropagation during the train-ing process. The detachable structure allows\nexisting off-the-shelf models to be directly reused, which allows for much less\ntraining time, demands for the training data and requirements for expertise. A\nhybrid initialization method is also intro-duced to boost the selection\naccuracy for datasets without enough samples for feature weight generation.\nExperimental results show that AFS achieves the best accuracy and stability in\ncomparison to several state-of-art feature selection algo-rithms upon both\nMNIST, noisy MNIST and several datasets with small samples.\n\n**Published:** 2019-02-28\n\n\n\n#### 5. Temporal Consistency Learning of inter-frames for Video Super-Resolution\n\n*From Search Query: \"hybrid attention architecture stability\"*\n\n*Yao Zhao, Chunyu Lin, Chao Yao, Shuo Jin, Meiqin Liu*\n\n**Abstract:** Video super-resolution (VSR) is a task that aims to reconstruct high-resolution (HR) frames from the low-resolution (LR) reference frame and multiple neighboring frames. The vital operation is to utilize the relative misaligned frames for the current frame reconstruction and preserve the consistency of the results. Existing methods generally explore information propagation and frame alignment to improve the performance of VSR. However, few studies focus on the temporal consistency of inter-frames. In this paper, we propose a Temporal Consistency learning Network (TCNet) for VSR in an end-to-end manner, to enhance the consistency of the reconstructed videos. A spatio-temporal stability module is designed to learn the self-alignment from inter-frames. Especially, the correlative matching is employed to exploit the spatial dependency from each frame to maintain structural stability. Moreover, a self-attention mechanism is utilized to learn the temporal correspondence to implement an adaptive warping operation for temporal consistency among multi-frames. Besides, a hybrid recurrent architecture is designed to leverage short-term and long-term information. We further present a progressive fusion module to perform a multistage fusion of spatio-temporal features. And the final reconstructed frames are refined by these fused features. Objective and subjective results of various experiments demonstrate that TCNet has superior performance on different benchmark datasets, compared to several state-of-the-art methods.\n\n**Published:** 2022-11-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the provided keywords and details, here are some key insights and potential integration techniques:\n\n### Integration Techniques for Combining Frequency-Domain Operations with Hierarchical Attention\n\n#### Hybrid Architectures\n- Combining frequency-domain operations with hierarchical attention can be achieved through hybrid architectures. For instance, the H-Transformer-1D uses a hierarchical attention mechanism that exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix), which has linear runtime and memory complexity. This approach can be integrated with frequency-domain operations to capture both local and global contextual information efficiently.\n\n#### Multi-scale Processing\n- Multi-scale processing can be implemented using hierarchical attention mechanisms that operate at different levels. This is similar to the approach in the \"Hierarchical Fusion of Infrared and Visible Images\" where features are integrated across different levels using a hierarchical multi-scale cross-attention feature fusion module. This can help in capturing a wide range of frequencies and contextual dependencies.\n\n#### Efficient Implementations\n- Efficient implementations can be achieved through techniques like matrix decomposition and kernel fusion. For example, FlashFFTConv uses matrix decomposition to compute FFTs using matrix multiply units and enables kernel fusion for long sequences, reducing I/O and memory bandwidth.\n\n### Memory Optimization for FFT Operations in Neural Networks\n\n#### Matrix Decomposition Techniques\n- Matrix decomposition techniques, as used in FlashFFTConv, can significantly reduce the computational and memory requirements for FFT operations. This involves breaking down the FFT computation into matrix multiplications, which can be more efficiently handled by modern hardware.\n\n#### Cache Optimization Strategies\n- Block-based processing, as seen in the Block Transformer, can improve cache utilization by isolating expensive global modeling to lower layers and applying fast local modeling in upper layers. This strategy helps in reducing memory access latency and improving overall efficiency.\n\n#### Bandwidth Reduction Methods\n- Kernel fusion, as mentioned earlier, reduces memory bandwidth by combining multiple operations into a single kernel. This approach can be particularly beneficial for FFT operations in neural networks, where repeated memory accesses can be a significant bottleneck.\n\n### Training Stability Analysis for Hybrid Architectures\n\n#### Gradient Flow Analysis\n- Training stability in hybrid architectures can be improved through frequency-aware training strategies. For example, progressive exposure of frequencies, as in the SAPE method, helps in stabilizing the training process by gradually introducing different frequency components.\n\n#### Convergence Properties\n- Spectral normalization can help control parameter growth and ensure convergence. This technique is crucial in hybrid architectures where the combination of different components can lead to unstable training dynamics.\n\n#### Stability Techniques\n- Hierarchical attention mechanisms can also contribute to training stability by reducing the complexity of the model. By breaking down the attention process into hierarchical levels, the model can better manage the flow of gradients and reduce the risk of vanishing or exploding gradients.\n\n### Hardware Optimization for Attention Models\n\n#### Parallel Processing Strategies\n- Parallel processing strategies can significantly improve the computational efficiency of attention models. By distributing the computation across multiple processing units, the model can leverage the parallel processing capabilities of modern hardware, such as GPUs and TPUs.\n\n#### Memory Access Patterns\n- Optimizing memory access patterns is crucial for efficient computation. Techniques like block-based processing and kernel fusion help in reducing memory access latency and improving cache utilization, which are essential for maintaining high performance in attention models.\n\n#### Computational Efficiency\n- Computational efficiency can be enhanced by using hierarchical attention mechanisms that reduce the computational complexity. For instance, the H-Transformer-1D achieves linear runtime and memory complexity, making it more efficient for large-scale sequences.\n\n### Summary of Insights\n\n- **Hybrid Architectures**: Combine frequency-domain operations with hierarchical attention to capture both local and global contextual information.\n- **Memory Optimization**: Use matrix decomposition, kernel fusion, and block-based processing to reduce memory bandwidth and improve cache utilization.\n- **Training Stability**: Employ frequency-aware training strategies, spectral normalization, and hierarchical attention to ensure stable training dynamics.\n- **Hardware Optimization**: Leverage parallel processing, optimize memory access patterns, and use computationally efficient hierarchical attention mechanisms.\n\nBy integrating these techniques, researchers can design more efficient, stable, and scalable autoregressive language models that outperform existing state-of-the-art models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about frequency-domain operations, hierarchical attention, and memory optimization in neural networks. Key findings include:\n\na) Frequency-Domain Operations:\n- FlashFFTConv demonstrates efficient FFT implementations using matrix decomposition and kernel fusion\n- Frequency-domain operations can be effectively combined with attention mechanisms\n- Memory access patterns and I/O optimization are crucial for performance\n\nb) Hierarchical Processing:\n- H-Transformer-1D shows promising results with hierarchical attention\n- Block Transformer demonstrates effective global-to-local modeling\n- Memory and computational trade-offs need careful consideration\n\nc) Training Stability:\n- Frequency-aware training strategies improve stability\n- Spectral normalization helps control parameter growth\n- Progressive exposure of frequencies can aid training\n\n2. Useful References with Key Excerpts:\n\nFrom FlashFFTConv:\n\"FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.\"\n\nFrom H-Transformer-1D:\n\"The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix)... and has linear run time and memory complexity.\"\n\nFrom Block Transformer:\n\"We notice that these costs stem from applying self-attention on the global context, therefore we isolate the expensive bottlenecks of global modeling to lower layers and apply fast local modeling in upper layers.\"\n\nFrom SAPE:\n\"A spatially adaptive progressive encoding scheme... enables them to better fit a wide range of frequencies without sacrificing training stability.\"\n\n3. Key Insights:\n\na) Architectural Design:\n- Matrix decomposition for efficient FFT computation\n- Hierarchical processing can effectively manage complexity\n- Global-to-local modeling improves efficiency\n- Progressive frequency exposure aids training\n\nb) Memory Optimization:\n- Kernel fusion reduces memory bandwidth\n- Block-based processing improves cache utilization\n- Hierarchical attention reduces memory requirements\n\nc) Training Considerations:\n- Frequency-aware training improves stability\n- Progressive exposure of frequencies helps convergence\n- Spectral normalization controls parameter growth\n\n4. Future Search Plan:\n\nNeed more information about:\n1. Integration of frequency-domain operations with hierarchical attention\n2. Memory-efficient implementations of FFT operations\n3. Training stability in hybrid architectures\n4. Hardware optimization techniques\n\nProposed search queries:\n- \"frequency domain hierarchical attention\"\n- \"memory efficient FFT neural networks\"\n- \"hybrid attention architecture stability\"\n- \"hardware optimization attention models\"\n\n5. References:\n\n[1] Fu, D. Y., et al. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\". arXiv preprint.\n\n[2] Zhu, Z., & Soricut, R. (2021). \"H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences\". arXiv preprint.\n\n[3] Yun, S. Y., et al. (2024). \"Block Transformer: Global-to-Local Language Modeling for Fast Inference\". arXiv preprint.\n\n[4] Hertz, A., et al. (2021). \"SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization\". Neural Information Processing Systems.\n\n[5] Wang, X., & Zhang, M. (2022). \"How Powerful are Spectral Graph Neural Networks\". International Conference on Machine Learning.\n\n[6] Lee, H., et al. (2024). \"A Training-free Sub-quadratic Cost Transformer Model Serving Framework With Hierarchically Pruned Attention\". arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"frequency domain attention\", \"FFT neural networks\", \"hierarchical attention\"",
            "detail": "Search for:\n1. Implementations and analysis of frequency-domain operations in attention mechanisms\n2. Performance comparisons between time-domain and frequency-domain attention\n3. Hardware optimization techniques for FFT operations in neural networks\n4. Memory efficiency analysis of frequency-domain transformations\n5. Training stability considerations for FFT-based neural networks\n6. Integration of hierarchical processing with frequency-domain operations\n7. Adaptive filtering techniques in neural networks",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for:\n1. Implementations and analysis of frequency-domain operations in attention mechanisms\n2. Performance comparisons between time-domain and frequency-domain attention\n3. Hardware optimization techniques for FFT operations in neural networks\n4. Memory efficiency analysis of frequency-domain transformations\n5. Training stability considerations for FFT-based neural networks\n6. Integration of hierarchical processing with frequency-domain operations\n7. Adaptive filtering techniques in neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.32)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.59)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 14/46 (Score: 0.05)*\n\n```\narXiv preprint arXiv:2204.02311, 2022. [18] Eleanor Chu and Alan George. Inside the FFT black box: serial and parallel fast Fourier transform algorithms. CRC press, 1999. [19] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965. [20] Fiona Cunningham, James E Allen, Jamie Allen, Jorge Alvarez-Jarreta, M Ridwan Amode, Irina M Armean, Olanrewaju Austine-Orimoloye, Andrey G Azov, If Barnes, Ruth Bennett, et al. Ensembl 2022. Nucleic acids research, 50(D1):D988-D995, 2022. [21] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, $33: 4271-4282,2020$\n[22] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [23] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training.\n```\n\n#### 2. Fnet: Mixing tokens with fourier transforms (Avg. Score: 0.09)\n\n*J. Lee-Thorp, J. Ainslie, Ilya Eckstein, Santiago Onta\u00f1\u00f3n*\n\n**Published in:** North American Chapter of the Association for Computational Linguistics (2021)\t**Cited by** 400  (*Influential: 56*)\n\n**TL;DR:** The FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\n\n**Abstract:** We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.\n\n##### *Relevant Chunk: No. 12/38 (Score: 0.09)*\n\n```\nHazem M El-Bakry and Qiangfu Zhao. 2004. Fast object/face detection using neural networks and fast fourier transform. International Journal of Signal Processing, 1(3):182-187. Matteo Frigo and Steven G Johnson. 2005. The design and implementation of fftw3. Proceedings of the IEEE, 93(2):216-231. Kfir Goldberg, Stav Shapiro, Elad Richardson, and Shai Avidan. 2020. Rethinking fun: Frequencydomain utilization networks. arXiv preprint arXiv:2012.03357.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.05)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 44/74 (Score: 0.05)*\n\n```\nIn: Advances in Neural Information Processing Systems (NeurIPS). 2023. [69] Shahar Lutati, Itamar Zimerman, and Lior Wolf. \"Focus Your Attention (with Adaptive IIR Filters)\". In: arXiv preprint arXiv:2305.14952 (2023). [70] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. \"Mega: Moving Average Equipped Gated Attention\". In: The International Conference on Learning Representations (ICLR). 2023. [71] Eric Martin and Chris Cundy. \"Parallelizing Linear Recurrent Neural Nets Over Sequence Length\". In: The International Conference on Learning Representations (ICLR). 2018. [72] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville, and Yoshua Bengio. \"SampleRNN: An Unconditional End-to-End Neural Audio Generation Model\".\n```\n\n#### 4. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.05)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 25/29 (Score: 0.05)*\n\n```\nArXiv, abs/2405.05254, 2024b. URL https://api. semanticscholar org/CorpusID:269626143. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention in transformer models, 2021a. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview net/forum?id=qVyeW-grC2k\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers \\& distillation through attention. In International Conference on Machine Learning, volume 139, pp. 10347-10357, July 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Jos van der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. CoRR, abs/1804.04849, 2018. Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M. Rush. Pretraining without attention. CoRR, abs/2212.10544, 2022. Songlin Yang and Yu Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, January 2024. URL https://github.com/ sustcsonglin/flash-linear-attention\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. CoRR, abs/2312.06635, 2023. doi: 10.48550/ARXIV.2312.06635. URL https://doi.org/10.48550/arXiv.2312.06635. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"frequency domain attention\", \"FFT neural networks\", \"hierarchical attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Selective Frequency Network for Image Restoration\n\n*From Search Query: \"frequency domain attention\"*\n\n*Yuning Cui, Y. Tao, Zhenshan Bing, Wenqi Ren, Xinwei Gao, Xiaochun Cao, Kai Huang, Alois Knoll*\n\n**TL;DR:** A multi-branch and content-aware module is exploited to decompose features into separate frequency subbands dynamically and locally, and then accentuate the useful ones via channel-wise attention weights to handle large-scale degradation blurs.\n\n**Abstract:** Image restoration aims to reconstruct the latent sharp image from its corrupted counterpart. Besides dealing with this long-standing task in the spatial domain, a few approaches seek solutions in the frequency domain in consideration of the large discrepancy between spectra of sharp/degraded image pairs. However, these works commonly utilize transformation tools, e.g., wavelet transform, to split features into several frequency parts, which is not flexible enough to select the most informative frequency component to recover. In this paper, we exploit a multi-branch and content-aware module to decompose features into separate frequency subbands dynamically and locally, and then accentuate the useful ones via channel-wise attention weights. In addition, to handle large-scale degradation blurs, we propose an extremely simple decoupling and modulation module to enlarge the receptive field via global and window-based average pooling. Integrating two developed modules into a U-Net backbone, the proposed Selective Frequency Network (SFNet) performs favorably against state-of-the-art algorithms on five image restoration tasks, including single-image defocus deblurring, image dehazing, image motion deblurring, image desnowing, and image deraining 1.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 63  (*Influential: 11*)\n\n#### 2. ReMask: A Robust Information-Masking Approach for Domain Counterfactual Generation\n\n*From Search Query: \"frequency domain attention\"*\n\n*Pengfei Hong, Rishabh Bhardwaj, Navonil Majumdar, Somak Aditya, Soujanya Poria*\n\n**TL;DR:** This work employs a three-step domain obfuscation approach that involves frequency and attention norm-based masking, to mask domain-specific cues, and unmasking to regain the domain generic context, and shows its domain adaptation efficacy on a large multi-domain intent classification dataset where it attains state-of-the-art results.\n\n**Abstract:** Domain shift is a big challenge in NLP, thus, many approaches resort to learning domain-invariant features to mitigate the inference phase domain shift. Such methods, however, fail to leverage the domain-specific nuances relevant to the task at hand. To avoid such drawbacks, domain counterfactual generation aims to transform a text from the source domain to a given target domain. However, due to the limited availability of data, such frequency-based methods often miss and lead to some valid and spurious domain-token associations. Hence, we employ a three-step domain obfuscation approach that involves frequency and attention norm-based masking, to mask domain-specific cues, and unmasking to regain the domain generic context. Our experiments empirically show that the counterfactual samples sourced from our masked text lead to improved domain transfer on 10 out of 12 domain sentiment classification settings, with an average of 2% accuracy improvement over the state-of-the-art for unsupervised domain adaptation (UDA). Further, our model outperforms the state-of-the-art by achieving 1.4% average accuracy improvement in the adversarial domain adaptation (ADA) setting. Moreover, our model also shows its domain adaptation efficacy on a large multi-domain intent classification dataset where it attains state-of-the-art results. We release the codes publicly at \\url{https://github.com/declare-lab/remask}.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. RTFS-Net: Recurrent time-frequency modelling for efficient audio-visual speech separation\n\n*From Search Query: \"frequency domain attention\"*\n\n*Samuel Pegg, Kai Li, Xiaolin Hu*\n\n**TL;DR:** This paper presents a novel time-frequency domain audio-visual speech separation method, RTFS-Net, which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform and introduces a unique attention-based fusion technique for the efficient integration of audio and visual information.\n\n**Abstract:** Audio-visual speech separation methods aim to integrate different modalities to generate high-quality separated speech, thereby enhancing the performance of downstream tasks such as speech recognition. Most existing state-of-the-art (SOTA) models operate in the time domain. However, their overly simplistic approach to modeling acoustic features often necessitates larger and more computationally intensive models in order to achieve SOTA performance. In this paper, we present a novel time-frequency domain audio-visual speech separation method: Recurrent Time-Frequency Separation Network (RTFS-Net), which applies its algorithms on the complex time-frequency bins yielded by the Short-Time Fourier Transform. We model and capture the time and frequency dimensions of the audio independently using a multi-layered RNN along each dimension. Furthermore, we introduce a unique attention-based fusion technique for the efficient integration of audio and visual information, and a new mask separation approach that takes advantage of the intrinsic spectral nature of the acoustic features for a clearer separation. RTFS-Net outperforms the prior SOTA method in both inference speed and separation quality while reducing the number of parameters by 90% and MACs by 83%. This is the first time-frequency domain audio-visual speech separation method to outperform all contemporary time-domain counterparts.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. MEC: Memory-efficient Convolution for Deep Neural Network\n\n*From Search Query: \"FFT neural networks\"*\n\n*Minsik Cho, D. Brand*\n\n**TL;DR:** This work proposes a memory-efficient convolution or MEC with compact lowering, which reduces memory-overhead substantially and accelerates convolution process and reduces memory consumption significantly with good speedup on both mobile and server platforms.\n\n**Abstract:** Convolution is a critical component in modern deep neural networks, thus several algorithms for convolution have been developed. Direct convolution is simple but suffers from poor performance. As an alternative, multiple indirect methods have been proposed including im2col-based convolution, FFT-based convolution, or Winograd-based algorithm. However, all these indirect methods have high memory-overhead, which creates performance degradation and offers a poor trade-off between performance and memory consumption. In this work, we propose a memory-efficient convolution or MEC with compact lowering, which reduces memory-overhead substantially and accelerates convolution process. MEC lowers the input matrix in a simple yet efficient/compact way (i.e., much less memory-overhead), and then executes multiple small matrix multiplications in parallel to get convolution completed. Additionally, the reduced memory footprint improves memory sub-system efficiency, improving performance. Our experimental results show that MEC reduces memory consumption significantly with good speedup on both mobile and server platforms, compared with other indirect convolution algorithms.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 80  (*Influential: 15*)\n\n#### 5. Spherical CNNs\n\n*From Search Query: \"FFT neural networks\"*\n\n*Taco Cohen, M. Geiger, Jonas K\u00f6hler, M. Welling*\n\n**TL;DR:** A definition for the spherical cross-correlation is proposed that is both expressive and rotation-equivariant and satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm.\n\n**Abstract:** Convolutional Neural Networks (CNNs) have become the method of choice for learning problems involving 2D planar images. However, a number of problems of recent interest have created a demand for models that can analyze spherical images. Examples include omnidirectional vision for drones, robots, and autonomous cars, molecular regression problems, and global weather and climate modelling. A naive application of convolutional networks to a planar projection of the spherical signal is destined to fail, because the space-varying distortions introduced by such a projection will make translational weight sharing ineffective. In this paper we introduce the building blocks for constructing spherical CNNs. We propose a definition for the spherical cross-correlation that is both expressive and rotation-equivariant. The spherical correlation satisfies a generalized Fourier theorem, which allows us to compute it efficiently using a generalized (non-commutative) Fast Fourier Transform (FFT) algorithm. We demonstrate the computational efficiency, numerical accuracy, and effectiveness of spherical CNNs applied to 3D model recognition and atomization energy regression.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 844  (*Influential: 124*)\n\n#### 6. General Nonlinearities in SO(2)-Equivariant CNNs\n\n*From Search Query: \"FFT neural networks\"*\n\n*Daniel Franzen, Michael Wand*\n\n**TL;DR:** A novel FFT-based algorithm for computing representations of non-linearly transformed activations while maintaining band-limitation is developed and yields exact equivariance for polynomial (approximations of) nonlinearities, as well as approximate solutions with tunable accuracy for general functions.\n\n**Abstract:** Invariance under symmetry is an important problem in machine learning. Our paper looks speci\ufb01cally at equivariant neural networks where transformations of inputs yield homomorphic transformations of outputs. Here, steerable CNNs have emerged as the standard solution. An inherent problem of steerable representations is that general nonlinear layers break equivariance, thus restricting architectural choices. Our paper applies harmonic distortion analysis to illuminate the effect of nonlinearities on Fourier representations of SO(2). We develop a novel FFT-based algorithm for computing representations of non-linearly transformed activations while maintaining band-limitation. It yields exact equivariance for polynomial (approximations of) nonlinearities, as well as approximate solutions with tunable accuracy for general functions. We apply the approach to build a fully E(3)- equivariant network for sampled 3D surface data. In experiments with 2D and 3D data, we obtain results that compare favorably to the state-of-the-art in terms of accuracy while permitting continuous symmetry and exact equivariance.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 7. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: \"hierarchical attention\"*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4301  (*Influential: 554*)\n\n#### 8. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: \"hierarchical attention\"*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 38  (*Influential: 3*)\n\n#### 9. HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level\n\n*From Search Query: \"hierarchical attention\"*\n\n*Haoran Luo, E. Haihong, Yuhao Yang, Yikai Guo, Mingzhi Sun, Tianyu Yao, Zichen Tang, Kaiyang Wan, Meina Song, Wei Lin*\n\n**TL;DR:** A novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention, that addresses the issue of HKG multi-position prediction for the first time and achieves state-of-the-art performance in link prediction tasks on HKG standard datasets.\n\n**Abstract:** Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs\u2019 representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Aspect Based Sentiment Analysis Using Spectral Temporal Graph Neural\n  Network\n\n*From Search Query: \"frequency domain attention\"*\n\n*Abir Chakraborty*\n\n**Abstract:** The objective of Aspect Based Sentiment Analysis is to capture the sentiment\nof reviewers associated with different aspects. However, complexity of the\nreview sentences, presence of double negation and specific usage of words found\nin different domains make it difficult to predict the sentiment accurately and\noverall a challenging natural language understanding task. While recurrent\nneural network, attention mechanism and more recently, graph attention based\nmodels are prevalent, in this paper we propose graph Fourier transform based\nnetwork with features created in the spectral domain. While this approach has\nfound considerable success in the forecasting domain, it has not been explored\nearlier for any natural language processing task. The method relies on creating\nand learning an underlying graph from the raw data and thereby using the\nadjacency matrix to shift to the graph Fourier domain. Subsequently, Fourier\ntransform is used to switch to the frequency (spectral) domain where new\nfeatures are created. These series of transformation proved to be extremely\nefficient in learning the right representation as we have found that our model\nachieves the best result on both the SemEval-2014 datasets, i.e., \"Laptop\" and\n\"Restaurants\" domain. Our proposed model also found competitive results on the\ntwo other recently proposed datasets from the e-commerce domain.\n\n**Published:** 2022-02-14T14:55:00Z  (*Updated: 2022-02-14T14:55:00Z*)\n\n\n\n#### 2. Pre-trained Large Language Models Use Fourier Features to Compute\n  Addition\n\n*From Search Query: \"frequency domain attention\"*\n\n*Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia*\n\n**Abstract:** Pre-trained large language models (LLMs) exhibit impressive mathematical\nreasoning capabilities, yet how they compute basic arithmetic, such as\naddition, remains unclear. This paper shows that pre-trained LLMs add numbers\nusing Fourier features -- dimensions in the hidden state that represent numbers\nvia a set of features sparse in the frequency domain. Within the model, MLP and\nattention layers use Fourier features in complementary ways: MLP layers\nprimarily approximate the magnitude of the answer using low-frequency features,\nwhile attention layers primarily perform modular addition (e.g., computing\nwhether the answer is even or odd) using high-frequency features. Pre-training\nis crucial for this mechanism: models trained from scratch to add numbers only\nexploit low-frequency features, leading to lower accuracy. Introducing\npre-trained token embeddings to a randomly initialized model rescues its\nperformance. Overall, our analysis demonstrates that appropriate pre-trained\nrepresentations (e.g., Fourier features) can unlock the ability of Transformers\nto learn precise mechanisms for algorithmic tasks.\n\n**Published:** 2024-06-05T16:40:53Z  (*Updated: 2024-06-05T16:40:53Z*)\n\n\n\n#### 3. diagNNose: A Library for Neural Activation Analysis\n\n*From Search Query: \"FFT neural networks\"*\n\n*Jaap Jumelet*\n\n**Abstract:** In this paper we introduce diagNNose, an open source library for analysing\nthe activations of deep neural networks. diagNNose contains a wide array of\ninterpretability techniques that provide fundamental insights into the inner\nworkings of neural networks. We demonstrate the functionality of diagNNose with\na case study on subject-verb agreement within language models. diagNNose is\navailable at https://github.com/i-machine-think/diagnnose.\n\n**Published:** 2020-11-13T09:19:48Z  (*Updated: 2020-11-13T09:19:48Z*)\n\n\n\n#### 4. Training and Generating Neural Networks in Compressed Weight Space\n\n*From Search Query: \"FFT neural networks\"*\n\n*Kazuki Irie, J\u00fcrgen Schmidhuber*\n\n**Abstract:** The inputs and/or outputs of some neural nets are weight matrices of other\nneural nets. Indirect encodings or end-to-end compression of weight matrices\ncould help to scale such approaches. Our goal is to open a discussion on this\ntopic, starting with recurrent neural networks for character-level language\nmodelling whose weight matrices are encoded by the discrete cosine transform.\nOur fast weight version thereof uses a recurrent neural network to parameterise\nthe compressed weights. We present experimental results on the enwik8 dataset.\n\n**Published:** 2021-12-31T16:50:31Z  (*Updated: 2021-12-31T16:50:31Z*)\n\n\n\n#### 5. Tree-structured Attention with Hierarchical Accumulation\n\n*From Search Query: \"hierarchical attention\"*\n\n*Xuan-Phi Nguyen, Shafiq Joty, Steven C. H. Hoi, Richard Socher*\n\n**Abstract:** Incorporating hierarchical structures like constituency trees has been shown\nto be effective for various natural language processing (NLP) tasks. However,\nit is evident that state-of-the-art (SOTA) sequence-based models like the\nTransformer struggle to encode such structures inherently. On the other hand,\ndedicated models like the Tree-LSTM, while explicitly modeling hierarchical\nstructures, do not perform as efficiently as the Transformer. In this paper, we\nattempt to bridge this gap with \"Hierarchical Accumulation\" to encode parse\ntree structures into self-attention at constant time complexity. Our approach\noutperforms SOTA methods in four IWSLT translation tasks and the WMT'14\nEnglish-German translation task. It also yields improvements over Transformer\nand Tree-LSTM on three text classification tasks. We further demonstrate that\nusing hierarchical priors can compensate for data shortage, and that our model\nprefers phrase-level attentions over token-level attentions.\n\n**Published:** 2020-02-19T08:17:00Z  (*Updated: 2020-02-19T08:17:00Z*)\n\n\n\n#### 6. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for\n  Sequences\n\n*From Search Query: \"hierarchical attention\"*\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.\n\n**Published:** 2021-07-25T23:07:03Z  (*Updated: 2021-07-25T23:07:03Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. HartleyMHA: Self-Attention in Frequency Domain for Resolution-Robust and Parameter-Efficient 3D Image Segmentation\n\n*From Search Query: \"frequency domain attention\"*\n\n*Tanveer Syeda-Mahmood, Hongzhi Wang, Ken C. L. Wong*\n\n**Abstract:** With the introduction of Transformers, different attention-based models have been proposed for image segmentation with promising results. Although self-attention allows capturing of long-range dependencies, it suffers from a quadratic complexity in the image size especially in 3D. To avoid the out-of-memory error during training, input size reduction is usually required for 3D segmentation, but the accuracy can be suboptimal when the trained models are applied on the original image size. To address this limitation, inspired by the Fourier neural operator (FNO), we introduce the HartleyMHA model which is robust to training image resolution with efficient self-attention. FNO is a deep learning framework for learning mappings between functions in partial differential equations, which has the appealing properties of zero-shot super-resolution and global receptive field. We modify the FNO by using the Hartley transform with shared parameters to reduce the model size by orders of magnitude, and this allows us to further apply self-attention in the frequency domain for more expressive high-order feature combination with improved efficiency. When tested on the BraTS'19 dataset, it achieved superior robustness to training image resolution than other tested models with less than 1% of their model parameters.\n\n**Published:** 2023-10-05\n\n\n\n#### 2. FAITH: Frequency-domain Attention In Two Horizons for Time Series Forecasting\n\n*From Search Query: \"frequency domain attention\"*\n\n*Xiufang Zhou, Yue Sun, Quangao Liu, Kaiduo Feng, Kai Wang, Maowei Jiang, RuiQi Li*\n\n**Abstract:** Time Series Forecasting plays a crucial role in various fields such as industrial equipment maintenance, meteorology, energy consumption, traffic flow and financial investment. However, despite their considerable advantages over traditional statistical approaches, current deep learning-based predictive models often exhibit a significant deviation between their forecasting outcomes and the ground truth. This discrepancy is largely due to an insufficient emphasis on extracting the sequence's latent information, particularly its global information within the frequency domain and the relationship between different variables. To address this issue, we propose a novel model Frequency-domain Attention In Two Horizons, which decomposes time series into trend and seasonal components using a multi-scale sequence adaptive decomposition and fusion architecture, and processes them separately. FAITH utilizes Frequency Channel feature Extraction Module and Frequency Temporal feature Extraction Module to capture inter-channel relationships and temporal global information in the sequence, significantly improving its ability to handle long-term dependencies and complex patterns. Furthermore, FAITH achieves theoretically linear complexity by modifying the time-frequency domain transformation method, effectively reducing computational costs. Extensive experiments on 6 benchmarks for long-term forecasting and 3 benchmarks for short-term forecasting demonstrate that FAITH outperforms existing models in many fields, such as electricity, weather and traffic, proving its effectiveness and superiority both in long-term and short-term time series forecasting tasks. Our codes and data are available at https://github.com/LRQ577/FAITH.\n\n**Published:** 2024-05-22\n\n\n\n#### 3. SuperNeurons: FFT-based Gradient Sparsification in the Distributed Training of Deep Neural Networks\n\n*From Search Query: \"FFT neural networks\"*\n\n*Anonymous*\n\n**Abstract:** The performance and efficiency of distributed training of Deep Neural\nNetworks highly depend on the performance of gradient averaging among all\nparticipating nodes, which is bounded by the communication between nodes. There\nare two major strategies to reduce communication overhead: one is to hide\ncommunication by overlapping it with computation, and the other is to reduce\nmessage sizes. The first solution works well for linear neural architectures,\nbut latest networks such as ResNet and Inception offer limited opportunity for\nthis overlapping. Therefore, researchers have paid more attention to minimizing\ncommunication. In this paper, we present a novel gradient compression framework\nderived from insights of real gradient distributions, and which strikes a\nbalance between compression ratio, accuracy, and computational overhead. Our\nframework has two major novel components: sparsification of gradients in the\nfrequency domain, and a range-based floating point representation to quantize\nand further compress gradients frequencies. Both components are dynamic, with\ntunable parameters that achieve different compression ratio based on the\naccuracy requirement and systems' platforms, and achieve very high throughput\non GPUs. We prove that our techniques guarantee the convergence with a\ndiminishing compression ratio. Our experiments show that the proposed\ncompression framework effectively improves the scalability of most popular\nneural networks on a 32 GPU cluster to the baseline of no compression, without\ncompromising the accuracy and convergence speed.\n\n**Published:** 2018-11-21\n\n\n\n#### 4. Fast Algorithms for Convolutional Neural Networks\n\n*From Search Query: \"FFT neural networks\"*\n\n*Scott Gray, Andrew Lavin*\n\n**Abstract:** Deep convolutional neural networks take GPU days of compute time to train on\nlarge data sets. Pedestrian detection for self driving cars requires very low\nlatency. Image recognition for mobile phones is constrained by limited\nprocessing resources. The success of convolutional neural networks in these\nsituations is limited by how fast we can compute them. Conventional FFT based\nconvolution is fast for large filters, but state of the art convolutional\nneural networks use small, 3x3 filters. We introduce a new class of fast\nalgorithms for convolutional neural networks using Winograd's minimal filtering\nalgorithms. The algorithms compute minimal complexity convolution over small\ntiles, which makes them fast with small filters and small batch sizes. We\nbenchmark a GPU implementation of our algorithm with the VGG network and show\nstate of the art throughput at batch sizes from 1 to 64.\n\n**Conference:** fast-algorithms-for-convolutional-neural-1\n\n**Published:** 2015-09-30\n\n\n\n#### 5. Hierarchical Multi-Scale Attention for Semantic Segmentation\n\n*From Search Query: \"hierarchical attention\"*\n\n*Bryan Catanzaro, Karan Sapra, Andrew Tao*\n\n**Abstract:** Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).\n\n**Published:** 2020-05-21\n\n\n\n#### 6. HAHE: Hierarchical Attentive Heterogeneous Information Network Embedding\n\n*From Search Query: \"hierarchical attention\"*\n\n*Jia-Wei Chen, Sheng Zhou, Jiajun Bu, Xin Wang, Can Wang*\n\n**Abstract:** Heterogeneous information network (HIN) embedding has recently attracted much attention due to its effectiveness in dealing with the complex heterogeneous data. Meta path, which connects different object types with various semantic meanings, is widely used by existing HIN embedding works. However, several challenges have not been addressed so far. First, different meta paths convey different semantic meanings, while existing works assume that all nodes share same weights for meta paths and ignore the personalized preferences of different nodes on different meta paths. Second, given a meta path, nodes in HIN are connected by path instances while existing works fail to fully explore the differences between path instances that reflect nodes' preferences in the semantic space. rTo tackle the above challenges, we propose aHierarchical Attentive Heterogeneous information network Embedding (HAHE) model to capture the personalized preferences on meta paths and path instances in each semantic space. As path instances are based on a particular meta path, a hierarchical attention mechanism is naturally utilized to model the personalized preference on meta paths and path instances. Extensive experiments on several real-world datasets show that our proposed \\model model significantly outperforms the state-of-the-art methods in terms of various data mining tasks.\n\n**Published:** 2019-01-31\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model incorporating frequency-domain operations, hierarchical attention, and other specified aspects, here are some key findings and insights from the provided sources and additional context:\n\n## Implementations and Analysis of Frequency-Domain Operations in Attention Mechanisms\n\n- The concept of using frequency-domain operations in neural networks is explored in several works. For instance, the \"FreDF\" method uses Fourier transforms to align predicted values and label sequences in the frequency domain, reducing the impact of autocorrelation and improving forecast quality. This approach can be adapted to attention mechanisms by transforming the input sequences into the frequency domain before applying attention, potentially enhancing long-range dependency modeling.\n- The use of frequency-domain MLPs in FreTS is another example, where frequency-domain operations are used for global signal analysis and compact energy representation. This could be integrated into attention mechanisms to improve their ability to capture long-range dependencies.\n\n## Performance Comparisons Between Time-Domain and Frequency-Domain Attention\n\n- The \"FreDF\" method highlights the benefits of frequency-domain transformations by showing that they can bypass the effects of autocorrelation, which is a common issue in time-domain forecasting. This suggests that frequency-domain attention could offer similar advantages in capturing dependencies in sequences.\n- The TFCFN model, which uses a cross-attention mechanism to fuse time-domain and frequency-domain features, demonstrates better performance in detecting signals in both Gaussian and non-Gaussian noise environments. This indicates that combining both domains can lead to more robust and accurate models.\n\n## Hardware Optimization Techniques for FFT Operations in Neural Networks\n\n- Efficient FFT implementations are crucial for reducing computational overhead. Works like \"FlashFFTConv\" focus on optimizing convolutions for long sequences using tensor cores, which can be applied to FFT operations in neural networks. These optimizations can significantly improve the feasibility of using FFT-based operations in real-time applications[Reference 1 from Initial Analysis].\n\n## Memory Efficiency Analysis of Frequency-Domain Transformations\n\n- The memory efficiency of frequency-domain transformations depends on the specific implementation. For example, using FFT-based operations can be memory-efficient if the data is processed in chunks and if the FFT algorithm is optimized for cache utilization. However, the overall memory bandwidth requirements need careful consideration to avoid bottlenecks[Initial Analysis].\n\n## Training Stability Considerations for FFT-Based Neural Networks\n\n- Training stability is a concern when introducing frequency-domain transformations. The \"FreDF\" method suggests that combining time and frequency domains with an appropriate coefficient (\u03b1) can help maintain stability and improve model accuracy. This balanced approach can mitigate the instability that might arise from using frequency-domain transformations alone.\n\n## Integration of Hierarchical Processing with Frequency-Domain Operations\n\n- Integrating hierarchical attention with frequency-domain operations is a novel approach. By transforming the input sequences into the frequency domain and then applying hierarchical attention, the model can capture both local and global dependencies more effectively. This integration can leverage the strengths of both hierarchical attention and frequency-domain operations to improve long-range dependency modeling[Initial Analysis].\n\n## Adaptive Filtering Techniques in Neural Networks\n\n- Adaptive filtering techniques can be applied in neural networks to dynamically select frequency components. The TFCFN model uses a cross-attention mechanism to adaptively fuse time-domain and frequency-domain features, which can be seen as a form of adaptive filtering. This approach helps in enhancing the model's sensitivity to signal variations and improving its performance in various noise environments.\n\n### Additional Insights\n\n- **Hierarchical Attention**: Combining hierarchical attention with frequency-domain operations can enhance the model's ability to capture complex dependencies. Hierarchical attention can process the frequency-domain representations at multiple scales, further improving the model's expressiveness[Initial Analysis].\n- **Hardware-Aware Design**: Optimizing FFT operations for modern hardware, such as using tensor cores and efficient memory access patterns, is crucial for practical implementation. This ensures that the computational overhead of FFT operations is minimized, making the model more feasible for real-world applications[Reference 1 from Initial Analysis].\n\nBy integrating these insights, the researcher can develop a more robust and efficient autoregressive language model that leverages the strengths of frequency-domain operations, hierarchical attention, and adaptive filtering, while addressing concerns related to computational overhead, memory efficiency, and training stability.\n",
            "analysis": "Initial Analysis of FreqHAA-GPT Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Frequency-Domain Operations in Attention:\n- Novel integration of FFT-based operations with hierarchical attention\n- Potential benefits and overhead of frequency-domain transformations\n- Impact on computational efficiency and memory usage\n\n2. Adaptive Frequency Filtering:\n- Effectiveness of dynamic frequency component selection\n- Trade-offs between expressiveness and computational cost\n- Comparison with existing frequency-based attention mechanisms\n\n3. Hardware Optimization:\n- Feasibility of efficient FFT implementations on modern hardware\n- Memory access patterns and cache utilization\n- Parallel processing capabilities\n\nInvestigation Plan:\n1. Search for existing work combining frequency-domain operations with attention mechanisms\n2. Analyze implementations of FFT-based neural network components\n3. Evaluate hardware optimization techniques for frequency-domain operations\n4. Compare with existing hierarchical attention approaches\n5. Assess memory efficiency claims and scalability potential\n\nInitial Observations:\n\nStrengths:\n- Novel combination of frequency-domain operations with hierarchical attention\n- Potential for improved long-range dependency modeling\n- Hardware-aware design considerations\n- Clear mathematical formulation\n\nConcerns:\n- Computational overhead of FFT operations\n- Memory bandwidth requirements\n- Training stability with frequency-domain transformations\n- Implementation complexity\n\nThe proposal appears to be novel in its specific combination of frequency-domain operations with hierarchical adaptive attention. While both FFT-based neural networks and hierarchical attention exist separately, their integration in this manner seems unique.\n\nComparison with Previous Designs:\n- Different from stategpt and hieraroute_gpt which focused on normalization\n- Distinct from r_ssmwkv which modified RWKV architecture\n- Novel compared to expertswiglu which used mixture of experts\n- Different approach from memoryaugmentedtttlinear which used external memory\n\nReferences found so far:\n1. Fu, D. Y., et al. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\"\n2. Lee-Thorp, J., et al. (2021). \"FNet: Mixing tokens with fourier transforms\"\n3. Fu, D. Y., et al. (2023). \"Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\""
        },
        {
            "ready": false,
            "query": "\"frequency domain neural networks\", \"attention FFT implementation\", \"hardware efficient attention\"",
            "detail": "Search for:\n1. Specific implementations of frequency-domain attention mechanisms in neural networks\n2. Training stability analysis for frequency-domain neural networks\n3. Memory optimization techniques for FFT operations in deep learning\n4. Hardware-specific optimizations for attention mechanisms\n5. Integration strategies for combining frequency-domain operations with attention\n6. Performance benchmarks comparing different frequency-domain approaches\n7. State-of-the-art architectures using frequency-domain operations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for:\n1. Specific implementations of frequency-domain attention mechanisms in neural networks\n2. Training stability analysis for frequency-domain neural networks\n3. Memory optimization techniques for FFT operations in deep learning\n4. Hardware-specific optimizations for attention mechanisms\n5. Integration strategies for combining frequency-domain operations with attention\n6. Performance benchmarks comparing different frequency-domain approaches\n7. State-of-the-art architectures using frequency-domain operations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.83)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.83)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 2. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.17)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 25/29 (Score: 0.17)*\n\n```\nArXiv, abs/2405.05254, 2024b. URL https://api. semanticscholar org/CorpusID:269626143. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention in transformer models, 2021a. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview net/forum?id=qVyeW-grC2k\n\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers \\& distillation through attention. In International Conference on Machine Learning, volume 139, pp. 10347-10357, July 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. Jos van der Westhuizen and Joan Lasenby. The unreasonable effectiveness of the forget gate. CoRR, abs/1804.04849, 2018. Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M. Rush. Pretraining without attention. CoRR, abs/2212.10544, 2022. Songlin Yang and Yu Zhang. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism, January 2024. URL https://github.com/ sustcsonglin/flash-linear-attention\n\nSonglin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. CoRR, abs/2312.06635, 2023. doi: 10.48550/ARXIV.2312.06635. URL https://doi.org/10.48550/arXiv.2312.06635. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length.\n```\n\n#### 3. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.06)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.06)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 4. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.05)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.05)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 5. Building Blocks for a Complex-Valued Transformer Architecture (Avg. Score: 0.05)\n\n*Florian Eilers, Xiaoyi Jiang*\n\n**Published in:** IEEE International Conference on Acoustics, Speech, and Signal Processing (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work adds to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain and shows improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n**Abstract:** Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into \u211d2. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n##### *Relevant Chunk: No. 16/22 (Score: 0.05)*\n\n```\n23, no. 4, pp. 541-551, 2012. [19] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, \"DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement,\" in INTERSPEECH, 2020. [20] H. Zhang et al., \"An optical neural chip for implementing complex-valued neural network,\" Nature Communications, vol. 12, pp. 457, 2021. [21] D. W. Otter, J. R. Medina, and J. K. Kalita, \"A survey of the usages of deep learning for natural language processing,\" IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 2, pp. 604-624, 2020. [22] D. Hu, \"An introductory survey on attention mechanisms in NLP problems,\" in IntelliSys, 2019. [23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \"An image is worth 16x16 words: Transformers for image recognition at scale,\" in ICLR, 2021. [24] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, \"Transformers in vision: A survey,\" ACM Computing Surveys, vol. 54, pp. 200:1-200:41, 2021. [25] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck, \"Music transformer: Generating music with long-term structure,\" in ICLR, 2019. [26] T. Lin, Y. Wang, X. Liu, and X. Qiu, \"A survey of transformers,\" AI Open, vol. 3, pp. 111-132, 2022. [27] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, \"Efficient transformers: A survey,\" ACM Computing Surveys, vol.\n```\n\n\n\n---\n## Found 14 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"frequency domain neural networks\", \"attention FFT implementation\", \"hardware efficient attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Rethinking and Improving Robustness of Convolutional Neural Networks: a Shapley Value-based Approach in Frequency Domain\n\n*From Search Query: \"frequency domain neural networks\"*\n\n*Yiting Chen, Qibing Ren, Junchi Yan*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 1*)\n\n#### 2. Frequency-Domain Dynamic Pruning for Convolutional Neural Networks\n\n*From Search Query: \"frequency domain neural networks\"*\n\n*Z. Liu, Jizheng Xu, Xiulian Peng, Ruiqin Xiong*\n\n**TL;DR:** Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin and can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-110.\n\n**Abstract:** Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-110.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 130  (*Influential: 11*)\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"attention FFT implementation\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 71  (*Influential: 13*)\n\n#### 4. Point Transformer V2: Grouped Vector Attention and Partition-based Pooling\n\n*From Search Query: \"attention FFT implementation\"*\n\n*Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, Hengshuang Zhao*\n\n**TL;DR:** This work proposes a powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work, and first proposes group vector attention, which is more effective than the previous version of vector attention.\n\n**Abstract:** As a pioneering work exploring transformer architecture for 3D point cloud understanding, Point Transformer achieves impressive results on multiple highly competitive benchmarks. In this work, we analyze the limitations of the Point Transformer and propose our powerful and efficient Point Transformer V2 model with novel designs that overcome the limitations of previous work. In particular, we first propose group vector attention, which is more effective than the previous version of vector attention. Inheriting the advantages of both learnable weight encoding and multi-head attention, we present a highly effective implementation of grouped vector attention with a novel grouped weight encoding layer. We also strengthen the position information for attention by an additional position encoding multiplier. Furthermore, we design novel and lightweight partition-based pooling methods which enable better spatial alignment and more efficient sampling. Extensive experiments show that our model achieves better performance than its predecessor and achieves state-of-the-art on several challenging 3D point cloud understanding benchmarks, including 3D point cloud segmentation on ScanNet v2 and S3DIS and 3D point cloud classification on ModelNet40. Our code will be available at https://github.com/Gofinge/PointTransformerV2.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 238  (*Influential: 32*)\n\n#### 5. Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator\n\n*From Search Query: \"attention FFT implementation\"*\n\n*Ziwei He, Meng-Da Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, Zhouhan Lin*\n\n**TL;DR:** Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models, and achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA.\n\n**Abstract:** The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. \\footnote{Our code is publicly available at \\url{https://github.com/LUMIA-Group/FourierTransformer}}\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han*\n\n**TL;DR:** This work designs Hardware-Aware Transformers with neural architecture search, and trains a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing, and performs an evolutionary search with a hardware latency constraint.\n\n**Abstract:** Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 243  (*Influential: 45*)\n\n### 4 related papers from ArXiv\n\n#### 1. Aspect Based Sentiment Analysis Using Spectral Temporal Graph Neural\n  Network\n\n*From Search Query: \"frequency domain neural networks\"*\n\n*Abir Chakraborty*\n\n**Abstract:** The objective of Aspect Based Sentiment Analysis is to capture the sentiment\nof reviewers associated with different aspects. However, complexity of the\nreview sentences, presence of double negation and specific usage of words found\nin different domains make it difficult to predict the sentiment accurately and\noverall a challenging natural language understanding task. While recurrent\nneural network, attention mechanism and more recently, graph attention based\nmodels are prevalent, in this paper we propose graph Fourier transform based\nnetwork with features created in the spectral domain. While this approach has\nfound considerable success in the forecasting domain, it has not been explored\nearlier for any natural language processing task. The method relies on creating\nand learning an underlying graph from the raw data and thereby using the\nadjacency matrix to shift to the graph Fourier domain. Subsequently, Fourier\ntransform is used to switch to the frequency (spectral) domain where new\nfeatures are created. These series of transformation proved to be extremely\nefficient in learning the right representation as we have found that our model\nachieves the best result on both the SemEval-2014 datasets, i.e., \"Laptop\" and\n\"Restaurants\" domain. Our proposed model also found competitive results on the\ntwo other recently proposed datasets from the e-commerce domain.\n\n**Published:** 2022-02-14T14:55:00Z  (*Updated: 2022-02-14T14:55:00Z*)\n\n\n\n#### 2. Efficient Feature Selection techniques for Sentiment Analysis\n\n*From Search Query: \"frequency domain neural networks\"*\n\n*Avinash Madasu, Sivasankar E*\n\n**Abstract:** Sentiment analysis is a domain of study that focuses on identifying and\nclassifying the ideas expressed in the form of text into positive, negative and\nneutral polarities. Feature selection is a crucial process in machine learning.\nIn this paper, we aim to study the performance of different feature selection\ntechniques for sentiment analysis. Term Frequency Inverse Document Frequency\n(TF-IDF) is used as the feature extraction technique for creating feature\nvocabulary. Various Feature Selection (FS) techniques are experimented to\nselect the best set of features from feature vocabulary. The selected features\nare trained using different machine learning classifiers Logistic Regression\n(LR), Support Vector Machines (SVM), Decision Tree (DT) and Naive Bayes (NB).\nEnsemble techniques Bagging and Random Subspace are applied on classifiers to\nenhance the performance on sentiment analysis. We show that, when the best FS\ntechniques are trained using ensemble methods achieve remarkable results on\nsentiment analysis. We also compare the performance of FS methods trained using\nBagging, Random Subspace with varied neural network architectures. We show that\nFS techniques trained using ensemble classifiers outperform neural networks\nrequiring significantly less training time and parameters thereby eliminating\nthe need for extensive hyper-parameter tuning.\n\n**Published:** 2019-11-01T10:20:05Z  (*Updated: 2020-02-02T13:55:21Z*)\n\n\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"attention FFT implementation\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n#### 4. Generalized Probabilistic Attention Mechanism in Transformers\n\n*From Search Query: \"attention FFT implementation\"*\n\n*DongNyeong Heo, Heeyoul Choi*\n\n**Abstract:** The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.\n\n**Published:** 2024-10-21T01:55:52Z  (*Updated: 2024-10-21T01:55:52Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. Training behavior of deep neural network in frequency domain\n\n*From Search Query: \"frequency domain neural networks\"*\n\n*Yaoyu Zhang, Zhi-Qin John Xu, Yanyang Xiao*\n\n**Abstract:** Why deep neural networks (DNNs) capable of overfitting often generalize well in practice is a mystery [#zhang2016understanding]. To find a potential mechanism, we focus on the study of implicit biases underlying the training process of DNNs. In this work, for both real and synthetic datasets, we empirically find that a DNN with common settings first quickly captures the dominant low-frequency components, and then relatively slowly captures the high-frequency ones. We call this phenomenon Frequency Principle (F-Principle). The F-Principle can be observed over DNNs of various structures, activation functions, and training algorithms in our experiments. We also illustrate how the F-Principle help understand the effect of early-stopping as well as the generalization of DNNs. This F-Principle potentially provides insights into a general principle underlying DNN optimization and generalization.\n\n**Published:** 2018-07-03\n\n\n\n#### 2. Learning Frequency Domain Approximation for Binary Neural Networks\n\n*From Search Query: \"frequency domain neural networks\"*\n\n*Yunhe Wang, Chunjing Xu, Yehui Tang, Chang Xu, Kai Han, Yixing Xu*\n\n**Abstract:** Binary neural networks (BNNs) represent original full-precision weights and activations into 1-bit with sign function. Since the gradient of the conventional sign function is almost zero everywhere which cannot be used for back-propagation, several attempts have been proposed to alleviate the optimization difficulty by using approximate gradient. However, those approximations corrupt the main direction of factual gradient. To this end, we propose to estimate the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. In addition, we embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using our method achieves the state-of-the-art accuracy. Code will be available at \\textit{https://gitee.com/mindspore/models/tree/master/research/cv/FDA-BNN}.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-03-01\n\n\n\n#### 3. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using frequency-domain operations and attention mechanisms, here are some key points and strategies based on the provided details and references:\n\n### Specific Implementations of Frequency-Domain Attention Mechanisms\n\n- **FNet**: This model uses Fourier transforms to mix tokens, which can be seen as a form of frequency-domain attention. FNet replaces the self-attention mechanism in transformers with Fourier transforms, demonstrating that frequency-domain operations can be effective in capturing long-range dependencies.\n\n- **FlashFFTConv**: While primarily focused on efficient convolutions, the use of FFT operations here can be adapted for attention mechanisms. The method's efficiency in using tensor cores for FFT computations can be leveraged to optimize frequency-domain attention mechanisms.\n\n### Training Stability Analysis for Frequency-Domain Neural Networks\n\n- The **frequency principle/spectral bias** in deep neural networks indicates that these networks tend to fit target functions from low to high frequencies during training. This principle can help in understanding and stabilizing the training process by ensuring that the network first captures low-frequency components before moving to high-frequency details.\n\n- **Frequency Domain Inference**: The work on accelerating network inference in the frequency domain suggests that training in the spatial domain and inference in the frequency domain can maintain accuracy while speeding up the process. This approach can help in stabilizing the training by avoiding the need for frequent frequency transformations during training.\n\n### Memory Optimization Techniques for FFT Operations\n\n- **FlashFFTConv** demonstrates efficient implementation of FFT operations using tensor cores, which reduces I/O and enables kernel fusion for long sequences. This approach can significantly optimize memory usage and computational efficiency for FFT operations in deep learning models.\n\n- **Hierarchical Attention**: Models like H-Transformer-1D and FasterViT use hierarchical attention mechanisms that have linear runtime and memory complexity. Integrating these with frequency-domain operations can help in optimizing memory usage by reducing the computational complexity of attention mechanisms.\n\n### Hardware-Specific Optimizations for Attention Mechanisms\n\n- **Tensor Core Utilization**: Efficient use of tensor cores, as seen in FlashFFTConv, is crucial for optimizing FFT operations and, by extension, frequency-domain attention mechanisms. This can significantly improve the performance and efficiency of these operations on hardware that supports tensor cores.\n\n- **Memory Access Patterns**: Optimizing memory access patterns is vital for performance. Models like H-Transformer-1D and FasterViT emphasize the importance of memory hierarchy utilization and parallel processing capabilities, which can be applied to frequency-domain attention mechanisms as well.\n\n### Integration Strategies for Combining Frequency-Domain Operations with Attention\n\n- **Cross-Attention Mechanism**: The time-frequency cross fusion network (TFCFN) integrates time-domain and frequency-domain features using a cross-attention mechanism. This approach can be adapted to combine frequency-domain operations with hierarchical attention mechanisms, enhancing the model's ability to capture both local and global dependencies.\n\n- **Hierarchical Attention with Fourier Transforms**: Combining hierarchical attention mechanisms (like those in H-Transformer-1D and FasterViT) with Fourier transforms (as in FNet) could provide a robust way to integrate frequency-domain operations with attention. This would leverage the strengths of both approaches in capturing long-range dependencies and optimizing computational costs.\n\n### Performance Benchmarks Comparing Different Frequency-Domain Approaches\n\n- **Comparison with State-of-the-Art Methods**: Studies like the one on FNet and FlashFFTConv provide performance benchmarks that show the efficacy of frequency-domain operations compared to traditional spatial domain methods. These benchmarks can help in evaluating the performance of different frequency-domain approaches integrated with attention mechanisms.\n\n### State-of-the-Art Architectures Using Frequency-Domain Operations\n\n- **FNet and FlashFFTConv**: These architectures are state-of-the-art in using frequency-domain operations. FNet uses Fourier transforms to mix tokens, and FlashFFTConv optimizes FFT operations for long sequences. These can serve as foundational models for integrating frequency-domain operations with attention mechanisms in autoregressive language models.\n\n### Additional Considerations\n\n- **Frequency Principle and Spectral Bias**: Understanding the frequency principle can help in designing networks that efficiently learn both low and high-frequency components, which is crucial for stable and effective training.\n\n- **Hybrid Training and Inference**: Combining training in the spatial domain with inference in the frequency domain, as suggested by the work on accelerating network inference, can offer a balanced approach that leverages the strengths of both domains.\n\nBy integrating these strategies and insights, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage the benefits of frequency-domain operations and hierarchical attention mechanisms.\n",
            "analysis": "Summary of Analysis:\n\nThe initial search has revealed several important aspects about frequency-domain operations in neural networks and hierarchical attention mechanisms, but more investigation is needed in specific areas.\n\nKey Findings:\n\n1. Frequency-Domain Operations:\n- FlashFFTConv demonstrates efficient implementation of FFT operations using tensor cores\n- FNet shows successful application of Fourier transforms in transformer architectures\n- Memory efficiency considerations are crucial for practical implementation\n\n2. Hierarchical Attention:\n- H-Transformer-1D shows promising results with hierarchical attention\n- FasterViT demonstrates successful integration of hierarchical attention in vision models\n- Memory and computational trade-offs need careful consideration\n\n3. Hardware Optimization:\n- Tensor core utilization is key for efficient FFT operations\n- Memory access patterns significantly impact performance\n- Need more investigation into specific hardware optimizations\n\nUseful References with Excerpts:\n\n1. From FlashFFTConv:\n\"FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.\"\n\n2. From H-Transformer-1D:\n\"The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix)... and has linear run time and memory complexity.\"\n\n3. From FasterViT:\n\"Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs.\"\n\nKey Insights Needing Further Investigation:\n\n1. Integration Mechanisms:\n- How to effectively combine frequency-domain operations with hierarchical attention\n- Trade-offs between different FFT implementations\n- Impact on model training stability\n\n2. Performance Considerations:\n- Memory bandwidth requirements\n- Computational overhead of FFT operations\n- Scalability with sequence length\n\n3. Hardware Efficiency:\n- Optimal implementation strategies\n- Memory hierarchy utilization\n- Parallel processing capabilities\n\nFuture Search Plan:\n\n1. Need more information about:\n- Specific implementations of frequency-domain attention mechanisms\n- Training stability in frequency-domain neural networks\n- Memory optimization techniques for FFT operations\n- Hardware-specific optimizations for attention mechanisms\n\n2. Areas requiring deeper investigation:\n- Integration strategies for frequency and attention mechanisms\n- Performance benchmarks of similar approaches\n- State-of-the-art in frequency-domain neural networks\n\nReferences:\n\n1. Fu, D. Y., et al. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\". arXiv preprint.\n\n2. Zhu, Z., & Soricut, R. (2021). \"H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences\". arXiv preprint.\n\n3. Hatamizadeh, A., et al. (2023). \"FasterViT: Fast Vision Transformers with Hierarchical Attention\". ICLR 2023.\n\n4. Lee-Thorp, J., et al. (2021). \"FNet: Mixing tokens with fourier transforms\". NAACL 2021.\n\n5. Yang, Z., et al. (2016). \"Hierarchical Attention Networks for Document Classification\". NAACL 2016."
        },
        {
            "ready": false,
            "query": "\"adaptive frequency filtering neural networks\", \"frequency domain training stability\", \"hierarchical attention memory optimization\"",
            "detail": "Search for:\n1. Adaptive frequency filtering techniques in neural networks:\n   - Dynamic filter generation\n   - Content-dependent frequency selection\n   - Frequency band adaptation\n\n2. Training stability analysis:\n   - Gradient flow in frequency domain\n   - Spectral normalization techniques\n   - Frequency-aware optimization\n\n3. Memory optimization strategies:\n   - Hierarchical memory management\n   - Efficient state compression\n   - Cache-friendly implementations\n\n4. Integration with hierarchical attention:\n   - Multi-scale processing\n   - Frequency-domain attention mechanisms\n   - Efficient implementation techniques",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for:\n1. Adaptive frequency filtering techniques in neural networks:\n   - Dynamic filter generation\n   - Content-dependent frequency selection\n   - Frequency band adaptation\n\n2. Training stability analysis:\n   - Gradient flow in frequency domain\n   - Spectral normalization techniques\n   - Frequency-aware optimization\n\n3. Memory optimization strategies:\n   - Hierarchical memory management\n   - Efficient state compression\n   - Cache-friendly implementations\n\n4. Integration with hierarchical attention:\n   - Multi-scale processing\n   - Frequency-domain attention mechanisms\n   - Efficient implementation techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.57)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.57)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 2. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.04)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 16/24 (Score: 0.04)*\n\n```\nAdvances in neural information processing systems, 32, 2019. [34] Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive iir filters). arXiv preprint arXiv:2305.14952, 2023. [35] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention.\n```\n\n#### 3. Building Blocks for a Complex-Valued Transformer Architecture (Avg. Score: 0.03)\n\n*Florian Eilers, Xiaoyi Jiang*\n\n**Published in:** IEEE International Conference on Acoustics, Speech, and Signal Processing (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work adds to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain and shows improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n**Abstract:** Most deep learning pipelines are built on real-valued operations to deal with real-valued inputs such as images, speech or music signals. However, a lot of applications naturally make use of complex-valued signals or images, such as MRI or remote sensing. Additionally the Fourier transform of signals is complex-valued and has numerous applications. We aim to make deep learning directly applicable to these complex-valued signals without using projections into \u211d2. Thus we add to the recent developments of complex-valued neural networks by presenting building blocks to transfer the transformer architecture to the complex domain. We present multiple versions of a complex-valued Scaled Dot-Product Attention mechanism as well as a complex-valued layer normalization. We test on a classification and a sequence generation task on the MusicNet dataset and show improved robustness to overfitting while maintaining on-par performance when compared to the real-valued transformer architecture.\n\n##### *Relevant Chunk: No. 16/22 (Score: 0.03)*\n\n```\n23, no. 4, pp. 541-551, 2012. [19] Y. Hu, Y. Liu, S. Lv, M. Xing, S. Zhang, Y. Fu, J. Wu, B. Zhang, and L. Xie, \"DCCRN: Deep complex convolution recurrent network for phase-aware speech enhancement,\" in INTERSPEECH, 2020. [20] H. Zhang et al., \"An optical neural chip for implementing complex-valued neural network,\" Nature Communications, vol. 12, pp. 457, 2021. [21] D. W. Otter, J. R. Medina, and J. K. Kalita, \"A survey of the usages of deep learning for natural language processing,\" IEEE Transactions on Neural Networks and Learning Systems, vol. 32, no. 2, pp. 604-624, 2020. [22] D. Hu, \"An introductory survey on attention mechanisms in NLP problems,\" in IntelliSys, 2019. [23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \"An image is worth 16x16 words: Transformers for image recognition at scale,\" in ICLR, 2021. [24] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, \"Transformers in vision: A survey,\" ACM Computing Surveys, vol. 54, pp. 200:1-200:41, 2021. [25] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, I. Simon, C. Hawthorne, N. Shazeer, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck, \"Music transformer: Generating music with long-term structure,\" in ICLR, 2019. [26] T. Lin, Y. Wang, X. Liu, and X. Qiu, \"A survey of transformers,\" AI Open, vol. 3, pp. 111-132, 2022. [27] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, \"Efficient transformers: A survey,\" ACM Computing Surveys, vol.\n```\n\n#### 4. Linear Transformers Are Secretly Fast Weight Programmers (Avg. Score: 0.02)\n\n*Imanol Schlag, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Machine Learning (2021)\t**Cited by** 151  (*Influential: 18*)\n\n**TL;DR:** This work infer a memory capacity limitation of recent linearised softmax attention variants, and replaces the purely additive outer products of self-invented activation patterns by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values.\n\n**Abstract:** We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow\"neural net learns by gradient descent to program the ``fast weights\"of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.\n\n##### *Relevant Chunk: No. 22/47 (Score: 0.02)*\n\n```\nHinton, G. E. and Plaut, D. C. Using fast weights to deblur old memories. In Proc. Conf. of Cognitive Science Society, pp. 177-186, Seatle, WA, USA, July 1987. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proc. of the national academy of sciences, 79(8):2554-2558, 1982. Irie, K., Zeyer, A., Schl\u00fcter, R., and Ney, H. Language modeling with deep Transformers. In Proc. Interspeech, pp. 3905-3909, Graz, Austria, September 2019a. Irie, K., Zeyer, A., Schl\u00fcter, R., and Ney, H. Training language models for long-span cross-sentence evaluation. In Proc. IEEE Automatic Speech Recog. and Understanding Workshop (ASRU), pp. 419-426, Sentosa, Singapore, December 2019b. Irie, K., Gerstenberger, A., Schl\u00fcter, R., and Ney, H. How much self-attention do we need? Trading attention for feed-forward layers. In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 61546158, Virtual only, May 2020. Jia, X., De Brabandere, B., Tuytelaars, T., and Gool, L. V. Dynamic filter networks. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 667-675, Barcelona, Spain, 2016. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 5. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.01)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive frequency filtering neural networks\", \"frequency domain training stability\", \"hierarchical attention memory optimization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Implicit Maximum a Posteriori Filtering via Adaptive Optimization\n\n*From Search Query: \"adaptive frequency filtering neural networks\"*\n\n*Gianluca M Bencomo, Jake C. Snell, Thomas L. Griffiths*\n\n**TL;DR:** This work frames the standard Bayesian filtering problem as optimization over a time-varying objective, and suggests that it is easier to fine-tune an optimizer than it is to specify the correct filtering equations, making this framework an attractive option for high-dimensional filtering problems.\n\n**Abstract:** Bayesian filtering approximates the true underlying behavior of a time-varying system by inverting an explicit generative model to convert noisy measurements into state estimates. This process typically requires either storage, inversion, and multiplication of large matrices or Monte Carlo estimation, neither of which are practical in high-dimensional state spaces such as the weight spaces of artificial neural networks. Here, we frame the standard Bayesian filtering problem as optimization over a time-varying objective. Instead of maintaining matrices for the filtering equations or simulating particles, we specify an optimizer that defines the Bayesian filter implicitly. In the linear-Gaussian setting, we show that every Kalman filter has an equivalent formulation using K steps of gradient descent. In the nonlinear setting, our experiments demonstrate that our framework results in filters that are effective, robust, and scalable to high-dimensional systems, comparing well against the standard toolbox of Bayesian filtering solutions. We suggest that it is easier to fine-tune an optimizer than it is to specify the correct filtering equations, making our framework an attractive option for high-dimensional filtering problems.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 2. How Powerful are Spectral Graph Neural Networks\n\n*From Search Query: \"adaptive frequency filtering neural networks\"*\n\n*Xiyuan Wang, Muhan Zhang*\n\n**TL;DR:** It is proved that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality, and JacobiConv is proposed, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions.\n\n**Abstract:** Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 145  (*Influential: 29*)\n\n#### 3. SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization\n\n*From Search Query: \"adaptive frequency filtering neural networks\"*\n\n*Amir Hertz, Or Perel, R. Giryes, O. Sorkine-Hornung, D. Cohen-Or*\n\n**TL;DR:** A spatially adaptive progressive encoding scheme for input signals of MLP networks is presented, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing.\n\n**Abstract:** Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better fit a wide range of frequencies without sacrificing training stability or requiring any domain specific preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 60  (*Influential: 6*)\n\n#### 4. LOT: Layer-wise Orthogonal Training on Improving l2 Certified Robustness\n\n*From Search Query: \"frequency domain training stability\"*\n\n*Xiaojun Xu, Linyi Li, Bo Li*\n\n**TL;DR:** A layer-wise orthogonal training method to effectively train 1-Lipschitz convolution layers via parametrizing an Orthogonal matrix with an unconstrained matrix and efficiently compute the inverse square root of a convolution kernel by transforming the input domain to the Fourier frequency domain is proposed.\n\n**Abstract:** Recent studies show that training deep neural networks (DNNs) with Lipschitz constraints are able to enhance adversarial robustness and other model properties such as stability. In this paper, we propose a layer-wise orthogonal training method (LOT) to effectively train 1-Lipschitz convolution layers via parametrizing an orthogonal matrix with an unconstrained matrix. We then efficiently compute the inverse square root of a convolution kernel by transforming the input domain to the Fourier frequency domain. On the other hand, as existing works show that semi-supervised training helps improve empirical robustness, we aim to bridge the gap and prove that semi-supervised learning also improves the certified robustness of Lipschitz-bounded models. We conduct comprehensive evaluations for LOT under different settings. We show that LOT significantly outperforms baselines regarding deterministic l2 certified robustness, and scales to deeper neural networks. Under the supervised scenario, we improve the state-of-the-art certified robustness for all architectures (e.g. from 59.04% to 63.50% on CIFAR-10 and from 32.57% to 34.59% on CIFAR-100 at radius rho = 36/255 for 40-layer networks). With semi-supervised learning over unlabelled data, we are able to improve state-of-the-art certified robustness on CIFAR-10 at rho = 108/255 from 36.04% to 42.39%. In addition, LOT consistently outperforms baselines on different model architectures with only 1/3 evaluation time.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 27  (*Influential: 2*)\n\n#### 5. Weakly-Supervised Spoken Video Grounding via Semantic Interaction Learning\n\n*From Search Query: \"frequency domain training stability\"*\n\n*Ye Wang, Wang Lin, Shengyu Zhang, Tao Jin, Lin Li, Xize Cheng, Zhou Zhao*\n\n**TL;DR:** This work investigates weakly-supervised spoken video grounding and proposes Semantic Interaction Learning (SIL), a novel framework consisting of the acoustic-semantic pre-training (ASP) and acoustic-visual contrastive learning (AVCL), which directly strengthen the interaction between speech and video by maximizing their mutual information.\n\n**Abstract:** The task of spoken video grounding aims to localize moments in videos that are relevant to descriptive spoken queries. However, extracting semantic information from speech and modeling the cross-modal correlation pose two critical challenges. Previous studies solve them by representing spoken queries based on the matched video frames, which require tremendous effort for frame-level labeling. In this work, we investigate weakly-supervised spoken video grounding, i.e., learning to localize moments without expensive temporal annotations. To effectively represent the cross-modal semantics, we propose Semantic Interaction Learning (SIL), a novel framework consisting of the acoustic-semantic pre-training (ASP) and acoustic-visual contrastive learning (AVCL). In ASP, we pre-train an effective encoder for the grounding task with three comprehensive tasks, where the robustness task enhances stability by explicitly capturing the invariance between time- and frequency-domain features, the conciseness task avoids over-smooth attention by compressing long sequence into segments, and the semantic task improves spoken language understanding by modeling the precise semantics. In AVCL, we mine pseudo labels with discriminative sampling strategies and directly strengthen the interaction between speech and video by maximizing their mutual information. Extensive experiments demonstrate the effectiveness and superiority of our method.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 6. Self-Attention Networks Can Process Bounded Hierarchical Languages\n\n*From Search Query: \"hierarchical attention memory optimization\"*\n\n*Shunyu Yao, Binghui Peng, C. Papadimitriou, Karthik Narasimhan*\n\n**TL;DR:** It is proved that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.\n\n**Abstract:** Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types. This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited. We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language. Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D). Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 65  (*Influential: 8*)\n\n#### 7. Towards mental time travel: a hierarchical memory for reinforcement learning agents\n\n*From Search Query: \"hierarchical attention memory optimization\"*\n\n*Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, Felix Hill*\n\n**TL;DR:** Hierarchical Chunk Attention Memory improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures), and is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Abstract:** Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore\"mentally time-travel\"-- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 42  (*Influential: 5*)\n\n### 6 related papers from ArXiv\n\n#### 1. Deep CNNs along the Time Axis with Intermap Pooling for Robustness to\n  Spectral Variations\n\n*From Search Query: \"adaptive frequency filtering neural networks\"*\n\n*Hwaran Lee, Geonmin Kim, Ho-Gyeong Kim, Sang-Hoon Oh, Soo-Young Lee*\n\n**Abstract:** Convolutional neural networks (CNNs) with convolutional and pooling\noperations along the frequency axis have been proposed to attain invariance to\nfrequency shifts of features. However, this is inappropriate with regard to the\nfact that acoustic features vary in frequency. In this paper, we contend that\nconvolution along the time axis is more effective. We also propose the addition\nof an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each\ngroup extract common but spectrally variant features, then the layer pools the\nfeature maps of each group. As a result, the proposed IMP CNN can achieve\ninsensitivity to spectral variations characteristic of different speakers and\nutterances. The effectiveness of the IMP CNN architecture is demonstrated on\nseveral LVCSR tasks. Even without speaker adaptation techniques, the\narchitecture achieved a WER of 12.7% on the SWB part of the Hub5'2000\nevaluation test set, which is competitive with other state-of-the-art methods.\n\n**Published:** 2016-06-10T06:44:21Z  (*Updated: 2016-07-12T07:23:53Z*)\n\n\n\n#### 2. Unsupervised Domain Adaptation on Reading Comprehension\n\n*From Search Query: \"adaptive frequency filtering neural networks\"*\n\n*Yu Cao, Meng Fang, Baosheng Yu, Joey Tianyi Zhou*\n\n**Abstract:** Reading comprehension (RC) has been studied in a variety of datasets with the\nboosted performance brought by deep neural networks. However, the\ngeneralization capability of these models across different domains remains\nunclear. To alleviate this issue, we are going to investigate unsupervised\ndomain adaptation on RC, wherein a model is trained on labeled source domain\nand to be applied to the target domain with only unlabeled samples. We first\nshow that even with the powerful BERT contextual representation, the\nperformance is still unsatisfactory when the model trained on one dataset is\ndirectly applied to another target dataset. To solve this, we provide a novel\nconditional adversarial self-training method (CASe). Specifically, our approach\nleverages a BERT model fine-tuned on the source dataset along with the\nconfidence filtering to generate reliable pseudo-labeled samples in the target\ndomain for self-training. On the other hand, it further reduces domain\ndistribution discrepancy through conditional adversarial learning across\ndomains. Extensive experiments show our approach achieves comparable accuracy\nto supervised models on multiple large-scale benchmark datasets.\n\n**Published:** 2019-11-13T00:54:39Z  (*Updated: 2020-07-27T02:44:59Z*)\n\n\n\n#### 3. Understanding the Stability of Medical Concept Embeddings\n\n*From Search Query: \"frequency domain training stability\"*\n\n*Grace E. Lee, Aixin Sun*\n\n**Abstract:** Frequency is one of the major factors for training quality word embeddings.\nSeveral work has recently discussed the stability of word embeddings in general\ndomain and suggested factors influencing the stability. In this work, we\nconduct a detailed analysis on the stability of concept embeddings in medical\ndomain, particularly the relation with concept frequency. The analysis reveals\nthe surprising high stability of low-frequency concepts: low-frequency (<100)\nconcepts have the same high stability as high-frequency (>1000) concepts. To\ndevelop a deeper understanding of this finding, we propose a new factor, the\nnoisiness of context words, which influences the stability of medical concept\nembeddings, regardless of frequency. We evaluate the proposed factor by showing\nthe linear correlation with the stability of medical concept embeddings. The\ncorrelations are clear and consistent with various groups of medical concepts.\nBased on the linear relations, we make suggestions on ways to adjust the\nnoisiness of context words for the improvement of stability. Finally, we\ndemonstrate that the proposed factor extends to the word embedding stability in\ngeneral domain.\n\n**Published:** 2019-04-21T07:07:48Z  (*Updated: 2020-05-10T07:07:36Z*)\n\n\n\n#### 4. PERL: Pivot-based Domain Adaptation for Pre-trained Deep Contextualized\n  Embedding Models\n\n*From Search Query: \"frequency domain training stability\"*\n\n*Eyal Ben-David, Carmel Rabinovitz, Roi Reichart*\n\n**Abstract:** Pivot-based neural representation models have lead to significant progress in\ndomain adaptation for NLP. However, previous works that follow this approach\nutilize only labeled data from the source domain and unlabeled data from the\nsource and target domains, but neglect to incorporate massive unlabeled corpora\nthat are not necessarily drawn from these domains. To alleviate this, we\npropose PERL: A representation learning model that extends contextualized word\nembedding models such as BERT with pivot-based fine-tuning. PERL outperforms\nstrong baselines across 22 sentiment classification domain adaptation setups,\nimproves in-domain model performance, yields effective reduced-size models and\nincreases model stability.\n\n**Published:** 2020-06-16T11:14:06Z  (*Updated: 2020-06-16T11:14:06Z*)\n\n\n\n#### 5. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for\n  Sequences\n\n*From Search Query: \"hierarchical attention memory optimization\"*\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the\nTransformer architecture. The proposed attention mechanism exploits a matrix\nstructure similar to the Hierarchical Matrix (H-Matrix) developed by the\nnumerical analysis community, and has linear run time and memory complexity. We\nperform extensive experiments to show that the inductive bias embodied by our\nhierarchical attention is effective in capturing the hierarchical structure in\nthe sequences typical for natural language and vision tasks. Our method is\nsuperior to alternative sub-quadratic proposals by over +6 points on average on\nthe Long Range Arena benchmark. It also sets a new SOTA test perplexity on\nOne-Billion Word dataset with 5x fewer model parameters than that of the\nprevious-best Transformer-based models.\n\n**Published:** 2021-07-25T23:07:03Z  (*Updated: 2021-07-25T23:07:03Z*)\n\n\n\n#### 6. A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention\n\n*From Search Query: \"hierarchical attention memory optimization\"*\n\n*Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyoung Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang*\n\n**Abstract:** In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.\n\n**Published:** 2024-06-14T08:32:45Z  (*Updated: 2024-10-15T06:09:35Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. AdaGNN: Graph Neural Networks with Adaptive Frequency Response Filter\n\n*From Search Query: \"adaptive frequency filtering neural networks\"*\n\n*Jundong Li, Shuiwang Ji, Brian Jalaian, Kaize Ding, Yushun Dong*\n\n**Abstract:** Graph Neural Networks have recently become a prevailing paradigm for various high-impact graph analytical problems. Existing efforts can be mainly categorized as spectral-based and spatial-based methods. The major challenge for the former is to find an appropriate graph filter to distill discriminative information from input signals for learning. Recently, myriads of explorations are made to achieve better graph filters, e.g., Graph Convolutional Network (GCN), which leverages Chebyshev polynomial truncation to seek an approximation of graph filters and bridge these two families of methods. Nevertheless, it has been shown in recent studies that GCN and its variants are essentially employing fixed low-pass filters to perform information denoising. Thus their learning capability is rather limited and may over-smooth node representations at deeper layers. To tackle these problems, we develop a novel graph neural network framework AdaGNN with a well-designed adaptive frequency response filter. At its core, AdaGNN leverages a simple but elegant trainable filter that spans across multiple layers to capture the varying importance of different frequency components for node representation learning. The inherent differences among different feature channels are also well captured by the filter. As such, it empowers AdaGNN with stronger expressiveness and naturally alleviates the over-smoothing problem. We empirically validate the effectiveness of the proposed framework on various benchmark datasets. Theoretical analysis is also provided to show the superiority of the proposed AdaGNN. The open-source implementation of AdaGNN can be found here: https://github.com/yushundong/AdaGNN.\n\n**Published:** 2021-04-26\n\n\n\n#### 2. Learnable Multi-level Frequency Decomposition and Hierarchical Attention Mechanism for Generalized Face Presentation Attack Detection\n\n*From Search Query: \"adaptive frequency filtering neural networks\"*\n\n*Arjan Kuijper, Florian Kirchbuchner, Naser Damer, Meiling Fang*\n\n**Abstract:** With the increased deployment of face recognition systems in our daily lives, face presentation attack detection (PAD) is attracting much attention and playing a key role in securing face recognition systems. Despite the great performance achieved by the hand-crafted and deep-learning-based methods in intra-dataset evaluations, the performance drops when dealing with unseen scenarios. In this work, we propose a dual-stream convolution neural networks (CNNs) framework. One stream adapts four learnable frequency filters to learn features in the frequency domain, which are less influenced by variations in sensors/illuminations. The other stream leverages the RGB images to complement the features of the frequency domain. Moreover, we propose a hierarchical attention module integration to join the information from the two streams at different stages by considering the nature of deep features in different layers of the CNN. The proposed method is evaluated in the intra-dataset and cross-dataset setups, and the results demonstrate that our proposed approach enhances the generalizability in most experimental setups in comparison to state-of-the-art, including the methods designed explicitly for domain adaption/shift problems. We successfully prove the design of our proposed PAD solution in a step-wise ablation study that involves our proposed learnable frequency decomposition, our hierarchical attention module design, and the used loss function. Training codes and pre-trained models are publicly released\n\n**Published:** 2021-09-16\n\n\n\n#### 3. Stabilizing GANs with Soft Octave Convolutions\n\n*From Search Query: \"frequency domain training stability\"*\n\n*Franz-Josef Pfreundt, Ricard Durall, Janis Keuper*\n\n**Abstract:** Motivated by recently published methods using frequency decompositions of convolutions (e.g. Octave Convolutions), we propose a novel convolution scheme to stabilize the training and reduce the likelihood of a mode collapse. The basic idea of our approach is to split convolutional filters into additive high and low frequency parts, while shifting weight updates from low to high during the training. Intuitively, this method forces GANs to learn low frequency coarse image structures before descending into fine (high frequency) details. We also show, that the use of the proposed soft octave convolutions reduces common artifacts in the frequency domain of generated images. Our approach is orthogonal and complementary to existing stabilization methods and can simply be plugged into any CNN based GAN architecture. Experiments on the CelebA dataset show the effectiveness of the proposed method.\n\n**Published:** 2019-05-29\n\n\n\n#### 4. ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation\n\n*From Search Query: \"hierarchical attention memory optimization\"*\n\n*WangMeng Zuo, Lei Zhang, Jinfeng Bai, Zhilong Ji, Yabo Zhang, Yuxiang Wei*\n\n**Abstract:** In addition to the unprecedented ability in imaginary creation, large text-to-image models are expected to take customized concepts in image generation. Existing works generally learn such concepts in an optimization-based manner, yet bringing excessive computation or memory burden. In this paper, we instead propose a learning-based encoder, which consists of a global and a local mapping networks for fast and accurate customized text-to-image generation. In specific, the global mapping network projects the hierarchical features of a given image into multiple new words in the textual word embedding space, i.e., one primary word for well-editable concept and other auxiliary words to exclude irrelevant disturbances (e.g., background). In the meantime, a local mapping network injects the encoded patch features into cross attention layers to provide omitted details, without sacrificing the editability of primary concepts. We compare our method with existing optimization-based approaches on a variety of user-defined concepts, and demonstrate that our method enables high-fidelity inversion and more robust editability with a significantly faster encoding process. Our code is publicly available at https://github.com/csyxwei/ELITE.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-02-27\n\n\n\n#### 5. Block Transformer: Global-to-Local Language Modeling for Fast Inference\n\n*From Search Query: \"hierarchical attention memory optimization\"*\n\n*Se-Young Yun, James Thorne, Adam Fisch, Tal Schuster, Yireun Kim, Hyunjik Jo, Taehyeon Kim, Sangmin Bae, Namgyu Ho*\n\n**Abstract:** This paper presents the Block Transformer architecture which adopts hierarchical global-to-local modeling to autoregressive transformers to mitigate the inference bottlenecks of self-attention. To apply self-attention, the key-value (KV) cache of all previous sequences must be retrieved from memory at every decoding step. Thereby, this KV cache IO becomes a significant bottleneck in batch inference. We notice that these costs stem from applying self-attention on the global context, therefore we isolate the expensive bottlenecks of global modeling to lower layers and apply fast local modeling in upper layers. To mitigate the remaining costs in the lower layers, we aggregate input tokens into fixed size blocks and then apply self-attention at this coarse level. Context information is aggregated into a single embedding to enable upper layers to decode the next block of tokens, without global attention. Free of global attention bottlenecks, the upper layers can fully utilize the compute hardware to maximize inference throughput. By leveraging global and local modules, the Block Transformer architecture demonstrates 10-20x gains in inference throughput compared to vanilla transformers with equivalent perplexity. Our work introduces a new approach to optimize language model inference through novel application of global-to-local modeling. Code is available at https://github.com/itsnamgyu/block-transformer.\n\n**Published:** 2024-06-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the provided keywords and details, here are some key insights and techniques that can be leveraged:\n\n### Adaptive Frequency Filtering Techniques in Neural Networks\n\n- **Dynamic Filter Generation**: The concept of dynamic convolutions can be applied to autoregressive models to adaptively learn parameters based on different inputs. For instance, the work on image super-resolution using dynamic convolutions can be adapted to text models by dynamically generating filters that capture varying contextual information.\n- **Content-Dependent Frequency Selection**: Learnable filters that adapt to different frequency bands can be integrated into autoregressive models. This approach, similar to the one used in refining universal adversarial perturbations, can help the model focus on relevant frequency components of the input data.\n- **Frequency Band Adaptation**: Techniques like PhaseDNN, which convert high-frequency components to low-frequency spectra for easier learning and then convert them back, can be explored to improve the model's ability to capture high-frequency information in text data.\n\n### Training Stability Analysis\n\n- **Gradient Flow in Frequency Domain**: Understanding the frequency principle (F-Principle) is crucial, as it explains how deep neural networks learn from low to high frequencies. This principle can guide the design of training protocols to ensure stability by focusing on the learning of low-frequency components first and gradually moving to high-frequency components.\n- **Spectral Normalization Techniques**: Spectral normalization can help stabilize the training process by controlling the spectral components of the network. This technique ensures that the network's parameters do not grow too large, which can lead to instability.\n- **Frequency-Aware Optimization**: Optimizing the training process with frequency-aware techniques can help in stabilizing the gradient flow. For example, using frequency regularization during training can help in achieving high compression ratios and improving stability.\n\n### Memory Optimization Strategies\n\n- **Hierarchical Memory Management**: Hierarchical processing and memory management, such as those proposed in CHELA, can help manage the computational complexity and memory usage of autoregressive models. This involves using short-long convolutions and linear attention in a divide-and-conquer manner[Liu et al., 2024].\n- **Efficient State Compression**: Techniques like those used in FlashFFTConv, which involve matrix decomposition and kernel fusion, can be adapted to compress states efficiently in autoregressive models. This reduces memory bandwidth and improves performance[Fu et al., 2023].\n- **Cache-Friendly Implementations**: Optimizing memory access patterns and using tiling strategies can significantly improve the performance of autoregressive models by reducing memory bandwidth and improving cache utilization[Liu et al., 2024].\n\n### Integration with Hierarchical Attention\n\n- **Multi-Scale Processing**: Integrating hierarchical attention mechanisms that process information at multiple scales can help capture both low and high-frequency components of the input data. This is similar to the approach used in image super-resolution models that enhance context and salient information at different scales.\n- **Frequency-Domain Attention Mechanisms**: Using Fourier transforms to replace or complement attention mechanisms, as seen in the Fourier Transformer, can help in capturing long-range dependencies efficiently. This approach can be integrated with hierarchical attention to improve the model's ability to process complex sequences[He et al., 2023].\n- **Efficient Implementation Techniques**: Implementing hierarchical attention in a way that is hardware-efficient, such as using linear attention in a divide-and-conquer manner, can help in managing computational complexity and improving performance[Liu et al., 2024].\n\nBy combining these techniques, researchers can design autoregressive language models that are more efficient, stable, and capable of capturing a wide range of frequency components, leading to improved performance in terms of perplexity, accuracy, and robustness.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about frequency-domain operations in neural networks and their integration with attention mechanisms. Key findings include:\n\na) Hardware Efficiency:\n- FlashFFTConv demonstrates efficient FFT implementations using tensor cores\n- Memory access patterns and I/O optimization are crucial for performance\n- Hardware-specific optimizations can significantly improve throughput\n\nb) Integration Approaches:\n- FNet shows successful replacement of attention with Fourier transforms\n- Hybrid approaches combining frequency and spatial domain operations show promise\n- Hierarchical processing can help manage computational complexity\n\nc) Training Considerations:\n- Frequency principle suggests networks learn from low to high frequencies\n- Training stability requires careful consideration of spectral components\n- Memory optimization techniques are crucial for efficient implementation\n\n2. Useful References with Excerpts:\n\nFrom FlashFFTConv:\n\"FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O.\"\n\nFrom Fourier Transformer:\n\"Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models.\"\n\nFrom CHELA:\n\"leverages the thought of tiling and hierarchy to propose CHELA... which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.\"\n\n3. Key Insights:\n\na) Hardware Optimization:\n- Matrix decomposition for efficient FFT computation\n- Kernel fusion for reducing memory bandwidth\n- Tiling strategies for better cache utilization\n\nb) Architectural Design:\n- Hierarchical processing can help manage complexity\n- Frequency-domain operations can complement attention\n- Adaptive filtering may improve efficiency\n\nc) Implementation Considerations:\n- Memory access patterns significantly impact performance\n- Hardware-specific optimizations are crucial\n- Training stability requires careful design\n\n4. Future Search Plan:\n\nNeed more information about:\n1. Adaptive frequency filtering techniques\n2. Training stability in frequency-domain models\n3. Memory optimization for long sequences\n4. Integration with hierarchical attention\n\nProposed search queries:\n- \"adaptive frequency filtering neural networks\"\n- \"frequency domain training stability\"\n- \"hierarchical attention memory optimization\"\n- \"long sequence FFT optimization\"\n\n5. References:\n\n[1] Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[2] He, Z., Yang, M. D., Feng, M., Yin, J., Wang, X., Leng, J., & Lin, Z. (2023). Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator. ACL 2023.\n\n[3] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. ICLM 2024.\n\n[4] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. ICLM 2023.\n\n[5] Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., & Han, S. (2020). HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. ACL 2020."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.9388620000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.1450280000000002,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalAdaptiveAttention",
    "proposal": "# FreqHAA-GPT: Frequency-Domain Enhanced Hierarchical Adaptive Attention for Efficient Language Modeling\n\n## Motivation\n\nCurrent language models face significant challenges in efficiently modeling long-range dependencies while maintaining computational efficiency. While HieraNorm-AttnGPT introduced hierarchical adaptive attention to address these challenges, there remain opportunities for improvement through frequency-domain operations, which can capture global patterns more efficiently and enable better hardware utilization.\n\nThe key limitations of current approaches include:\n1. Inefficient computation of long-range dependencies\n2. Limited expressiveness in capturing global patterns\n3. Suboptimal hardware utilization\n4. High memory bandwidth requirements\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **Computational Efficiency**:\n   - Traditional attention mechanisms scale quadratically with sequence length\n   - Linear attention variants often sacrifice expressiveness for efficiency\n   - Hardware resources not fully utilized\n\n2. **Long-Range Dependencies**:\n   - Difficulty in capturing global patterns efficiently\n   - Limited by locality bias in current architectures\n   - Memory bandwidth bottlenecks\n\n3. **Hardware Utilization**:\n   - Inefficient use of specialized hardware (e.g., tensor cores)\n   - High memory I/O overhead\n   - Suboptimal cache utilization\n\n### Core Philosophy\n\nFreqHAA-GPT addresses these limitations by integrating frequency-domain operations into hierarchical adaptive attention. The core ideas are:\n\n1. **Frequency-Domain Processing**:\n   - Transform attention computations to frequency domain for efficient global pattern capture\n   - Enable multi-scale frequency analysis for better feature representation\n   - Utilize hardware-optimized FFT operations\n\n2. **Adaptive Frequency Filtering**:\n   - Dynamic selection of relevant frequency components\n   - Content-dependent frequency band emphasis\n   - Efficient compression in frequency domain\n\n3. **Hardware-Aware Implementation**:\n   - Matrix decomposition for efficient FFT computation\n   - IO-aware algorithm design\n   - Cache-friendly memory access patterns\n\n## Design Plan\n\n### Modifications to Hierarchical Adaptive Attention\n\nThe FreqHAA variant enhances the original hierarchical adaptive attention through:\n\n1. **Frequency-Domain Transformation**:\n   ```python\n   def frequency_transform(x):\n       # Shape: (B, H, L, D) -> (B, H, L, D)\n       return torch.fft.rfft2(x, dim=(-2, -1))\n   ```\n\n2. **Adaptive Frequency Filtering**:\n   ```python\n   def adaptive_frequency_filter(x_freq, filter_params):\n       # Generate dynamic filters\n       filters = self.filter_generator(filter_params)\n       # Apply filters in frequency domain\n       return x_freq * filters\n   ```\n\n3. **Efficient Implementation**:\n   ```python\n   class FreqHAA(nn.Module):\n       def __init__(self, dim, num_heads, num_scales):\n           super().__init__()\n           self.freq_proj = nn.Linear(dim, dim)\n           self.filter_generator = nn.Sequential(\n               nn.Linear(dim, dim // 2),\n               nn.ReLU(),\n               nn.Linear(dim // 2, dim)\n           )\n           \n       def forward(self, x):\n           # Transform to frequency domain\n           x_freq = frequency_transform(x)\n           \n           # Generate and apply adaptive filters\n           filter_params = self.freq_proj(x)\n           x_filtered = adaptive_frequency_filter(x_freq, filter_params)\n           \n           # Transform back to time domain\n           return torch.fft.irfft2(x_filtered, dim=(-2, -1))\n   ```\n\n### Mathematical Formulation\n\n1. **Frequency-Domain Attention**:\n   \\[\n   \\text{FreqAttn}(Q, K, V) = \\text{IFFT}(\\text{FFT}(QK^T) \\odot F(V))\n   \\]\n   where F(V) represents the adaptive frequency filters.\n\n2. **Adaptive Filtering**:\n   \\[\n   F(V) = \\sigma(W_f \\cdot \\text{FFT}(V))\n   \\]\n   where W_f are learnable parameters and \u03c3 is a non-linear activation.\n\n3. **Multi-Scale Integration**:\n   \\[\n   Y = \\sum_{s=1}^S \\alpha_s \\cdot \\text{FreqAttn}_s(X)\n   \\]\n   where S is the number of scales and \u03b1_s are learned scale weights.\n\n### Implementation Guidelines\n\n1. **Initialization**:\n   ```python\n   def init_frequency_filters(module):\n       if isinstance(module, FreqHAA):\n           nn.init.kaiming_normal_(module.freq_proj.weight)\n           nn.init.zeros_(module.freq_proj.bias)\n   ```\n\n2. **Memory Management**:\n   ```python\n   def optimize_memory_access(x, chunk_size):\n       # Process in memory-efficient chunks\n       chunks = torch.split(x, chunk_size, dim=-2)\n       results = []\n       for chunk in chunks:\n           result = process_chunk(chunk)\n           results.append(result)\n       return torch.cat(results, dim=-2)\n   ```\n\n3. **Hardware Optimization**:\n   ```python\n   def hardware_efficient_fft(x):\n       # Use matrix multiplication for small FFTs\n       if x.shape[-1] <= 32:\n           return matrix_fft(x)\n       # Use FFT for larger sizes\n       return torch.fft.fft(x)\n   ```\n\n## Theoretical Analysis\n\n1. **Computational Complexity**:\n   - Time complexity: O(N log N) for frequency transforms\n   - Space complexity: O(N) for intermediate representations\n   - Overall complexity remains linear in sequence length\n\n2. **Memory Efficiency**:\n   - Reduced memory bandwidth through frequency-domain compression\n   - Efficient cache utilization through blocked processing\n   - Adaptive precision in frequency domain\n\n3. **Expressiveness**:\n   - Enhanced global pattern capture through frequency analysis\n   - Multi-scale representation through hierarchical processing\n   - Adaptive feature selection through learned filters\n\n## Conclusion\n\nFreqHAA-GPT introduces a novel approach to enhancing hierarchical adaptive attention through frequency-domain operations. The design maintains computational efficiency while improving expressiveness and hardware utilization. Key advantages include:\n\n1. Efficient long-range dependency modeling through frequency-domain operations\n2. Improved hardware utilization through optimized implementations\n3. Enhanced expressiveness through multi-scale frequency analysis\n4. Reduced memory bandwidth requirements through adaptive compression\n\n## References\n\n[1] Fu, D. Y., et al. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\". arXiv preprint.\n\n[2] Lee-Thorp, J., et al. (2021). \"FNet: Mixing tokens with fourier transforms\". NAACL.\n\n[3] Fu, D. Y., et al. (2023). \"Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture\". NeurIPS.\n\n[4] Poli, M., et al. (2022). \"Transform Once: Efficient Operator Learning in Frequency Domain\". NeurIPS.\n\n[5] Lee, C., & Kim, H. S. (2023). \"Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\". ICLR.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "FreqHAA-GPT enhances HieraNorm-AttnGPT by integrating frequency-domain operations into hierarchical adaptive attention, enabling efficient long-range dependency modeling through adaptive frequency filtering and hardware-optimized FFT computations. The design maintains linear complexity while improving expressiveness through multi-scale frequency analysis.",
    "ideation": null,
    "modelname": "freqhaa_gpt",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of FFT computational overhead\n- Include concrete examples of frequency filter adaptation\n- Add guidelines for initialization and training procedures\n\n2. Memory Analysis:\n- Include quantitative analysis of memory savings\n- Provide detailed cache utilization strategies\n- Address potential memory bottlenecks in large-scale deployment\n\n3. Training Considerations:\n- Add specific guidance for handling gradient flow in frequency domain\n- Include stability analysis across different scales\n- Provide recommendations for hyperparameter tuning\n\n4. Hardware Optimization:\n- Provide specific guidelines for different hardware platforms\n- Include memory bandwidth analysis\n- Add cache utilization strategies\n- Consider different parallel processing approaches\n\n5. Empirical Validation:\n- Suggest specific benchmarks for evaluating the proposed improvements\n- Include ablation study designs\n- Propose metrics for measuring efficiency gains\n\n6. Integration Guidelines:\n- Provide detailed integration steps with existing transformer architectures\n- Include guidance for adapting the design to different model sizes\n- Add recommendations for fine-tuning procedures",
    "user_input": ""
}