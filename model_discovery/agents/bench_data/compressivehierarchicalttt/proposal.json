{
    "variantname": "CompressiveTTTLinear",
    "review": "The CompressiveTTTLinear proposal presents an innovative approach to enhancing FastTTTLinear through hierarchical compressive memory and multi-scale attention mechanisms. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\n1. CLARITY (4.2/5)\nThe proposal provides:\n- Clear articulation of objectives and motivation\n- Well-structured design components\n- Detailed mathematical formulations\n- Comprehensive implementation guidelines\n\nHowever, some aspects need clarification:\n- Specific interaction between memory levels\n- Details of compression rate adaptation\n- Gradient flow mechanisms across hierarchy levels\n\n2. INNOVATION (4.3/5)\nThe proposal demonstrates significant innovation through:\n- Novel integration of hierarchical memory with TTT\n- Adaptive compression mechanisms\n- Multi-scale attention patterns\n- Dynamic routing for efficient computation\n\nComparison with existing research shows several unique contributions:\n- More efficient than LoMA's lossless compression\n- More adaptive than Infini-attention's fixed memory structure\n- Better scaling properties than traditional hierarchical approaches\n\n3. FEASIBILITY (4.0/5)\nImplementation appears feasible within the framework:\n- Clear pseudo-code implementation\n- Modular design structure\n- Reasonable computational requirements\n- Hardware-aware optimizations\n\n4. SCALABILITY (4.4/5)\nStrong potential for scaling:\n- Hierarchical structure supports longer sequences\n- Adaptive compression enables efficient memory usage\n- Multi-scale processing optimizes computation\n- Hardware-efficient implementation strategies\n\n5. ACCURACY AND ROBUSTNESS (4.1/5)\nExpected improvements in:\n- Better handling of long-term dependencies through hierarchical memory\n- Enhanced robustness through multi-scale processing\n- Improved adaptation to varying input patterns\n- Stable performance across different sequence lengths\n\n6. EFFICIENCY (4.5/5)\nSignificant efficiency considerations:\n- Reduced memory requirements through adaptive compression\n- Optimized computation through multi-scale attention\n- Efficient gradient propagation\n- Hardware-aware implementation strategies",
    "search_stack": [
        {
            "ready": false,
            "query": "memory augmented neural networks, adaptive computation, efficient attention mechanisms",
            "detail": "Analyze recent developments in:\n1. Memory compression and management techniques for neural networks\n2. Adaptive computation methods for transformers\n3. Efficient implementations of hybrid attention mechanisms\n4. Dynamic architecture adaptation approaches\n5. Memory-augmented neural networks with focus on:\n   - Compression techniques\n   - Dynamic allocation\n   - Hierarchical structures\n   - Integration with attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in:\n1. Memory compression and management techniques for neural networks\n2. Adaptive computation methods for transformers\n3. Efficient implementations of hybrid attention mechanisms\n4. Dynamic architecture adaptation approaches\n5. Memory-augmented neural networks with focus on:\n   - Compression techniques\n   - Dynamic allocation\n   - Hierarchical structures\n   - Integration with attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Recurrent Memory Transformer (Avg. Score: 0.98)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 0.98)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n#### 2. Memformer: A Memory-Augmented Transformer for Sequence Modeling (Avg. Score: 0.97)\n\n*Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, A. Geramifard, Zhou Yu*\n\n**Published in:** AACL/IJCNLP (2020)\t**Cited by** 36  (*Influential: 2*)\n\n**TL;DR:** This work presents Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information, and proposes a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back- Propagation through time with a significantly reduced memory requirement.\n\n**Abstract:** Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.97)*\n\n```\nTransformers have achieved great success in various natural language processing tasks. Nevertheless, the quadratic computation complexity can be costly. Some works try to address the limitations of selfattention, including Reformer, Sparse Transformer, Longformer, Linformer (Child et al., 2019; Kitaev et al., 2020; Wang et al., 2020), etc. They successfully reduce the complexity of self-attention and thus enable processing longer sequences. However, most of them still require linear memory space complexity. Transformer-XL (Dai et al., 2019) re-introduces the concept of memory and recurrence. It caches each layer's hidden states of self-attention into a fixed-size queue and re-uses them in the later attention computation. However, the memory as raw hidden states cannot effectively compress highlevel information. Thus, Transformer-XL in practice needs a massive memory size to perform well, and spends huge computation in using its memory. Compressive Transformer (Rae et al., 2020) improves upon Transformer-XL by further compressing its memories into fewer vectors via a compression network. However, as mentioned in the papers, both Transformer-XL and Compressive Transformer discard the information from the distant past, which causes a theoretical maximum tempo-\nral range given the fixed memory size. Inspired by the previous external memory networks, we propose Memformer, which incorporates a fixed-size external dynamic memory combined with the recent Transformer architecture. Memformer interacts with its external dynamic memory through the memory reading and writing modules. Also, we introduce a forgetting mechanism to improve the effectiveness of memorizing new information. By utilizing recurrence and a fixedsize memory, our model has a theoretically infinite temporal range of memorization and implies a linear computation complexity and constant memory space complexity. As the traditional backpropagation through time (BPTT) has an unaffordable memory cost in our model, we introduce a new optimization scheme, memory replay backpropagation (MRBP), to significantly reduce the memory cost in training recurrent neural networks with large size of memory representations. We evaluate Memformer on the autoregressive image generation and language modeling task. Experimental results show that Memformer performs on par with Transformer and Transformer XL with large memory size, while being much more efficient in terms of computation speed and memory space consumption. We also conduct an analysis showing that Memformer can retain information for an extended period. ## 2 Related Work\n\nThis section introduces some recent research directions that aim to alleviate the quadratic cost of selfattention. Moreover, we analyze their assumptions and limitations under the autoregressive setting to provide a broader view of these models. ### 2.1 Sparse Attention\n\nOne influential direction is to replace the full selfattention with sparse attention patterns to speed up the computation. Child et al. (2019) proposed Sparse Transformer, using a block sparse attention pattern to reduce the computation complexity to $\\mathcal{O}(N \\sqrt{N})$. Later, Longformer (Beltagy et al., 2020) and Big Bird (Zaheer et al., 2020) further explored this direction and proposed an even more sparse attention pattern to reduce the cost to $\\mathcal{O}(N)$. They introduced global tokens to encode the information from the entire sequence and kept the selfattention to the closest $k$ tokens and the global tokens to achieve linear complexity. Although linear sparse attention's theoretical soundness is proven for bidirectional encoders, it does not hold for the decoder. The main reason is that the global tokens cannot leak information to the future tokens in the autoregressive setting, where all the tokens can only see their previous tokens. Thus, linear sparse attention cannot guarantee a token to see its all past tokens. Only Sparse Transformer here with $\\mathcal{O}(N \\sqrt{N})$ complexity can theoretically cover all the past tokens for the sequence generation. ### 2.2 Linear Attention\n\nAnother direction is focusing on improving the softmax operation in the self-attention. Linformer (Wang et al., 2020) reduced the complexity to $\\mathcal{O}(N)$ by projecting the entire sequence to a constant size of keys and values, but this method has not been applied to autoregressive decoding. Performer (Choromanski et al., 2020) and Linear Transformer (Katharopoulos et al., 2020) used a linear dot-product of kernel feature maps to replace softmax. However, for Linear Transformer under the autoregressive setting, it needs to compute the cumulative summation to aggregate the history information. This assumption is too strong if the input sequence is long and the length is not fixed. After thousands of steps, the numerical values can become very large due to the summation, causing overflow and gradient instability. ### 2.3 Recurrence and Memory\n\nApplying recurrence and memory to Transformers is an orthogonal direction comparing to the efficient attention approaches. If the memory size is constant, recurrence enables the model to have constant memory complexity during inference. There are mainly two works exploring this direction. Transformer-XL (Dai et al., 2019) used relative positional encoding and consisted of a segmentlevel recurrence mechanism to encode beyond a fixed-length context. Compressive Transformer (Rae et al., 2020) extended from Transformer-XL by further compressing the previous cached hidden states to achieve a longer context. However, using past hidden states as memory would cause a theoretical maximum temporal range of context, meaning that a token is not guaranteed to see all the past tokens. Thus, in practice, Transformer-XL and Compressive Transformer need huge memory size to achieve good performance. ![](https://cdn.mathpix.com/cropped/2024_09_12_a5b0bfb29582fd0f4766g-03.jpg?height=915&width=717&top_left_y=225&top_left_x=224)\n\nFigure 1: Memformer overall architecture for the encoder (left) and decoder (right). Transformer encoder is responsible to interact with the memory. Sequence modeling is achieved by predicting the next segment conditioned to the current segment and memory. ### 2.3.1 Dynamic Memorization\n\nWithin the scope of memory networks, there are dynamic memorization techniques. Different from Transformer-XL which stores the token-level history representations as memory, dynamic memorization does not have a theoretical upper bound for the temporal range. Neural Turing Machine (NTM) (Graves et al., 2014) and Differential Neural Computer (DNC) (Graves et al., 2016) are two early models that can control external memory resources to achieve long-lasting memory.\n```\n\n#### 3. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.94)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.94)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n#### 4. \u221e-former: Infinite Memory Transformer (Avg. Score: 0.76)\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins*\n\n**Published in:** Volume 1 (2022)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** N/A\n\n##### *Relevant Chunk: No. 17/32 (Score: 0.76)*\n\n```\nIn Proc. $A C L$. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. 2020. Location Attention for Extrapolation to Longer Sequences. In Proc. ACL. Angela Fan, Claire Gardent, Chlo\u00e9 Braud, and Antoine Bordes. 2021. Augmenting Transformers with KNN-Based Composite Memory for Dialog. Transactions of the Association for Computational Linguistics. Ant\u00f3nio Farinhas, Andr\u00e9 F. T. Martins, and P. Aguiar. 2021. Multimodal Continuous Visual Attention Mechanisms. Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016. Improving Neural Language Models with a Continuous Cache. In Proc. ICLR. Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. 2015. Learning to transduce with unbounded memory. Proc. NeurIPS. Maosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaussian Transformer: A Lightweight Approach for Natural Language Inference.\n```\n\n#### 5. Memorizing Transformers (Avg. Score: 0.70)\n\n*Yuhuai Wu, M. Rabe, DeLesley S. Hutchins, Christian Szegedy*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 138  (*Influential: 15*)\n\n**TL;DR:** It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).\n\n**Abstract:** Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.\n\n##### *Relevant Chunk: No. 7/26 (Score: 0.70)*\n\n```\nIn $A C L, 2019$. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. Angela Fan, Claire Gardent, Chlo\u00e9 Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computational Linguistics, 9:82-99, 2021. Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In ICLR, 2017. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In ICML, 2020. Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. URL/https://arxiv.org/ $\\mathrm{abs} / 2106.06899$. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In ICML, 2020. Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: memory augmented neural networks, adaptive computation, efficient attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Meta-Learning with Memory-Augmented Neural Networks\n\n*From Search Query: memory augmented neural networks*\n\n*Adam Santoro, Sergey Bartunov, M. Botvinick, Daan Wierstra, T. Lillicrap*\n\n**TL;DR:** The ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples is demonstrated.\n\n**Abstract:** Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of \"one-shot learning.\" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2016\n\n**Citations:** 1697  (*Influential: 92*)\n\n#### 2. Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\n\n*From Search Query: memory augmented neural networks*\n\n*Jack W. Rae, Jonathan J. Hunt, Ivo Danihelka, Tim Harley, A. Senior, Greg Wayne, Alex Graves, T. Lillicrap*\n\n**TL;DR:** This work presents an end-to-end differentiable memory access scheme, which they call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories, and achieves asymptotic lower bounds in space and time complexity.\n\n**Abstract:** Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows \u2014 limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,000 x faster and with 3,000x less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 156  (*Influential: 20*)\n\n#### 3. Sentence Simplification with Memory-Augmented Neural Networks\n\n*From Search Query: memory augmented neural networks*\n\n*Tu Vu, Baotian Hu, Tsendsuren Munkhdalai, Hong Yu*\n\n**TL;DR:** This paper adapts an architecture with augmented memory capacities called Neural Semantic Encoders for sentence simplification, and demonstrates the effectiveness of this approach on different simplification datasets, both in terms of automatic evaluation measures and human judgments.\n\n**Abstract:** Sentence simplification aims to simplify the content and structure of complex sentences, and thus make them easier to interpret for human readers, and easier to process for downstream NLP applications. Recent advances in neural machine translation have paved the way for novel approaches to the task. In this paper, we adapt an architecture with augmented memory capacities called Neural Semantic Encoders (Munkhdalai and Yu, 2017) for sentence simplification. Our experiments demonstrate the effectiveness of our approach on different simplification datasets, both in terms of automatic evaluation measures and human judgments.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2018\n\n**Citations:** 57  (*Influential: 6*)\n\n#### 4. Scalable Adaptive Computation for Iterative Generation\n\n*From Search Query: adaptive computation*\n\n*A. Jabri, David J. Fleet, Ting Chen*\n\n**TL;DR:** The Recurrent Interface Networks (RINs) are proposed, an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data.\n\n**Abstract:** Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024X1024 images without cascades or guidance, while being domain-agnostic and up to 10X more efficient than 2D and 3D U-Nets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 85  (*Influential: 13*)\n\n#### 5. AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation\n\n*From Search Query: adaptive computation*\n\n*Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, L. Lakshmanan, A. Awadallah, S\u00e9bastien Bubeck, Jianfeng Gao*\n\n**TL;DR:** AutoMoE is developed -- a framework for designing heterogeneous MoE's under computational constraints and leverages Neural Architecture Search to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, on aggregate over benchmark datasets for NMT.\n\n**Abstract:** Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 6. Efficient Attention via Control Variates\n\n*From Search Query: efficient attention mechanisms*\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 7. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: efficient attention mechanisms*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 8. Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning\n\n*From Search Query: efficient attention mechanisms*\n\n*Mattia Atzeni, Mrinmaya Sachan, Andreas Loukas*\n\n**TL;DR:** This work focuses on geometry priors and introduces LatFormer, a model that incorporates lattice symmetry priors in attention masks that provides preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Abstract:** The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer requires 2 orders of magnitude fewer data than standard attention and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric priors provide preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Memory-Augmented Neural Networks for Machine Translation\n\n*From Search Query: memory augmented neural networks*\n\n*Mark Collier, Joeran Beel*\n\n**Abstract:** Memory-augmented neural networks (MANNs) have been shown to outperform other\nrecurrent neural network architectures on a series of artificial sequence\nlearning tasks, yet they have had limited application to real-world tasks. We\nevaluate direct application of Neural Turing Machines (NTM) and Differentiable\nNeural Computers (DNC) to machine translation. We further propose and evaluate\ntwo models which extend the attentional encoder-decoder with capabilities\ninspired by memory augmented neural networks. We evaluate our proposed models\non IWSLT Vietnamese to English and ACL Romanian to English datasets. Our\nproposed models and the memory augmented neural networks perform similarly to\nthe attentional encoder-decoder on the Vietnamese to English translation task\nwhile have a 0.3-1.9 lower BLEU score for the Romanian to English task.\nInterestingly, our analysis shows that despite being equipped with additional\nflexibility and being randomly initialized memory augmented neural networks\nlearn an algorithm for machine translation almost identical to the attentional\nencoder-decoder.\n\n**Published:** 2019-09-18T09:39:14Z  (*Updated: 2019-09-18T09:39:14Z*)\n\n\n\n#### 2. A2P-MANN: Adaptive Attention Inference Hops Pruned Memory-Augmented\n  Neural Networks\n\n*From Search Query: memory augmented neural networks*\n\n*Mohsen Ahmadzadeh, Mehdi Kamal, Ali Afzali-Kusha, Massoud Pedram*\n\n**Abstract:** In this work, to limit the number of required attention inference hops in\nmemory-augmented neural networks, we propose an online adaptive approach called\nA2P-MANN. By exploiting a small neural network classifier, an adequate number\nof attention inference hops for the input query is determined. The technique\nresults in elimination of a large number of unnecessary computations in\nextracting the correct answer. In addition, to further lower computations in\nA2P-MANN, we suggest pruning weights of the final FC (fully-connected) layers.\nTo this end, two pruning approaches, one with negligible accuracy loss and the\nother with controllable loss on the final accuracy, are developed. The efficacy\nof the technique is assessed by using the twenty question-answering (QA) tasks\nof bAbI dataset. The analytical assessment reveals, on average, more than 42%\nfewer computations compared to the baseline MANN at the cost of less than 1%\naccuracy loss. In addition, when used along with the previously published\nzero-skipping technique, a computation count reduction of up to 68% is\nachieved. Finally, when the proposed approach (without zero-skipping) is\nimplemented on the CPU and GPU platforms, up to 43% runtime reduction is\nachieved.\n\n**Published:** 2021-01-24T12:02:12Z  (*Updated: 2022-02-23T07:07:03Z*)\n\n\n\n#### 3. Duo-LLM: A Framework for Studying Adaptive Computation in Large Language\n  Models\n\n*From Search Query: adaptive computation*\n\n*Keivan Alizadeh, Iman Mirzadeh, Hooman Shahrokhi, Dmitry Belenko, Frank Sun, Minsik Cho, Mohammad Hossein Sekhavat, Moin Nabi, Mehrdad Farajtabar*\n\n**Abstract:** Large Language Models (LLMs) typically generate outputs token by token using\na fixed compute budget, leading to inefficient resource utilization. To address\nthis shortcoming, recent advancements in mixture of expert (MoE) models,\nspeculative decoding, and early exit strategies leverage the insight that\ncomputational demands can vary significantly based on the complexity and nature\nof the input. However, identifying optimal routing patterns for dynamic\nexecution remains an open challenge, limiting the full potential of these\nadaptive methods. To address this need, we study adaptive computation in LLMs\nmore systematically. We propose a novel framework that integrates smaller\nauxiliary modules within each Feed-Forward Network layer of the LLM. This\ndesign enables dynamic routing of tokens based on task complexity: tokens can\nbe processed by either the small or big modules at each layer, or even bypass\ncertain layers entirely. This allows us to introduce a novel notion of a\ntoken's difficulty, defined by its potential to benefit from additional\ncomputational resources. Importantly, by employing oracles to identify optimal\npatterns of adaptive computations, we gain valuable insights into the internal\nworkings of LLMs and the routing processes in a simplified heterogeneous MoE\nsetup. We show that trained routers operate differently from oracles and often\nyield suboptimal solutions. Notably, activating a large module in just one\nlayer outperforms models that use large modules across all layers, underscoring\nthe gap between practical implementations of routing in MoE models and\ntheoretical optima for adaptive computation.\n\n**Published:** 2024-10-01T16:10:21Z  (*Updated: 2024-10-01T16:10:21Z*)\n\n\n\n#### 4. CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs\n\n*From Search Query: adaptive computation*\n\n*Weijie Lv, Xuan Xia, Sheng-Jun Huang*\n\n**Abstract:** Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.\n\n**Published:** 2024-08-05T02:38:48Z  (*Updated: 2024-08-05T02:38:48Z*)\n\n\n\n#### 5. LSTM-based Deep Learning Models for Non-factoid Answer Selection\n\n*From Search Query: efficient attention mechanisms*\n\n*Ming Tan, Cicero dos Santos, Bing Xiang, Bowen Zhou*\n\n**Abstract:** In this paper, we apply a general deep learning (DL) framework for the answer\nselection task, which does not depend on manually defined features or\nlinguistic tools. The basic framework is to build the embeddings of questions\nand answers based on bidirectional long short-term memory (biLSTM) models, and\nmeasure their closeness by cosine similarity. We further extend this basic\nmodel in two directions. One direction is to define a more composite\nrepresentation for questions and answers by combining convolutional neural\nnetwork with the basic framework. The other direction is to utilize a simple\nbut efficient attention mechanism in order to generate the answer\nrepresentation according to the question context. Several variations of models\nare provided. The models are examined by two datasets, including TREC-QA and\nInsuranceQA. Experimental results demonstrate that the proposed models\nsubstantially outperform several strong baselines.\n\n**Published:** 2015-11-12T22:01:54Z  (*Updated: 2016-03-28T04:12:45Z*)\n\n\n\n#### 6. Efficient Transformer Knowledge Distillation: A Performance Review\n\n*From Search Query: efficient attention mechanisms*\n\n*Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence*\n\n**Abstract:** As pretrained transformer language models continue to achieve\nstate-of-the-art performance, the Natural Language Processing community has\npushed for advances in model compression and efficient attention mechanisms to\naddress high computational requirements and limited input sequence length.\nDespite these separate efforts, no investigation has been done into the\nintersection of these two fields. In this work, we provide an evaluation of\nmodel compression via knowledge distillation on efficient attention\ntransformers. We provide cost-performance trade-offs for the compression of\nstate-of-the-art efficient attention architectures and the gains made in\nperformance in comparison to their full attention counterparts. Furthermore, we\nintroduce a new long-context Named Entity Recognition dataset, GONERD, to train\nand test the performance of NER models on long sequences. We find that\ndistilled efficient attention transformers can preserve a significant amount of\noriginal model performance, preserving up to 98.6% across short-context tasks\n(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context\nQuestion-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on\nlong-context Named Entity Recognition (GONERD), while decreasing inference\ntimes by up to 57.8%. We find that, for most models on most tasks, performing\nknowledge distillation is an effective method to yield high-performing\nefficient attention models with low costs.\n\n**Published:** 2023-11-22T19:19:37Z  (*Updated: 2023-11-22T19:19:37Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. One-shot Learning with Memory-Augmented Neural Networks\n\n*From Search Query: memory augmented neural networks*\n\n*Matthew Botvinick, Adam Santoro, Sergey Bartunov, Timothy Lillicrap, Daan Wierstra*\n\n**Abstract:** Despite recent breakthroughs in the applications of deep neural networks, one\nsetting that presents a persistent challenge is that of \"one-shot learning.\"\nTraditional gradient-based networks require a lot of data to learn, often\nthrough extensive iterative training. When new data is encountered, the models\nmust inefficiently relearn their parameters to adequately incorporate the new\ninformation without catastrophic interference. Architectures with augmented\nmemory capacities, such as Neural Turing Machines (NTMs), offer the ability to\nquickly encode and retrieve new information, and hence can potentially obviate\nthe downsides of conventional models. Here, we demonstrate the ability of a\nmemory-augmented neural network to rapidly assimilate new data, and leverage\nthis data to make accurate predictions after only a few samples. We also\nintroduce a new method for accessing an external memory that focuses on memory\ncontent, unlike previous methods that additionally use memory location-based\nfocusing mechanisms.\n\n**Published:** 2016-05-19\n\n\n\n#### 2. Text normalization using memory augmented neural networks\n\n*From Search Query: memory augmented neural networks*\n\n*Aman Hussain, Subhojeet Pramanik*\n\n**Abstract:** We perform text normalization, i.e. the transformation of words from the\nwritten to the spoken form, using a memory augmented neural network. With the\naddition of dynamic memory access and storage mechanism, we present a neural\narchitecture that will serve as a language-agnostic text normalization system\nwhile avoiding the kind of unacceptable errors made by the LSTM-based recurrent\nneural networks. By successfully reducing the frequency of such mistakes, we\nshow that this novel architecture is indeed a better alternative. Our proposed\nsystem requires significantly lesser amounts of data, training time and compute\nresources. Additionally, we perform data up-sampling, circumventing the data\nsparsity problem in some semiotic classes, to show that sufficient examples in\nany particular class can improve the performance of our text normalization\nsystem. Although a few occurrences of these errors still remain in certain\nsemiotic classes, we demonstrate that memory augmented networks with\nmeta-learning capabilities can open many doors to a superior text normalization\nsystem.\n\n**Published:** 2018-05-31\n\n\n\n#### 3. Adaptive Computation Time for Recurrent Neural Networks\n\n*From Search Query: adaptive computation*\n\n*Alex Graves*\n\n**Abstract:** This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.\n\n**Published:** 2016-03-29\n\n\n\n#### 4. Adaptive Computation with Elastic Input Sequence\n\n*From Search Query: adaptive computation*\n\n*Yang You, Mostafa Dehghani, Neil Houlsby, Anurag Arnab, Valerii Likhosherstov, Fuzhao Xue*\n\n**Abstract:** Humans have the ability to adapt the type of information they use, the procedure they employ, and the amount of time they spend when solving problems. However, most standard neural networks have a fixed function type and computation budget regardless of the sample's nature or difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we introduce a new approach called AdaTape, which allows for dynamic computation in neural networks through adaptive tape tokens. AdaTape utilizes an elastic input sequence by equipping an architecture with a dynamic read-and-write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank which can be either trainable or derived from input data. We examine the challenges and requirements to obtain dynamic sequence content and length, and propose the Adaptive Tape Reading (ATR) algorithm to achieve both goals. Through extensive experiments on image recognition tasks, we show that AdaTape can achieve better performance while maintaining the computational cost. To facilitate further research, we have released code at https://github.com/google-research/scenic.\n\n**Published:** 2023-01-30\n\n\n\n#### 5. Linear Attention Mechanism: An Efficient Attention for Semantic Segmentation\n\n*From Search Query: efficient attention mechanisms*\n\n*Jianlin Su, Rui Li, Shunyi Zheng, Chenxi Duan*\n\n**Abstract:** In this paper, to remedy this deficiency, we propose a Linear Attention Mechanism which is approximate to dot-product attention with much less memory and computational costs. The efficient design makes the incorporation between attention mechanisms and neural networks more flexible and versatile. Experiments conducted on semantic segmentation demonstrated the effectiveness of linear attention mechanism. Code is available at https://github.com/lironui/Linear-Attention-Mechanism.\n\n**Published:** 2020-07-29\n\n\n\n#### 6. BAM: A Balanced Attention Mechanism for Single Image Super Resolution\n\n*From Search Query: efficient attention mechanisms*\n\n*Cheng Shen, Haotian Hu, Fanyi Wang*\n\n**Abstract:** Recovering texture information from the aliasing regions has always been a major challenge for Single Image Super Resolution (SISR) task. These regions are often submerged in noise so that we have to restore texture details while suppressing noise. To address this issue, we propose a Balanced Attention Mechanism (BAM), which consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial Attention Module (MSAM) in parallel. ACAM is designed to suppress extreme noise in the large scale feature maps while MSAM preserves high-frequency texture details. Thanks to the parallel structure, these two modules not only conduct self-optimization, but also mutual optimization to obtain the balance of noise reduction and high-frequency texture restoration during the back propagation process, and the parallel structure makes the inference faster. To verify the effectiveness and robustness of BAM, we applied it to 10 SOTA SISR networks. The results demonstrate that BAM can efficiently improve the networks performance, and for those originally with attention mechanism, the substitution with BAM further reduces the amount of parameters and increases the inference speed. Moreover, we present a dataset with rich texture aliasing regions in real scenes, named realSR7. Experiments prove that BAM achieves better super-resolution results on the aliasing area.\n\n**Published:** 2021-04-15\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on memory-augmented neural networks, adaptive computation, and efficient attention mechanisms, here are some key insights and recent developments from the provided sources and additional relevant information:\n\n## Memory Compression and Management Techniques\n\n### Memory-Augmented Neural Networks (MANNs)\n- MANNs integrate external memory components to enhance the ability to store and retrieve information. Techniques like Memory-Augmented Neural Networks can achieve explicit information storage and precise access to specific features through an externally addressable memory network.\n- The Fragmented-Memory-based Trajectory Prediction (FMTP) model uses a learnable memory array to consolidate continuous trajectory representations into discrete forms, reducing redundancy and maintaining essential features. This approach can be adapted for language models to handle long-term dependencies efficiently.\n\n### Dynamic Memory Allocation\n- Dynamic memory allocation based on input complexity can improve efficiency. For instance, techniques like Experience Replay and Generative Replay can help manage memory by retaining a subset of past experiences or generating synthetic examples, allowing the model to revisit and reinforce previous knowledge without storing all previous data.\n\n### Hierarchical Structures\n- Hierarchical memory structures can better capture multi-scale dependencies. The HierarchicalFastTTTLinear design combines temporal latent bottlenecks with memory optimization, suggesting that hierarchical structures can be effective in managing memory for long sequences[Analysis Summary].\n\n## Adaptive Computation Methods for Transformers\n\n### Efficient Attention Mechanisms\n- Efficient attention mechanisms like LSH (Local Sensitive Hashing) attention can reduce the complexity from \\(O(L^2)\\) to \\(O(L \\log L)\\), where \\(L\\) is the sequence length. This is demonstrated in the Reformer model, which also uses reversible residual layers to reduce memory requirements[Analysis Summary: Reformer].\n\n### Hybrid Attention Mechanisms\n- Combining different attention mechanisms (local, global, LSH) can provide better trade-offs between efficiency and expressiveness. Infini-attention incorporates both masked local attention and long-term linear attention mechanisms in a single Transformer block, enabling efficient infinite context handling[Analysis Summary: Leave No Context Behind].\n\n### Adaptive Computation\n- Adaptive switching between attention types based on context can optimize computation. This can be achieved by dynamically selecting the most appropriate attention mechanism based on the input complexity and context length[Analysis Summary].\n\n## Efficient Implementations of Hybrid Attention Mechanisms\n\n### Compressive Memory and Attention\n- Infini-attention integrates a compressive memory into the vanilla attention mechanism, allowing for efficient infinite context handling. This approach combines the strengths of different attention types within a single Transformer block[Analysis Summary: Leave No Context Behind].\n\n### Reversible Architectures\n- Reversible architectures, such as those used in the Reformer model, can significantly reduce memory requirements by storing activations only once instead of multiple times. This is particularly useful for deep architectures where memory efficiency is crucial[Analysis Summary: Reformer].\n\n## Dynamic Architecture Adaptation Approaches\n\n### Progressive Neural Networks\n- Progressive Neural Networks involve creating new neural network columns for new tasks while keeping the old columns intact. This approach allows the model to learn new sequences without losing the context of earlier ones, which can be adapted for dynamic architecture adaptation in autoregressive models.\n\n### Dynamic Architectures\n- Dynamic architectures can add new neurons or layers for new tasks while maintaining the performance on old tasks. This can be particularly useful for autoregressive models that need to adapt to changing input sequences and contexts.\n\n## Memory-Augmented Neural Networks with Focus on Specific Aspects\n\n### Compression Techniques\n- Techniques like quantization operations in the FMTP model can reduce redundancy in stored information while preserving essential features. This can be applied to compress memory in autoregressive models, making them more efficient.\n\n### Dynamic Allocation\n- Dynamic memory allocation can be achieved through methods like Experience Replay and Generative Replay, which allow the model to manage memory based on the complexity of the input data.\n\n### Hierarchical Structures\n- Hierarchical memory structures, as seen in the HierarchicalFastTTTLinear design, can capture multi-scale dependencies more effectively. This hierarchical approach can be integrated into memory-augmented neural networks to improve their performance on long sequences[Analysis Summary].\n\n### Integration with Attention Mechanisms\n- Integrating memory-augmented neural networks with attention mechanisms can enhance the model's ability to focus on relevant information. For example, using a learnable memory array in conjunction with attention mechanisms can help the model recall critical information from past experiences while processing new inputs.\n\nBy incorporating these techniques, researchers can develop autoregressive language models that are more efficient, scalable, and capable of handling long-term dependencies and complex input sequences. The integration of compressive memory, dynamic allocation, hierarchical structures, and hybrid attention mechanisms can significantly enhance the performance and adaptability of these models.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design HierarchicalFastTTTLinear presents a hierarchical multi-scale architecture combining temporal latent bottlenecks with memory optimization. From analyzing the seed design and initial references, several key areas emerge for potential innovation:\n\n1. Infinite Context Handling:\n- The seed design still has limitations in handling very long sequences efficiently\n- References like LM-Infinite and Infini-attention suggest novel approaches for unbounded context length\n\n2. Memory Management:\n- Current approaches use fixed memory structures\n- Opportunity to develop more dynamic and adaptive memory mechanisms\n\n3. Attention Mechanisms:\n- Existing designs often trade off between efficiency and expressiveness\n- Potential to combine strengths of different attention types (LSH, linear, local)\n\nKey References and Excerpts:\n\n1. From \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\":\n\"Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"\n\n2. From \"Reformer: The Efficient Transformer\":\n\"LSH attention changes complexity from O(L^2) to O(L log L), where L is sequence length... reversible residual layers allow storing activations only once instead of N times\"\n\n3. From \"DiffusionBERT\":\n\"Diffusion models offer a promising training strategy that helps improve generation quality... pre-trained denoising language models can be used as good initialization\"\n\nKey Insights:\n\n1. Memory Architecture:\n- Compressive memory mechanisms can enable infinite context handling\n- Dynamic memory allocation based on input complexity could improve efficiency\n- Hierarchical memory structures could better capture multi-scale dependencies\n\n2. Attention Integration:\n- Combining different attention mechanisms (local, global, LSH) could provide better trade-offs\n- Adaptive switching between attention types based on context could optimize computation\n\n3. Training Dynamics:\n- Diffusion-based training could improve generation quality\n- Reversible architectures could significantly reduce memory requirements\n- Pre-trained initialization could accelerate convergence\n\nFuture Search Plan:\n\n1. Need to investigate:\n- Recent advances in memory-augmented neural networks\n- Efficient implementations of hybrid attention mechanisms\n- Dynamic architecture adaptation techniques\n\n2. Specific areas to search:\n- Memory compression techniques\n- Adaptive computation methods\n- Efficient gradient propagation in deep architectures\n\nReferences:\n\n1. Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv preprint.\n\n2. Kitaev, N., Kaiser, L., & Levskaya, A. (2020). Reformer: The Efficient Transformer. International Conference on Learning Representations.\n\n3. He, Z., Sun, T., Wang, K., Huang, X., & Qiu, X. (2022). DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models. Annual Meeting of the Association for Computational Linguistics.\n\n4. Han, C., Wang, Q., Peng, H., Xiong, W., Chen, Y., Ji, H., & Wang, S. (2023). LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models. North American Chapter of the Association for Computational Linguistics."
        },
        {
            "ready": false,
            "query": "adaptive routing mechanisms, state compression neural networks, hybrid attention architectures",
            "detail": "Analyze recent developments in:\n1. Adaptive routing mechanisms in neural networks\n2. State compression techniques for memory models\n3. Hybrid attention architectures focusing on:\n   - Integration methods\n   - Efficiency improvements\n   - Dynamic adaptation\n4. Memory-efficient attention implementations\n5. Dynamic architecture adaptation",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in:\n1. Adaptive routing mechanisms in neural networks\n2. State compression techniques for memory models\n3. Hybrid attention architectures focusing on:\n   - Integration methods\n   - Efficiency improvements\n   - Dynamic adaptation\n4. Memory-efficient attention implementations\n5. Dynamic architecture adaptation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.51)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.51)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Hierarchical Transformers Are More Efficient Language Models (Avg. Score: 0.15)\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, H. Michalewski*\n\n**Published in:** NAACL-HLT (2021)\t**Cited by** 40  (*Influential: 4*)\n\n**TL;DR:** Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.\n\n##### *Relevant Chunk: No. 17/25 (Score: 0.15)*\n\n```\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient content-based sparse attention with routing transformers. Jianlin $\\mathrm{Su}, \\mathrm{Yu} \\mathrm{Lu}$, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, and Y-Lan Boureau. 2020. Multi-scale transformer language models. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers.\n```\n\n#### 3. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.13)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.13)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n#### 4. An Attention Free Transformer (Avg. Score: 0.04)\n\n*Shuangfei Zhai, Walter A. Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, J. Susskind*\n\n**Published in:** arXiv.org (2021)\t**Cited by** 90  (*Influential: 10*)\n\n**TL;DR:** Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention, is introduced and demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.\n\n**Abstract:** We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.04)*\n\n```\nIn $A C L, 2019$. [24] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. ArXiv, abs/2003.05997, 2020. [25] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. ArXiv, abs/1901.10430, 2019. [26] Yi Tay, Dara Bahri, L. Yang, Donald Metzler, and D. Juan. Sparse sinkhorn attention. ArXiv, abs/2002.11296, 2020. [27] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021.\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.04)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.04)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive routing mechanisms, state compression neural networks, hybrid attention architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Adaptive Privacy Composition for Accuracy-first Mechanisms\n\n*From Search Query: adaptive routing mechanisms*\n\n*Ryan M. Rogers, G. Samorodnitsky, Zhiwei Steven Wu, Aaditya Ramdas*\n\n**TL;DR:** Privacy filters are developed that allow an analyst to adaptively switch between differentially private and ex-post private mechanisms subject to an overall differential privacy guarantee.\n\n**Abstract:** In many practical applications of differential privacy, practitioners seek to provide the best privacy guarantees subject to a target level of accuracy. A recent line of work by Ligett et al. '17 and Whitehouse et al. '22 has developed such accuracy-first mechanisms by leveraging the idea of noise reduction that adds correlated noise to the sufficient statistic in a private computation and produces a sequence of increasingly accurate answers. A major advantage of noise reduction mechanisms is that the analysts only pay the privacy cost of the least noisy or most accurate answer released. Despite this appealing property in isolation, there has not been a systematic study on how to use them in conjunction with other differentially private mechanisms. A fundamental challenge is that the privacy guarantee for noise reduction mechanisms is (necessarily) formulated as ex-post privacy that bounds the privacy loss as a function of the released outcome. Furthermore, there has yet to be any study on how ex-post private mechanisms compose, which allows us to track the accumulated privacy over several mechanisms. We develop privacy filters [Rogers et al. '16, Feldman and Zrnic '21, and Whitehouse et al. '22'] that allow an analyst to adaptively switch between differentially private and ex-post private mechanisms subject to an overall differential privacy guarantee.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Scalable Modular Network: A Framework for Adaptive Learning via Agreement Routing\n\n*From Search Query: adaptive routing mechanisms*\n\n*Minyang Hu, Hong Chang, Bingpeng Ma, Shiguang Shan, Xilin Chen*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Fast Routing under Uncertainty: Adaptive Learning in Congestion Games via Exponential Weights\n\n*From Search Query: adaptive routing mechanisms*\n\n*Dong Quan Vu, Kimon Antonakopoulos, P. Mertikopoulos*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks\n\n*From Search Query: state compression neural networks*\n\n*Zhiwei Deng, Olga Russakovsky*\n\n**Abstract:** We propose an algorithm that compresses the critical information of a large dataset into compact addressable memories. These memories can then be recalled to quickly re-train a neural network and recover the performance (instead of storing and re-training on the full original dataset). Building upon the dataset distillation framework, we make a key observation that a shared common representation allows for more efficient and effective distillation. Concretely, we learn a set of bases (aka ``memories'') which are shared between classes and combined through learned flexible addressing functions to generate a diverse set of training examples. This leads to several benefits: 1) the size of compressed data does not necessarily grow linearly with the number of classes; 2) an overall higher compression rate with more effective distillation is achieved; and 3) more generalized queries are allowed beyond recalling the original classes. We demonstrate state-of-the-art results on the dataset distillation task across six benchmarks, including up to 16.5% and 9.7% in retained accuracy improvement when distilling CIFAR10 and CIFAR100 respectively. We then leverage our framework to perform continual learning, achieving state-of-the-art results on four benchmarks, with 23.2% accuracy improvement on MANY. The code is released on our project webpage https://github.com/princetonvisualai/RememberThePast-DatasetDistillation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 78  (*Influential: 17*)\n\n#### 5. Compressing multidimensional weather and climate data into neural networks\n\n*From Search Query: state compression neural networks*\n\n*La-mei Huang, T. Hoefler*\n\n**TL;DR:** A coordinate-based neural network is trained to overfit the data, and the resulting parameters are taken as a compact representation of the original grid-based data to democratize access to high-resolution climate data and enables numerous new research directions.\n\n**Abstract:** Weather and climate simulations produce petabytes of high-resolution data that are later analyzed by researchers in order to understand climate change or severe weather. We propose a new method of compressing this multidimensional weather and climate data: a coordinate-based neural network is trained to overfit the data, and the resulting parameters are taken as a compact representation of the original grid-based data. While compression ratios range from 300x to more than 3,000x, our method outperforms the state-of-the-art compressor SZ3 in terms of weighted RMSE, MAE. It can faithfully preserve important large scale atmosphere structures and does not introduce artifacts. When using the resulting neural network as a 790x compressed dataloader to train the WeatherBench forecasting model, its RMSE increases by less than 2%. The three orders of magnitude compression democratizes access to high-resolution climate data and enables numerous new research directions.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 19  (*Influential: 3*)\n\n#### 6. DRAGONN: Distributed Randomized Approximate Gradients of Neural Networks\n\n*From Search Query: state compression neural networks*\n\n*Zhuang Wang, Zhaozhuo Xu, X. Wu, Anshumali Shrivastava, T. Ng*\n\n**TL;DR:** DRAGN is proposed, a randomized hashing algorithm for GS in DDT that can significantly reduce the compression time by up to 70% compared to state-of-the-art GS approaches, and achieve up to 3 .\n\n**Abstract:** Data-parallel distributed training (DDT) has become the de-facto standard for accelerating the training of most deep learning tasks on massively parallel hardware. In the DDT paradigm, the communication overhead of gradient synchronization is the major efficiency bottleneck. A widely adopted approach to tackle this issue is gradient sparsification (GS). However, the current GS methods introduce significant new overhead in compressing the gradients, outweighing the communication overhead and becoming the new efficiency bottleneck. In this paper, we propose DRAGONN, a randomized hashing algorithm for GS in DDT. DRAGONN can significantly reduce the compression time by up to 70% compared to state-of-the-art GS approaches, and achieve up to 3 . 52 \u00d7 speedup in total training throughput.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 7. CoAtNet: Marrying Convolution and Attention for All Data Sizes\n\n*From Search Query: hybrid attention architectures*\n\n*Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan*\n\n**TL;DR:** This work presents CoAtNets, a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention and (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency.\n\n**Abstract:** Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced\"coat\"nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 995  (*Influential: 105*)\n\n#### 8. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid attention architectures*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 9. LambdaNetworks: Modeling Long-Range Interactions Without Attention\n\n*From Search Query: hybrid attention architectures*\n\n*Irwan Bello*\n\n**TL;DR:** The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, C OCO object detection and COCO instance segmentation, while being more computationally efficient.\n\n**Abstract:** We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and COCO instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 167  (*Influential: 11*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Tutel: Adaptive Mixture-of-Experts at Scale\n\n*From Search Query: adaptive routing mechanisms*\n\n*Yongqiang Xiong, Mao Yang, Fan Yang, Peng Cheng, Joe Chau, Prabhat Ram, Jithin Jose, Rafael Salas, Zilong Wang, Han Hu, Ze Liu, Ziyue Yang, Yifan Xiong, Wei Cui, Changho Hwang*\n\n**Abstract:** Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep learning models to trillion-plus parameters with fixed computational cost. The algorithmic performance of MoE relies on its token routing mechanism that forwards each input token to the right sub-models or experts. While token routing dynamically determines the amount of expert workload at runtime, existing systems suffer inefficient computation due to their static execution, namely static parallelism and pipelining, which does not adapt to the dynamic workload. We present Flex, a highly scalable stack design and implementation for MoE with dynamically adaptive parallelism and pipelining. Flex designs an identical layout for distributing MoE model parameters and input data, which can be leveraged by all possible parallelism or pipelining methods without any mathematical inequivalence or tensor migration overhead. This enables adaptive parallelism/pipelining optimization at zero cost during runtime. Based on this key design, Flex also implements various MoE acceleration techniques. Aggregating all techniques, Flex finally delivers huge speedup at any scale -- 4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs, respectively, over the previous state-of-the-art. Our evaluation shows that Flex efficiently and effectively runs a real-world MoE-based model named SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x and 2.11x speedup in training and inference over Fairseq, respectively. On effectiveness, the SwinV2-MoE model achieves superior accuracy in both pre-training and down-stream computer vision tasks such as COCO object detection than the counterpart dense model, indicating the readiness of Flex for end-to-end real-world model training and inference.\n\n**Published:** 2022-06-07\n\n\n\n#### 2. PredRNN++: Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning\n\n*From Search Query: adaptive routing mechanisms*\n\n*Jian-Min Wang, Yunbo Wang, Philip S. Yu, Mingsheng Long, Zhifeng Gao*\n\n**Abstract:** We present PredRNN++, an improved recurrent network for video predictive\nlearning. In pursuit of a greater spatiotemporal modeling capability, our\napproach increases the transition depth between adjacent states by leveraging a\nnovel recurrent unit, which is named Causal LSTM for re-organizing the spatial\nand temporal memories in a cascaded mechanism. However, there is still a\ndilemma in video predictive learning: increasingly deep-in-time models have\nbeen designed for capturing complex variations, while introducing more\ndifficulties in the gradient back-propagation. To alleviate this undesirable\neffect, we propose a Gradient Highway architecture, which provides alternative\nshorter routes for gradient flows from outputs back to long-range inputs. This\narchitecture works seamlessly with causal LSTMs, enabling PredRNN++ to capture\nshort-term and long-term dependencies adaptively. We assess our model on both\nsynthetic and real video datasets, showing its ability to ease the vanishing\ngradient problem and yield state-of-the-art prediction results even in a\ndifficult objects occlusion scenario.\n\n**Conference:** predrnn-towards-a-resolution-of-the-deep-in-1\n\n**Published:** 2018-04-17\n\n\n\n#### 3. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: state compression neural networks*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 4. HPTQ: Hardware-Friendly Post Training Quantization\n\n*From Search Query: state compression neural networks*\n\n*Arnon Netzer, Roy H. Jennings, Idit Diamant, Oranit Dror, Lior Dikstein, Elad Cohen, Reuven Peretz, Hai Victor Habi*\n\n**Abstract:** Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.\n\n**Published:** 2021-09-19\n\n\n\n#### 5. Hybrid Window Attention Based Transformer Architecture for Brain Tumor Segmentation\n\n*From Search Query: hybrid attention architectures*\n\n*Mehrtash Harandi, Gary Egan, Zhaolin Chen, Munawar Hayat, Himashi Peiris*\n\n**Abstract:** As intensities of MRI volumes are inconsistent across institutes, it is essential to extract universal features of multi-modal MRIs to precisely segment brain tumors. In this concept, we propose a volumetric vision transformer that follows two windowing strategies in attention for extracting fine features and local distributional smoothness (LDS) during model training inspired by virtual adversarial training (VAT) to make the model robust. We trained and evaluated network architecture on the FeTS Challenge 2022 dataset. Our performance on the online validation dataset is as follows: Dice Similarity Score of 81.71%, 91.38% and 85.40%; Hausdorff Distance (95%) of 14.81 mm, 3.93 mm, 11.18 mm for the enhancing tumor, whole tumor, and tumor core, respectively. Overall, the experimental results verify our method's effectiveness by yielding better performance in segmentation accuracy for each tumor sub-region. Our code implementation is publicly available : https://github.com/himashi92/vizviva_fets_2022\n\n**Published:** 2022-09-16\n\n\n\n#### 6. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: hybrid attention architectures*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive routing mechanisms, state compression techniques, hybrid attention architectures, and dynamic architecture adaptation, here are some key insights and recent developments:\n\n## Adaptive Routing Mechanisms\n\nRecent studies have highlighted the importance of adaptive routing in optimizing computational resources and improving model efficiency.\n\n- The **Duo-LLM framework** introduces a novel approach by integrating smaller auxiliary modules within each Feed-Forward Network (FFN) layer. This allows for dynamic routing of tokens based on task complexity, enabling the model to balance efficiency and performance. The use of oracles to identify optimal routing patterns provides valuable insights into the internal workings of LLMs and highlights the gap between practical implementations and theoretical optima for adaptive computation.\n\n- **EC-DiT (Expert-Choice Diffusion Transformers)** employs adaptive expert-choice routing, which is particularly beneficial for diffusion models. This approach leverages global context information to optimize computational resource allocation, dynamically adjusting computation across model depth and denoising timesteps. This method shows superior scalability and adaptive compute allocation, making it relevant for text-to-image generation and potentially other tasks.\n\n## State Compression Techniques for Memory Models\n\nState compression is crucial for reducing memory overhead and improving the efficiency of memory-augmented models.\n\n- **Memory Compression Techniques**: Studies like those on the **Memformer** and **\u221e-former** models suggest that using external dynamic memory and continuous-space attention can help in encoding and retrieving past information efficiently. These models achieve linear time complexity and constant memory space complexity, which can be enhanced further with compression techniques to reduce memory overhead while preserving information.\n\n- **Hierarchical Memory Structures**: Implementing hierarchical memory structures can better capture multi-scale dependencies. Dynamic memory allocation based on input complexity can improve efficiency, and compression techniques can be integrated to reduce memory overhead[Analysis Note].\n\n## Hybrid Attention Architectures\n\nHybrid attention architectures combine different types of attention mechanisms to achieve better trade-offs between efficiency and performance.\n\n- **Combining Attention Types**: Research suggests that combining local, global, and linear attention mechanisms can offer better trade-offs. For example, **Simple linear attention language models** balance the recall-throughput tradeoff by applying linear attention variants, which reduce complexity while maintaining recall ability[Analysis Note].\n\n- **Memory-Augmented Attention**: Integrating memory-augmented attention can enhance long-term dependency modeling. Models like the **Recurrent Memory Transformer** and **Memformer** use special memory tokens or external dynamic memory to encode and retrieve past information, which can be combined with different attention types for improved performance[Analysis Note].\n\n## Efficiency Improvements and Dynamic Adaptation\n\nEfficiency improvements and dynamic adaptation are key to optimizing the performance of autoregressive language models.\n\n- **Linear Attention Variants**: Linear attention variants reduce the complexity of attention mechanisms, making them more efficient. This is evident in models like the **Simple linear attention language models**, which identify a key tradeoff between state size and recall ability[Analysis Note].\n\n- **Dynamic Routing and Adaptive Computation**: Dynamic routing based on input complexity can optimize computation. The **Duo-LLM framework** and **EC-DiT** demonstrate how adaptive computation can be achieved through dynamic routing, which can be integrated with hybrid attention mechanisms for further efficiency improvements.\n\n## Dynamic Architecture Adaptation\n\nDynamic architecture adaptation involves adjusting the model's architecture based on the input characteristics.\n\n- **Adaptive Memory Allocation**: Dynamic memory allocation based on input complexity can improve efficiency. This can be combined with compression techniques to reduce memory overhead while preserving information[Analysis Note].\n\n- **Flexible Attention Patterns**: Adaptive attention patterns based on input characteristics can enhance the model's ability to capture long-term dependencies and improve overall performance. This can be achieved by integrating different attention types and using memory-augmented attention mechanisms[Analysis Note].\n\nIn summary, recent developments in adaptive routing mechanisms, state compression techniques, and hybrid attention architectures offer promising directions for improving the efficiency, performance, and scalability of autoregressive language models. Integrating these techniques with dynamic architecture adaptation can further enhance the model's ability to handle complex inputs and optimize computational resources.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nFrom analyzing the search results, several promising directions emerge for improving the HierarchicalFastTTTLinear design:\n\na) Memory Management:\n- Current approaches often use fixed memory structures\n- Opportunity exists for more dynamic and adaptive memory mechanisms\n- Compression techniques can significantly reduce memory overhead\n\nb) Attention Mechanisms:\n- Hybrid attention approaches show promise (combining local, global, LSH)\n- Linear attention variants offer efficiency improvements\n- Memory-augmented attention could enhance long-term dependencies\n\nc) Adaptive Computation:\n- Dynamic routing based on input complexity\n- Adaptive memory allocation and compression\n- Flexible attention patterns\n\n2. Useful References with Excerpts:\n\nFrom \"Recurrent Memory Transformer\":\n\"Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence.\"\n\nFrom \"Memformer\":\n\"Utilizes an external dynamic memory to encode and retrieve past information... achieves linear time complexity and constant memory space complexity when processing long sequences.\"\n\nFrom \"Simple linear attention language models\":\n\"By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability.\"\n\nFrom \"\u221e-former\":\n\"Utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded.\"\n\n3. Key Insights and Analysis:\n\na) Memory Architecture:\n- Dynamic memory allocation based on input complexity could improve efficiency\n- Hierarchical memory structures better capture multi-scale dependencies\n- Compression techniques can reduce memory overhead while preserving information\n\nb) Attention Integration:\n- Combining different attention types (local, global, linear) offers better trade-offs\n- Memory-augmented attention could enhance long-term dependency modeling\n- Adaptive attention patterns based on input characteristics\n\nc) Computational Efficiency:\n- Linear attention variants reduce complexity\n- Dynamic routing can optimize computation\n- Memory compression techniques reduce resource requirements\n\n4. Future Search Plan:\nNeed to investigate:\n1. Recent advances in adaptive routing mechanisms\n2. State compression techniques for memory-augmented models\n3. Integration methods for hybrid attention mechanisms\n4. Dynamic architecture adaptation approaches\n\n5. References:\n\n1. Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). Recurrent Memory Transformer. Neural Information Processing Systems.\n\n2. Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). Memformer: A Memory-Augmented Transformer for Sequence Modeling. AACL/IJCNLP.\n\n3. Arora, S., et al. (2024). Simple linear attention language models balance the recall-throughput tradeoff. arXiv.org.\n\n4. Martins, P.H., Marinho, Z., & Martins, A.F.T. (2022). \u221e-former: Infinite Memory Transformer. Volume 1.\n\n5. Graves, A. (2016). Adaptive Computation Time for Recurrent Neural Networks. arXiv preprint."
        },
        {
            "ready": false,
            "query": "efficient gradient propagation, dynamic routing attention, memory compression language models",
            "detail": "Analyze recent developments in:\n1. Efficient gradient propagation techniques for deep architectures\n2. Dynamic routing mechanisms specifically for attention layers\n3. Memory compression methods optimized for language models\n4. Hardware-efficient implementations focusing on:\n   - Memory bandwidth optimization\n   - Computation-memory trade-offs\n   - Efficient gradient flow\n5. Integration approaches for hybrid architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in:\n1. Efficient gradient propagation techniques for deep architectures\n2. Dynamic routing mechanisms specifically for attention layers\n3. Memory compression methods optimized for language models\n4. Hardware-efficient implementations focusing on:\n   - Memory bandwidth optimization\n   - Computation-memory trade-offs\n   - Efficient gradient flow\n5. Integration approaches for hybrid architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.13)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.13)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.06)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.06)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 3. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.04)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 24/28 (Score: 0.04)*\n\n```\narXiv preprint arXiv:1911.05507, 2019. [31] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [32] Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, and Douglas Orr. Sparq attention: Bandwidth-efficient llm inference. arXiv preprint arXiv:2312.04985, 2023. [33] Selim Sandal and Ismail Akturk. Zero-shot rtl code generation with attention sink augmented large language models.\n```\n\n#### 4. Scalable MatMul-free Language Modeling (Avg. Score: 0.04)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.04)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 5. Universal Transformers  (Avg. Score: 0.03)\n\n*Mostafa Dehghani, Stephan Gouws, O. Vinyals, Jakob Uszkoreit, Lukasz Kaiser*\n\n**Published in:** International Conference on Learning Representations (2018)\t**Cited by** 673  (*Influential: 65*)\n\n**TL;DR:** The Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses issues of parallelizability and global receptive field, is proposed.\n\n**Abstract:** Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.\n\n##### *Relevant Chunk: No. 21/32 (Score: 0.03)*\n\n```\narXiv preprint arXiv:1610.02357, 2016. Zewei Chu, Hai Wang, Kevin Gimpel, and David McAllester. Broad context language modeling as reading comprehension. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, volume 2, pp. 52-57, 2017. Bhuwan Dhingra, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Linguistic knowledge as memory for recurrent neural networks. arXiv preprint arXiv:1703.02620, 2017. Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for reasoning over multiple mentions using coreference. arXiv preprint arXiv:1804.05922, 2018. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. CoRR, abs/1705.03122, 2017. URL/http://arxiv.org/abs/1705.03122\n\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016. Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850\n\nAlex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401\n\nCaglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786, 2018. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2003. A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in Neural Information Processing Systems, (NIPS), 2015. \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1511.08228\n\n\u0141ukasz Kaiser, Aidan N. Gomez, and Francois Chollet. Depthwise separable convolutions for neural machine translation. CoRR, abs/1706.03059, 2017. URLhttp://arxiv.org/abs/1706.03059. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378-1387, 2016. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of 1stms to learn syntax-sensitive dependencies.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient gradient propagation, dynamic routing attention, memory compression language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting\n\n*From Search Query: efficient gradient propagation*\n\n*Shi-Wee Deng, Yuhang Li, Shanghang Zhang, Shi Gu*\n\n**TL;DR:** This paper analyzes why the current direct training approach with surrogate gradient results in SNNs with poor generalizability, and introduces the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalIZability.\n\n**Abstract:** Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. Still, it is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we first analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, including CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained 83$\\%$ top-1 accuracy, over 10$\\%$ improvement compared to existing state of the art. Codes are available at \\url{https://github.com/Gus-Lab/temporal_efficient_training}.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 196  (*Influential: 49*)\n\n#### 2. Efficient Neural Network Training via Forward and Backward Propagation Sparsification\n\n*From Search Query: efficient gradient propagation*\n\n*Xiao Zhou, Weizhong Zhang, Zonghao Chen, Shizhe Diao, Tong Zhang*\n\n**TL;DR:** This paper proposes an efficient sparse training method with completely sparse forward and backward passes, and proposes a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training.\n\n**Abstract:** Sparse training is a natural idea to accelerate the training speed of deep neural networks and save the memory usage, especially since large modern neural networks are significantly over-parameterized. However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step. This paper solves this problem by proposing an efficient sparse training method with completely sparse forward and backward passes. We first formulate the training process as a continuous minimization problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure. For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algorithm is much more effective in accelerating the training process, up to an order of magnitude faster.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 41  (*Influential: 1*)\n\n#### 3. PLASTIC: Improving Input and Label Plasticity for Sample Efficient Reinforcement Learning\n\n*From Search Query: efficient gradient propagation*\n\n*Hojoon Lee, Hanseul Cho, Hyunseung Kim, Daehoon Gwak, Joonkee Kim, J. Choo, Se-Young Yun, Chulhee Yun*\n\n**TL;DR:** The PLASTIC algorithm is introduced, which harmoniously combines techniques to address input and label plasticity and achieves competitive performance on benchmarks including Atari-100k and Deepmind Control Suite.\n\n**Abstract:** In Reinforcement Learning (RL), enhancing sample efficiency is crucial, particularly in scenarios when data acquisition is costly and risky. In principle, off-policy RL algorithms can improve sample efficiency by allowing multiple updates per environment interaction. However, these multiple updates often lead the model to overfit to earlier interactions, which is referred to as the loss of plasticity. Our study investigates the underlying causes of this phenomenon by dividing plasticity into two aspects. Input plasticity, which denotes the model's adaptability to changing input data, and label plasticity, which denotes the model's adaptability to evolving input-output relationships. Synthetic experiments on the CIFAR-10 dataset reveal that finding smoother minima of loss landscape enhances input plasticity, whereas refined gradient propagation improves label plasticity. Leveraging these findings, we introduce the PLASTIC algorithm, which harmoniously combines techniques to address both concerns. With minimal architectural modifications, PLASTIC achieves competitive performance on benchmarks including Atari-100k and Deepmind Control Suite. This result emphasizes the importance of preserving the model's plasticity to elevate the sample efficiency in RL. The code is available at https://github.com/dojeon-ai/plastic.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 3*)\n\n#### 4. Dynamic Routing Transformer Network for Multimodal Sarcasm Detection\n\n*From Search Query: dynamic routing attention*\n\n*Yuan Tian, Nan Xu, Ruike Zhang, W. Mao*\n\n**TL;DR:** This work model the dynamic mechanism in multimodal sarcasm detection and proposes the Dynamic Routing Transformer Network (DynRT-Net), which utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity.\n\n**Abstract:** Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods. Our codes are available at https://github.com/TIAN-viola/DynRT.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 5. Attention-Based Capsule Network with Dynamic Routing for Relation Extraction\n\n*From Search Query: dynamic routing attention*\n\n*Ningyu Zhang, Shumin Deng, Zhanlin Sun, Xi Chen, Wei Zhang, Huajun Chen*\n\n**Abstract:** None\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 6. A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models\n\n*From Search Query: memory compression language models*\n\n*Krithika Ramesh, Arnav Chavan, Shrey Pandit, Sunayana Sitaram*\n\n**TL;DR:** This analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification and indicates that compression strategies can have an adverse effect on fairness measures.\n\n**Abstract:** Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 0*)\n\n#### 7. The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models\n\n*From Search Query: memory compression language models*\n\n*Satya Sai, Srinath Namburi, Makesh Narsimhan Sreedhar, Srinath Srinivasan, Frederic Sala*\n\n**TL;DR:** A comprehensive analysis across multiple model families using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance, with a particular focus on tradeoffs involving parametric knowledge.\n\n**Abstract:** Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families (ENCODER, ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. We release our codebase1 to enable further research.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 8. History Compression via Language Models in Reinforcement Learning\n\n*From Search Query: memory compression language models*\n\n*Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, S. Lehner, Hamid Eghbalzadeh, Sepp Hochreiter*\n\n**TL;DR:** This work proposes to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency, and introduces FrozenHopfield, which automatically associates observations with pretrained token embeddings.\n\n**Abstract:** In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with pretrained token embeddings. To form these associations, a modern Hopfield network stores these token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 2*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Practical recommendations for gradient-based training of deep architectures\n\n*From Search Query: efficient gradient propagation*\n\n*Yoshua Bengio*\n\n**Abstract:** Learning algorithms related to artificial neural networks and in particular\nfor Deep Learning may seem to involve many bells and whistles, called\nhyper-parameters. This chapter is meant as a practical guide with\nrecommendations for some of the most commonly used hyper-parameters, in\nparticular in the context of learning algorithms based on back-propagated\ngradient and gradient-based optimization. It also discusses how to deal with\nthe fact that more interesting results can be obtained when allowing one to\nadjust many hyper-parameters. Overall, it describes elements of the practice\nused to successfully and efficiently train and debug large-scale and often deep\nmulti-layer neural networks. It closes with open questions about the training\ndifficulties observed with deeper architectures.\n\n**Published:** 2012-06-24\n\n\n\n#### 2. Gradient Flow of Energy: A General and Efficient Approach for Entity Alignment Decoding\n\n*From Search Query: efficient gradient propagation*\n\n*Jianxin Liao, Shaoling Sun, Qi Qi, Jingyu Wang, Haifeng Sun, Yuanyi Wang*\n\n**Abstract:** Entity alignment (EA), a pivotal process in integrating multi-source Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these graphs. Most existing approaches regard EA as a graph representation learning task, concentrating on enhancing graph encoders. However, the decoding process in EA - essential for effective operation and alignment accuracy - has received limited attention and remains tailored to specific datasets and model architectures, necessitating both entity and additional explicit relation embeddings. This specificity limits its applicability, particularly in GNN-based models. To address this gap, we introduce a novel, generalized, and efficient decoding approach for EA, relying solely on entity embeddings. Our method optimizes the decoding process by minimizing Dirichlet energy, leading to the gradient flow within the graph, to maximize graph homophily. The discretization of the gradient flow produces a fast and scalable approach, termed Triple Feature Propagation (TFP). TFP innovatively generalizes adjacency matrices to multi-views matrices:entity-to-entity, entity-to-relation, relation-to-entity, and relation-to-triple. The gradient flow through generalized matrices enables TFP to harness the multi-view structural information of KGs. Rigorous experimentation on diverse public datasets demonstrates that our approach significantly enhances various EA methods. Notably, the approach achieves these advancements with less than 6 seconds of additional computational time, establishing a new benchmark in efficiency and adaptability for future EA methods.\n\n**Published:** 2024-01-23\n\n\n\n#### 3. Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction\n\n*From Search Query: dynamic routing attention*\n\n*Wei zhang, Xi Chen, Zhanlin Sun, Shumin Deng, Ningyu Zhang, Huajun Chen*\n\n**Abstract:** A capsule is a group of neurons, whose activity vector represents the\ninstantiation parameters of a specific type of entity. In this paper, we\nexplore the capsule networks used for relation extraction in a multi-instance\nmulti-label learning framework and propose a novel neural approach based on\ncapsule networks with attention mechanisms. We evaluate our method with\ndifferent benchmarks, and it is demonstrated that our method improves the\nprecision of the predicted relations. Particularly, we show that capsule\nnetworks improve multiple entity pairs relation extraction.\n\n**Conference:** attention-based-capsule-networks-with-dynamic-1\n\n**Published:** 2018-12-29\n\n\n\n#### 4. A Deep Reinforcement Learning Algorithm Using Dynamic Attention Model for Vehicle Routing Problems\n\n*From Search Query: dynamic routing attention*\n\n*Bo Peng, Zizhen Zhang, Jiahai Wang*\n\n**Abstract:** Recent researches show that machine learning has the potential to learn better heuristics than the one designed by human for solving combinatorial optimization problems. The deep neural network is used to characterize the input instance for constructing a feasible solution incrementally. Recently, an attention model is proposed to solve routing problems. In this model, the state of an instance is represented by node features that are fixed over time. However, the fact is, the state of an instance is changed according to the decision that the model made at different construction steps, and the node features should be updated correspondingly. Therefore, this paper presents a dynamic attention model with dynamic encoder-decoder architecture, which enables the model to explore node features dynamically and exploit hidden structure information effectively at different construction steps. This paper focuses on a challenging NP-hard problem, vehicle routing problem. The experiments indicate that our model outperforms the previous methods and also shows a good generalization performance.\n\n**Published:** 2020-02-09\n\n\n\n#### 5. Compressed Context Memory For Online Language Model Interaction\n\n*From Search Query: memory compression language models*\n\n*Hyun Oh Song, Sangdoo Yun, Junyoung Yeom, Jang-Hyun Kim*\n\n**Abstract:** This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.\n\n**Published:** 2023-12-06\n\n\n\n#### 6. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n\n*From Search Query: memory compression language models*\n\n*Carole-Jean Wu, Xing Liu, Bilge Acun, Chunxing Yin*\n\n**Abstract:** The memory capacity of embedding tables in deep learning recommendation models (DLRMs) is increasing dramatically from tens of GBs to TBs across the industry. Given the fast growth in DLRMs, novel solutions are urgently needed, in order to enable fast and efficient DLRM innovations. At the same time, this must be done without having to exponentially increase infrastructure capacity demands. In this paper, we demonstrate the promising potential of Tensor Train decomposition for DLRMs (TT-Rec), an important yet under-investigated context. We design and implement optimized kernels (TT-EmbeddingBag) to evaluate the proposed TT-Rec design. TT-EmbeddingBag is 3 times faster than the SOTA TT implementation. The performance of TT-Rec is further optimized with the batched matrix multiplication and caching strategies for embedding vector lookup operations. In addition, we present mathematically and empirically the effect of weight initialization distribution on DLRM accuracy and propose to initialize the tensor cores of TT-Rec following the sampled Gaussian distribution. We evaluate TT-Rec across three important design space dimensions -- memory capacity, accuracy, and timing performance -- by training MLPerf-DLRM with Criteo's Kaggle and Terabyte data sets. TT-Rec achieves 117 times and 112 times model size compression, for Kaggle and Terabyte, respectively. This impressive model size reduction can come with no accuracy nor training time overhead as compared to the uncompressed baseline.\n\n**Published:** 2021-01-25\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, focusing on efficient gradient propagation, dynamic routing mechanisms, memory compression, and hardware-efficient implementations, here are some key insights and techniques derived from the provided analysis and additional research:\n\n### Efficient Gradient Propagation Techniques\n\n1. **Residual Connections**:\n   Residual connections, or skip connections, are crucial for maintaining gradient flow in deep networks. This architectural motif allows part of the gradient to flow through the identity connections, preventing the vanishing gradient problem. This technique is particularly effective in deep architectures and has been widely adopted in models like ResNet.\n\n2. **Batch Normalization**:\n   Batch normalization helps stabilize the training process by normalizing the inputs to each layer, which can mitigate both the vanishing and exploding gradient problems. This method ensures that the gradients do not vanish or explode as they propagate through the network.\n\n3. **Weight Initialization**:\n   Proper weight initialization can also help in maintaining efficient gradient flow. Techniques such as initializing weights based on the activation function used can prevent gradients from vanishing in deep networks.\n\n### Dynamic Routing Mechanisms for Attention Layers\n\n1. **Adaptive Routing**:\n   Dynamic routing mechanisms can be implemented to adapt the attention patterns based on the input complexity. This can involve selecting different attention types (local, global, linear) or adjusting the attention weights dynamically. For example, models like HybridBERT use a hybrid mixture of attention mechanisms to adapt to different input scenarios.\n\n2. **Memory-Augmented Attention**:\n   Incorporating memory-augmented attention mechanisms can help in modeling long-term dependencies efficiently. This involves using hierarchical memory structures with dynamic allocation to store and retrieve relevant information from previous time steps or layers[Analysis Note].\n\n### Memory Compression Methods Optimized for Language Models\n\n1. **Low-Rank Key Representations**:\n   Techniques like those described in \"Loki: Low-Rank Keys for Efficient Sparse Attention\" can significantly reduce memory requirements. By representing key vectors in a lower-dimensional space, the memory footprint can be reduced without compromising performance[Analysis Note].\n\n2. **Adaptive KV Cache Compression**:\n   Dynamic compression of the KV cache based on importance can further optimize memory usage. This involves selecting and compressing tokens in the cache based on their attention scores, ensuring that only the most relevant information is retained[Analysis Note].\n\n3. **Hierarchical Memory Structures**:\n   Using hierarchical memory structures with adaptive allocation can efficiently manage memory. This approach allows for dynamic allocation of memory resources based on the input complexity and the needs of the model[Analysis Note].\n\n### Hardware-Efficient Implementations\n\n1. **Memory Bandwidth Optimization**:\n   Optimizing memory bandwidth is crucial for efficient training and inference. Techniques such as sparse neural networks can reduce the memory requirements and improve the training speed by removing unnecessary parts and connections.\n\n2. **Computation-Memory Trade-offs**:\n   Balancing computation and memory usage is essential. Models like CoAtNet, which unify convolution and attention layers, can achieve this balance by vertically stacking convolution and attention layers in a principled way, reducing the computational and memory overhead[Analysis Note].\n\n3. **Efficient Gradient Flow**:\n   Implementing shortcut connections, such as residual connections, not only improves gradient flow but also accelerates the training process by enabling more efficient gradient propagation.\n\n### Integration Approaches for Hybrid Architectures\n\n1. **Unified Framework for Attention**:\n   Developing a unified framework that integrates different attention types (local, global, linear) can provide flexibility and efficiency. This approach allows the model to adapt to different input scenarios and optimize performance based on the specific task requirements[Analysis Note].\n\n2. **Hybrid Mixture of Mechanisms**:\n   Combining different mechanisms, such as convolution and attention, or using hybrid attention mechanisms like in HybridBERT, can provide better performance and efficiency. These hybrid architectures can leverage the strengths of different components to achieve improved results[Analysis Note].\n\nBy incorporating these techniques, researchers can develop autoregressive language models that are more efficient in terms of gradient propagation, memory usage, and computational resources, while also achieving better performance and scalability.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nFrom analyzing the search results, several promising directions emerge for enhancing the HierarchicalFastTTTLinear design:\n\na) Efficient Memory Management:\n- Low-rank key representations for sparse attention (Loki)\n- Adaptive KV cache compression techniques\n- Hierarchical memory structures with dynamic allocation\n\nb) Hybrid Attention Mechanisms:\n- Combination of different attention types (local, global, linear)\n- Hardware-friendly attention implementations\n- Memory-augmented attention for long-term dependencies\n\nc) Dynamic Architecture Adaptation:\n- Adaptive routing based on input complexity\n- Dynamic memory allocation and compression\n- Flexible attention patterns\n\n2. Useful References with Excerpts:\n\nFrom \"Loki: Low-Rank Keys for Efficient Sparse Attention\":\n\"Key vectors lie in a significantly lower-dimensional space... ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space.\"\n\nFrom \"Self-attention Does Not Need O(n^2) Memory\":\n\"Presents a very simple algorithm for attention that requires O(1) memory with respect to sequence length... practical implementation for accelerators that requires O(\u221an) memory.\"\n\nFrom \"LambdaNetworks: Modeling Long-Range Interactions Without Attention\":\n\"Lambda layers capture interactions by transforming available contexts into linear functions... bypass expensive attention maps while modeling both content and position-based interactions.\"\n\nFrom \"CoAtNet: Marrying Convolution and Attention\":\n\"Depthwise Convolution and self-Attention can be naturally unified via simple relative attention... vertically stacking convolution layers and attention layers in a principled way.\"\n\n3. Key Insights and Analysis:\n\na) Memory Optimization:\n- Low-rank approximations can significantly reduce memory requirements while maintaining performance\n- Dynamic compression of KV cache based on importance\n- Hierarchical memory structures with adaptive allocation\n\nb) Attention Integration:\n- Unified framework for different attention types\n- Hardware-efficient implementations possible through careful design\n- Position and content-based interactions can be modeled efficiently\n\nc) Dynamic Adaptation:\n- Input-dependent routing mechanisms\n- Adaptive computation paths\n- Flexible memory allocation\n\n4. Future Search Plan:\nNeed to investigate:\n1. Efficient gradient propagation in deep architectures\n2. Dynamic routing mechanisms for attention\n3. Memory compression techniques specific to language models\n4. Hardware-efficient implementations of hybrid architectures\n\n5. References:\n\n1. Singhania, P., Singh, S., He, S., Feizi, S., & Bhatele, A. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint.\n\n2. Rabe, M., & Staats, C. (2021). Self-attention Does Not Need O(n^2) Memory. arXiv preprint.\n\n3. Bello, I. (2021). LambdaNetworks: Modeling Long-Range Interactions Without Attention. International Conference on Learning Representations.\n\n4. Dai, Z., Liu, H., Le, Q.V., & Tan, M. (2021). CoAtNet: Marrying Convolution and Attention for All Data Sizes. Neural Information Processing Systems.\n\n5. Srinivasagan, G., & Ostermann, S. (2024). HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms. NAACL."
        },
        {
            "ready": false,
            "query": "hierarchical memory systems, efficient attention variants, knowledge preservation compression",
            "detail": "Analyze recent developments in:\n1. Hierarchical memory architectures focusing on:\n   - Multi-scale memory systems\n   - Dynamic memory allocation\n   - Memory compression techniques\n2. Efficient attention mechanisms focusing on:\n   - Hardware-efficient implementations\n   - Sparse attention patterns\n   - Memory-efficient attention\n3. Knowledge preservation methods focusing on:\n   - Compression techniques\n   - Knowledge distillation\n   - Feature preservation",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in:\n1. Hierarchical memory architectures focusing on:\n   - Multi-scale memory systems\n   - Dynamic memory allocation\n   - Memory compression techniques\n2. Efficient attention mechanisms focusing on:\n   - Hardware-efficient implementations\n   - Sparse attention patterns\n   - Memory-efficient attention\n3. Knowledge preservation methods focusing on:\n   - Compression techniques\n   - Knowledge distillation\n   - Feature preservation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.18)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.18)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.03)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.03)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n#### 3. CORM: Cache Optimization with Recent Message for Large Language Model Inference (Avg. Score: 0.03)\n\n*Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi*\n\n**Published in:**  (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper presents CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning, and shows that CORM reduces the inference memory usage of KV cache by up to 70\\% with negligible performance degradation across six tasks in LongBench.\n\n**Abstract:** Large Language Models (LLMs), despite their remarkable performance across a wide range of tasks, necessitate substantial GPU memory and consume significant computational resources. Beyond the memory taken up by model weights, the memory used by the KV cache rises linearly with sequence length, becoming a primary bottleneck for inference. In this paper, we introduce an innovative method for optimizing the KV cache, which considerably minimizes its memory footprint. Upon thorough investigation, we discover that in most Transformer models, (i) there is a striking similarity between adjacent tokens' query vectors, and (ii) the attention calculation of the current query can rely exclusively on the attention information of a small fraction of preceding queries. Based on these observations, we present CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning. Our validation shows that CORM reduces the inference memory usage of KV cache by up to 70\\% with negligible performance degradation across six tasks in LongBench. Furthermore, we demonstrate that CORM is compatible with GQA for further compression rate.\n\n##### *Relevant Chunk: No. 7/18 (Score: 0.03)*\n\n```\narXiv preprint arXiv:2402.06262, 2024. [15] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. [16] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [18] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. Advances in Neural Information Processing Systems, 36, 2024. [19] Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, and Edoardo M Ponti. Dynamic memory compression: Retrofitting llms for accelerated inference. arXiv preprint arXiv:2403.09636, 2024. [20] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023 . [21] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory.\n```\n\n#### 4. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.03)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.03)*\n\n```\narXiv preprint arXiv:1910.06611, 2019. Imanol Schlag, Tsendsuren Munkhdalai, and J\u00fcrgen Schmidhuber. Learning associative inference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355-9366. PMLR, 2021. J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131-139, 1992. Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. arXiv preprint arXiv:1812.01243, 2018. Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159-216, 1990. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015. Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In International Conference on Machine Learning, pp. 9902-9912. PMLR, 2021. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, $30,2017$. Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural networks, 1(4):339-356, 1988. Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory.\n```\n\n#### 5. Latent Attention for Linear Time Transformers (Avg. Score: 0.02)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.02)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. Glorot, X., Bordes, A., and Bengio, Y. Deep Sparse Rectifier Neural Networks. In JMLR Workshop and Conference Proceedings, pp. 315-323, 2011. Gokaslan, A. and Cohen, V. OpenWebText Corpus, 2019. URL http://Skylion007.github.io/ OpenWebTextCorpus. Gu, A., Goel, K., and R\u00e9, C. Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396, 2021. Hutter, M. The Human Knowledge Compression Prize, 2002. URL https:// www.kurzweilai.net/hutter-prizefor-lossless-compression-of-humanknowledge. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General Perception with Iterative Attention. In International Conference on Machine Learning, pp. 4651-4664. PMLR, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Khan, S., Naseer, M., Hayat, M., Zamir, S.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical memory systems, efficient attention variants, knowledge preservation compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Towards mental time travel: a hierarchical memory for reinforcement learning agents\n\n*From Search Query: hierarchical memory systems*\n\n*Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, Felix Hill*\n\n**TL;DR:** Hierarchical Chunk Attention Memory improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures), and is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Abstract:** Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore\"mentally time-travel\"-- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 42  (*Influential: 5*)\n\n#### 2. Online Neural Sequence Detection with Hierarchical Dirichlet Point Process\n\n*From Search Query: hierarchical memory systems*\n\n*Weihan Li, Yu Qi, Gang Pan*\n\n**TL;DR:** A hierarchical Dirichlet point process model for efficient neural sequence detection that can sequentially detect sequences in an online unsupervised manner with Particle filters and enables the Dirichlet prior to automatically introduce new sequence types on the fly as needed, thus avoiding specifying the number of types in advance.\n\n**Abstract:** Neural sequence detection plays a vital role in neuroscience research. Recent impressive works utilize convolutive nonnegative matrix factorization and Neyman-Scott process to solve this problem. However, they still face two limitations. Firstly, they accommodate the entire dataset into memory and perform iterative updates of multiple passes, which can be inefficient when the dataset is large or grows frequently. Secondly, they rely on the prior knowledge of the number of sequence types, which can be impractical with data when the future situation is unknown. To tackle these limitations, we propose a hierarchical Dirichlet point process model for efficient neural sequence detection. Instead of computing the entire data, our model can sequentially detect sequences in an online unsupervised manner with Particle filters. Besides, the Dirichlet prior enables our model to automatically introduce new sequence types on the fly as needed, thus avoiding specifying the number of types in advance. We manifest these advantages on synthetic data and neural recordings from songbird higher vocal center and rodent hippocampus.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 3. OctField: Hierarchical Implicit Functions for 3D Modeling\n\n*From Search Query: hierarchical memory systems*\n\n*Jiapeng Tang, Weikai Chen, Jie Yang, Bo Wang, Songrun Liu, Bo Yang, Lin Gao*\n\n**TL;DR:** This work presents a learnable hierarchical implicit representation for 3D surfaces, coded OctField, that allows high-precision encoding of intricate surfaces with low memory and computational budget and demonstrates the value of OctField for a range of shape modeling and reconstruction tasks, showing superiority over alternative approaches.\n\n**Abstract:** Recent advances in localized implicit functions have enabled neural implicit representation to be scalable to large scenes. However, the regular subdivision of 3D space employed by these approaches fails to take into account the sparsity of the surface occupancy and the varying granularities of geometric details. As a result, its memory footprint grows cubically with the input volume, leading to a prohibitive computational cost even at a moderately dense decomposition. In this work, we present a learnable hierarchical implicit representation for 3D surfaces, coded OctField, that allows high-precision encoding of intricate surfaces with low memory and computational budget. The key to our approach is an adaptive decomposition of 3D scenes that only distributes local implicit functions around the surface of interest. We achieve this goal by introducing a hierarchical octree structure to adaptively subdivide the 3D space according to the surface occupancy and the richness of part geometry. As octree is discrete and non-differentiable, we further propose a novel hierarchical network that models the subdivision of octree cells as a probabilistic process and recursively encodes and decodes both octree structure and surface geometry in a differentiable manner. We demonstrate the value of OctField for a range of shape modeling and reconstruction tasks, showing superiority over alternative approaches.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 36  (*Influential: 3*)\n\n#### 4. COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models\n\n*From Search Query: efficient attention variants*\n\n*Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan*\n\n**TL;DR:** This paper develops a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods and can be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training and lower extra storage cost than the existing works.\n\n**Abstract:** Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to $2.6\\times$ speedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the existing works.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 5. HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions\n\n*From Search Query: efficient attention variants*\n\n*Yongming Rao, Wenliang Zhao, Yansong Tang, Jie Zhou, S. Lim, Jiwen Lu*\n\n**TL;DR:** The Recursive Gated Convolution, a new operation that performs high-order spatial interactions with gated convolutions and recursive designs, is presented that can serve as a plug-and-play module to improve various vision Transformers and convolution-based models.\n\n**Abstract:** Recent progress in vision Transformers exhibits great success in various tasks driven by the new spatial modeling mechanism based on dot-product self-attention. In this paper, we show that the key ingredients behind the vision Transformers, namely input-adaptive, long-range and high-order spatial interactions, can also be efficiently implemented with a convolution-based framework. We present the Recursive Gated Convolution ($\\textit{g}^\\textit{n}$Conv) that performs high-order spatial interactions with gated convolutions and recursive designs. The new operation is highly flexible and customizable, which is compatible with various variants of convolution and extends the two-order interactions in self-attention to arbitrary orders without introducing significant extra computation. $\\textit{g}^\\textit{n}$Conv can serve as a plug-and-play module to improve various vision Transformers and convolution-based models. Based on the operation, we construct a new family of generic vision backbones named HorNet. Extensive experiments on ImageNet classification, COCO object detection and ADE20K semantic segmentation show HorNet outperform Swin Transformers and ConvNeXt by a significant margin with similar overall architecture and training configurations. HorNet also shows favorable scalability to more training data and larger model sizes. Apart from the effectiveness in visual encoders, we also show $\\textit{g}^\\textit{n}$Conv can be applied to task-specific decoders and consistently improve dense prediction performance with less computation. Our results demonstrate that $\\textit{g}^\\textit{n}$Conv can be a new basic module for visual modeling that effectively combines the merits of both vision Transformers and CNNs. Code is available at https://github.com/raoyongming/HorNet\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 186  (*Influential: 23*)\n\n#### 6. Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks\n\n*From Search Query: knowledge preservation compression*\n\n*Qingbin Liu, Pengfei Cao, Cao Liu, Jiansong Chen, Xunliang Cai, Fan Yang, Shizhu He, Kang Liu, Jun Zhao*\n\n**TL;DR:** This paper proposes a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task.\n\n**Abstract:** Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 7. AD-KD: Attribution-Driven Knowledge Distillation for Language Model Compression\n\n*From Search Query: knowledge preservation compression*\n\n*Siyue Wu, Hongzhan Chen, Xiaojun Quan, Qifan Wang, Rui Wang*\n\n**TL;DR:** This paper explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model and further explores multi-view attribution distillation on all potential decisions of the teacher.\n\n**Abstract:** Knowledge distillation has attracted a great deal of interest recently to compress large language models. However, existing knowledge distillation methods suffer from two limitations. First, the student model simply imitates the teacher\u2019s behavior while ignoring the reasoning behind it. Second, these methods usually focus on the transfer of sophisticated model-specific knowledge but overlook data-specific knowledge. In this paper, we present a novel attribution-driven knowledge distillation approach, which explores the token-level rationale behind the teacher model based on Integrated Gradients (IG) and transfers attribution knowledge to the student model. To enhance the knowledge transfer of model reasoning and generalization, we further explore multi-view attribution distillation on all potential decisions of the teacher. Comprehensive experiments are conducted with BERT on the GLUE benchmark. The experimental results demonstrate the superior performance of our approach to several state-of-the-art methods.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 3*)\n\n#### 8. Adaptive Contrastive Knowledge Distillation for BERT Compression\n\n*From Search Query: knowledge preservation compression*\n\n*Jinyang Guo, Jiaheng Liu, Z. Wang, Yuqing Ma, Ruihao Gong, Ke Xu, Xianglong Liu*\n\n**TL;DR:** This paper introduces a novel contrastive distillation loss based on hidden state features in BERT as the explicit supervision to learn discriminative student features and incorporates this strategy into a new strategy called sample adaptive reweighting (SAR) to adaptively pay more attention to these hard samples and strengthen their discrimination abilities.\n\n**Abstract:** In this paper, we propose a new knowledge distillation approach called adaptive contrastive knowledge distillation (ACKD) for BERT compression. Different from existing knowledge distillation methods for BERT that implicitly learn discriminative student features by mimicking the teacher features, we first introduce a novel contrastive distillation loss (CDL) based on hidden state features in BERT as the explicit supervision to learn discriminative student features. We further observe sentences with similar features may have completely different meanings, which makes them hard to distinguish. Existing methods do not pay sufficient attention to these hard samples with less discriminative features. Therefore, we propose a new strategy called sample adaptive reweighting (SAR) to adaptively pay more attention to these hard samples and strengthen their discrimination abilities. We incorporate our SAR strategy into our CDL and form the adaptive contrastive distillation loss, based on which we construct our ACKD framework. Comprehensive experiments on multiple natural language processing tasks demonstrate the effectiveness of our ACKD framework.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n### 2 related papers from ArXiv\n\n#### 1. Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for\n  Long Sequences\n\n*From Search Query: efficient attention variants*\n\n*Yanming Kang, Giang Tran, Hans De Sterck*\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance in many\nareas. However, the quadratic complexity of self-attention with respect to the\ninput length hinders the applicability of Transformer-based models to long\nsequences. To address this, we present Fast Multipole Attention, a new\nattention mechanism that uses a divide-and-conquer strategy to reduce the time\nand memory complexity of attention for sequences of length $n$ from\n$\\mathcal{O}(n^2)$ to $\\mathcal{O}(n \\log n)$ or $O(n)$, while retaining a\nglobal receptive field. The hierarchical approach groups queries, keys, and\nvalues into $\\mathcal{O}( \\log n)$ levels of resolution, where groups at\ngreater distances are increasingly larger in size and the weights to compute\ngroup quantities are learned. As such, the interaction between tokens far from\neach other is considered in lower resolution in an efficient hierarchical\nmanner. The overall complexity of Fast Multipole Attention is $\\mathcal{O}(n)$\nor $\\mathcal{O}(n \\log n)$, depending on whether the queries are down-sampled\nor not. This multi-level divide-and-conquer strategy is inspired by fast\nsummation methods from $n$-body physics and the Fast Multipole Method. We\nperform evaluation on autoregressive and bidirectional language modeling tasks\nand compare our Fast Multipole Attention model with other efficient attention\nvariants on medium-size datasets. We find empirically that the Fast Multipole\nTransformer performs much better than other efficient transformers in terms of\nmemory size and accuracy. The Fast Multipole Attention mechanism has the\npotential to empower large language models with much greater sequence lengths,\ntaking the full context into account in an efficient, naturally hierarchical\nmanner during training and when generating long sequences.\n\n**Published:** 2023-10-18T13:40:41Z  (*Updated: 2024-07-30T15:02:51Z*)\n\n\n\n#### 2. LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive\n  Compression Strategy\n\n*From Search Query: efficient attention variants*\n\n*Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, Yelong Shen*\n\n**Abstract:** The Key-Value (KV) cache is a crucial component in serving transformer-based\nautoregressive large language models (LLMs), enabling faster inference by\nstoring previously computed KV vectors. However, its memory consumption scales\nlinearly with sequence length and batch size, posing a significant bottleneck\nin LLM deployment. Existing approaches to mitigate this issue include: (1)\nefficient attention variants integrated in upcycling stages, which requires\nextensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache\ncompression at test time, primarily through token eviction policies, which\noften overlook inter-layer dependencies and can be task-specific.\n  This paper introduces an orthogonal approach to KV cache compression. We\npropose a low-rank approximation of KV weight matrices, allowing for plug-in\nintegration with existing transformer-based LLMs without model retraining. To\neffectively compress KV cache at the weight level, we adjust for layerwise\nsensitivity and introduce a progressive compression strategy, which is\nsupported by our theoretical analysis on how compression errors accumulate in\ndeep networks. Our method is designed to function without model tuning in\nupcycling stages or task-specific profiling in test stages. Extensive\nexperiments with LLaMA models ranging from 8B to 70B parameters across various\ntasks show that our approach significantly reduces the GPU memory footprint\nwhile maintaining performance.\n\n**Published:** 2024-10-04T03:10:53Z  (*Updated: 2024-10-04T03:10:53Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation\n\n*From Search Query: hierarchical memory systems*\n\n*Qiang Li, Hao Zhang, Tong Xiao, Jingbo Zhu*\n\n**Abstract:** \n\n**Proceeding:** acl-2012-7\n\n**Published:** 2012-07-01\n\n\n\n#### 2. Multi-timescale memory dynamics in a reinforcement learning network with attention-gated memory\n\n*From Search Query: hierarchical memory systems*\n\n*Wulfram Gerstner, Aditya Gilra, Marco Martinolli*\n\n**Abstract:** Learning and memory are intertwined in our brain and their relationship is at\nthe core of several recent neural network models. In particular, the\nAttention-Gated MEmory Tagging model (AuGMEnT) is a reinforcement learning\nnetwork with an emphasis on biological plausibility of memory dynamics and\nlearning. We find that the AuGMEnT network does not solve some hierarchical\ntasks, where higher-level stimuli have to be maintained over a long time, while\nlower-level stimuli need to be remembered and forgotten over a shorter\ntimescale. To overcome this limitation, we introduce hybrid AuGMEnT, with leaky\nor short-timescale and non-leaky or long-timescale units in memory, that allow\nto exchange lower-level information while maintaining higher-level one, thus\nsolving both hierarchical and distractor tasks.\n\n**Published:** 2017-12-28\n\n\n\n#### 3. When to Use Efficient Self Attention? Profiling Text, Speech and Image Transformer Variants\n\n*From Search Query: efficient attention variants*\n\n*David Harwath, Eunsol Choi, Anuj Diwan*\n\n**Abstract:** We present the first unified study of the efficiency of self-attention-based Transformer variants spanning text, speech and vision. We identify input length thresholds (tipping points) at which efficient Transformer variants become more efficient than vanilla models, using a variety of efficiency metrics (latency, throughput, and memory). To conduct this analysis for speech, we introduce L-HuBERT, a novel local-attention variant of a self-supervised speech model. We observe that these thresholds are (a) much higher than typical dataset sequence lengths and (b) dependent on the metric and modality, showing that choosing the right model depends on modality, task type (long-form vs. typical context) and resource constraints (time vs. memory). By visualising the breakdown of the computational costs for transformer components, we also show that non-self-attention components exhibit significant computational costs. We release our profiling toolkit at https://github.com/ajd12342/profiling-transformers .\n\n**Published:** 2023-06-14\n\n\n\n#### 4. Mega: Moving Average Equipped Gated Attention\n\n*From Search Query: efficient attention variants*\n\n*Luke Zettlemoyer, Jonathan May, Graham Neubig, Liangke Gui, Junxian He, Xiang Kong, Chunting Zhou, Xuezhe Ma*\n\n**Abstract:** The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.\n\n**Published:** 2022-09-21\n\n\n\n#### 5. InDistill: Information flow-preserving knowledge distillation for model compression\n\n*From Search Query: knowledge preservation compression*\n\n*Symeon Papadopoulos, Giorgos Kordopatis-Zilos, Ioannis Kompatsiaris, Christos Koutlis, Ioannis Sarridis*\n\n**Abstract:** In this paper we introduce InDistill, a model compression approach that combines knowledge distillation and channel pruning in a unified framework for the transfer of the critical information flow paths from a heavyweight teacher to a lightweight student. Such information is typically collapsed in previous methods due to an encoding stage prior to distillation. By contrast, InDistill leverages a pruning operation applied to the teacher's intermediate layers reducing their width to the corresponding student layers' width. In that way, we force architectural alignment enabling the intermediate layers to be directly distilled without the need of an encoding stage. Additionally, a curriculum learning-based training scheme is adopted considering the distillation difficulty of each layer and the critical learning periods in which the information flow paths are created. The proposed method surpasses state-of-the-art performance on three standard benchmarks, i.e. CIFAR-10, CUB-200, and FashionMNIST by 3.08%, 14.27%, and 1% mAP, respectively, as well as on more challenging evaluation settings, i.e. ImageNet and CIFAR-100 by 1.97% and 5.65% mAP, respectively.\n\n**Published:** 2022-05-20\n\n\n\n#### 6. Extreme Compression for Pre-trained Transformers Made Simple and Efficient\n\n*From Search Query: knowledge preservation compression*\n\n*Yuxiong He, Conglong Li, Minjia Zhang, Zhewei Yao, Xiaoxia Wu*\n\n**Abstract:** Extreme compression, particularly ultra-low bit precision (binary/ternary) quantization, has been proposed to fit large NLP models on resource-constraint devices. However, to preserve the accuracy for such aggressive compression schemes, cutting-edge methods usually introduce complicated compression pipelines, e.g., multi-stage expensive knowledge distillation with extensive hyperparameter tuning. Also, they oftentimes focus less on smaller transformer models that have already been heavily compressed via knowledge distillation and lack a systematic study to show the effectiveness of their methods. In this paper, we perform a very comprehensive systematic study to measure the impact of many key hyperparameters and training strategies from previous works. As a result, we find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. Based on our study, we propose a simple yet effective compression pipeline for extreme compression, named XTC. XTC demonstrates that (1) we can skip the pre-training knowledge distillation to obtain a 5-layer BERT while achieving better performance than previous state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization plus layer reduction is able to reduce the model size by 50x, resulting in new state-of-the-art results on GLUE tasks.\n\n**Published:** 2022-06-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model (LM) design, focusing on hierarchical memory systems, efficient attention mechanisms, and knowledge preservation methods, here are some key findings and directions from the provided sources and analysis:\n\n### Hierarchical Memory Architectures\n\n1. **Dynamic Tree Memory Representation**:\n   - The MemTree algorithm introduced in the arXiv paper is a promising approach. It uses a dynamic, tree-structured memory representation to optimize the organization, retrieval, and integration of information. This hierarchical structure allows for efficient insertion and retrieval with a complexity of O(log N), making it suitable for real-time applications. MemTree adapts the memory structure by computing and comparing semantic embeddings, which enhances the model's context-awareness and handles complex reasoning effectively.\n\n2. **Multi-scale Memory Systems**:\n   - MemTree's hierarchical organization can be seen as a multi-scale memory system, where each node in the tree represents aggregated textual content and semantic embeddings at varying abstraction levels. This structure facilitates the aggregation of knowledge at parent nodes, capturing high-level semantics as the tree expands.\n\n3. **Memory Compression Techniques**:\n   - While the MemTree paper does not explicitly discuss compression, the concept of hierarchical memory can be combined with compression techniques to reduce resource requirements. For instance, compressing the semantic embeddings at higher levels of the tree could maintain the hierarchical structure while reducing memory usage.\n\n### Efficient Attention Mechanisms\n\n1. **Hardware-Efficient Implementations**:\n   - Efficient attention mechanisms can be achieved through sparse attention patterns. The analysis notes the importance of sparse training methods, which can reduce computational overhead. Implementations like sparse forward and backward passes can make attention mechanisms more hardware-efficient[3 in Analysis Note].\n\n2. **Memory-Efficient Attention**:\n   - Hierarchical attention mechanisms, such as those used in dynamic routing transformer networks, can improve feature extraction and adapt to input complexity. These mechanisms can be optimized to be memory-efficient by using techniques like hierarchical co-attention, which reduces the need for full attention matrices[3 in Analysis Note].\n\n### Knowledge Preservation Methods\n\n1. **Compression Techniques**:\n   - The analysis highlights the need to balance compression with model capabilities. Studies like \"The Cost of Compression\" systematically quantify the effect of compression techniques on model performance, focusing on tradeoffs involving parametric knowledge. This is crucial for ensuring that knowledge is preserved during compression[4 in Analysis Note].\n\n2. **Knowledge Distillation**:\n   - Knowledge distillation can be used to preserve the knowledge of a larger model in a smaller, compressed version. This method involves training a smaller model (the student) to mimic the behavior of a larger, pre-trained model (the teacher), ensuring that the essential knowledge is retained[4 in Analysis Note].\n\n### Additional Considerations\n\n1. **Dynamic Routing and Memory**:\n   - Dynamic routing transformers and capsule networks with attention demonstrate improved feature extraction and adaptability to input complexity. These architectures can be integrated with hierarchical memory systems to enhance the overall performance of the LM[3 in Analysis Note].\n\n2. **Hardware Optimization**:\n   - The design should consider trade-offs between computation and memory access patterns. Efficient gradient flow and hardware-aware design are crucial for optimizing the performance of the LM, especially when implementing hierarchical memory and attention mechanisms[3 in Analysis Note].\n\nBy combining these approaches, researchers can develop an autoregressive LM that leverages hierarchical memory architectures, efficient attention mechanisms, and effective knowledge preservation techniques, leading to improved performance, efficiency, and scalability.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nFrom analyzing the search results, several promising directions emerge for enhancing the HierarchicalFastTTTLinear design:\n\na) Efficient Gradient Flow:\n- Temporal efficient training approaches show promise for better gradient propagation\n- Forward and backward propagation sparsification can improve training efficiency\n- Gradient re-weighting mechanisms can enhance model stability\n\nb) Dynamic Routing and Memory:\n- Dynamic routing transformers show effectiveness in multimodal scenarios\n- Capsule networks with attention demonstrate improved feature extraction\n- Memory compression techniques can significantly reduce resource requirements\n\nc) Hardware Optimization:\n- Need for specialized implementations considering memory bandwidth\n- Trade-offs between computation and memory access patterns\n- Importance of efficient gradient flow in hardware implementations\n\n2. Useful References with Excerpts:\n\nFrom \"Temporal Efficient Training of Spiking Neural Network\":\n\"introduces the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent [...] so that the training process can converge into flatter minima with better generalizability.\"\n\nFrom \"Efficient Neural Network Training via Forward and Backward Propagation Sparsification\":\n\"proposes an efficient sparse training method with completely sparse forward and backward passes [...] achieving completely sparse training.\"\n\nFrom \"Dynamic Routing Transformer Network\":\n\"utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity.\"\n\nFrom \"The Cost of Compression\":\n\"systematically quantify the effect of commonly employed compression techniques on model performance [...] with a particular focus on tradeoffs involving parametric knowledge.\"\n\n3. Key Insights and Analysis:\n\na) Gradient Optimization:\n- Temporal efficient training can improve gradient flow stability\n- Sparse training methods can reduce computational overhead\n- Gradient re-weighting helps achieve better minima\n\nb) Dynamic Architecture:\n- Dynamic routing can adapt to input complexity\n- Hierarchical attention mechanisms improve feature extraction\n- Memory compression needs careful consideration of knowledge preservation\n\nc) Implementation Considerations:\n- Need to balance compression vs. model capabilities\n- Hardware-aware design crucial for efficiency\n- Importance of maintaining model knowledge during compression\n\n4. Future Search Plan:\nNeed to investigate:\n1. Recent advances in hierarchical memory architectures\n2. Hardware-efficient attention implementations\n3. Knowledge preservation in compressed models\n4. Specific areas to search:\n   - Hierarchical memory systems\n   - Efficient attention variants\n   - Knowledge distillation techniques\n\n5. References:\n\n1. Deng, S., Li, Y., Zhang, S., & Gu, S. (2022). Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting. International Conference on Learning Representations.\n\n2. Zhou, X., Zhang, W., Chen, Z., Diao, S., & Zhang, T. (2021). Efficient Neural Network Training via Forward and Backward Propagation Sparsification. Neural Information Processing Systems.\n\n3. Tian, Y., Xu, N., Zhang, R., & Mao, W. (2023). Dynamic Routing Transformer Network for Multimodal Sarcasm Detection. Annual Meeting of the Association for Computational Linguistics.\n\n4. Sai, S., Namburi, S., Sreedhar, M.N., Srinivasan, S., & Sala, F. (2023). The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models. Conference on Empirical Methods in Natural Language Processing.\n\n5. Paischer, F., et al. (2022). History Compression via Language Models in Reinforcement Learning. International Conference on Machine Learning.\n\n6. Bengio, Y. (2012). Practical recommendations for gradient-based training of deep architectures. Neural Networks: Tricks of the Trade."
        },
        {
            "ready": false,
            "query": "\"gradient propagation hierarchical\", \"stability analysis compression\", \"hardware efficient attention\"",
            "detail": "Search for papers discussing:\n1. Gradient propagation in hierarchical networks:\n   - Gradient flow analysis\n   - Stability considerations\n   - Training dynamics\n\n2. Implementation details of:\n   - Stable compression methods\n   - Convergence analysis\n   - Error propagation\n\n3. Hardware efficiency:\n   - Memory-efficient attention\n   - Computational optimization\n   - Implementation strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Gradient propagation in hierarchical networks:\n   - Gradient flow analysis\n   - Stability considerations\n   - Training dynamics\n\n2. Implementation details of:\n   - Stable compression methods\n   - Convergence analysis\n   - Error propagation\n\n3. Hardware efficiency:\n   - Memory-efficient attention\n   - Computational optimization\n   - Implementation strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.02)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.02)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.00)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.00)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.00)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.00)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. RRWKV: Capturing Long-range Dependencies in RWKV (Avg. Score: 0.00)\n\n*Leilei Wang*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** The paper devises the Retrospected Receptance Weighted Key Value architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\n\n**Abstract:** Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\n\n##### *Relevant Chunk: No. 11/11 (Score: 0.00)*\n\n```\nIn 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7132-7141, 2018. doi 10.1109/CVPR.2018.00745. Fakultit Informatik, Y. Bengio, Paolo Frasconi, and Jfirgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 032003. Yiyi Zhou, Tianhe Ren, Chaoyang Zhu, Xiaoshuai Sun, Jianzhuang Liu, Xinghao Ding, Mingliang Xu, and Rongrong Ji. Trar: Routing the attention spans in transformer for visual question answering. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2054-2064, 2021. doi 10.1109/ICCV48922.2021.00208. [^0]:    * the first author\n\n\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.00)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.00)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"gradient propagation hierarchical\", \"stability analysis compression\", \"hardware efficient attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Multi-Layered Gradient Boosting Decision Trees\n\n*From Search Query: \"gradient propagation hierarchical\"*\n\n*Ji Feng, Yang Yu, Zhi-Hua Zhou*\n\n**TL;DR:** This work proposes the multi-layered GBDT forest, with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block, and confirms the effectiveness of the model in terms of performance and representation learning ability.\n\n**Abstract:** Multi-layered representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments and visualizations confirmed the effectiveness of the model in terms of performance and representation learning ability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 112  (*Influential: 8*)\n\n#### 2. Gradient-EM Bayesian Meta-learning\n\n*From Search Query: \"gradient propagation hierarchical\"*\n\n*Yayi Zou, Xiaoqi Lu*\n\n**TL;DR:** This work proposes a variant of Bayesian meta-learning that improves computational efficiency by avoiding back-propagation computation in the meta-update step, which is exhausting for deep neural networks, and provides flexibility to the inner-update optimization procedure by decoupling it from meta- update.\n\n**Abstract:** Bayesian meta-learning enables robust and fast adaptation to new tasks with uncertainty assessment. The key idea behind Bayesian meta-learning is empirical Bayes inference of hierarchical model. In this work, we extend this framework to include a variety of existing methods, before proposing our variant based on gradient-EM algorithm. Our method improves computational efficiency by avoiding back-propagation computation in the meta-update step, which is exhausting for deep neural networks. Furthermore, it provides flexibility to the inner-update optimization procedure by decoupling it from meta-update. Experiments on sinusoidal regression, few-shot image classification, and policy-based reinforcement learning show that our method not only achieves better accuracy with less computation cost, but is also more robust to uncertainty.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 3. Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies\n\n*From Search Query: \"gradient propagation hierarchical\"*\n\n*Sungryull Sohn, Junhyuk Oh, Honglak Lee*\n\n**TL;DR:** A novel non-parametric gradient-based policy, graph reward propagation, is proposed to pre-train the NSGS agent and further finetune it through actor-critic method, showing that the agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs.\n\n**Abstract:** We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 60  (*Influential: 3*)\n\n#### 4. Stronger generalization bounds for deep nets via a compression approach\n\n*From Search Query: \"stability analysis compression\"*\n\n*Sanjeev Arora, Rong Ge, Behnam Neyshabur, Yi Zhang*\n\n**TL;DR:** These results provide some theoretical justification for widespread empirical success in compressing deep nets and show generalization bounds that're orders of magnitude better in practice.\n\n**Abstract:** Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that're orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net --- a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified \\textquotedblleft noise stability\\textquotedblright properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 608  (*Influential: 92*)\n\n#### 5. Linear Convergent Decentralized Optimization with Compression\n\n*From Search Query: \"stability analysis compression\"*\n\n*Xiaorui Liu, Yao Li, Rongrong Wang, Jiliang Tang, Ming Yan*\n\n**TL;DR:** This paper delineates two key obstacles in the algorithm design -- data heterogeneity and compression error and attempts to explicitly overcome these obstacles, leading to a novel decentralized algorithm named LEAD, the first consensus error bound without assuming bounded gradients.\n\n**Abstract:** Communication compression has been extensively adopted to speed up large-scale distributed optimization. However, most existing decentralized algorithms with compression are unsatisfactory in terms of convergence rate and stability. In this paper, we delineate two key obstacles in the algorithm design -- data heterogeneity and compression error. Our attempt to explicitly overcome these obstacles leads to a novel decentralized algorithm named LEAD. This algorithm is the first \\underline{L}in\\underline{EA}r convergent \\underline{D}ecentralized algorithm with communication compression. Our theory describes the coupled dynamics of the inaccurate model propagation and optimization process. We also provide the first consensus error bound without assuming bounded gradients. Empirical experiments validate our theoretical analysis and show that the proposed algorithm achieves state-of-the-art computation and communication efficiency.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 43  (*Influential: 11*)\n\n#### 6. The alignment property of SGD noise and how it helps select flat minima: A stability analysis\n\n*From Search Query: \"stability analysis compression\"*\n\n*Lei Wu, Mingze Wang, Weijie J. Su*\n\n**TL;DR:** It is proved that if a global minimum is linearly stable for SGD, then it must satisfy the Frobenius norm of Hessian at $\\theta^*$, batch size, and learning rate, respectively, otherwise SGD will escape from that minimum fast.\n\n**Abstract:** The phenomenon that stochastic gradient descent (SGD) favors flat minima has played a critical role in understanding the implicit regularization of SGD. In this paper, we provide an explanation of this striking phenomenon by relating the particular noise structure of SGD to its \\emph{linear stability} (Wu et al., 2018). Specifically, we consider training over-parameterized models with square loss. We prove that if a global minimum $\\theta^*$ is linearly stable for SGD, then it must satisfy $\\|H(\\theta^*)\\|_F\\leq O(\\sqrt{B}/\\eta)$, where $\\|H(\\theta^*)\\|_F, B,\\eta$ denote the Frobenius norm of Hessian at $\\theta^*$, batch size, and learning rate, respectively. Otherwise, SGD will escape from that minimum \\emph{exponentially} fast. Hence, for minima accessible to SGD, the sharpness -- as measured by the Frobenius norm of the Hessian -- is bounded \\emph{independently} of the model size and sample size. The key to obtaining these results is exploiting the particular structure of SGD noise: The noise concentrates in sharp directions of local landscape and the magnitude is proportional to loss value. This alignment property of SGD noise provably holds for linear networks and random feature models (RFMs), and is empirically verified for nonlinear networks. Moreover, the validity and practical relevance of our theoretical findings are also justified by extensive experiments on CIFAR-10 dataset.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 31  (*Influential: 4*)\n\n#### 7. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han*\n\n**TL;DR:** This work designs Hardware-Aware Transformers with neural architecture search, and trains a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing, and performs an evolutionary search with a hardware latency constraint.\n\n**Abstract:** Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 243  (*Influential: 45*)\n\n### 6 related papers from ArXiv\n\n#### 1. Hierarchical Neural Network Approaches for Long Document Classification\n\n*From Search Query: \"gradient propagation hierarchical\"*\n\n*Snehal Khandve, Vedangi Wagh, Apurva Wani, Isha Joshi, Raviraj Joshi*\n\n**Abstract:** Text classification algorithms investigate the intricate relationships\nbetween words or phrases and attempt to deduce the document's interpretation.\nIn the last few years, these algorithms have progressed tremendously.\nTransformer architecture and sentence encoders have proven to give superior\nresults on natural language processing tasks. But a major limitation of these\narchitectures is their applicability for text no longer than a few hundred\nwords. In this paper, we explore hierarchical transfer learning approaches for\nlong document classification. We employ pre-trained Universal Sentence Encoder\n(USE) and Bidirectional Encoder Representations from Transformers (BERT) in a\nhierarchical setup to capture better representations efficiently. Our proposed\nmodels are conceptually simple where we divide the input data into chunks and\nthen pass this through base models of BERT and USE. Then output representation\nfor each chunk is then propagated through a shallow neural network comprising\nof LSTMs or CNNs for classifying the text data. These extensions are evaluated\non 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its\nstand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its\nstand-alone counterpart. However, the hierarchical BERT models are still\ndesirable as it avoids the quadratic complexity of the attention mechanism in\nBERT. Along with the hierarchical approaches, this work also provides a\ncomparison of different deep learning algorithms like USE, BERT, HAN,\nLongformer, and BigBird for long document classification. The Longformer\napproach consistently performs well on most of the datasets.\n\n**Published:** 2022-01-18T07:17:40Z  (*Updated: 2022-01-18T07:17:40Z*)\n\n\n\n#### 2. Approximation-Aware Dependency Parsing by Belief Propagation\n\n*From Search Query: \"gradient propagation hierarchical\"*\n\n*Matthew R. Gormley, Mark Dredze, Jason Eisner*\n\n**Abstract:** We show how to train the fast dependency parser of Smith and Eisner (2008)\nfor improved accuracy. This parser can consider higher-order interactions among\nedges while retaining O(n^3) runtime. It outputs the parse with maximum\nexpected recall -- but for speed, this expectation is taken under a posterior\ndistribution that is constructed only approximately, using loopy belief\npropagation through structured factors. We show how to adjust the model\nparameters to compensate for the errors introduced by this approximation, by\nfollowing the gradient of the actual loss on training data. We find this\ngradient by back-propagation. That is, we treat the entire parser\n(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)\nand Domke (2010) did for loopy CRFs. The resulting trained parser obtains\nhigher accuracy with fewer iterations of belief propagation than one trained by\nconditional log-likelihood.\n\n**Published:** 2015-08-10T19:48:33Z  (*Updated: 2015-08-10T19:48:33Z*)\n\n\n\n#### 3. Training LLMs over Neurally Compressed Text\n\n*From Search Query: \"stability analysis compression\"*\n\n*Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant*\n\n**Abstract:** In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers.\n\n**Published:** 2024-04-04T17:48:28Z  (*Updated: 2024-08-13T22:01:42Z*)\n\n\n\n#### 4. An Empirical Investigation of Matrix Factorization Methods for\n  Pre-trained Transformers\n\n*From Search Query: \"stability analysis compression\"*\n\n*Ashim Gupta, Sina Mahdipour Saravani, P. Sadayappan, Vivek Srikumar*\n\n**Abstract:** The increasing size of transformer-based models in NLP makes the question of\ncompressing them important. In this work, we present a comprehensive analysis\nof factorization based model compression techniques. Specifically, we focus on\ncomparing straightforward low-rank factorization against the recently\nintroduced Monarch factorization, which exhibits impressive performance\npreservation on the GLUE benchmark. To mitigate stability issues associated\nwith low-rank factorization of the matrices in pre-trained transformers, we\nintroduce a staged factorization approach wherein layers are factorized one by\none instead of being factorized simultaneously. Through this strategy we\nsignificantly enhance the stability and reliability of the compression process.\nFurther, we introduce a simple block-wise low-rank factorization method, which\nhas a close relationship to Monarch factorization. Our experiments lead to the\nsurprising conclusion that straightforward low-rank factorization consistently\noutperforms Monarch factorization across both different compression ratios and\nsix different text classification tasks.\n\n**Published:** 2024-06-17T08:14:23Z  (*Updated: 2024-06-17T08:14:23Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 6. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Discrete Autoencoders for Sequence Models\n\n*From Search Query: \"gradient propagation hierarchical\"*\n\n*\u0141ukasz Kaiser, Samy Bengio*\n\n**Abstract:** Recurrent models for sequences have been recently successful at many tasks,\nespecially for language modeling and machine translation. Nevertheless, it\nremains challenging to extract good representations from these models. For\ninstance, even though language has a clear hierarchical structure going from\ncharacters through words to sentences, it is not apparent in current language\nmodels. We propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to\npropagate gradients though this discrete representation we introduce an\nimproved semantic hashing technique. We show that this technique performs well\non a newly proposed quantitative efficiency measure. We also analyze latent\ncodes produced by the model showing how they correspond to words and phrases.\nFinally, we present an application of the autoencoder-augmented model to\ngenerating diverse translations.\n\n**Conference:** discrete-autoencoders-for-sequence-models-1\n\n**Published:** 2018-01-29\n\n\n\n#### 2. Decoupled Neural Interfaces using Synthetic Gradients\n\n*From Search Query: \"gradient propagation hierarchical\"*\n\n*Wojciech Marian Czarnecki, Koray Kavukcuoglu, David Silver, Simon Osindero, Oriol Vinyals, Max Jaderberg, Alex Graves*\n\n**Abstract:** Training directed neural networks typically requires forward-propagating data\nthrough a computation graph, followed by backpropagating error signal, to\nproduce weight updates. All layers, or more generally, modules, of the network\nare therefore locked, in the sense that they must wait for the remainder of the\nnetwork to execute forwards and propagate error backwards before they can be\nupdated. In this work we break this constraint by decoupling modules by\nintroducing a model of the future computation of the network graph. These\nmodels predict what the result of the modelled subgraph will produce using only\nlocal information. In particular we focus on modelling error gradients: by\nusing the modelled synthetic gradient in place of true backpropagated error\ngradients we decouple subgraphs, and can update them independently and\nasynchronously i.e. we realise decoupled neural interfaces. We show results for\nfeed-forward models, where every layer is trained asynchronously, recurrent\nneural networks (RNNs) where predicting one's future gradient extends the time\nover which the RNN can effectively model, and also a hierarchical RNN system\nwith ticking at different timescales. Finally, we demonstrate that in addition\nto predicting gradients, the same framework can be used to predict inputs,\nresulting in models which are decoupled in both the forward and backwards pass\n-- amounting to independent networks which co-learn such that they can be\ncomposed into a single functioning corporation.\n\n**Conference:** decoupled-neural-interfaces-using-synthetic-1\n\n**Published:** 2016-08-18\n\n\n\n#### 3. LIT: Learned Intermediate Representation Training for Model Compression\n\n*From Search Query: \"stability analysis compression\"*\n\n*Matei Zaharia, Peter Bailis, Daniel Kang, Animesh Koratana*\n\n**Abstract:** Researchers have proposed a range of model compression techniques to reduce the computational\r\nand memory footprint of deep neural networks\r\n(DNNs). In this work, we introduce Learned\r\nIntermediate representation Training (LIT), a\r\nnovel model compression technique that outperforms a range of recent model compression techniques by leveraging the highly repetitive structure of modern DNNs (e.g., ResNet). LIT uses a\r\nteacher DNN to train a student DNN of reduced\r\ndepth by leveraging two key ideas: 1) LIT directly compares intermediate representations of\r\nthe teacher and student model and 2) LIT uses\r\nthe intermediate representation from the teacher\r\nmodel\u2019s previous block as input to the current\r\nstudent block during training, improving stability of intermediate representations in the student\r\nnetwork. We show that LIT can substantially reduce network size without loss in accuracy on a\r\nrange of DNN architectures and datasets. For example, LIT can compress ResNet on CIFAR10 by\r\n3.4\u00d7 outperforming network slimming and FitNets. Furthermore, LIT can compress, by depth,\r\nResNeXt 5.5\u00d7 on CIFAR10 (image classification), VDCNN by 1.7\u00d7 on Amazon Reviews\r\n(sentiment analysis), and StarGAN by 1.8\u00d7 on\r\nCelebA (style transfer, i.e., GANs).\n\n**Published:** 2019-09-04\n\n\n\n#### 4. MechElastic: A Python Library for Analysis of Mechanical and Elastic Properties of Bulk and 2D Materials\n\n*From Search Query: \"stability analysis compression\"*\n\n*Anonymous*\n\n**Abstract:** The MechElastic Python package evaluates the mechanical and elastic properties of bulk and 2D materials using the elastic coefficient matrix ($C_{ij}$) obtained from any ab-initio density-functional theory (DFT) code. The current version of this package reads the output of VASP, ABINIT, and Quantum Espresso codes (but it can be easily generalized to any other DFT code) and performs the appropriate post-processing of elastic constants as per the requirement of the user. This program can also detect the input structure's crystal symmetry and test the mechanical stability of all crystal classes using the Born-Huang criteria. Various useful material-specific properties such as elastic moduli, longitudinal and transverse elastic wave velocities, Debye temperature, elastic anisotropy, 2D layer modulus, hardness, Pugh's ratio, Cauchy's pressure, Kleinman parameter, and Lame's coefficients, can be estimated using this program. Another existing feature of this program is to employ the ELATE package [J. Phys.: Condens. Matter 28, 275201 (2016)] and plot the spatial variation of several elastic properties such as Poisson's ratio, linear compressibility, shear modulus, and Young's modulus in three dimensions. Further, the MechElastic package can plot the equation of state (EOS) curves for energy and pressure for a variety of EOS models such as Murnaghan, Birch, Birch-Murnaghan, and Vinet, by reading the inputted energy/pressure versus volume data obtained via numerical calculations or experiments. This package is particularly useful for the high-throughput analysis of elastic and mechanical properties of materials.\n\n**Published:** 2020-12-08\n\n\n\n#### 5. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on gradient propagation in hierarchical networks, stability analysis of compression methods, and hardware-efficient attention implementations, here are some key insights and relevant information from the provided sources and additional context:\n\n## Gradient Propagation in Hierarchical Networks\n\n### Gradient Flow Analysis\n- The vanishing gradient problem is a significant issue in deep and hierarchical networks. To address this, residual connections can be used, which allow part of the gradient to flow through the identity matrix, preventing the gradient from vanishing in deep networks.\n- Hierarchical structures, as analyzed using the Ladderpath approach, indicate that networks with richer hierarchical structures (characterized by an order-rate \u03b7 around 0.5) tend to perform better. This suggests that the hierarchical structure can influence the gradient flow and overall performance of the network.\n\n### Stability Considerations\n- Stability in hierarchical networks can be enhanced by using techniques such as batch normalization and weight initialization strategies. For example, initializing weights according to specific distributions can help reduce the vanishing gradient problem.\n- The use of residual connections also helps in stabilizing the gradient flow, ensuring that gradients do not vanish or explode in deep networks.\n\n### Training Dynamics\n- The training dynamics in hierarchical networks can be influenced by the initial connectivity and weight magnitudes. Studies on biologically plausible learning rules for RNNs show that the initial weight magnitude significantly affects learning performance, and methods like gradient flossing can help regularize the Lyapunov exponents to improve stability.\n\n## Implementation Details of Stable Compression Methods\n\n### Stable Compression Methods\n- Hierarchical Graph Neural Networks (h-GNNs) use a hierarchical structure by successively stacking compressed-size graphs, which helps in preserving both local and global information. This approach can be seen as a form of stable compression, where the network's depth is increased through clustering and message passing mechanisms.\n\n### Convergence Analysis\n- The convergence analysis of compressed representations can be related to the stability of the training process. For instance, the rate-based backpropagation method for Spiking Neural Networks (SNNs) provides a bound on the approximation error, ensuring the robustness of training under practical conditions. This method streamlines the computational graph and reduces memory and computational demands, which is crucial for stable compression.\n\n### Error Propagation\n- Error propagation in compressed representations can be managed by techniques such as rate-based backpropagation. This method approximates the exact inputs with average inputs over time, reducing the complexity of backpropagation through time (BPTT) and ensuring that the approximation errors are effectively bounded.\n\n## Hardware Efficiency\n\n### Memory-Efficient Attention\n- Memory-efficient attention mechanisms are crucial for hardware efficiency. Approaches like multi-scale linear attention in EfficientViT achieve a global receptive field with lightweight and hardware-efficient operations. Similarly, learnable linear attention mechanisms like Hedgehog retain the properties of softmax attention while maintaining linear complexity, which is more hardware-efficient.\n\n### Computational Optimization\n- Computational optimization can be achieved through scale-specific computation methods and hybrid attention patterns. For example, the h-GNN uses message passing mechanisms that propagate local and global messages among nodes and across graphs, which optimizes computational resources by maintaining both local and global information.\n\n### Implementation Strategies\n- Implementation strategies for hardware efficiency include using dynamic compression ratios and adaptive compression techniques. These methods can be combined with efficient attention mechanisms to improve scalability. For instance, hierarchical compression can be integrated with memory-efficient attention to balance compression and information preservation.\n\nIn summary, to improve the autoregressive language model design, the researcher should focus on:\n\n- Using residual connections and appropriate weight initialization to address the vanishing gradient problem.\n- Implementing hierarchical structures that promote richer hierarchical relationships, as indicated by the Ladderpath approach.\n- Employing stable compression methods like hierarchical graph neural networks and rate-based backpropagation to manage error propagation and ensure robust training.\n- Utilizing memory-efficient attention mechanisms such as multi-scale linear attention and learnable linear attention.\n- Optimizing computational resources through scale-specific computation and hybrid attention patterns.\n\nThese strategies will help in achieving better gradient propagation, stability, and hardware efficiency, which are critical for improving the overall performance of autoregressive language models.\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\nThe third round of searches has revealed important insights about hierarchical compression and multi-scale attention architectures:\n\na) Hierarchical Compression:\n- Lossless compression techniques for KV cache (LoMA)\n- Adaptive compression with dynamic ratios\n- Multi-level feature representation\n\nb) Multi-scale Attention:\n- Scale-specific computation methods\n- Memory-efficient implementations\n- Hardware-aware optimizations\n\n2. Useful References with Key Excerpts:\n\nFrom \"EfficientViT\":\n\"Unlike prior high-resolution dense prediction models that rely on heavy softmax attention, hardware-inefficient large-kernel convolution, or complicated topology structure [...] our multi-scale linear attention achieves the global receptive field and multi-scale learning with only lightweight and hardware-efficient operations.\"\n\nFrom \"The Hedgehog & the Porcupine\":\n\"We propose Hedgehog, a learnable linear attention that retains the spiky and monotonic properties of softmax attention while maintaining linear complexity. Hedgehog uses simple trainable MLPs to produce attention weights mimicking softmax attention.\"\n\nFrom \"Learning to Summarize Long Texts with Memory Compression\":\n\"Our memory regularization compresses an encoded input article into a more compact set of sentence representations [...] the memory compression step performs implicit extraction without labels.\"\n\n3. Key Insights:\n\na) Memory Efficiency:\n- Hierarchical compression can be combined with efficient attention\n- Dynamic compression ratios improve adaptability\n- Memory-efficient implementations crucial for scalability\n\nb) Multi-scale Processing:\n- Scale-specific attention computation improves efficiency\n- Hardware-aware implementations enhance performance\n- Hybrid attention patterns (local + global) show promise\n\nc) Integration Challenges:\n- Balancing compression and information preservation\n- Managing computational overhead\n- Maintaining model stability\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Gradient flow in hierarchical architectures\n2. Stability analysis of compressed representations\n3. Hardware-efficient implementation details\n\nNext Search Queries:\n- Gradient propagation in hierarchical networks\n- Stability analysis compression methods\n- Hardware-efficient attention implementations\n\n5. References:\n\n[1] D\u00e9fossez, A., et al. (2023). Simple and Controllable Music Generation. NeurIPS.\n\n[2] Zhang, M., et al. (2024). The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry. arXiv preprint.\n\n[3] Park, J., et al. (2020). Learning to Summarize Long Texts with Memory Compression and Transfer. arXiv preprint.\n\n[4] Gan, C., et al. (2022). EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction.\n\n[5] Panda, R., et al. (2021). CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification. ICCV."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"hierarchical memory compression\", \"multi-scale attention\", \"adaptive neural compression\"",
            "detail": "Search for papers discussing:\n1. Hierarchical memory architectures in neural networks, focusing on:\n   - Multi-level memory organization\n   - Adaptive compression techniques\n   - Memory-efficient attention patterns\n\n2. Implementation details of:\n   - Scale-specific attention computation\n   - Dynamic routing mechanisms\n   - Efficient gradient propagation\n\n3. Applications in language models:\n   - Test-time training with compressed memory\n   - Multi-scale processing of sequences\n   - Adaptive state management",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hierarchical memory architectures in neural networks, focusing on:\n   - Multi-level memory organization\n   - Adaptive compression techniques\n   - Memory-efficient attention patterns\n\n2. Implementation details of:\n   - Scale-specific attention computation\n   - Dynamic routing mechanisms\n   - Efficient gradient propagation\n\n3. Applications in language models:\n   - Test-time training with compressed memory\n   - Multi-scale processing of sequences\n   - Adaptive state management\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.62)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 7/30 (Score: 0.62)*\n\n```\nV. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 77-87, 2017. [6] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [8] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [9] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [11] Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052, 2022. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding.\n```\n\n#### 2. Universal Transformers  (Avg. Score: 0.60)\n\n*Mostafa Dehghani, Stephan Gouws, O. Vinyals, Jakob Uszkoreit, Lukasz Kaiser*\n\n**Published in:** International Conference on Learning Representations (2018)\t**Cited by** 673  (*Influential: 65*)\n\n**TL;DR:** The Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses issues of parallelizability and global receptive field, is proposed.\n\n**Abstract:** Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions, UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.\n\n##### *Relevant Chunk: No. 21/32 (Score: 0.60)*\n\n```\narXiv preprint arXiv:1610.02357, 2016. Zewei Chu, Hai Wang, Kevin Gimpel, and David McAllester. Broad context language modeling as reading comprehension. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, volume 2, pp. 52-57, 2017. Bhuwan Dhingra, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Linguistic knowledge as memory for recurrent neural networks. arXiv preprint arXiv:1703.02620, 2017. Bhuwan Dhingra, Qiao Jin, Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Neural models for reasoning over multiple mentions using coreference. arXiv preprint arXiv:1804.05922, 2018. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. CoRR, abs/1705.03122, 2017. URL/http://arxiv.org/abs/1705.03122\n\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. arXiv preprint arXiv:1612.04426, 2016. Alex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http://arxiv.org/abs/1308.0850\n\nAlex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. CoRR, abs/1410.5401, 2014. URL http://arxiv.org/abs/1410.5401\n\nCaglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, et al. Hyperbolic attention networks. arXiv preprint arXiv:1805.09786, 2018. Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, and Yann LeCun. Tracking the world state with recurrent entity networks. arXiv preprint arXiv:1612.03969, 2016. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, 2003. A. Joulin and T. Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in Neural Information Processing Systems, (NIPS), 2015. \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2016. URL https://arxiv.org/abs/1511.08228\n\n\u0141ukasz Kaiser, Aidan N. Gomez, and Francois Chollet. Depthwise separable convolutions for neural machine translation. CoRR, abs/1706.03059, 2017. URLhttp://arxiv.org/abs/1706.03059. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In International Conference on Machine Learning, pp. 1378-1387, 2016. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint arXiv:1703.03130, 2017. Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of 1stms to learn syntax-sensitive dependencies.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.54)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.54)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.35)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 31/54 (Score: 0.35)*\n\n```\n## A. 2 Memory in Machine Learning\n\nMemory in sequence models Sequential or temporal data in areas such as language, reinforcement learning, and continual learning can involve increasingly long dependencies. However, direct parametric modeling cannot handle inputs of unknown and potentially unbounded lengths. Many modern solutions such as attention [70] and dilated convolutions [5], are functions on finite windows, thus sidestepping the need for an explicit memory representation. While this suffices for certain tasks, these approaches can only process a finite context window instead of an entire sequence. Naively increasing the window length poses significant compute and memory challenges. This has spurred various approaches to extend this fixed context window subjected to compute and storage constraints [6, 15, 18, 42, 59, 60, 64, 74]. We instead focus on the core problem of online processing and memorization of continuous and discrete signals, and anticipate that the study of this foundational problem will be useful in improving a variety of models. Recurrent memory Recurrent neural networks are a natural tool for modeling sequential data online, with the appealing property of having unbounded context; in other words they can summarize history indefinitely. However, due to difficulties in the optimization process (vanishing/exploding gradients [56]), particular care must be paid to endow them with longer memory. The ubiquitous LSTM 34 and simplifications such as the GRU [17] control the update with gates to smooth the optimization process. With more careful parametrization, the addition of gates alone make RNNs significantly more robust and able to address long-term dependencies [31. Tallec and Ollivier [66] show that gates are in fact fundamental for recurrent dynamics by allowing time dilations. Many other approaches to endowing RNNs with better memory exist, such as noise injection [32] or non-saturating gates [9], which can suffer from instability issues.\n```\n\n#### 5. Reformer: The Efficient Transformer (Avg. Score: 0.16)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.16)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hierarchical memory compression\", \"multi-scale attention\", \"adaptive neural compression\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Weighted Mutual Learning with Diversity-Driven Model Compression\n\n*From Search Query: \"hierarchical memory compression\"*\n\n*Miao Zhang, Li Wang, David Campos, Wei Huang, Chenjuan Guo, B. Yang*\n\n**TL;DR:** A framework called Weighted Mutual Learning with Diversity-Driven Model Compression (WML), which outperforms existing online distillation methods on a variety of deep neural networks and produces a series of students with different model sizes in a single run, which also achieves competitive results compared with existing channel pruning methods.\n\n**Abstract:** Online distillation attracts attention from the community as it simplifies the traditional two-stage knowledge distillation process into a single stage. Online distillation collaboratively trains a group of peer models, which are treated as students, and all students gain extra knowledge from each other. However, memory consumption and diversity among students are two key challenges to the scalability and quality of online distillation. To address the two challenges, this paper presents a framework called Weighted Mutual Learning with Diversity-Driven Model Compression ( WML ) for online distillation. First, at the base of a hierarchical structure where students share different parts, we leverage the structured network pruning to generate diversified students with different models sizes, thus also helping reduce the memory requirements. Second, rather than taking the average of students, this paper, for the first time, leverages a bi-level formulation to estimate the relative importance of students with a close-form, to further boost the effectiveness of the distillation from each other. Extensive experiments show the generalization of the proposed framework, which outperforms existing online distillation methods on a variety of deep neural networks. More interesting, as a byproduct, WML produces a series of students with different model sizes in a single run, which also achieves competitive results compared with existing channel pruning methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 2. The Kanerva Machine: A Generative Distributed Memory\n\n*From Search Query: \"hierarchical memory compression\"*\n\n*Yan Wu, Greg Wayne, Alex Graves, T. Lillicrap*\n\n**TL;DR:** An end-to-end trained memory system that quickly adapts to new data and generates samples like them that significantly improves generative models trained on both the Omniglot and CIFAR datasets is presented.\n\n**Abstract:** We present an end-to-end trained memory system that quickly adapts to new data and generates samples like them. Inspired by Kanerva's sparse distributed memory, it has a robust distributed reading and writing mechanism. The memory is analytically tractable, which enables optimal on-line compression via a Bayesian update-rule. We formulate it as a hierarchical conditional generative model, where memory provides a rich data-dependent prior distribution. Consequently, the top-down memory and bottom-up perception are combined to produce the code representing an observation. Empirically, we demonstrate that the adaptive memory significantly improves generative models trained on both the Omniglot and CIFAR datasets. Compared with the Differentiable Neural Computer (DNC) and its variants, our memory model has greater capacity and is significantly easier to train.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 37  (*Influential: 6*)\n\n#### 3. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: \"hierarchical memory compression\"*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 23  (*Influential: 2*)\n\n#### 4. MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting\n\n*From Search Query: \"multi-scale attention\"*\n\n*Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, Yifei Xiao*\n\n**TL;DR:** The proposed method, termed as Multi-scale Isometric Convolution Network (MICN), is more efficient with linear complexity about the sequence length with suitable convolution kernels, and is more efficient with linear complexity about the sequence length with suitable convolution kernels.\n\n**Abstract:** Recently, Transformer-based methods have achieved surprising performance in the field of long-term series forecasting, but the attention mechanism for computing global correlations entails high complexity. And they do not allow for targeted modeling of local features as CNN structures do. To solve the above problems, we propose to combine local features and global correlations to capture the overall view of time series (e.g., fluctuations, trends). To fully exploit the underlying information in the time series, a multi-scale branch structure is adopted to model different potential patterns separately. Each pattern is extracted with down-sampled convolution and isometric convolution for local features and global correlations, respectively. In addition to being more effective, our proposed method, termed as Multi-scale Isometric Convolution Network (MICN), is more efficient with linear complexity about the sequence length with suitable convolution kernels. Our experiments on six benchmark datasets show that compared with state-of-the-art methods, MICN yields 17.2% and 21.6% relative improvements for multivariate and univariate time series, respectively. Code is available at https://github. com/wanghq21/MICN.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 122  (*Influential: 20*)\n\n#### 5. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: \"multi-scale attention\"*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 190  (*Influential: 27*)\n\n#### 6. Multi-Scale Progressive Attention Network for Video Question Answering\n\n*From Search Query: \"multi-scale attention\"*\n\n*Zhicheng Guo, Jiaxuan Zhao, L. Jiao, Xu Liu, Lingling Li*\n\n**TL;DR:** This work proposes a novel Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information to achieve state-of-the-art performance in Video Question Answering.\n\n**Abstract:** Understanding the multi-scale visual information in a video is essential for Video Question Answering (VideoQA). Therefore, we propose a novel Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information. We construct clips of different lengths to represent different scales of the video. Then, the clip-level features are aggregated into node features by using max-pool, and a graph is generated for each scale of clips. For cross-scale feature interaction, we design a message passing strategy between adjacent scale graphs, i.e., top-down scale interaction and bottom-up scale interaction. Under the question\u2019s guidance of progressive attention, we realize the fusion of all-scale video features. Experimental evaluations on three benchmarks: TGIF-QA, MSVD-QA and MSRVTT-QA show our method has achieved state-of-the-art performance.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 2*)\n\n#### 7. Adaptive Estimators Show Information Compression in Deep Neural Networks\n\n*From Search Query: \"adaptive neural compression\"*\n\n*Ivan Chelombiev, Conor J. Houghton, Cian O\u2019Donnell*\n\n**TL;DR:** More robust mutual information estimation techniques are developed, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions, which explore compression in networks with a range of different activation functions.\n\n**Abstract:** To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 7*)\n\n#### 8. Variable-Bitrate Neural Compression via Bayesian Arithmetic Coding\n\n*From Search Query: \"adaptive neural compression\"*\n\n*Yibo Yang, Robert Bamler, S. Mandt*\n\n**TL;DR:** A new algorithm for compressing latent representations in deep probabilistic models, such as variational autoencoders, in post-processing, using adaptive discretization accuracy that exploits estimates of posterior uncertainty is proposed.\n\n**Abstract:** Deep Bayesian latent variable models have enabled new approaches to both model and data compression. Here, we propose a new algorithm for compressing latent representations in deep probabilistic models, such as variational autoencoders, in post-processing. The approach thus separates model design and training from the compression task. Our algorithm generalizes arithmetic coding to the continuous domain, using adaptive discretization accuracy that exploits estimates of posterior uncertainty. A consequence of the \"plug and play\" nature of our approach is that various rate-distortion trade-offs can be achieved with a single trained model, eliminating the need to train multiple models for different bit rates. Our experimental results demonstrate the importance of taking into account posterior uncertainties, and show that image compression with the proposed algorithm outperforms JPEG over a wide range of bit rates using only a single machine learning model. Further experiments on Bayesian neural word embeddings demonstrate the versatility of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 9. Flexible Neural Image Compression via Code Editing\n\n*From Search Query: \"adaptive neural compression\"*\n\n*Chenjian Gao, Tongda Xu, Dailan He, Hongwei Qin, Yan Wang*\n\n**TL;DR:** Experimental results show that the proposed Code Editing method surpasses existing variable-rate methods, and achieves ROI coding and multi-distortion trade-off with a single decoder.\n\n**Abstract:** Neural image compression (NIC) has outperformed traditional image codecs in rate-distortion (R-D) performance. However, it usually requires a dedicated encoder-decoder pair for each point on R-D curve, which greatly hinders its practical deployment. While some recent works have enabled bitrate control via conditional coding, they impose strong prior during training and provide limited flexibility. In this paper we propose Code Editing, a highly flexible coding method for NIC based on semi-amortized inference and adaptive quantization. Our work is a new paradigm for variable bitrate NIC. Furthermore, experimental results show that our method surpasses existing variable-rate methods, and achieves ROI coding and multi-distortion trade-off with a single decoder.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 21  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. Learning to Summarize Long Texts with Memory Compression and Transfer\n\n*From Search Query: \"hierarchical memory compression\"*\n\n*Jaehong Park, Jonathan Pilault, Christopher Pal*\n\n**Abstract:** We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent\nneural network based encoder decoder architectures and we explore its use for\nabstractive document summarization. Mem2Mem transfers \"memories\" via\nreadable/writable external memory modules that augment both the encoder and\ndecoder. Our memory regularization compresses an encoded input article into a\nmore compact set of sentence representations. Most importantly, the memory\ncompression step performs implicit extraction without labels, sidestepping\nissues with suboptimal ground-truth data and exposure bias of hybrid\nextractive-abstractive summarization techniques. By allowing the decoder to\nread/write over the encoded input memory, the model learns to read salient\ninformation about the input article while keeping track of what has been\ngenerated. Our Mem2Mem approach yields results that are competitive with state\nof the art transformer based summarization methods, but with 16 times fewer\nparameters\n\n**Published:** 2020-10-21T21:45:44Z  (*Updated: 2020-10-21T21:45:44Z*)\n\n\n\n#### 2. LoMA: Lossless Compressed Memory Attention\n\n*From Search Query: \"hierarchical memory compression\"*\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.\n\n**Published:** 2024-01-16T09:18:46Z  (*Updated: 2024-02-04T03:14:08Z*)\n\n\n\n#### 3. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry\n\n*From Search Query: \"multi-scale attention\"*\n\n*Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher R\u00e9*\n\n**Abstract:** Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.\n\n**Published:** 2024-02-06T19:31:26Z  (*Updated: 2024-02-06T19:31:26Z*)\n\n\n\n#### 4. Generalized Probabilistic Attention Mechanism in Transformers\n\n*From Search Query: \"multi-scale attention\"*\n\n*DongNyeong Heo, Heeyoul Choi*\n\n**Abstract:** The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.\n\n**Published:** 2024-10-21T01:55:52Z  (*Updated: 2024-10-21T01:55:52Z*)\n\n\n\n#### 5. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural\n  Architecture Search\n\n*From Search Query: \"adaptive neural compression\"*\n\n*Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, Jingren Zhou*\n\n**Abstract:** Large pre-trained language models such as BERT have shown their effectiveness\nin various natural language processing tasks. However, the huge parameter size\nmakes them difficult to be deployed in real-time applications that require\nquick inference with limited resources. Existing methods compress BERT into\nsmall models while such compression is task-independent, i.e., the same\ncompressed BERT for all different downstream tasks. Motivated by the necessity\nand benefits of task-oriented BERT compression, we propose a novel compression\nmethod, AdaBERT, that leverages differentiable Neural Architecture Search to\nautomatically compress BERT into task-adaptive small models for specific tasks.\nWe incorporate a task-oriented knowledge distillation loss to provide search\nhints and an efficiency-aware loss as search constraints, which enables a good\ntrade-off between efficiency and effectiveness for task-adaptive BERT\ncompression. We evaluate AdaBERT on several NLP tasks, and the results\ndemonstrate that those task-adaptive compressed models are 12.7x to 29.3x\nfaster than BERT in inference time and 11.5x to 17.0x smaller in terms of\nparameter size, while comparable performance is maintained.\n\n**Published:** 2020-01-13T14:03:26Z  (*Updated: 2021-01-22T10:58:24Z*)\n\n\n\n#### 6. Long-Short Range Context Neural Networks for Language Modeling\n\n*From Search Query: \"adaptive neural compression\"*\n\n*Youssef Oualil, Mittul Singh, Clayton Greenberg, Dietrich Klakow*\n\n**Abstract:** The goal of language modeling techniques is to capture the statistical and\nstructural properties of natural languages from training corpora. This task\ntypically involves the learning of short range dependencies, which generally\nmodel the syntactic properties of a language and/or long range dependencies,\nwhich are semantic in nature. We propose in this paper a new multi-span\narchitecture, which separately models the short and long context information\nwhile it dynamically merges them to perform the language modeling task. This is\ndone through a novel recurrent Long-Short Range Context (LSRC) network, which\nexplicitly models the local (short) and global (long) context using two\nseparate hidden states that evolve in time. This new architecture is an\nadaptation of the Long-Short Term Memory network (LSTM) to take into account\nthe linguistic properties. Extensive experiments conducted on the Penn Treebank\n(PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a\nsignificant reduction of the perplexity when compared to state-of-the-art\nlanguage modeling techniques.\n\n**Published:** 2017-08-22T10:26:41Z  (*Updated: 2017-08-22T10:26:41Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Residual Dense Network for Image Restoration\n\n*From Search Query: \"hierarchical memory compression\"*\n\n*Yu Kong, Yulun Zhang, Yapeng Tian, Bineng Zhong, Yun Fu*\n\n**Abstract:** Convolutional neural network has recently achieved great success for image restoration (IR) and also offered hierarchical features. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in IR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.\n\n**Published:** 2018-12-25\n\n\n\n#### 2. Learning for Video Compression with Hierarchical Quality and Recurrent Enhancement\n\n*From Search Query: \"hierarchical memory compression\"*\n\n*Luc van Gool, Radu Timofte, Ren Yang, Fabian Mentzer*\n\n**Abstract:** In this paper, we propose a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. The frames in the first layer are compressed by an image compression method with the highest quality. Using these frames as references, we propose the Bi-Directional Deep Compression (BDDC) network to compress the second layer with relatively high quality. Then, the third layer frames are compressed with the lowest quality, by the proposed Single Motion Deep Compression (SMDC) network, which adopts a single motion map to estimate the motions of multiple frames, thus saving bits for motion information. In our deep decoder, we develop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement. In our HLVC approach, the hierarchical quality benefits the coding efficiency, since the high quality information facilitates the compression and enhancement of low quality frames at encoder and decoder sides, respectively. Finally, the experiments validate that our HLVC approach advances the state-of-the-art of deep video compression methods, and outperforms the \"Low-Delay P (LDP) very fast\" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at https://github.com/RenYang-home/HLVC.\n\n**Conference:** learning-for-video-compression-with-1\n\n**Published:** 2020-03-04\n\n\n\n#### 3. EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction\n\n*From Search Query: \"multi-scale attention\"*\n\n*Chuang Gan, Junyan Li, Muyan Hu, Song Han, Han Cai*\n\n**Abstract:** High-resolution dense prediction enables many appealing real-world applications, such as computational photography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art high-resolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of high-resolution vision models with novel multi-scale linear attention. Unlike prior high-resolution dense prediction models that rely on heavy softmax attention, hardware-inefficient large-kernel convolution, or complicated topology structure to obtain good performances, our multi-scale linear attention achieves the global receptive field and multi-scale learning (two desirable features for high-resolution dense prediction) with only lightweight and hardware-efficient operations. As such, EfficientViT delivers remarkable performance gains over previous state-of-the-art models with significant speedup on diverse hardware platforms, including mobile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 13.9$\\times$ and 6.2$\\times$ GPU latency reduction over SegFormer and SegNeXt, respectively. For super-resolution, EfficientViT delivers up to 6.4x speedup over Restormer while providing 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers 48.9x higher throughput on A100 GPU while achieving slightly better zero-shot instance segmentation performance on COCO.\n\n**Published:** 2022-05-29\n\n\n\n#### 4. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\n\n*From Search Query: \"multi-scale attention\"*\n\n*Rameswar Panda, Quanfu Fan, Chun-Fu Chen*\n\n**Abstract:** The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at \\url{https://github.com/IBM/CrossViT}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-27\n\n\n\n#### 5. Context-adaptive neural network based prediction for image compression\n\n*From Search Query: \"adaptive neural compression\"*\n\n*Thierry Dumas, Aline Roumy, Christine Guillemot*\n\n**Abstract:** This paper describes a set of neural network architectures, called Prediction Neural Networks Set (PNNS), based on both fully-connected and convolutional neural networks, for intra image prediction. The choice of neural network for predicting a given image block depends on the block size, hence does not need to be signalled to the decoder. It is shown that, while fully-connected neural networks give good performance for small block sizes, convolutional neural networks provide better predictions in large blocks with complex textures. Thanks to the use of masks of random sizes during training, the neural networks of PNNS well adapt to the available context that may vary, depending on the position of the image block to be predicted. When integrating PNNS into a H.265 codec, PSNR-rate performance gains going from 1.46% to 5.20% are obtained. These gains are on average 0.99% larger than those of prior neural network based methods. Unlike the H.265 intra prediction modes, which are each specialized in predicting a specific texture, the proposed PNNS can model a large set of complex textures.\n\n**Published:** 2018-07-17\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the concepts of hierarchical memory compression, multi-scale attention, and adaptive neural compression, here are some key points and references that can guide the research:\n\n## Hierarchical Memory Architectures and Adaptive Compression\n\n### Multi-Level Memory Organization\n- The paper on efficient arbitrary precision acceleration for large language models discusses a multi-level memory hierarchy on GPUs, which includes global memory, shared memory, registers, and caches. This hierarchy can be leveraged to design hierarchical memory mechanisms in neural networks, optimizing memory access and reducing latency.\n\n### Adaptive Compression Techniques\n- The work on Sparse Autoencoders (SAEs) using the Minimal Description Length (MDL) principle provides insights into adaptive compression. SAEs can be optimized to produce compressed explanations of neural activations, which could be applied to hierarchical memory architectures to manage memory efficiency. This approach ensures that the features are both accurate and concise, which is crucial for adaptive compression in language models.\n\n### Memory-Efficient Attention Patterns\n- The concept of efficient attention mechanisms, such as those in the Reformer model, can be integrated with hierarchical memory. Reformer uses reversible attention and other techniques to reduce memory requirements, which aligns with the goal of memory-efficient attention patterns.\n\n## Implementation Details of Attention and Gradient Propagation\n\n### Scale-Specific Attention Computation\n- Multi-scale attention can be inspired by models like Infini-attention, which handles infinite contexts efficiently. This approach can be adapted to compute attention at different scales, leveraging hierarchical memory to manage context lengths effectively.\n\n### Dynamic Routing Mechanisms\n- Dynamic routing mechanisms can be explored in the context of hierarchical memory and attention. For instance, the use of sparse attention patterns and dynamic routing can help in efficiently processing sequences at multiple scales.\n\n### Efficient Gradient Propagation\n- The paper on efficient arbitrary precision acceleration for large language models discusses efficient matrix preprocessing and data recovery-oriented memory management. These techniques can be adapted to optimize gradient propagation in hierarchical memory architectures, ensuring temporal efficiency in training.\n\n## Applications in Language Models\n\n### Test-Time Training with Compressed Memory\n- The concept of test-time training can be enhanced by using compressed memory. By leveraging hierarchical memory and adaptive compression, models can be trained efficiently even with limited memory resources, which is particularly useful for test-time training scenarios.\n\n### Multi-Scale Processing of Sequences\n- Multi-scale processing can be achieved by integrating hierarchical memory with attention mechanisms that operate at different scales. This approach can improve the model's understanding of long-term dependencies and handle sequences of varying lengths more effectively.\n\n### Adaptive State Management\n- Adaptive state management can be implemented using techniques similar to those in SAEs, where the model adapts to the complexity of the input data by adjusting the compression rate and memory allocation. This ensures that the model can handle varying input patterns efficiently.\n\n## Additional Considerations\n\n### Technical Aspects and Theoretical Foundation\n- For detailed compression mechanism specifications, convergence analysis, and complexity bounds, integrating information-theoretic principles like MDL can provide a solid theoretical foundation. This can help in ensuring that the compression mechanisms preserve important information and converge efficiently.\n\n### Integration Challenges\n- Addressing integration challenges such as memory level synchronization, compression rate adaptation, and gradient flow across levels will be crucial. The use of reversible attention and dynamic routing can help in managing these challenges effectively.\n\nBy combining these insights, researchers can develop an autoregressive language model that leverages hierarchical memory architectures, multi-scale attention, and adaptive neural compression to achieve better performance, efficiency, and scalability.\n",
            "analysis": "Initial Analysis of CompressiveTTTLinear Proposal\n\n1. Key Aspects Requiring Investigation:\n\na) Hierarchical Memory Mechanism:\n- Novel integration of multi-level memory organization with TTT\n- Adaptive compression techniques for memory efficiency\n- Relationship to existing hierarchical memory approaches\n\nb) Compressive Attention:\n- Scale-specific attention computation methods\n- Memory-efficient attention patterns\n- Dynamic routing mechanisms\n\nc) Gradient Optimization:\n- Efficient gradient propagation techniques\n- Temporal efficient training approaches\n- Adaptive learning rate strategies\n\n2. Investigation Plan:\n\nPhase 1: Hierarchical Memory & Compression\n- Research existing hierarchical memory architectures\n- Investigate compression techniques in language models\n- Study adaptive memory management approaches\n\nPhase 2: Attention Mechanisms\n- Analyze efficient attention implementations\n- Research multi-scale attention patterns\n- Study dynamic routing in neural networks\n\nPhase 3: Training & Optimization\n- Investigate gradient optimization techniques\n- Research temporal efficient training\n- Study adaptive learning methods\n\n3. Initial Findings from References:\n\nThe proposal draws inspiration from several recent works:\n- Infini-attention's approach to handling infinite contexts\n- Reformer's efficient attention mechanisms\n- DiffusionBERT's denoising strategies\n\n4. Novelty Assessment:\n\nCompared to existing designs:\n- Unique combination of hierarchical memory with TTT\n- Novel approach to compression in attention mechanisms\n- Distinctive integration of multi-scale processing\n\n5. Key References Found:\n1. \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\" - Munkhdalai et al. (2024)\n2. \"Reformer: The Efficient Transformer\" - Kitaev et al. (2020)\n3. \"DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models\" - He et al. (2022)\n\n6. Potential Impact:\n\nAccuracy:\n- Multi-scale processing could improve model understanding\n- Hierarchical memory might enhance long-term dependency modeling\n- Adaptive compression may preserve important information\n\nRobustness:\n- Multiple memory levels could provide redundancy\n- Adaptive mechanisms might handle varying input patterns\n- Dynamic routing could improve stability\n\nEfficiency:\n- Compression reduces memory requirements\n- Multi-scale processing optimizes computation\n- Efficient gradient propagation saves resources\n\nScalability:\n- Hierarchical structure supports longer sequences\n- Adaptive compression enables larger models\n- Memory-efficient design allows better scaling\n\n7. Research Gaps Identified:\n\na) Technical Aspects:\n- Detailed compression mechanism specifications\n- Multi-scale attention implementation details\n- Gradient optimization strategies\n\nb) Theoretical Foundation:\n- Information preservation guarantees\n- Convergence analysis\n- Complexity bounds\n\nc) Integration Challenges:\n- Memory level synchronization\n- Compression rate adaptation\n- Gradient flow across levels\n\n8. Next Steps:\n\n1. Investigate hierarchical memory architectures in recent literature\n2. Research compression techniques in attention mechanisms\n3. Study efficient gradient propagation methods\n4. Analyze multi-scale processing approaches"
        },
        {
            "ready": false,
            "query": "\"training stability compressed memory\", \"hierarchical gradient propagation\", \"efficient attention implementation\"",
            "detail": "Search for papers discussing:\n1. Training dynamics and stability in models with compressed representations:\n   - Gradient flow through compressed states\n   - Loss function design for compressed memory\n   - Optimization strategies\n\n2. Implementation details of:\n   - Efficient gradient propagation in hierarchical networks\n   - Memory management during training\n   - Stability mechanisms\n\n3. Technical aspects of:\n   - Memory-efficient attention implementations\n   - Compression-aware attention mechanisms\n   - Integration of compression with self-attention",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Training dynamics and stability in models with compressed representations:\n   - Gradient flow through compressed states\n   - Loss function design for compressed memory\n   - Optimization strategies\n\n2. Implementation details of:\n   - Efficient gradient propagation in hierarchical networks\n   - Memory management during training\n   - Stability mechanisms\n\n3. Technical aspects of:\n   - Memory-efficient attention implementations\n   - Compression-aware attention mechanisms\n   - Integration of compression with self-attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.27)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 8/16 (Score: 0.35)*\n\n```\nSee Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.\n```\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.20)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n#### 2. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.19)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.19)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 3. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.17)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.17)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n#### 4. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.14)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.14)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"training stability compressed memory\", \"hierarchical gradient propagation\", \"efficient attention implementation\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training\n\n*From Search Query: \"training stability compressed memory\"*\n\n*Jianfei Chen, Lianmin Zheng, Z. Yao, Dequan Wang, Ion Stoica, Michael W. Mahoney, Joseph E. Gonzalez*\n\n**TL;DR:** The convergence of ActNN is proved, the impact of quantization on the convergence via an exact expression for the gradient variance is characterized, and novel mixed-precision quantization strategies that exploit the activation's heterogeneity across feature dimensions, samples, and layers are proposed.\n\n**Abstract:** The increasing size of neural network models has been critical for improvements in their accuracy, but device memory is not growing at the same rate. This creates fundamental challenges for training neural networks within limited memory environments. In this work, we propose ActNN, a memory-efficient training framework that stores randomly quantized activations for back propagation. We prove the convergence of ActNN for general network architectures, and we characterize the impact of quantization on the convergence via an exact expression for the gradient variance. Using our theory, we propose novel mixed-precision quantization strategies that exploit the activation's heterogeneity across feature dimensions, samples, and layers. These techniques can be readily applied to existing dynamic graph frameworks, such as PyTorch, simply by substituting the layers. We evaluate ActNN on mainstream computer vision models for classification, detection, and segmentation tasks. On all these tasks, ActNN compresses the activation to 2 bits on average, with negligible accuracy loss. ActNN reduces the memory footprint of the activation by 12x, and it enables training with a 6.6x to 14x larger batch size.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 64  (*Influential: 14*)\n\n#### 2. Compressed Context Memory For Online Language Model Interaction\n\n*From Search Query: \"training stability compressed memory\"*\n\n*Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh Song*\n\n**TL;DR:** This paper proposes a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments.\n\n**Abstract:** This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 3. GACT: Activation Compressed Training for Generic Network Architectures\n\n*From Search Query: \"training stability compressed memory\"*\n\n*Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael W. Mahoney, Alvin Cheung*\n\n**TL;DR:** GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge is presented, and the convergence of GACT is proved by analyzing a linearized version of ACT's approximate gradient.\n\n**Abstract:** Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT's approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 20  (*Influential: 3*)\n\n#### 4. Multi-Layered Gradient Boosting Decision Trees\n\n*From Search Query: \"hierarchical gradient propagation\"*\n\n*Ji Feng, Yang Yu, Zhi-Hua Zhou*\n\n**TL;DR:** This work proposes the multi-layered GBDT forest, with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block, and confirms the effectiveness of the model in terms of performance and representation learning ability.\n\n**Abstract:** Multi-layered representation is believed to be the key ingredient of deep neural networks especially in cognitive tasks like computer vision. While non-differentiable models such as gradient boosting decision trees (GBDTs) are the dominant methods for modeling discrete or tabular data, they are hard to incorporate with such representation learning ability. In this work, we propose the multi-layered GBDT forest (mGBDTs), with an explicit emphasis on exploring the ability to learn hierarchical representations by stacking several layers of regression GBDTs as its building block. The model can be jointly trained by a variant of target propagation across layers, without the need to derive back-propagation nor differentiability. Experiments and visualizations confirmed the effectiveness of the model in terms of performance and representation learning ability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 112  (*Influential: 8*)\n\n#### 5. Gradient-EM Bayesian Meta-learning\n\n*From Search Query: \"hierarchical gradient propagation\"*\n\n*Yayi Zou, Xiaoqi Lu*\n\n**TL;DR:** This work proposes a variant of Bayesian meta-learning that improves computational efficiency by avoiding back-propagation computation in the meta-update step, which is exhausting for deep neural networks, and provides flexibility to the inner-update optimization procedure by decoupling it from meta- update.\n\n**Abstract:** Bayesian meta-learning enables robust and fast adaptation to new tasks with uncertainty assessment. The key idea behind Bayesian meta-learning is empirical Bayes inference of hierarchical model. In this work, we extend this framework to include a variety of existing methods, before proposing our variant based on gradient-EM algorithm. Our method improves computational efficiency by avoiding back-propagation computation in the meta-update step, which is exhausting for deep neural networks. Furthermore, it provides flexibility to the inner-update optimization procedure by decoupling it from meta-update. Experiments on sinusoidal regression, few-shot image classification, and policy-based reinforcement learning show that our method not only achieves better accuracy with less computation cost, but is also more robust to uncertainty.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 6. Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies\n\n*From Search Query: \"hierarchical gradient propagation\"*\n\n*Sungryull Sohn, Junhyuk Oh, Honglak Lee*\n\n**TL;DR:** A novel non-parametric gradient-based policy, graph reward propagation, is proposed to pre-train the NSGS agent and further finetune it through actor-critic method, showing that the agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs.\n\n**Abstract:** We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 60  (*Influential: 3*)\n\n#### 7. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\n\n*From Search Query: \"efficient attention implementation\"*\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 8. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: \"efficient attention implementation\"*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. LoMA: Lossless Compressed Memory Attention\n\n*From Search Query: \"training stability compressed memory\"*\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.\n\n**Published:** 2024-01-16T09:18:46Z  (*Updated: 2024-02-04T03:14:08Z*)\n\n\n\n#### 2. CompAct: Compressed Activations for Memory-Efficient LLM Training\n\n*From Search Query: \"training stability compressed memory\"*\n\n*Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*\n\n**Abstract:** We introduce CompAct, a technique that reduces peak memory utilization on GPU\nby 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory\nis a major limiting factor in training LLMs, with various recent works aiming\nto reduce model memory. However most works don't target the largest component\nof allocated memory during training: the model's compute graph, which is stored\nfor the backward pass. By storing low-rank, compressed activations to be used\nin the backward pass we greatly reduce the required memory, unlike previous\nmethods which only reduce optimizer overheads or the number of trained\nparameters. Our compression uses random projection matrices, thus avoiding\nadditional memory overheads. Comparisons with previous techniques for either\npretraining or fine-tuning show that CompAct substantially improves existing\ncompute-performance tradeoffs. We expect CompAct's savings to scale even higher\nfor larger models.\n\n**Published:** 2024-10-20T10:24:38Z  (*Updated: 2024-10-20T10:24:38Z*)\n\n\n\n#### 3. Hierarchical Neural Network Approaches for Long Document Classification\n\n*From Search Query: \"hierarchical gradient propagation\"*\n\n*Snehal Khandve, Vedangi Wagh, Apurva Wani, Isha Joshi, Raviraj Joshi*\n\n**Abstract:** Text classification algorithms investigate the intricate relationships\nbetween words or phrases and attempt to deduce the document's interpretation.\nIn the last few years, these algorithms have progressed tremendously.\nTransformer architecture and sentence encoders have proven to give superior\nresults on natural language processing tasks. But a major limitation of these\narchitectures is their applicability for text no longer than a few hundred\nwords. In this paper, we explore hierarchical transfer learning approaches for\nlong document classification. We employ pre-trained Universal Sentence Encoder\n(USE) and Bidirectional Encoder Representations from Transformers (BERT) in a\nhierarchical setup to capture better representations efficiently. Our proposed\nmodels are conceptually simple where we divide the input data into chunks and\nthen pass this through base models of BERT and USE. Then output representation\nfor each chunk is then propagated through a shallow neural network comprising\nof LSTMs or CNNs for classifying the text data. These extensions are evaluated\non 6 benchmark datasets. We show that USE + CNN/LSTM performs better than its\nstand-alone baseline. Whereas the BERT + CNN/LSTM performs on par with its\nstand-alone counterpart. However, the hierarchical BERT models are still\ndesirable as it avoids the quadratic complexity of the attention mechanism in\nBERT. Along with the hierarchical approaches, this work also provides a\ncomparison of different deep learning algorithms like USE, BERT, HAN,\nLongformer, and BigBird for long document classification. The Longformer\napproach consistently performs well on most of the datasets.\n\n**Published:** 2022-01-18T07:17:40Z  (*Updated: 2022-01-18T07:17:40Z*)\n\n\n\n#### 4. Approximation-Aware Dependency Parsing by Belief Propagation\n\n*From Search Query: \"hierarchical gradient propagation\"*\n\n*Matthew R. Gormley, Mark Dredze, Jason Eisner*\n\n**Abstract:** We show how to train the fast dependency parser of Smith and Eisner (2008)\nfor improved accuracy. This parser can consider higher-order interactions among\nedges while retaining O(n^3) runtime. It outputs the parse with maximum\nexpected recall -- but for speed, this expectation is taken under a posterior\ndistribution that is constructed only approximately, using loopy belief\npropagation through structured factors. We show how to adjust the model\nparameters to compensate for the errors introduced by this approximation, by\nfollowing the gradient of the actual loss on training data. We find this\ngradient by back-propagation. That is, we treat the entire parser\n(approximations and all) as a differentiable circuit, as Stoyanov et al. (2011)\nand Domke (2010) did for loopy CRFs. The resulting trained parser obtains\nhigher accuracy with fewer iterations of belief propagation than one trained by\nconditional log-likelihood.\n\n**Published:** 2015-08-10T19:48:33Z  (*Updated: 2015-08-10T19:48:33Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"efficient attention implementation\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 6. Simple linear attention language models balance the recall-throughput\n  tradeoff\n\n*From Search Query: \"efficient attention implementation\"*\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.\n\n**Published:** 2024-02-28T19:28:27Z  (*Updated: 2024-02-28T19:28:27Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Residual Dense Network for Image Restoration\n\n*From Search Query: \"training stability compressed memory\"*\n\n*Yu Kong, Yulun Zhang, Yapeng Tian, Bineng Zhong, Yun Fu*\n\n**Abstract:** Convolutional neural network has recently achieved great success for image restoration (IR) and also offered hierarchical features. However, most deep CNN based IR models do not make full use of the hierarchical features from the original low-quality images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in IR. We fully exploit the hierarchical features from all the convolutional layers. Specifically, we propose residual dense block (RDB) to extract abundant local features via densely connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory mechanism. To adaptively learn more effective features from preceding and current local features and stabilize the training of wider network, we proposed local feature fusion in RDB. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. We demonstrate the effectiveness of RDN with several representative IR applications, single image super-resolution, Gaussian image denoising, image compression artifact reduction, and image deblurring. Experiments on benchmark and real-world datasets show that our RDN achieves favorable performance against state-of-the-art methods for each IR task quantitatively and visually.\n\n**Published:** 2018-12-25\n\n\n\n#### 2. Normalization Helps Training of Quantized LSTM\n\n*From Search Query: \"training stability compressed memory\"*\n\n*Tie-Yan Liu, Lu Hou, Jinhua Zhu, Fei Gao, Tao Qin, James Kwok*\n\n**Abstract:** The long-short-term memory (LSTM), though powerful, is memory and computa\\x02tion expensive. To alleviate this problem, one approach is to compress its weights by quantization. However, existing quantization methods usually have inferior performance when used on LSTMs. In this paper, we first show theoretically that training a quantized LSTM is difficult because quantization makes the exploding gradient problem more severe, particularly when the LSTM weight matrices are large. We then show that the popularly used weight/layer/batch normalization schemes can help stabilize the gradient magnitude in training quantized LSTMs. Empirical results show that the normalized quantized LSTMs achieve significantly better results than their unnormalized counterparts. Their performance is also comparable with the full-precision LSTM, while being much smaller in size.\n\n**Proceeding:** neurips-2019-12\n\n**Published:** 2019-12-01\n\n\n\n#### 3. Discrete Autoencoders for Sequence Models\n\n*From Search Query: \"hierarchical gradient propagation\"*\n\n*\u0141ukasz Kaiser, Samy Bengio*\n\n**Abstract:** Recurrent models for sequences have been recently successful at many tasks,\nespecially for language modeling and machine translation. Nevertheless, it\nremains challenging to extract good representations from these models. For\ninstance, even though language has a clear hierarchical structure going from\ncharacters through words to sentences, it is not apparent in current language\nmodels. We propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to\npropagate gradients though this discrete representation we introduce an\nimproved semantic hashing technique. We show that this technique performs well\non a newly proposed quantitative efficiency measure. We also analyze latent\ncodes produced by the model showing how they correspond to words and phrases.\nFinally, we present an application of the autoencoder-augmented model to\ngenerating diverse translations.\n\n**Conference:** discrete-autoencoders-for-sequence-models-1\n\n**Published:** 2018-01-29\n\n\n\n#### 4. Decoupled Neural Interfaces using Synthetic Gradients\n\n*From Search Query: \"hierarchical gradient propagation\"*\n\n*Wojciech Marian Czarnecki, Koray Kavukcuoglu, David Silver, Simon Osindero, Oriol Vinyals, Max Jaderberg, Alex Graves*\n\n**Abstract:** Training directed neural networks typically requires forward-propagating data\nthrough a computation graph, followed by backpropagating error signal, to\nproduce weight updates. All layers, or more generally, modules, of the network\nare therefore locked, in the sense that they must wait for the remainder of the\nnetwork to execute forwards and propagate error backwards before they can be\nupdated. In this work we break this constraint by decoupling modules by\nintroducing a model of the future computation of the network graph. These\nmodels predict what the result of the modelled subgraph will produce using only\nlocal information. In particular we focus on modelling error gradients: by\nusing the modelled synthetic gradient in place of true backpropagated error\ngradients we decouple subgraphs, and can update them independently and\nasynchronously i.e. we realise decoupled neural interfaces. We show results for\nfeed-forward models, where every layer is trained asynchronously, recurrent\nneural networks (RNNs) where predicting one's future gradient extends the time\nover which the RNN can effectively model, and also a hierarchical RNN system\nwith ticking at different timescales. Finally, we demonstrate that in addition\nto predicting gradients, the same framework can be used to predict inputs,\nresulting in models which are decoupled in both the forward and backwards pass\n-- amounting to independent networks which co-learn such that they can be\ncomposed into a single functioning corporation.\n\n**Conference:** decoupled-neural-interfaces-using-synthetic-1\n\n**Published:** 2016-08-18\n\n\n\n#### 5. HGRN2: Gated Linear RNNs with State Expansion\n\n*From Search Query: \"efficient attention implementation\"*\n\n*Yiran Zhong, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Songlin Yang, Zhen Qin*\n\n**Abstract:** Hierarchically gated linear RNN (HGRN, \\citealt{HGRN}) has demonstrated competitive training speed and performance in language modeling while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, limiting its expressiveness. To address this issue, we introduce a simple outer product-based state expansion mechanism, which significantly enlarges the recurrent state size without introducing any additional parameters. This enhancement also provides a linear attention interpretation for HGRN2, enabling hardware-efficient training. Our extensive experiments verify the advantage of HGRN2 over HGRN consistently across different settings and competitive with other recurrent models.\n\n**Published:** 2024-04-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on training stability, efficient gradient propagation, and memory-efficient attention implementations, here are some key points and references that align with your requirements:\n\n## Training Dynamics and Stability with Compressed Representations\n\n### Gradient Flow and Loss Function Design\n- **Compression Techniques**: Papers like \"Compressing Optimizer states and Activation for Memory-Efficient FP8 Training\" discuss the use of low-precision training (e.g., FP8) which can reduce memory footprint while maintaining performance. This involves dynamic range expansion and mixed-granularity activation quantization, which are crucial for stable gradient flow through compressed states.\n- **Optimization Strategies**: Techniques such as activation checkpointing and recomputation can reduce memory usage during training, trading off runtime for memory savings. This is discussed in the context of protein language models, which can be adapted to other domains.\n\n### Optimization Strategies\n- **Memory Optimization**: Strategies like eviction and regeneration, recomputation, quantization, and compression are essential for managing memory efficiently during training. These methods help in reducing memory pressure and ensuring stable training dynamics.\n\n## Efficient Gradient Propagation in Hierarchical Networks\n\n### Hierarchical Gradient Propagation\n- **Hierarchical Gated Recurrent Neural Networks**: The concept of hierarchically gated recurrent neural networks, where forget gates are lower bounded by a learnable value that increases monotonically across layers, allows for efficient modeling of long-term and short-term dependencies. This hierarchical structure can be adapted for efficient gradient propagation[Analysis Note, \"Hierarchically Gated Recurrent Neural Network\"].\n- **Distributed Training**: Techniques such as asynchronous updates and gradient compression (quantization or sparsification) can reduce communication overhead and speed up the training process in distributed settings, which is crucial for hierarchical networks.\n\n## Memory-Efficient Attention Implementations\n\n### Memory-Efficient Attention Mechanisms\n- **Infini-attention**: This technique incorporates a compressive memory into the vanilla attention mechanism, combining masked local attention and long-term linear attention. This approach is memory-efficient and can handle both local and global context processing[Analysis Note, \"Leave No Context Behind\"].\n- **LoMA: Lossless Compressed Memory Attention**: This method enables lossless compression of the KV cache, reducing memory and computational demands during autoregressive generation. This is particularly useful for integrating compression with self-attention mechanisms[Analysis Note, \"LoMA: Lossless Compressed Memory Attention\"].\n\n### Compression-Aware Attention Mechanisms\n- **FlashAttention**: This IO-aware self-attention algorithm optimizes memory usage and runtime for long sequences by minimizing read/write operations and fusing operations. This can significantly reduce memory overhead and improve training efficiency.\n\n## Integration of Compression with Self-Attention\n\n### Technical Aspects\n- **Quantization and Compression**: Integrating quantization and compression techniques into self-attention mechanisms can significantly reduce memory usage. For example, rounding model weights to lower-precision numbers can reduce memory consumption, as seen in protein language models.\n- **Activation Checkpointing and Offloading**: Techniques like activation checkpointing and offloading optimization states to CPU memory can further reduce GPU memory usage, ensuring stable and efficient training.\n\nBy leveraging these techniques, researchers can design autoregressive language models that are more stable during training, efficient in gradient propagation, and optimized for memory usage, particularly in the context of hierarchical networks and attention mechanisms.\n",
            "analysis": "Analysis Note for CompressiveTTTLinear Proposal\n\n1. Summary of Analysis:\nThe initial search reveals several promising directions and relevant techniques that could inform the CompressiveTTTLinear design:\n\na) Hierarchical Memory Architectures:\n- Multiple approaches exist for hierarchical memory organization in neural networks\n- Compression techniques range from lossless to adaptive methods\n- Integration with attention mechanisms is still an open research area\n\nb) Multi-scale Processing:\n- Several successful implementations in vision and language models\n- Different approaches to scale-specific attention computation\n- Trade-offs between efficiency and expressiveness\n\n2. Useful References with Key Excerpts:\n\nFrom \"Leave No Context Behind\":\n\"A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"\n\nFrom \"Hierarchically Gated Recurrent Neural Network\":\n\"...includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies.\"\n\nFrom \"LoMA: Lossless Compressed Memory Attention\":\n\"...enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\"\n\n3. Key Insights:\n\na) Memory Architecture:\n- Hierarchical organization should balance local and global information\n- Compression can be integrated at multiple levels\n- Dynamic adaptation of memory structure is beneficial\n\nb) Attention Mechanisms:\n- Multi-scale attention can be implemented efficiently\n- Local and global context processing can be separated\n- Compression can be applied to attention computations\n\nc) Integration Challenges:\n- Need to carefully manage information flow between levels\n- Compression and decompression overhead must be considered\n- Training stability with compressed representations\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Training dynamics with compressed memory\n2. Efficient gradient propagation methods\n3. Specific compression techniques for attention mechanisms\n\nNext Search Queries:\n1. Training stability and optimization with compressed representations\n2. Gradient propagation in hierarchical neural networks\n3. Memory-efficient attention implementation details\n\n5. References:\n\n[1] Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv preprint.\n\n[2] Qin, Z., Yang, S., & Zhong, Y. (2023). Hierarchically Gated Recurrent Neural Network for Sequence Modeling. Neural Information Processing Systems.\n\n[3] Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv preprint.\n\n[4] Zhang, M., Bhatia, K., Kumbong, H., & R\u00e9, C. (2024). The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry. arXiv preprint.\n\n[5] Park, J., Pilault, J., & Pal, C. (2020). Learning to Summarize Long Texts with Memory Compression and Transfer. arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"hierarchical compression language models\", \"test-time adaptation compression\", \"multi-scale attention\"",
            "detail": "Search for papers discussing:\n1. Hierarchical compression techniques in language models:\n   - Multi-level compression strategies\n   - Adaptive compression methods\n   - Integration with test-time training\n\n2. Implementation details of:\n   - Test-time adaptation with compressed states\n   - Dynamic compression mechanisms\n   - Memory-efficient state updates\n\n3. Multi-scale attention architectures:\n   - Hierarchical attention patterns\n   - Scale-specific attention computation\n   - Memory-efficient implementations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hierarchical compression techniques in language models:\n   - Multi-level compression strategies\n   - Adaptive compression methods\n   - Integration with test-time training\n\n2. Implementation details of:\n   - Test-time adaptation with compressed states\n   - Dynamic compression mechanisms\n   - Memory-efficient state updates\n\n3. Multi-scale attention architectures:\n   - Hierarchical attention patterns\n   - Scale-specific attention computation\n   - Memory-efficient implementations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.70)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.70)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.36)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 12/24 (Score: 0.36)*\n\n```\narXiv preprint arXiv:1511.07289, 2015. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128 k context. arXiv preprint arXiv:2402.10171, 2024. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.30)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.30)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.26)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.26)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n#### 5. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.26)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.26)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hierarchical compression language models\", \"test-time adaptation compression\", \"multi-scale attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression\n\n*From Search Query: \"hierarchical compression language models\"*\n\n*Chenhe Dong, Yaliang Li, Ying Shen, Minghui Qiu*\n\n**TL;DR:** To enhance the model capability and transferability, the idea of meta-learning is leveraged and set up domain-relational graphs to capture the relational information across different domains, and to dynamically select the most representative prototypes for each domain, a hierarchical compare-aggregate mechanism to capture hierarchical relationships.\n\n**Abstract:** On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https://github.com/cheneydon/hrkd.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 2. Language-guided Skill Learning with Temporal Variational Inference\n\n*From Search Query: \"hierarchical compression language models\"*\n\n*Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, G. Konidaris, Nicolas Le Roux, Marc-Alexandre Cot'e, Xingdi Yuan*\n\n**TL;DR:** The results demonstrate that agents equipped with the algorithm are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.\n\n**Abstract:** We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 3. LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation\n\n*From Search Query: \"hierarchical compression language models\"*\n\n*Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo Zhao*\n\n**TL;DR:** LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix, which significantly outperforms existing compression methods.\n\n**Abstract:** Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compression methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 43  (*Influential: 4*)\n\n#### 4. Towards Stable Test-Time Adaptation in Dynamic Wild World\n\n*From Search Query: \"test-time adaptation compression\"*\n\n*Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Z. Wen, Yaofo Chen, P. Zhao, Mingkui Tan*\n\n**TL;DR:** This paper proposes a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples.\n\n**Abstract:** Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \\ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \\ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 179  (*Influential: 57*)\n\n#### 5. TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation\n\n*From Search Query: \"test-time adaptation compression\"*\n\n*Hyesu Lim, Byeonggeun Kim, J. Choo, Sungha Choi*\n\n**TL;DR:** A new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer is presented and improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios.\n\n**Abstract:** This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modified batch normalization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from the source data, i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degradation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous methods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adaptation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 68  (*Influential: 6*)\n\n#### 6. Tent: Fully Test-Time Adaptation by Entropy Minimization\n\n*From Search Query: \"test-time adaptation compression\"*\n\n*Dequan Wang, Evan Shelhamer, Shaoteng Liu, B. Olshausen, Trevor Darrell*\n\n**TL;DR:** Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C, and optimize the model for confidence as measured by the entropy of its predictions.\n\n**Abstract:** A model must adapt itself to generalize to new and different data during testing. In this setting of fully test-time adaptation the model has only the test data and its own parameters. We propose to adapt by test entropy minimization (tent1): we optimize the model for confidence as measured by the entropy of its predictions. Our method estimates normalization statistics and optimizes channel-wise affine transformations to update online on each batch. Tent reduces generalization error for image classification on corrupted ImageNet and CIFAR-10/100 and reaches a new state-of-the-art error on ImageNet-C. Tent handles source-free domain adaptation on digit recognition from SVHN to MNIST/MNIST-M/USPS, on semantic segmentation from GTA to Cityscapes, and on the VisDA-C benchmark. These results are achieved in one epoch of test-time optimization without altering training.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 827  (*Influential: 299*)\n\n#### 7. MICN: Multi-scale Local and Global Context Modeling for Long-term Series Forecasting\n\n*From Search Query: \"multi-scale attention\"*\n\n*Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, Yifei Xiao*\n\n**TL;DR:** The proposed method, termed as Multi-scale Isometric Convolution Network (MICN), is more efficient with linear complexity about the sequence length with suitable convolution kernels, and is more efficient with linear complexity about the sequence length with suitable convolution kernels.\n\n**Abstract:** Recently, Transformer-based methods have achieved surprising performance in the field of long-term series forecasting, but the attention mechanism for computing global correlations entails high complexity. And they do not allow for targeted modeling of local features as CNN structures do. To solve the above problems, we propose to combine local features and global correlations to capture the overall view of time series (e.g., fluctuations, trends). To fully exploit the underlying information in the time series, a multi-scale branch structure is adopted to model different potential patterns separately. Each pattern is extracted with down-sampled convolution and isometric convolution for local features and global correlations, respectively. In addition to being more effective, our proposed method, termed as Multi-scale Isometric Convolution Network (MICN), is more efficient with linear complexity about the sequence length with suitable convolution kernels. Our experiments on six benchmark datasets show that compared with state-of-the-art methods, MICN yields 17.2% and 21.6% relative improvements for multivariate and univariate time series, respectively. Code is available at https://github. com/wanghq21/MICN.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 122  (*Influential: 20*)\n\n#### 8. Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training\n\n*From Search Query: \"multi-scale attention\"*\n\n*Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bingyan Zhao, Dong Wang, Y. Qiao, Hongsheng Li*\n\n**TL;DR:** Point-M2AE is proposed, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds that modifications the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of3D shapes.\n\n**Abstract:** Masked Autoencoders (MAE) have shown great potentials in self-supervised pre-training for language and 2D image transformers. However, it still remains an open question on how to exploit masked autoencoding for learning 3D representations of irregular point clouds. In this paper, we propose Point-M2AE, a strong Multi-scale MAE pre-training framework for hierarchical self-supervised learning of 3D point clouds. Unlike the standard transformer in MAE, we modify the encoder and decoder into pyramid architectures to progressively model spatial geometries and capture both fine-grained and high-level semantics of 3D shapes. For the encoder that downsamples point tokens by stages, we design a multi-scale masking strategy to generate consistent visible regions across scales, and adopt a local spatial self-attention mechanism during fine-tuning to focus on neighboring patterns. By multi-scale token propagation, the lightweight decoder gradually upsamples point tokens with complementary skip connections from the encoder, which further promotes the reconstruction from a global-to-local perspective. Extensive experiments demonstrate the state-of-the-art performance of Point-M2AE for 3D representation learning. With a frozen encoder after pre-training, Point-M2AE achieves 92.9% accuracy for linear SVM on ModelNet40, even surpassing some fully trained methods. By fine-tuning on downstream tasks, Point-M2AE achieves 86.43% accuracy on ScanObjectNN, +3.36% to the second-best, and largely benefits the few-shot classification, part segmentation and 3D object detection with the hierarchical pre-training scheme. Code is available at https://github.com/ZrrSkywalker/Point-M2AE.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 190  (*Influential: 27*)\n\n#### 9. Multi-Scale Progressive Attention Network for Video Question Answering\n\n*From Search Query: \"multi-scale attention\"*\n\n*Zhicheng Guo, Jiaxuan Zhao, L. Jiao, Xu Liu, Lingling Li*\n\n**TL;DR:** This work proposes a novel Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information to achieve state-of-the-art performance in Video Question Answering.\n\n**Abstract:** Understanding the multi-scale visual information in a video is essential for Video Question Answering (VideoQA). Therefore, we propose a novel Multi-Scale Progressive Attention Network (MSPAN) to achieve relational reasoning between cross-scale video information. We construct clips of different lengths to represent different scales of the video. Then, the clip-level features are aggregated into node features by using max-pool, and a graph is generated for each scale of clips. For cross-scale feature interaction, we design a message passing strategy between adjacent scale graphs, i.e., top-down scale interaction and bottom-up scale interaction. Under the question\u2019s guidance of progressive attention, we realize the fusion of all-scale video features. Experimental evaluations on three benchmarks: TGIF-QA, MSVD-QA and MSRVTT-QA show our method has achieved state-of-the-art performance.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. Self-organized Hierarchical Softmax\n\n*From Search Query: \"hierarchical compression language models\"*\n\n*Yikang Shen, Shawn Tan, Chrisopher Pal, Aaron Courville*\n\n**Abstract:** We propose a new self-organizing hierarchical softmax formulation for\nneural-network-based language models over large vocabularies. Instead of using\na predefined hierarchical structure, our approach is capable of learning word\nclusters with clear syntactical and semantic meaning during the language model\ntraining process. We provide experiments on standard benchmarks for language\nmodeling and sentence compression tasks. We find that this approach is as fast\nas other efficient softmax approximations, while achieving comparable or even\nbetter performance relative to similar full softmax models.\n\n**Published:** 2017-07-26T18:01:32Z  (*Updated: 2017-07-26T18:01:32Z*)\n\n\n\n#### 2. Learning to Summarize Long Texts with Memory Compression and Transfer\n\n*From Search Query: \"hierarchical compression language models\"*\n\n*Jaehong Park, Jonathan Pilault, Christopher Pal*\n\n**Abstract:** We introduce Mem2Mem, a memory-to-memory mechanism for hierarchical recurrent\nneural network based encoder decoder architectures and we explore its use for\nabstractive document summarization. Mem2Mem transfers \"memories\" via\nreadable/writable external memory modules that augment both the encoder and\ndecoder. Our memory regularization compresses an encoded input article into a\nmore compact set of sentence representations. Most importantly, the memory\ncompression step performs implicit extraction without labels, sidestepping\nissues with suboptimal ground-truth data and exposure bias of hybrid\nextractive-abstractive summarization techniques. By allowing the decoder to\nread/write over the encoded input memory, the model learns to read salient\ninformation about the input article while keeping track of what has been\ngenerated. Our Mem2Mem approach yields results that are competitive with state\nof the art transformer based summarization methods, but with 16 times fewer\nparameters\n\n**Published:** 2020-10-21T21:45:44Z  (*Updated: 2020-10-21T21:45:44Z*)\n\n\n\n#### 3. AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural\n  Architecture Search\n\n*From Search Query: \"test-time adaptation compression\"*\n\n*Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, Jingren Zhou*\n\n**Abstract:** Large pre-trained language models such as BERT have shown their effectiveness\nin various natural language processing tasks. However, the huge parameter size\nmakes them difficult to be deployed in real-time applications that require\nquick inference with limited resources. Existing methods compress BERT into\nsmall models while such compression is task-independent, i.e., the same\ncompressed BERT for all different downstream tasks. Motivated by the necessity\nand benefits of task-oriented BERT compression, we propose a novel compression\nmethod, AdaBERT, that leverages differentiable Neural Architecture Search to\nautomatically compress BERT into task-adaptive small models for specific tasks.\nWe incorporate a task-oriented knowledge distillation loss to provide search\nhints and an efficiency-aware loss as search constraints, which enables a good\ntrade-off between efficiency and effectiveness for task-adaptive BERT\ncompression. We evaluate AdaBERT on several NLP tasks, and the results\ndemonstrate that those task-adaptive compressed models are 12.7x to 29.3x\nfaster than BERT in inference time and 11.5x to 17.0x smaller in terms of\nparameter size, while comparable performance is maintained.\n\n**Published:** 2020-01-13T14:03:26Z  (*Updated: 2021-01-22T10:58:24Z*)\n\n\n\n#### 4. LadaBERT: Lightweight Adaptation of BERT through Hybrid Model\n  Compression\n\n*From Search Query: \"test-time adaptation compression\"*\n\n*Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai Tong, Jing Bai*\n\n**Abstract:** BERT is a cutting-edge language representation model pre-trained by a large\ncorpus, which achieves superior performances on various natural language\nunderstanding tasks. However, a major blocking issue of applying BERT to online\nservices is that it is memory-intensive and leads to unsatisfactory latency of\nuser requests, raising the necessity of model compression. Existing solutions\nleverage the knowledge distillation framework to learn a smaller model that\nimitates the behaviors of BERT. However, the training procedure of knowledge\ndistillation is expensive itself as it requires sufficient training data to\nimitate the teacher model. In this paper, we address this issue by proposing a\nhybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid\nmodel compression), which combines the advantages of different model\ncompression methods, including weight pruning, matrix factorization and\nknowledge distillation. LadaBERT achieves state-of-the-art accuracy on various\npublic datasets while the training overheads can be reduced by an order of\nmagnitude.\n\n**Published:** 2020-04-08T17:18:56Z  (*Updated: 2020-10-21T15:15:11Z*)\n\n\n\n#### 5. The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry\n\n*From Search Query: \"multi-scale attention\"*\n\n*Michael Zhang, Kush Bhatia, Hermann Kumbong, Christopher R\u00e9*\n\n**Abstract:** Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.\n\n**Published:** 2024-02-06T19:31:26Z  (*Updated: 2024-02-06T19:31:26Z*)\n\n\n\n#### 6. Generalized Probabilistic Attention Mechanism in Transformers\n\n*From Search Query: \"multi-scale attention\"*\n\n*DongNyeong Heo, Heeyoul Choi*\n\n**Abstract:** The Transformer architecture has become widely adopted due to its\ndemonstrated success, attributed to the attention mechanism at its core.\nDespite these successes, the attention mechanism of Transformers is associated\nwith two well-known issues: rank-collapse and gradient vanishing. In this\npaper, we present a theoretical analysis that it is inherently difficult to\naddress both issues simultaneously in the conventional attention mechanism. To\nhandle these issues, we introduce a novel class of attention mechanism,\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\ndual-attention implementation within the Transformer architecture. Unlike\nconventional attention mechanisms, GPAM allows for negative attention scores\nwhile preserving a fixed total sum. We provide theoretical evidence that the\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\nrank-collapse and gradient vanishing issues which are difficult to resolve\nsimultaneously with the conventional attention mechanisms. Furthermore, we\nempirically validate this theoretical evidence, demonstrating the superiority\nof daGPAM compared to other alternative attention mechanisms that were proposed\nto address the same issues. Additionally, we demonstrate the practical benefits\nof GPAM in natural language processing tasks, such as language modeling and\nneural machine translation.\n\n**Published:** 2024-10-21T01:55:52Z  (*Updated: 2024-10-21T01:55:52Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Simple and Controllable Music Generation\n\n*From Search Query: \"hierarchical compression language models\"*\n\n*Alexandre D\u00e9fossez, Yossi Adi, Gabriel Synnaeve, David Kant, Tal Remez, Itai Gat, Felix Kreuk, Jade Copet*\n\n**Abstract:** We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-06-08\n\n\n\n#### 2. Discrete Autoencoders for Sequence Models\n\n*From Search Query: \"hierarchical compression language models\"*\n\n*\u0141ukasz Kaiser, Samy Bengio*\n\n**Abstract:** Recurrent models for sequences have been recently successful at many tasks,\nespecially for language modeling and machine translation. Nevertheless, it\nremains challenging to extract good representations from these models. For\ninstance, even though language has a clear hierarchical structure going from\ncharacters through words to sentences, it is not apparent in current language\nmodels. We propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to\npropagate gradients though this discrete representation we introduce an\nimproved semantic hashing technique. We show that this technique performs well\non a newly proposed quantitative efficiency measure. We also analyze latent\ncodes produced by the model showing how they correspond to words and phrases.\nFinally, we present an application of the autoencoder-augmented model to\ngenerating diverse translations.\n\n**Conference:** discrete-autoencoders-for-sequence-models-1\n\n**Published:** 2018-01-29\n\n\n\n#### 3. Autoregressive Diffusion Models\n\n*From Search Query: \"test-time adaptation compression\"*\n\n*Tim Salimans, Rianne van den Berg, Ben Poole, Jasmijn Bastings, Alexey A. Gritsenko, Emiel Hoogeboom*\n\n**Abstract:** We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.\n\n**Conference:** autoregressive-diffusion-models-1\n\n**Published:** 2021-10-05\n\n\n\n#### 4. A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness\n\n*From Search Query: \"test-time adaptation compression\"*\n\n*Bhavya Kailkhura, Jize Zhang, Shreya Chaganti, Brian R. Bartoldson, James Diffenderfer*\n\n**Abstract:** Successful adoption of deep learning (DL) in the wild requires models to be: (1) compact, (2) accurate, and (3) robust to distributional shifts. Unfortunately, efforts towards simultaneously meeting these requirements have mostly been unsuccessful. This raises an important question: Is the inability to create Compact, Accurate, and Robust Deep neural networks (CARDs) fundamental? To answer this question, we perform a large-scale analysis of popular model compression techniques which uncovers several intriguing patterns. Notably, in contrast to traditional pruning approaches (e.g., fine tuning and gradual magnitude pruning), we find that \"lottery ticket-style\" approaches can surprisingly be used to produce CARDs, including binary-weight CARDs. Specifically, we are able to create extremely compact CARDs that, compared to their larger counterparts, have similar test accuracy and matching (or better) robustness -- simply by pruning and (optionally) quantizing. Leveraging the compactness of CARDs, we develop a simple domain-adaptive test-time ensembling approach (CARD-Decks) that uses a gating module to dynamically select appropriate CARDs from the CARD-Deck based on their spectral-similarity with test samples. The proposed approach builds a \"winning hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on CIFAR-10-C accuracies (i.e., 96.8% standard and 92.75% robust) and CIFAR-100-C accuracies (80.6% standard and 71.3% robust) with better memory usage than non-compressed baselines (pretrained CARDs and CARD-Decks available at https://github.com/RobustBench/robustbench). Finally, we provide theoretical support for our empirical findings.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-16\n\n\n\n#### 5. EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction\n\n*From Search Query: \"multi-scale attention\"*\n\n*Chuang Gan, Junyan Li, Muyan Hu, Song Han, Han Cai*\n\n**Abstract:** High-resolution dense prediction enables many appealing real-world applications, such as computational photography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art high-resolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of high-resolution vision models with novel multi-scale linear attention. Unlike prior high-resolution dense prediction models that rely on heavy softmax attention, hardware-inefficient large-kernel convolution, or complicated topology structure to obtain good performances, our multi-scale linear attention achieves the global receptive field and multi-scale learning (two desirable features for high-resolution dense prediction) with only lightweight and hardware-efficient operations. As such, EfficientViT delivers remarkable performance gains over previous state-of-the-art models with significant speedup on diverse hardware platforms, including mobile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 13.9$\\times$ and 6.2$\\times$ GPU latency reduction over SegFormer and SegNeXt, respectively. For super-resolution, EfficientViT delivers up to 6.4x speedup over Restormer while providing 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers 48.9x higher throughput on A100 GPU while achieving slightly better zero-shot instance segmentation performance on COCO.\n\n**Published:** 2022-05-29\n\n\n\n#### 6. CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification\n\n*From Search Query: \"multi-scale attention\"*\n\n*Rameswar Panda, Quanfu Fan, Chun-Fu Chen*\n\n**Abstract:** The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to combine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2\\% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at \\url{https://github.com/IBM/CrossViT}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-27\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using hierarchical compression, test-time adaptation, and multi-scale attention, here are some key findings and references that align with your requirements:\n\n## Hierarchical Compression Techniques in Language Models\n\n### Multi-level Compression Strategies\n- The concept of hierarchical compression can be drawn from techniques used in other domains, such as image processing and speech compression. For instance, the hierarchical multi-scale encoder in speech compression captures features at varying time scales, which can be analogous to hierarchical compression in language models. This involves encoding information at multiple levels, similar to the Hierarchical Multi-Scale Encoder described in the speech compression context, where features are captured at different scales.\n\n### Adaptive Compression Methods\n- Adaptive compression methods are crucial for maintaining efficiency and performance. Papers on retrieval-augmented generation highlight the use of contextual compression, which involves selecting the most relevant parts of the context or summarizing it to reduce the amount of information processed by the language model. This can be seen in the context selection and summarization techniques discussed in the survey on contextual compression in retrieval-augmented generation.\n\n### Integration with Test-Time Training\n- For integrating hierarchical compression with test-time training, the concept of dynamic compression mechanisms is important. Techniques like adaptive compression, where the compression ratio is dynamically selected based on the task complexity or retrieval quality, can be beneficial. For example, methods like AdaComp, which dynamically selects relevant documents based on query complexity, can be adapted for test-time training with compressed states.\n\n## Test-Time Adaptation with Compressed States\n\n### Dynamic Compression Mechanisms\n- Dynamic compression mechanisms are essential for test-time adaptation. The idea of using reinforcement learning (RL) for token selection, as seen in PCRL and TACO-RL, can be adapted to dynamically compress states during test-time training. These methods select tokens based on the task requirements, which can be extended to compressing states in a similar manner.\n\n### Memory-Efficient State Updates\n- Memory-efficient state updates can be achieved through techniques like lossless compression of the KV cache, as described in \"LoMA: Lossless Compressed Memory Attention.\" This method compresses the KV cache after every generated token, reducing memory and computational demands during autoregressive generation[Analysis Note].\n\n## Multi-Scale Attention Architectures\n\n### Hierarchical Attention Patterns\n- Hierarchical attention patterns can be inspired by the hierarchical message passing mechanisms used in graph neural networks. For example, the hierarchical GNN (h-GNN) uses within-level, bottom-up, and top-down message passing to update node representations at multiple hierarchical levels. This hierarchical attention can be adapted to language models to focus on different scales of context.\n\n### Scale-Specific Attention Computation\n- Scale-specific attention computation can be implemented by dividing the attention calculation into different scales or blocks, similar to the hierarchical multi-scale encoder. This approach ensures that attention is computed at various scales, capturing both fine-grained and long-term dependencies. The \"Lightning Attention\" method, which splits attention calculation into intra-blocks and inter-blocks, can be a starting point for this[Analysis Note].\n\n### Memory-Efficient Implementations\n- Memory-efficient implementations of multi-scale attention can be achieved through tiling techniques and hardware-aware designs. For instance, \"Gated Linear Attention Transformers with Hardware-Efficient Training\" discusses how to optimize memory usage through tiling and other hardware-efficient strategies[Analysis Note].\n\n## Additional Insights\n\n- **Hybrid Attention Patterns**: Combining local and global attention patterns can enhance the effectiveness of the model. This is similar to the hybrid attention mechanisms discussed in the context of efficient language modeling[Analysis Note].\n- **Gradient Impact Estimation**: Estimating the impact of compression on gradients at runtime, as in \"GACT: Activation Compressed Training,\" can help in maintaining training stability and optimizing compression ratios[Analysis Note].\n\nBy integrating these concepts\u2014hierarchical compression, adaptive compression, test-time adaptation with compressed states, and multi-scale attention architectures\u2014the researcher can develop a more efficient and scalable autoregressive language model.\n",
            "analysis": "Analysis Note for CompressiveTTTLinear Proposal - Round 2\n\n1. Summary of Analysis:\nThe second round of searches revealed important insights about memory-efficient attention mechanisms and compression techniques:\n\na) Memory Efficiency Approaches:\n- Lossless compression for KV cache (LoMA)\n- Hardware-efficient linear attention implementations\n- Hierarchical gradient propagation methods\n\nb) Training Stability:\n- Quantization and compression impact on gradient flow\n- Adaptive compression techniques\n- Memory-efficient optimization strategies\n\n2. Useful References with Key Excerpts:\n\nFrom \"LoMA: Lossless Compressed Memory Attention\":\n\"...enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. Our method compresses the KV cache after every tc generated tokens with a compression ratio of c...\"\n\nFrom \"Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\":\n\"...split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks...\"\n\nFrom \"GACT: Activation Compressed Training\":\n\"...decides the compression ratio for each tensor by estimating its impact on the gradient at run time...\"\n\n3. Key Insights:\n\na) Compression Techniques:\n- Lossless compression is achievable for KV cache\n- Dynamic compression ratios improve stability\n- Block-based processing enhances efficiency\n\nb) Attention Mechanisms:\n- Hybrid attention patterns (local + global) are effective\n- Hardware-aware implementations crucial for performance\n- Tiling techniques optimize memory usage\n\nc) Training Dynamics:\n- Gradient impact estimation guides compression\n- Adaptive compression ratios maintain stability\n- Memory-efficient optimization methods essential\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Integration of hierarchical compression with test-time training\n2. Multi-scale attention implementations\n3. Memory-efficient gradient propagation techniques\n\nNext Search Queries:\n- Hierarchical compression in language models\n- Test-time adaptation with compressed states\n- Multi-scale attention architectures\n\n5. References:\n\n[1] Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv preprint.\n\n[2] Qin, Z., et al. (2024). Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention. ICML.\n\n[3] Liu, X., et al. (2022). GACT: Activation Compressed Training for Generic Network Architectures. ICML.\n\n[4] Kim, J., et al. (2023). Compressed Context Memory For Online Language Model Interaction. ICLR.\n\n[5] Yang, S., et al. (2024). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.8796629999999999,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.9174000000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "# CompressiveTTTLinear: Enhancing FastTTTLinear with Hierarchical Compressive Memory\n\n## Motivation\n\nCurrent language models face significant challenges in efficiently handling long sequences while maintaining expressiveness and adaptability. While FastTTTLinear offers powerful test-time training capabilities, it still struggles with memory efficiency and effective long-term dependency modeling. By introducing hierarchical compressive memory and adaptive attention mechanisms, we can create a more efficient and scalable architecture.\n\n## Problem Analysis\n\n### Limitations of FastTTTLinear:\n1. Fixed memory structure limits scalability\n2. Inefficient handling of long-term dependencies\n3. High memory requirements for long sequences\n4. Limited adaptation to varying temporal scales\n\n### Opportunities for Improvement:\n1. Hierarchical memory organization\n2. Adaptive compression techniques\n3. Multi-scale attention mechanisms\n4. Efficient gradient propagation\n\n## Core Idea and Philosophy\n\nCompressiveTTTLinear introduces a hierarchical compressive memory mechanism that:\n1. Organizes memory at multiple temporal scales\n2. Adaptively compresses and retrieves information\n3. Uses dynamic routing for attention\n4. Maintains efficient test-time training capabilities\n\n## Design Plan\n\n### Selected GAU to Modify\n```selection FastTTTLinear```\n\n### Modification Overview\n\n1. **Hierarchical Memory Structure**:\n   - Multi-level memory organization\n   - Dynamic compression mechanisms\n   - Adaptive retrieval patterns\n\n2. **Compressive Attention**:\n   - Scale-specific attention computation\n   - Memory-efficient attention patterns\n   - Dynamic routing mechanisms\n\n3. **Gradient Optimization**:\n   - Efficient gradient propagation\n   - Temporal efficient training\n   - Adaptive learning rates\n\n### Implementation Guidelines\n\n```python\nclass CompressiveTTTLinear(GAUBase):\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n                 device=None, dtype=None, num_levels=3, compression_factor=4):\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_levels = num_levels\n        self.compression_factor = compression_factor\n        \n        # Memory levels\n        self.memory_levels = nn.ModuleList([\n            CompressiveMemoryLevel(\n                embed_dim,\n                compression_factor ** i\n            ) for i in range(num_levels)\n        ])\n        \n        # Attention components\n        self.query_proj = nn.Linear(embed_dim, embed_dim)\n        self.key_proj = nn.Linear(embed_dim, embed_dim)\n        self.value_proj = nn.Linear(embed_dim, embed_dim)\n        \n        # Memory compression\n        self.compressor = AdaptiveCompressor(embed_dim)\n        \n        # Output projection\n        self.output_proj = nn.Linear(embed_dim, embed_dim)\n        \n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        \n        # Project queries, keys, values\n        queries = self.query_proj(X)\n        keys = self.key_proj(X)\n        values = self.value_proj(X)\n        \n        # Process through memory levels\n        memory_outputs = []\n        for level in range(self.num_levels):\n            # Compress information for this level\n            compressed_keys = self.memory_levels[level](\n                keys, \n                self.compression_factor ** level\n            )\n            compressed_values = self.memory_levels[level](\n                values, \n                self.compression_factor ** level\n            )\n            \n            # Compute attention at this level\n            level_output = self.compute_level_attention(\n                queries,\n                compressed_keys,\n                compressed_values\n            )\n            memory_outputs.append(level_output)\n        \n        # Combine outputs from different levels\n        combined_output = self.combine_memory_levels(memory_outputs)\n        \n        # Final projection\n        output = self.output_proj(combined_output)\n        \n        return output, Z\n\nclass CompressiveMemoryLevel(nn.Module):\n    def __init__(self, embed_dim, compression_rate):\n        super().__init__()\n        self.compress = nn.Conv1d(\n            embed_dim,\n            embed_dim,\n            kernel_size=compression_rate,\n            stride=compression_rate\n        )\n        \n    def forward(self, x, rate):\n        # Reshape for 1D convolution\n        x = x.transpose(1, 2)\n        # Apply compression\n        compressed = self.compress(x)\n        # Restore shape\n        return compressed.transpose(1, 2)\n\nclass AdaptiveCompressor(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.importance_scorer = nn.Linear(embed_dim, 1)\n        \n    def forward(self, x):\n        # Compute importance scores\n        scores = self.importance_scorer(x)\n        # Apply soft thresholding\n        mask = torch.sigmoid(scores)\n        return x * mask\n```\n\n### Mathematical Formulations\n\n1. **Multi-level Memory Compression**:\n   For each level l:\n   ```\n   M_l = Compress(X, r_l)\n   where r_l = compression_factor^l\n   ```\n\n2. **Level-specific Attention**:\n   ```\n   A_l(Q, K_l, V_l) = softmax(QK_l^T/\u221ad)V_l\n   where K_l, V_l are compressed keys and values at level l\n   ```\n\n3. **Adaptive Compression**:\n   ```\n   s(x) = \u03c3(W_s x + b_s)\n   x_compressed = x * s(x)\n   ```\n\n### Integration Steps\n\n1. **Replace FastTTTLinear**:\n   - Initialize CompressiveTTTLinear with desired number of levels\n   - Configure compression factors for each level\n   - Set up memory hierarchies\n\n2. **Memory Configuration**:\n   - Initialize memory levels\n   - Configure compression mechanisms\n   - Set up attention patterns\n\n3. **Training Modifications**:\n   - Implement level-specific gradient scaling\n   - Configure adaptive compression thresholds\n   - Set up temporal efficient training\n\n## Theoretical Analysis\n\n1. **Memory Efficiency**:\n   - Hierarchical compression reduces memory requirements from O(N) to O(log N)\n   - Adaptive compression maintains important information while reducing storage\n   - Multi-scale attention allows efficient processing of long sequences\n\n2. **Computational Complexity**:\n   - Level-specific attention reduces complexity from O(N\u00b2) to O(N log N)\n   - Adaptive compression adds minimal overhead O(N)\n   - Overall complexity remains efficient for long sequences\n\n3. **Information Preservation**:\n   - Hierarchical structure maintains information at multiple scales\n   - Adaptive compression preserves important features\n   - Multi-level attention captures dependencies at different ranges\n\n## Expected Benefits\n\n1. **Improved Efficiency**:\n   - Reduced memory requirements through hierarchical compression\n   - Efficient attention computation through multi-scale processing\n   - Better gradient flow through temporal efficient training\n\n2. **Enhanced Performance**:\n   - Better handling of long-term dependencies\n   - Improved adaptation to varying temporal scales\n   - More efficient test-time training\n\n3. **Scalability**:\n   - Better scaling with sequence length\n   - Efficient handling of large-scale inputs\n   - Adaptable resource utilization\n\n## References\n\n1. Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv preprint.\n\n2. Singhania, P., et al. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint.\n\n3. Deng, S., et al. (2022). Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting. ICLR.\n\n4. Lampinen, A.K., et al. (2021). Towards mental time travel: a hierarchical memory for reinforcement learning agents. NeurIPS.\n\n5. Li, W., et al. (2022). Online Neural Sequence Detection with Hierarchical Dirichlet Point Process. NeurIPS.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "CompressiveTTTLinear enhances FastTTTLinear by introducing a hierarchical compressive memory mechanism that adaptively stores and retrieves information at multiple time scales. The design integrates dynamic memory compression, multi-scale attention patterns, and efficient gradient propagation to improve performance on long sequences while reducing memory requirements.",
    "ideation": null,
    "modelname": "compressivehierarchicalttt",
    "suggestions": "1. Memory Management Enhancement:\n- Provide more detailed analysis of memory level interactions\n- Include specific compression rate adaptation mechanisms\n- Add safeguards against information loss during compression\n\n2. Implementation Details:\n- Elaborate on gradient flow mechanisms\n- Include specific hardware optimization guidelines\n- Add detailed performance benchmarks\n- Provide concrete examples of adaptive compression\n\n3. Theoretical Analysis:\n- Add formal analysis of information preservation\n- Include convergence guarantees\n- Provide complexity bounds for different sequence lengths\n\n4. Integration Guidelines:\n- Add specific initialization strategies\n- Include failure mode analysis\n- Provide detailed hyperparameter tuning guidelines\n\n5. Evaluation Framework:\n- Define specific metrics for measuring compression quality\n- Include comparative benchmarks\n- Add analysis of computational overhead\n\n6. Hardware Optimization:\n- Add specific hardware-aware implementation details\n- Include memory bandwidth analysis\n- Provide optimization guidelines for different hardware platforms",
    "user_input": ""
}