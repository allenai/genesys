{
    "implementation": {
        "review": null,
        "root": "TTT",
        "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
        "units": {
            "TTT": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = LightningTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "LightningTTTLinear",
                    "SwiGluMLP",
                    "RMSNorm",
                    "Conv"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "SwiGluMLP": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "intermediate_size": null
                },
                "design_traces": null
            },
            "RotaryEmbedding": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rotaryembedding": "@gau_test\ndef test_RotaryEmbedding_test_rotaryembedding(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rotaryembedding = RotaryEmbedding(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rotaryembedding(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass RotaryEmbedding(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, dim=None, max_position_embeddings=16, base\n        =10000, scaling_factor=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scaling_factor = scaling_factor\n        self.dim = dim if dim is not None else embed_dim // 4\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n            torch.int64).float().to(device) / self.dim)\n        self.register_buffer('inv_freq', inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def _forward(self, X, input, position_ids, **Z):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = input.device.type\n        device_type = device_type if isinstance(device_type, str\n            ) and device_type != 'mps' else 'cpu'\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n                ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        Z['cos'] = cos.to(**self.factory_kwargs)\n        Z['sin'] = sin.to(**self.factory_kwargs)\n        return X, Z\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RotaryEmbedding\",\"document\":\"\\nRotaryEmbedding\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "scaling_factor": 1.0,
                    "dim": null,
                    "base": 10000,
                    "max_position_embeddings": 16
                },
                "design_traces": null
            },
            "RMSNorm": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "eps": 1e-05
                },
                "design_traces": null
            },
            "Conv": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "conv_kernel": 4,
                    "rms_norm_eps": 1e-06
                },
                "design_traces": null
            },
            "LightningTTTLinear": {
                "review": "# Comprehensive Review of LightningTTTLinear Implementation\n\n```rating 4.7```\n\n## Strengths\n\n1. **Architectural Innovation**\n- Successfully integrates Lightning Attention with TTT mechanisms\n- Implements efficient block-based processing with both local and global attention\n- Smart handling of variable sequence lengths through block-wise processing\n- Excellent balance between efficiency and expressiveness\n\n2. **Technical Implementation**\n- Clean and well-organized code structure\n- Robust handling of different sequence lengths\n- Efficient memory management through block-wise computation\n- Effective use of PyTorch's native operations for performance\n\n3. **Numerical Stability**\n- Good initialization practices (0.02 scale for weights)\n- Proper normalization layers\n- Smart use of attention scaling\n- Careful handling of data types\n\n4. **Flexibility and Extensibility**\n- Configurable block sizes and attention heads\n- Adaptable token weighting mechanism\n- Modular design allowing for easy modifications\n- Well-documented parameters and configurations\n\n## Areas for Improvement\n\n1. **Memory Optimization**\n```python\ndef _chunk_forward(self, Q, K, V, chunk_size=128):\n    \"\"\"Process attention in chunks to save memory\"\"\"\n    B, H, L, D = Q.shape\n    outputs = []\n    \n    @torch.jit.script\n    def process_chunk(q, k, v):\n        scores = torch.matmul(q, k.transpose(-2, -1))\n        scores = scores * self.attention_scale / math.sqrt(self.head_dim)\n        attn = F.softmax(scores, dim=-1)\n        return torch.matmul(attn, v)\n        \n    for i in range(0, L, chunk_size):\n        chunk_out = process_chunk(\n            Q[:,:,i:i+chunk_size], \n            K[:,:,:i+chunk_size],\n            V[:,:,:i+chunk_size]\n        )\n        outputs.append(chunk_out)\n    \n    return torch.cat(outputs, dim=2)\n```\n\n2. **Performance Optimization**\n```python\n# Add fused operations\n@torch.jit.script\ndef fused_attention(q, k, v, mask, scale):\n    scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n    scores = scores.masked_fill(mask, float('-inf'))\n    attn = F.softmax(scores, dim=-1)\n    return torch.matmul(attn, v)\n```\n\n3. **Gradient Flow**\n```python\nclass GradientCheckpointing(nn.Module):\n    def __init__(self, module, chunks=2):\n        super().__init__()\n        self.module = module\n        self.chunks = chunks\n        \n    def forward(self, *inputs):\n        if not torch.is_grad_enabled():\n            return self.module(*inputs)\n            \n        return torch.utils.checkpoint.checkpoint_sequential(\n            functions=[self.module],\n            segments=self.chunks,\n            input_tensors=inputs\n        )\n```\n\n## Innovation and Impact\n\n### Positive Aspects\n1. **Efficiency Gains**\n- Linear complexity through block-wise processing\n- Efficient memory usage through smart attention patterns\n- Good balance between local and global attention\n\n2. **Scalability**\n- Handles variable sequence lengths gracefully\n- Block-wise processing enables processing of very long sequences\n- Memory-efficient implementation\n\n3. **Integration Benefits**\n- Clean interface with other components\n- Well-defined input/output specifications\n- Modular design enables easy integration\n\n### Advanced Features to Consider\n\n1. **Adaptive Block Sizing**\n```python\ndef compute_optimal_block_size(self, seq_len, memory_limit):\n    \"\"\"Dynamically adjust block size based on sequence length and memory constraints\"\"\"\n    min_blocks = 4  # minimum number of blocks\n    max_block_size = min(seq_len // min_blocks, self.block_size)\n    return max_block_size\n```\n\n2. **Enhanced Global Attention**\n```python\ndef global_attention_pooling(self, K_prev, V_prev, sparse_factor=0.1):\n    \"\"\"Sparse global attention with pooling for efficiency\"\"\"\n    # Pool keys and values for global attention\n    K_pooled = F.adaptive_avg_pool2d(K_prev.transpose(-2, -1), \n                                    output_size=(int(K_prev.size(-2) * sparse_factor), 1))\n    V_pooled = F.adaptive_avg_pool2d(V_prev.transpose(-2, -1),\n                                    output_size=(int(V_prev.size(-2) * sparse_factor), 1))\n    return torch.einsum('bhnd,bhne->bhde', K_pooled.transpose(-2, -1), \n                       V_pooled.transpose(-2, -1))\n```\n\n## Recommendations\n\n1. **Immediate Optimizations**\n- Implement gradient checkpointing for very long sequences\n- Add adaptive block sizing\n- Integrate fused attention operations\n\n2. **Future Enhancements**\n```python\nclass Config:\n    \"\"\"Configuration for advanced features\"\"\"\n    def __init__(self):\n        self.use_adaptive_blocks = True\n        self.enable_gradient_checkpointing = True\n        self.sparse_global_attention = True\n        self.memory_efficient_attention = True\n```\n\n3. **Testing Recommendations**\n```python\n@gau_test\ndef test_long_sequence_stability():\n    \"\"\"Test stability with very long sequences\"\"\"\n    model = LightningTTTLinear(...)\n    x = torch.randn(2, 16384, embed_dim)\n    output = model(x)\n    assert output.isfinite().all()\n```\n\n## Final Notes\n\nThe implementation shows excellent attention to detail and innovative thinking in combining Lightning Attention with TTT mechanisms. The code is well-structured and maintains good balance between efficiency and functionality. The main areas for improvement are around memory optimization and advanced features for very long sequences, but these are enhancements rather than critical issues.\n\nThe high rating (4.7) reflects the strong technical implementation, innovative approach, and successful integration of complex components while maintaining efficiency and scalability. The implementation provides a solid foundation for further optimizations and extensions.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "test_lightning_ttt_linear": "@gau_test\ndef test_LightningTTTLinear_test_lightning_ttt_linear(device=None, dtype=None):\n    \"\"\"Test the LightningTTTLinear implementation.\"\"\"\n    model = LightningTTTLinear(embed_dim=64, block_loc=(0, 0), kwarg_all={},\n        num_attention_heads=4, block_size=16, mini_batch_size=16, device=\n        device, dtype=dtype)\n    x = torch.randn(2, 32, 64, device=device, dtype=dtype)\n    output = model(x)[0]\n    assert output.shape == x.shape, f\"Output shape {output.shape} doesn't match input shape {x.shape}\"\n    loss = output.sum()\n    loss.backward()\n    assert model.W.grad is not None, 'W has no gradients'\n    assert model.b.grad is not None, 'b has no gradients'\n    assert model.learnable_token_idx.grad is not None, 'learnable_token_idx has no gradients'\n    assert not torch.isnan(output).any(), 'Output contains NaN values'\n    assert not torch.isinf(output).any(), 'Output contains infinite values'\n    print('LightningTTTLinear tests passed!')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass LightningTTTLinear(GAUBase):\n    \"\"\"\n    Enhanced TTTLinear with Lightning Attention for efficient long-context modeling.\n    \n    This unit improves upon TTTLinear by:\n    1. Integrating Lightning Attention for efficient processing of long sequences\n    2. Using block-based attention computation (intra-block and inter-block)\n    3. Adding gating mechanisms for better expressiveness\n    4. Maintaining test-time training capabilities\n    \n    Args:\n        embed_dim (int): The embedding dimension\n        block_loc (tuple): Location of block in model (layer_idx, block_idx)\n        kwarg_all (dict): Additional arguments\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n        num_attention_heads (int, optional): Number of attention heads. Default: 4\n        block_size (int, optional): Size of attention blocks. Default: 128\n        mini_batch_size (int, optional): Size of mini-batches for TTT. Default: 16\n        rope_theta (float, optional): Base for rotary embeddings. Default: 10000.0\n        ttt_base_lr (float, optional): Base learning rate for TTT. Default: 1.0\n        dropout (float, optional): Dropout probability. Default: 0.0\n        \n    Shape:\n        - Input: (batch_size, seq_len, embed_dim)\n        - Output: (batch_size, seq_len, embed_dim)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, block_size=128,\n        mini_batch_size=16, rope_theta=10000.0, ttt_base_lr=1.0, dropout=\n        0.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        self.block_size = block_size\n        self.mini_batch_size = mini_batch_size\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        self.ttt_base_lr = ttt_base_lr\n        self.learnable_token_idx = nn.Parameter(torch.zeros(mini_batch_size,\n            **self.factory_kwargs))\n        self.W = nn.Parameter(torch.randn(num_attention_heads, self.\n            head_dim, self.head_dim, **self.factory_kwargs) * 0.02)\n        self.b = nn.Parameter(torch.zeros(num_attention_heads, 1, self.\n            head_dim, **self.factory_kwargs))\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-06, **self.factory_kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-06, **self.factory_kwargs)\n        kwargs['dim'] = self.head_dim\n        kwargs['max_position_embeddings'] = block_size\n        kwargs['base'] = rope_theta\n        self.rotary_emb = RotaryEmbedding(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        token_idx = 1.0 / torch.arange(1, mini_batch_size + 1, **self.\n            factory_kwargs)\n        self.register_buffer('token_idx', token_idx, persistent=False)\n        self.attention_scale = nn.Parameter(torch.ones(1, **self.\n            factory_kwargs))\n\n    def _forward(self, X, position_ids=None, **Z):\n        B, L, D = X.shape\n        if position_ids is None:\n            position_ids = torch.arange(0, L, dtype=torch.long, device=X.device\n                ).unsqueeze(0)\n        X_norm = self.norm1(X)\n        Q = self.q_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        K = self.k_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        V = self.v_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        K_transformed = torch.einsum('bhnd,hde->bhne', K, self.W) + self.b\n        Z['position_ids'] = position_ids % self.block_size\n        Z['input'] = V\n        _, Z = self.rotary_emb(X, **Z)\n        cos, sin = Z['cos'], Z['sin']\n        Q, K_transformed = self.apply_rotary_pos_emb(Q, K_transformed, cos, sin\n            )\n        num_blocks = (L + self.block_size - 1) // self.block_size\n        output = torch.zeros_like(Q)\n        base_token_weights = F.softplus(self.learnable_token_idx)\n        for i in range(num_blocks):\n            start_idx = i * self.block_size\n            end_idx = min((i + 1) * self.block_size, L)\n            block_size = end_idx - start_idx\n            Q_block = Q[:, :, start_idx:end_idx]\n            K_block = K_transformed[:, :, start_idx:end_idx]\n            V_block = V[:, :, start_idx:end_idx]\n            scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n            scores = scores * self.attention_scale / math.sqrt(self.head_dim)\n            causal_mask = torch.triu(torch.ones(block_size, block_size,\n                device=scores.device), diagonal=1).bool()\n            scores = scores.masked_fill(causal_mask, float('-inf'))\n            if block_size <= self.mini_batch_size:\n                block_weights = base_token_weights[:block_size]\n            else:\n                block_weights = F.interpolate(base_token_weights.view(1, 1,\n                    -1), size=block_size, mode='linear', align_corners=False\n                    ).squeeze(0).squeeze(0)\n            attn_weights = F.softmax(scores, dim=-1)\n            attn_weights = attn_weights * block_weights.view(1, 1, -1, 1)\n            attn_weights = F.dropout(attn_weights, p=self.dropout, training\n                =self.training)\n            block_output = torch.matmul(attn_weights, V_block)\n            if i > 0:\n                K_prev = K_transformed[:, :, :start_idx]\n                V_prev = V[:, :, :start_idx]\n                KV_global = torch.einsum('bhnd,bhne->bhde', K_prev, V_prev)\n                global_output = torch.einsum('bhmd,bhde->bhme', Q_block,\n                    KV_global)\n                block_output = block_output + global_output\n            output[:, :, start_idx:end_idx] = block_output\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        gate = torch.sigmoid(self.gate(X_norm))\n        output = gate * output\n        output = self.norm2(output)\n        output = self.o_proj(output)\n        return output\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Apply rotary positional embeddings to queries and keys.\"\"\"\n\n        def rotate_half(x):\n            x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n            return torch.cat((-x2, x1), dim=-1)\n        q_embed = q * cos + rotate_half(q) * sin\n        k_embed = k * cos + rotate_half(k) * sin\n        return q_embed, k_embed\n",
                "rating": 4.7,
                "spec": "{\"unitname\":\"LightningTTTLinear\",\"document\":\"Enhanced TTTLinear with Lightning Attention for efficient long-context modeling.\\n\\nThis unit improves upon TTTLinear by:\\n1. Integrating Lightning Attention for efficient processing of long sequences\\n2. Using block-based attention computation (intra-block and inter-block)\\n3. Adding gating mechanisms for better expressiveness\\n4. Maintaining test-time training capabilities\\n\\nArgs:\\n    embed_dim (int): The embedding dimension\\n    block_loc (tuple): Location of block in model (layer_idx, block_idx)\\n    kwarg_all (dict): Additional arguments\\n    device (torch.device, optional): Device to place tensors\\n    dtype (torch.dtype, optional): Data type of tensors\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4\\n    block_size (int, optional): Size of attention blocks. Default: 128\\n    mini_batch_size (int, optional): Size of mini-batches for TTT. Default: 16\\n    rope_theta (float, optional): Base for rotary embeddings. Default: 10000.0\\n    ttt_base_lr (float, optional): Base learning rate for TTT. Default: 1.0\\n    dropout (float, optional): Dropout probability. Default: 0.0\\n    \\nShape:\\n    - Input: (batch_size, seq_len, embed_dim)\\n    - Output: (batch_size, seq_len, embed_dim)\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RotaryEmbedding"
                ],
                "suggestions": null,
                "args": {
                    "rope_theta": 10000.0,
                    "mini_batch_size": 16,
                    "num_attention_heads": 4,
                    "dropout": 0.0,
                    "ttt_base_lr": 1.0,
                    "block_size": 128
                },
                "design_traces": null
            }
        },
        "rating": null,
        "declares": {
            "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Rotary position embeddings for attention\",\"inputs\":[\"X\",\"position_ids\"],\"outputs\":[\"cos\",\"sin\"]}",
            "LightningTTTLinear": "{\"unitname\":\"LightningTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
        },
        "proposal_traces": [],
        "suggestions": null,
        "name": "lightningtttlinear"
    },
    "status": "implemented",
    "history": [
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = LightningTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LightningTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    },
                    "RotaryEmbedding": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rotaryembedding": "@gau_test\ndef test_RotaryEmbedding_test_rotaryembedding(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rotaryembedding = RotaryEmbedding(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rotaryembedding(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass RotaryEmbedding(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, dim=None, max_position_embeddings=16, base\n        =10000, scaling_factor=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scaling_factor = scaling_factor\n        self.dim = dim if dim is not None else embed_dim // 4\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n            torch.int64).float().to(device) / self.dim)\n        self.register_buffer('inv_freq', inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def _forward(self, X, input, position_ids, **Z):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = input.device.type\n        device_type = device_type if isinstance(device_type, str\n            ) and device_type != 'mps' else 'cpu'\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n                ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        Z['cos'] = cos.to(**self.factory_kwargs)\n        Z['sin'] = sin.to(**self.factory_kwargs)\n        return X, Z\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RotaryEmbedding\",\"document\":\"\\nRotaryEmbedding\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "scaling_factor": 1.0,
                            "dim": null,
                            "base": 10000,
                            "max_position_embeddings": 16
                        },
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "LightningTTTLinear": {
                        "review": "# Comprehensive Review of LightningTTTLinear Implementation\n\n```rating 4.7```\n\n## Strengths\n\n1. **Architectural Innovation**\n- Successfully integrates Lightning Attention with TTT mechanisms\n- Implements efficient block-based processing with both local and global attention\n- Smart handling of variable sequence lengths through block-wise processing\n- Excellent balance between efficiency and expressiveness\n\n2. **Technical Implementation**\n- Clean and well-organized code structure\n- Robust handling of different sequence lengths\n- Efficient memory management through block-wise computation\n- Effective use of PyTorch's native operations for performance\n\n3. **Numerical Stability**\n- Good initialization practices (0.02 scale for weights)\n- Proper normalization layers\n- Smart use of attention scaling\n- Careful handling of data types\n\n4. **Flexibility and Extensibility**\n- Configurable block sizes and attention heads\n- Adaptable token weighting mechanism\n- Modular design allowing for easy modifications\n- Well-documented parameters and configurations\n\n## Areas for Improvement\n\n1. **Memory Optimization**\n```python\ndef _chunk_forward(self, Q, K, V, chunk_size=128):\n    \"\"\"Process attention in chunks to save memory\"\"\"\n    B, H, L, D = Q.shape\n    outputs = []\n    \n    @torch.jit.script\n    def process_chunk(q, k, v):\n        scores = torch.matmul(q, k.transpose(-2, -1))\n        scores = scores * self.attention_scale / math.sqrt(self.head_dim)\n        attn = F.softmax(scores, dim=-1)\n        return torch.matmul(attn, v)\n        \n    for i in range(0, L, chunk_size):\n        chunk_out = process_chunk(\n            Q[:,:,i:i+chunk_size], \n            K[:,:,:i+chunk_size],\n            V[:,:,:i+chunk_size]\n        )\n        outputs.append(chunk_out)\n    \n    return torch.cat(outputs, dim=2)\n```\n\n2. **Performance Optimization**\n```python\n# Add fused operations\n@torch.jit.script\ndef fused_attention(q, k, v, mask, scale):\n    scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n    scores = scores.masked_fill(mask, float('-inf'))\n    attn = F.softmax(scores, dim=-1)\n    return torch.matmul(attn, v)\n```\n\n3. **Gradient Flow**\n```python\nclass GradientCheckpointing(nn.Module):\n    def __init__(self, module, chunks=2):\n        super().__init__()\n        self.module = module\n        self.chunks = chunks\n        \n    def forward(self, *inputs):\n        if not torch.is_grad_enabled():\n            return self.module(*inputs)\n            \n        return torch.utils.checkpoint.checkpoint_sequential(\n            functions=[self.module],\n            segments=self.chunks,\n            input_tensors=inputs\n        )\n```\n\n## Innovation and Impact\n\n### Positive Aspects\n1. **Efficiency Gains**\n- Linear complexity through block-wise processing\n- Efficient memory usage through smart attention patterns\n- Good balance between local and global attention\n\n2. **Scalability**\n- Handles variable sequence lengths gracefully\n- Block-wise processing enables processing of very long sequences\n- Memory-efficient implementation\n\n3. **Integration Benefits**\n- Clean interface with other components\n- Well-defined input/output specifications\n- Modular design enables easy integration\n\n### Advanced Features to Consider\n\n1. **Adaptive Block Sizing**\n```python\ndef compute_optimal_block_size(self, seq_len, memory_limit):\n    \"\"\"Dynamically adjust block size based on sequence length and memory constraints\"\"\"\n    min_blocks = 4  # minimum number of blocks\n    max_block_size = min(seq_len // min_blocks, self.block_size)\n    return max_block_size\n```\n\n2. **Enhanced Global Attention**\n```python\ndef global_attention_pooling(self, K_prev, V_prev, sparse_factor=0.1):\n    \"\"\"Sparse global attention with pooling for efficiency\"\"\"\n    # Pool keys and values for global attention\n    K_pooled = F.adaptive_avg_pool2d(K_prev.transpose(-2, -1), \n                                    output_size=(int(K_prev.size(-2) * sparse_factor), 1))\n    V_pooled = F.adaptive_avg_pool2d(V_prev.transpose(-2, -1),\n                                    output_size=(int(V_prev.size(-2) * sparse_factor), 1))\n    return torch.einsum('bhnd,bhne->bhde', K_pooled.transpose(-2, -1), \n                       V_pooled.transpose(-2, -1))\n```\n\n## Recommendations\n\n1. **Immediate Optimizations**\n- Implement gradient checkpointing for very long sequences\n- Add adaptive block sizing\n- Integrate fused attention operations\n\n2. **Future Enhancements**\n```python\nclass Config:\n    \"\"\"Configuration for advanced features\"\"\"\n    def __init__(self):\n        self.use_adaptive_blocks = True\n        self.enable_gradient_checkpointing = True\n        self.sparse_global_attention = True\n        self.memory_efficient_attention = True\n```\n\n3. **Testing Recommendations**\n```python\n@gau_test\ndef test_long_sequence_stability():\n    \"\"\"Test stability with very long sequences\"\"\"\n    model = LightningTTTLinear(...)\n    x = torch.randn(2, 16384, embed_dim)\n    output = model(x)\n    assert output.isfinite().all()\n```\n\n## Final Notes\n\nThe implementation shows excellent attention to detail and innovative thinking in combining Lightning Attention with TTT mechanisms. The code is well-structured and maintains good balance between efficiency and functionality. The main areas for improvement are around memory optimization and advanced features for very long sequences, but these are enhancements rather than critical issues.\n\nThe high rating (4.7) reflects the strong technical implementation, innovative approach, and successful integration of complex components while maintaining efficiency and scalability. The implementation provides a solid foundation for further optimizations and extensions.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_lightning_ttt_linear": "@gau_test\ndef test_LightningTTTLinear_test_lightning_ttt_linear(device=None, dtype=None):\n    \"\"\"Test the LightningTTTLinear implementation.\"\"\"\n    model = LightningTTTLinear(embed_dim=64, block_loc=(0, 0), kwarg_all={},\n        num_attention_heads=4, block_size=16, mini_batch_size=16, device=\n        device, dtype=dtype)\n    x = torch.randn(2, 32, 64, device=device, dtype=dtype)\n    output = model(x)[0]\n    assert output.shape == x.shape, f\"Output shape {output.shape} doesn't match input shape {x.shape}\"\n    loss = output.sum()\n    loss.backward()\n    assert model.W.grad is not None, 'W has no gradients'\n    assert model.b.grad is not None, 'b has no gradients'\n    assert model.learnable_token_idx.grad is not None, 'learnable_token_idx has no gradients'\n    assert not torch.isnan(output).any(), 'Output contains NaN values'\n    assert not torch.isinf(output).any(), 'Output contains infinite values'\n    print('LightningTTTLinear tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass LightningTTTLinear(GAUBase):\n    \"\"\"\n    Enhanced TTTLinear with Lightning Attention for efficient long-context modeling.\n    \n    This unit improves upon TTTLinear by:\n    1. Integrating Lightning Attention for efficient processing of long sequences\n    2. Using block-based attention computation (intra-block and inter-block)\n    3. Adding gating mechanisms for better expressiveness\n    4. Maintaining test-time training capabilities\n    \n    Args:\n        embed_dim (int): The embedding dimension\n        block_loc (tuple): Location of block in model (layer_idx, block_idx)\n        kwarg_all (dict): Additional arguments\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n        num_attention_heads (int, optional): Number of attention heads. Default: 4\n        block_size (int, optional): Size of attention blocks. Default: 128\n        mini_batch_size (int, optional): Size of mini-batches for TTT. Default: 16\n        rope_theta (float, optional): Base for rotary embeddings. Default: 10000.0\n        ttt_base_lr (float, optional): Base learning rate for TTT. Default: 1.0\n        dropout (float, optional): Dropout probability. Default: 0.0\n        \n    Shape:\n        - Input: (batch_size, seq_len, embed_dim)\n        - Output: (batch_size, seq_len, embed_dim)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, block_size=128,\n        mini_batch_size=16, rope_theta=10000.0, ttt_base_lr=1.0, dropout=\n        0.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        self.block_size = block_size\n        self.mini_batch_size = mini_batch_size\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        self.ttt_base_lr = ttt_base_lr\n        self.learnable_token_idx = nn.Parameter(torch.zeros(mini_batch_size,\n            **self.factory_kwargs))\n        self.W = nn.Parameter(torch.randn(num_attention_heads, self.\n            head_dim, self.head_dim, **self.factory_kwargs) * 0.02)\n        self.b = nn.Parameter(torch.zeros(num_attention_heads, 1, self.\n            head_dim, **self.factory_kwargs))\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-06, **self.factory_kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-06, **self.factory_kwargs)\n        kwargs['dim'] = self.head_dim\n        kwargs['max_position_embeddings'] = block_size\n        kwargs['base'] = rope_theta\n        self.rotary_emb = RotaryEmbedding(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        token_idx = 1.0 / torch.arange(1, mini_batch_size + 1, **self.\n            factory_kwargs)\n        self.register_buffer('token_idx', token_idx, persistent=False)\n        self.attention_scale = nn.Parameter(torch.ones(1, **self.\n            factory_kwargs))\n\n    def _forward(self, X, position_ids=None, **Z):\n        B, L, D = X.shape\n        if position_ids is None:\n            position_ids = torch.arange(0, L, dtype=torch.long, device=X.device\n                ).unsqueeze(0)\n        X_norm = self.norm1(X)\n        Q = self.q_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        K = self.k_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        V = self.v_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        K_transformed = torch.einsum('bhnd,hde->bhne', K, self.W) + self.b\n        Z['position_ids'] = position_ids % self.block_size\n        Z['input'] = V\n        _, Z = self.rotary_emb(X, **Z)\n        cos, sin = Z['cos'], Z['sin']\n        Q, K_transformed = self.apply_rotary_pos_emb(Q, K_transformed, cos, sin\n            )\n        num_blocks = (L + self.block_size - 1) // self.block_size\n        output = torch.zeros_like(Q)\n        base_token_weights = F.softplus(self.learnable_token_idx)\n        for i in range(num_blocks):\n            start_idx = i * self.block_size\n            end_idx = min((i + 1) * self.block_size, L)\n            block_size = end_idx - start_idx\n            Q_block = Q[:, :, start_idx:end_idx]\n            K_block = K_transformed[:, :, start_idx:end_idx]\n            V_block = V[:, :, start_idx:end_idx]\n            scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n            scores = scores * self.attention_scale / math.sqrt(self.head_dim)\n            causal_mask = torch.triu(torch.ones(block_size, block_size,\n                device=scores.device), diagonal=1).bool()\n            scores = scores.masked_fill(causal_mask, float('-inf'))\n            if block_size <= self.mini_batch_size:\n                block_weights = base_token_weights[:block_size]\n            else:\n                block_weights = F.interpolate(base_token_weights.view(1, 1,\n                    -1), size=block_size, mode='linear', align_corners=False\n                    ).squeeze(0).squeeze(0)\n            attn_weights = F.softmax(scores, dim=-1)\n            attn_weights = attn_weights * block_weights.view(1, 1, -1, 1)\n            attn_weights = F.dropout(attn_weights, p=self.dropout, training\n                =self.training)\n            block_output = torch.matmul(attn_weights, V_block)\n            if i > 0:\n                K_prev = K_transformed[:, :, :start_idx]\n                V_prev = V[:, :, :start_idx]\n                KV_global = torch.einsum('bhnd,bhne->bhde', K_prev, V_prev)\n                global_output = torch.einsum('bhmd,bhde->bhme', Q_block,\n                    KV_global)\n                block_output = block_output + global_output\n            output[:, :, start_idx:end_idx] = block_output\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        gate = torch.sigmoid(self.gate(X_norm))\n        output = gate * output\n        output = self.norm2(output)\n        output = self.o_proj(output)\n        return output\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Apply rotary positional embeddings to queries and keys.\"\"\"\n\n        def rotate_half(x):\n            x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n            return torch.cat((-x2, x1), dim=-1)\n        q_embed = q * cos + rotate_half(q) * sin\n        k_embed = k * cos + rotate_half(k) * sin\n        return q_embed, k_embed\n",
                        "rating": 4.7,
                        "spec": "{\"unitname\":\"LightningTTTLinear\",\"document\":\"Enhanced TTTLinear with Lightning Attention for efficient long-context modeling.\\n\\nThis unit improves upon TTTLinear by:\\n1. Integrating Lightning Attention for efficient processing of long sequences\\n2. Using block-based attention computation (intra-block and inter-block)\\n3. Adding gating mechanisms for better expressiveness\\n4. Maintaining test-time training capabilities\\n\\nArgs:\\n    embed_dim (int): The embedding dimension\\n    block_loc (tuple): Location of block in model (layer_idx, block_idx)\\n    kwarg_all (dict): Additional arguments\\n    device (torch.device, optional): Device to place tensors\\n    dtype (torch.dtype, optional): Data type of tensors\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4\\n    block_size (int, optional): Size of attention blocks. Default: 128\\n    mini_batch_size (int, optional): Size of mini-batches for TTT. Default: 16\\n    rope_theta (float, optional): Base for rotary embeddings. Default: 10000.0\\n    ttt_base_lr (float, optional): Base learning rate for TTT. Default: 1.0\\n    dropout (float, optional): Dropout probability. Default: 0.0\\n    \\nShape:\\n    - Input: (batch_size, seq_len, embed_dim)\\n    - Output: (batch_size, seq_len, embed_dim)\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RotaryEmbedding"
                        ],
                        "suggestions": null,
                        "args": {
                            "rope_theta": 10000.0,
                            "mini_batch_size": 16,
                            "num_attention_heads": 4,
                            "dropout": 0.0,
                            "ttt_base_lr": 1.0,
                            "block_size": 128
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Rotary position embeddings for attention\",\"inputs\":[\"X\",\"position_ids\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "LightningTTTLinear": "{\"unitname\":\"LightningTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "lightningtttlinear"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_mini",
                    "IMPLEMENTATION_CODER": "claude3.5_sonnet",
                    "PROPOSAL_REVIEWER": "claude3.5_sonnet",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.14870699999999998,
                "IMPLEMENTATION_CODER": 4.443732,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 6.721920000000001,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = LightningTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LightningTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    },
                    "RotaryEmbedding": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rotaryembedding": "@gau_test\ndef test_RotaryEmbedding_test_rotaryembedding(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rotaryembedding = RotaryEmbedding(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rotaryembedding(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass RotaryEmbedding(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, dim=None, max_position_embeddings=16, base\n        =10000, scaling_factor=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.scaling_factor = scaling_factor\n        self.dim = dim if dim is not None else embed_dim // 4\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / self.base ** (torch.arange(0, self.dim, 2, dtype=\n            torch.int64).float().to(device) / self.dim)\n        self.register_buffer('inv_freq', inv_freq, persistent=False)\n\n    @torch.no_grad()\n    def _forward(self, X, input, position_ids, **Z):\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(\n            position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        device_type = input.device.type\n        device_type = device_type if isinstance(device_type, str\n            ) and device_type != 'mps' else 'cpu'\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()\n                ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        Z['cos'] = cos.to(**self.factory_kwargs)\n        Z['sin'] = sin.to(**self.factory_kwargs)\n        return X, Z\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RotaryEmbedding\",\"document\":\"\\nRotaryEmbedding\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "scaling_factor": 1.0,
                            "dim": null,
                            "base": 10000,
                            "max_position_embeddings": 16
                        },
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "LightningTTTLinear": {
                        "review": "# Comprehensive Review of LightningTTTLinear Implementation\n\n```rating 4.7```\n\n## Strengths\n\n1. **Architectural Innovation**\n- Successfully integrates Lightning Attention with TTT mechanisms\n- Implements efficient block-based processing with both local and global attention\n- Smart handling of variable sequence lengths through block-wise processing\n- Excellent balance between efficiency and expressiveness\n\n2. **Technical Implementation**\n- Clean and well-organized code structure\n- Robust handling of different sequence lengths\n- Efficient memory management through block-wise computation\n- Effective use of PyTorch's native operations for performance\n\n3. **Numerical Stability**\n- Good initialization practices (0.02 scale for weights)\n- Proper normalization layers\n- Smart use of attention scaling\n- Careful handling of data types\n\n4. **Flexibility and Extensibility**\n- Configurable block sizes and attention heads\n- Adaptable token weighting mechanism\n- Modular design allowing for easy modifications\n- Well-documented parameters and configurations\n\n## Areas for Improvement\n\n1. **Memory Optimization**\n```python\ndef _chunk_forward(self, Q, K, V, chunk_size=128):\n    \"\"\"Process attention in chunks to save memory\"\"\"\n    B, H, L, D = Q.shape\n    outputs = []\n    \n    @torch.jit.script\n    def process_chunk(q, k, v):\n        scores = torch.matmul(q, k.transpose(-2, -1))\n        scores = scores * self.attention_scale / math.sqrt(self.head_dim)\n        attn = F.softmax(scores, dim=-1)\n        return torch.matmul(attn, v)\n        \n    for i in range(0, L, chunk_size):\n        chunk_out = process_chunk(\n            Q[:,:,i:i+chunk_size], \n            K[:,:,:i+chunk_size],\n            V[:,:,:i+chunk_size]\n        )\n        outputs.append(chunk_out)\n    \n    return torch.cat(outputs, dim=2)\n```\n\n2. **Performance Optimization**\n```python\n# Add fused operations\n@torch.jit.script\ndef fused_attention(q, k, v, mask, scale):\n    scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n    scores = scores.masked_fill(mask, float('-inf'))\n    attn = F.softmax(scores, dim=-1)\n    return torch.matmul(attn, v)\n```\n\n3. **Gradient Flow**\n```python\nclass GradientCheckpointing(nn.Module):\n    def __init__(self, module, chunks=2):\n        super().__init__()\n        self.module = module\n        self.chunks = chunks\n        \n    def forward(self, *inputs):\n        if not torch.is_grad_enabled():\n            return self.module(*inputs)\n            \n        return torch.utils.checkpoint.checkpoint_sequential(\n            functions=[self.module],\n            segments=self.chunks,\n            input_tensors=inputs\n        )\n```\n\n## Innovation and Impact\n\n### Positive Aspects\n1. **Efficiency Gains**\n- Linear complexity through block-wise processing\n- Efficient memory usage through smart attention patterns\n- Good balance between local and global attention\n\n2. **Scalability**\n- Handles variable sequence lengths gracefully\n- Block-wise processing enables processing of very long sequences\n- Memory-efficient implementation\n\n3. **Integration Benefits**\n- Clean interface with other components\n- Well-defined input/output specifications\n- Modular design enables easy integration\n\n### Advanced Features to Consider\n\n1. **Adaptive Block Sizing**\n```python\ndef compute_optimal_block_size(self, seq_len, memory_limit):\n    \"\"\"Dynamically adjust block size based on sequence length and memory constraints\"\"\"\n    min_blocks = 4  # minimum number of blocks\n    max_block_size = min(seq_len // min_blocks, self.block_size)\n    return max_block_size\n```\n\n2. **Enhanced Global Attention**\n```python\ndef global_attention_pooling(self, K_prev, V_prev, sparse_factor=0.1):\n    \"\"\"Sparse global attention with pooling for efficiency\"\"\"\n    # Pool keys and values for global attention\n    K_pooled = F.adaptive_avg_pool2d(K_prev.transpose(-2, -1), \n                                    output_size=(int(K_prev.size(-2) * sparse_factor), 1))\n    V_pooled = F.adaptive_avg_pool2d(V_prev.transpose(-2, -1),\n                                    output_size=(int(V_prev.size(-2) * sparse_factor), 1))\n    return torch.einsum('bhnd,bhne->bhde', K_pooled.transpose(-2, -1), \n                       V_pooled.transpose(-2, -1))\n```\n\n## Recommendations\n\n1. **Immediate Optimizations**\n- Implement gradient checkpointing for very long sequences\n- Add adaptive block sizing\n- Integrate fused attention operations\n\n2. **Future Enhancements**\n```python\nclass Config:\n    \"\"\"Configuration for advanced features\"\"\"\n    def __init__(self):\n        self.use_adaptive_blocks = True\n        self.enable_gradient_checkpointing = True\n        self.sparse_global_attention = True\n        self.memory_efficient_attention = True\n```\n\n3. **Testing Recommendations**\n```python\n@gau_test\ndef test_long_sequence_stability():\n    \"\"\"Test stability with very long sequences\"\"\"\n    model = LightningTTTLinear(...)\n    x = torch.randn(2, 16384, embed_dim)\n    output = model(x)\n    assert output.isfinite().all()\n```\n\n## Final Notes\n\nThe implementation shows excellent attention to detail and innovative thinking in combining Lightning Attention with TTT mechanisms. The code is well-structured and maintains good balance between efficiency and functionality. The main areas for improvement are around memory optimization and advanced features for very long sequences, but these are enhancements rather than critical issues.\n\nThe high rating (4.7) reflects the strong technical implementation, innovative approach, and successful integration of complex components while maintaining efficiency and scalability. The implementation provides a solid foundation for further optimizations and extensions.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_lightning_ttt_linear": "@gau_test\ndef test_LightningTTTLinear_test_lightning_ttt_linear(device=None, dtype=None):\n    \"\"\"Test the LightningTTTLinear implementation.\"\"\"\n    model = LightningTTTLinear(embed_dim=64, block_loc=(0, 0), kwarg_all={},\n        num_attention_heads=4, block_size=16, mini_batch_size=16, device=\n        device, dtype=dtype)\n    x = torch.randn(2, 32, 64, device=device, dtype=dtype)\n    output = model(x)[0]\n    assert output.shape == x.shape, f\"Output shape {output.shape} doesn't match input shape {x.shape}\"\n    loss = output.sum()\n    loss.backward()\n    assert model.W.grad is not None, 'W has no gradients'\n    assert model.b.grad is not None, 'b has no gradients'\n    assert model.learnable_token_idx.grad is not None, 'learnable_token_idx has no gradients'\n    assert not torch.isnan(output).any(), 'Output contains NaN values'\n    assert not torch.isinf(output).any(), 'Output contains infinite values'\n    print('LightningTTTLinear tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass LightningTTTLinear(GAUBase):\n    \"\"\"\n    Enhanced TTTLinear with Lightning Attention for efficient long-context modeling.\n    \n    This unit improves upon TTTLinear by:\n    1. Integrating Lightning Attention for efficient processing of long sequences\n    2. Using block-based attention computation (intra-block and inter-block)\n    3. Adding gating mechanisms for better expressiveness\n    4. Maintaining test-time training capabilities\n    \n    Args:\n        embed_dim (int): The embedding dimension\n        block_loc (tuple): Location of block in model (layer_idx, block_idx)\n        kwarg_all (dict): Additional arguments\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n        num_attention_heads (int, optional): Number of attention heads. Default: 4\n        block_size (int, optional): Size of attention blocks. Default: 128\n        mini_batch_size (int, optional): Size of mini-batches for TTT. Default: 16\n        rope_theta (float, optional): Base for rotary embeddings. Default: 10000.0\n        ttt_base_lr (float, optional): Base learning rate for TTT. Default: 1.0\n        dropout (float, optional): Dropout probability. Default: 0.0\n        \n    Shape:\n        - Input: (batch_size, seq_len, embed_dim)\n        - Output: (batch_size, seq_len, embed_dim)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, block_size=128,\n        mini_batch_size=16, rope_theta=10000.0, ttt_base_lr=1.0, dropout=\n        0.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        self.block_size = block_size\n        self.mini_batch_size = mini_batch_size\n        self.embed_dim = embed_dim\n        self.dropout = dropout\n        self.ttt_base_lr = ttt_base_lr\n        self.learnable_token_idx = nn.Parameter(torch.zeros(mini_batch_size,\n            **self.factory_kwargs))\n        self.W = nn.Parameter(torch.randn(num_attention_heads, self.\n            head_dim, self.head_dim, **self.factory_kwargs) * 0.02)\n        self.b = nn.Parameter(torch.zeros(num_attention_heads, 1, self.\n            head_dim, **self.factory_kwargs))\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.norm1 = nn.LayerNorm(embed_dim, eps=1e-06, **self.factory_kwargs)\n        self.norm2 = nn.LayerNorm(embed_dim, eps=1e-06, **self.factory_kwargs)\n        kwargs['dim'] = self.head_dim\n        kwargs['max_position_embeddings'] = block_size\n        kwargs['base'] = rope_theta\n        self.rotary_emb = RotaryEmbedding(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        token_idx = 1.0 / torch.arange(1, mini_batch_size + 1, **self.\n            factory_kwargs)\n        self.register_buffer('token_idx', token_idx, persistent=False)\n        self.attention_scale = nn.Parameter(torch.ones(1, **self.\n            factory_kwargs))\n\n    def _forward(self, X, position_ids=None, **Z):\n        B, L, D = X.shape\n        if position_ids is None:\n            position_ids = torch.arange(0, L, dtype=torch.long, device=X.device\n                ).unsqueeze(0)\n        X_norm = self.norm1(X)\n        Q = self.q_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        K = self.k_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        V = self.v_proj(X_norm).view(B, L, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n        K_transformed = torch.einsum('bhnd,hde->bhne', K, self.W) + self.b\n        Z['position_ids'] = position_ids % self.block_size\n        Z['input'] = V\n        _, Z = self.rotary_emb(X, **Z)\n        cos, sin = Z['cos'], Z['sin']\n        Q, K_transformed = self.apply_rotary_pos_emb(Q, K_transformed, cos, sin\n            )\n        num_blocks = (L + self.block_size - 1) // self.block_size\n        output = torch.zeros_like(Q)\n        base_token_weights = F.softplus(self.learnable_token_idx)\n        for i in range(num_blocks):\n            start_idx = i * self.block_size\n            end_idx = min((i + 1) * self.block_size, L)\n            block_size = end_idx - start_idx\n            Q_block = Q[:, :, start_idx:end_idx]\n            K_block = K_transformed[:, :, start_idx:end_idx]\n            V_block = V[:, :, start_idx:end_idx]\n            scores = torch.matmul(Q_block, K_block.transpose(-2, -1))\n            scores = scores * self.attention_scale / math.sqrt(self.head_dim)\n            causal_mask = torch.triu(torch.ones(block_size, block_size,\n                device=scores.device), diagonal=1).bool()\n            scores = scores.masked_fill(causal_mask, float('-inf'))\n            if block_size <= self.mini_batch_size:\n                block_weights = base_token_weights[:block_size]\n            else:\n                block_weights = F.interpolate(base_token_weights.view(1, 1,\n                    -1), size=block_size, mode='linear', align_corners=False\n                    ).squeeze(0).squeeze(0)\n            attn_weights = F.softmax(scores, dim=-1)\n            attn_weights = attn_weights * block_weights.view(1, 1, -1, 1)\n            attn_weights = F.dropout(attn_weights, p=self.dropout, training\n                =self.training)\n            block_output = torch.matmul(attn_weights, V_block)\n            if i > 0:\n                K_prev = K_transformed[:, :, :start_idx]\n                V_prev = V[:, :, :start_idx]\n                KV_global = torch.einsum('bhnd,bhne->bhde', K_prev, V_prev)\n                global_output = torch.einsum('bhmd,bhde->bhme', Q_block,\n                    KV_global)\n                block_output = block_output + global_output\n            output[:, :, start_idx:end_idx] = block_output\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        gate = torch.sigmoid(self.gate(X_norm))\n        output = gate * output\n        output = self.norm2(output)\n        output = self.o_proj(output)\n        return output\n\n    def apply_rotary_pos_emb(self, q, k, cos, sin):\n        \"\"\"Apply rotary positional embeddings to queries and keys.\"\"\"\n\n        def rotate_half(x):\n            x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n            return torch.cat((-x2, x1), dim=-1)\n        q_embed = q * cos + rotate_half(q) * sin\n        k_embed = k * cos + rotate_half(k) * sin\n        return q_embed, k_embed\n",
                        "rating": 4.7,
                        "spec": "{\"unitname\":\"LightningTTTLinear\",\"document\":\"Enhanced TTTLinear with Lightning Attention for efficient long-context modeling.\\n\\nThis unit improves upon TTTLinear by:\\n1. Integrating Lightning Attention for efficient processing of long sequences\\n2. Using block-based attention computation (intra-block and inter-block)\\n3. Adding gating mechanisms for better expressiveness\\n4. Maintaining test-time training capabilities\\n\\nArgs:\\n    embed_dim (int): The embedding dimension\\n    block_loc (tuple): Location of block in model (layer_idx, block_idx)\\n    kwarg_all (dict): Additional arguments\\n    device (torch.device, optional): Device to place tensors\\n    dtype (torch.dtype, optional): Data type of tensors\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4\\n    block_size (int, optional): Size of attention blocks. Default: 128\\n    mini_batch_size (int, optional): Size of mini-batches for TTT. Default: 16\\n    rope_theta (float, optional): Base for rotary embeddings. Default: 10000.0\\n    ttt_base_lr (float, optional): Base learning rate for TTT. Default: 1.0\\n    dropout (float, optional): Dropout probability. Default: 0.0\\n    \\nShape:\\n    - Input: (batch_size, seq_len, embed_dim)\\n    - Output: (batch_size, seq_len, embed_dim)\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RotaryEmbedding"
                        ],
                        "suggestions": null,
                        "args": {
                            "rope_theta": 10000.0,
                            "mini_batch_size": 16,
                            "num_attention_heads": 4,
                            "dropout": 0.0,
                            "ttt_base_lr": 1.0,
                            "block_size": 128
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Rotary position embeddings for attention\",\"inputs\":[\"X\",\"position_ids\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "LightningTTTLinear": "{\"unitname\":\"LightningTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "lightningtttlinear"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_mini",
                    "IMPLEMENTATION_CODER": "claude3.5_sonnet",
                    "PROPOSAL_REVIEWER": "claude3.5_sonnet",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.14870699999999998,
                "IMPLEMENTATION_CODER": 4.443732,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 6.721920000000001,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}