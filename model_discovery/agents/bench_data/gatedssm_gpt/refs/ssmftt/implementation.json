{
    "implementation": {
        "review": null,
        "root": "TTT",
        "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
        "units": {
            "TTT": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "FastTTTLinear",
                    "SwiGluMLP",
                    "RMSNorm",
                    "Conv"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "RMSNorm": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "eps": 1e-05
                },
                "design_traces": null
            },
            "FastTTTLinear": {
                "review": "```rating 4.8\n```\n\n## Comprehensive Feedback Report for FastTTTLinear Implementation\n\n### 1. Overall Assessment\n\nThe **FastTTTLinear** implementation demonstrates a high level of sophistication, adhering closely to the proposed design while incorporating thoughtful optimizations and enhancements. The module successfully integrates advanced features such as adaptive chunk sizing, fused gate and projection operations, and optional Flash Attention support, all of which contribute to its performance and scalability. The recent updates address previous concerns, ensuring robust functionality and seamless integration within the larger language model (LM) framework. Given the passed functionality checks and the meticulous attention to performance and memory optimizations, the implementation deserves a strong rating.\n\n### 2. Strengths of the Implementation\n\n**A. Architectural Excellence**\n- **Adaptive Chunk Sizing**: The dynamic optimization of chunk size based on sequence length and available GPU memory ensures efficient resource utilization, enhancing scalability and flexibility across different hardware configurations.\n- **Fused Gate and Projection Operations**: Combining gate and projection operations for Q and K projections reduces computational overhead, streamlining the attention mechanism and improving processing speed.\n- **Optional Flash Attention Support**: Integration with Flash Attention when available leverages cutting-edge techniques for accelerated attention computation, further boosting performance.\n\n**B. Memory and Performance Optimizations**\n- **Gradient Checkpointing**: The incorporation of gradient checkpointing aids in memory-efficient training, allowing for the handling of longer sequences without exceeding memory limitations.\n- **Local Convolution Integration**: Utilizing depthwise convolutions (`nn.Conv1d` with `groups=embed_dim`) for local context processing enhances the model's ability to capture local dependencies efficiently.\n- **Performance Monitoring Capabilities**: The implementation includes tracking performance metrics (`perf_stats`) for both forward and attention operations, facilitating ongoing optimization and debugging.\n\n**C. Robust Code Quality and Documentation**\n- **Comprehensive Documentation**: Detailed docstrings provide clear explanations of the module\u2019s functionality, key features, hardware requirements, and performance characteristics, aiding both users and future developers in understanding and utilizing the component effectively.\n- **Initialization Stability**: Proper weight initialization methods (`xavier_uniform_`) ensure stable training dynamics, preventing issues such as vanishing or exploding gradients.\n- **Error Handling and Assertions**: Assertions, such as checking divisibility of `embed_dim` by `num_attention_heads`, enforce architectural constraints and prevent common configuration errors.\n\n### 3. Areas for Improvement and Specific Suggestions\n\n**A. Enhanced Memory Profiling**\n- **Memory Usage Reporting**: While memory tracking is implemented, providing more granular memory usage reports (e.g., per operation) could offer deeper insights into potential memory bottlenecks.\n  \n  **Suggestion**:\n  ```python\n  def _track_memory(self):\n      if self.memory_profiling and torch.cuda.is_available():\n          current = torch.cuda.memory_allocated()\n          reserved = torch.cuda.memory_reserved()\n          self.peak_memory = max(self.peak_memory, current)\n          logger.info(f\"Memory Allocated: {current}, Memory Reserved: {reserved}, Peak Memory: {self.peak_memory}\")\n  ```\n\n**B. Further Performance Enhancements**\n- **Batch Processing Optimizations**: Exploring parallel processing techniques or further optimizing tensor operations could yield additional speed improvements, especially for large batch sizes.\n  \n  **Suggestion**:\n  ```python\n  def _parallel_process_chunks(self, Q, K, V, mask):\n      \"\"\"Parallelize chunk processing using multi-threading or CUDA streams.\"\"\"\n      # Placeholder for parallel processing implementation\n      pass\n  ```\n\n**C. Comprehensive Unit Testing**\n- **Extended Test Coverage**: While functionality checks have passed, expanding unit tests to cover edge cases, varying sequence lengths, and different hardware configurations can ensure robustness across diverse scenarios.\n  \n  **Suggestion**:\n  ```python\n  @gau_test\n  def test_fasttttlinear_varying_seq_lengths():\n      model = FastTTTLinear(embed_dim=512, block_loc=(0,0), kwarg_all={})\n      for seq_len in [128, 1024, 8192, 32768]:\n          x = torch.randn(2, seq_len, 512)\n          y, _ = model(x)\n          assert y.shape == x.shape, f\"Output shape {y.shape} does not match input shape {x.shape}\"\n  ```\n\n**D. Code Refactoring for Modularity**\n- **Modular Attention Mechanism**: Separating different components of the attention mechanism into distinct methods or classes can enhance code readability and maintainability.\n  \n  **Suggestion**:\n  ```python\n  def _prepare_attention(self, X_norm):\n      Q, K = self._fused_gate_projection(X_norm)\n      V = self.W_V(X_norm)\n      Q = Q.view(B, L, H, D_H).transpose(1, 2)\n      K = K.view(B, L, H, D_H).transpose(1, 2)\n      V = V.view(B, L, H, D_H).transpose(1, 2)\n      return Q, K, V\n  ```\n\n### 4. Comments on Innovation and Potential Impact\n\n**Innovative Aspects**\n- **Fused Gate and Projection Operations**: This approach minimizes redundant computations within the attention mechanism, representing a novel optimization that can be pivotal for performance gains.\n- **Adaptive Chunk Sizing with Memory Awareness**: Tailoring chunk sizes based on real-time memory availability showcases a sophisticated method to balance computational load and memory usage dynamically.\n\n**Potential Impact**\n- **Scalability**: The ability to handle very long sequences efficiently positions **FastTTTLinear** as a formidable component for next-generation language models that require extensive context handling.\n- **Versatility Across Hardware**: By accommodating various GPU memory capacities and optionally leveraging Flash Attention, the module is well-suited for deployment in diverse computational environments.\n- **Performance Leadership**: These combined innovations could set new benchmarks in attention mechanism efficiency, influencing future research and development in language model architectures.\n\n**Concerns About Integration and Scalability**\n- **Compatibility with Other Modules**: Ensuring seamless interoperability with other GAUs and model components is crucial. Misalignments in tensor dimensions or operation sequences could introduce integration challenges.\n- **Resource Limitations**: While adaptive chunk sizing mitigates memory issues, extremely large embeddings or vocabularies might still pose challenges. Continuous monitoring and iterative optimization will be necessary as model scales increase.\n\n### 5. Recommendations for the Coder\n\nTo further refine and enhance the **FastTTTLinear** implementation, consider the following actions:\n\n1. **Implement Detailed Memory Profiling**:\n   - Enhance the `_track_memory` method to provide comprehensive memory usage reports during different phases of the forward pass.\n   - Utilize logging to capture and store memory metrics for post-run analysis.\n\n2. **Expand Unit Tests**:\n   - Develop a broader suite of unit tests that encompass various sequence lengths, batch sizes, and edge cases.\n   - Incorporate tests that specifically evaluate the behavior when Flash Attention is enabled versus disabled.\n\n3. **Optimize Parallel Processing**:\n   - Investigate opportunities for parallelizing chunk processing, potentially leveraging multi-threading or CUDA streams to expedite computations.\n   - Evaluate the benefits of such optimizations through empirical testing.\n\n4. **Refactor for Enhanced Modularity**:\n   - Reorganize the attention mechanism into discrete, reusable components or helper functions. This will improve code readability and facilitate easier maintenance and future enhancements.\n\n5. **Integrate Comprehensive Logging**:\n   - Utilize the logging framework to record key events, such as the start and end of attention computation, memory allocations, and performance metrics.\n   - Ensure that logs are informative yet concise to aid in debugging without overwhelming storage resources.\n\n6. **Conduct Empirical Evaluations**:\n   - Benchmark the module against existing attention mechanisms to quantify performance gains and identify potential bottlenecks.\n   - Perform ablation studies to understand the impact of each optimization, such as fused gate projections and adaptive chunk sizing.\n\n7. **Ensure Robust Error Handling**:\n   - Implement safeguards against potential errors arising from unexpected input shapes or invalid configurations.\n   - Provide meaningful error messages that can guide troubleshooting efforts.\n\n8. **Maintain Documentation Excellence**:\n   - Keep the module's documentation up-to-date with any new features or changes.\n   - Ensure that examples and usage guidelines are clear and cover a range of scenarios.\n\n9. **Collaborate with Peers for Code Reviews**:\n   - Engage in peer reviews to gain diverse perspectives on the implementation.\n   - Utilize feedback from colleagues to uncover hidden issues and explore alternative optimization strategies.\n\n10. **Plan for Future Extensions**:\n    - Design the module with extensibility in mind, allowing for the incorporation of additional features or adaptations to different model architectures in the future.\n\n### Final Thoughts\n\nThe **FastTTTLinear** implementation stands as a robust and highly optimized component within the GAU framework, embodying advanced techniques to enhance both performance and scalability. By addressing the highlighted areas for improvement and leveraging its inherent strengths, the module is well-positioned to make significant contributions to the next generation of language models. Continued diligence in testing, optimization, and documentation will ensure that **FastTTTLinear** remains a high-performing and reliable element within the broader LM architecture.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "test_fasttttlinear_causality": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_causality(device=None, dtype=None):\n    device = device or torch.device('cpu')\n    dtype = dtype or torch.float32\n    embed_dim = 32\n    seq_len = 64\n    model = FastTTTLinear(embed_dim, (0, 0), {}, device=device, dtype=dtype,\n        num_attention_heads=4)\n    x = torch.randn(2, seq_len, embed_dim, device=device, dtype=dtype)\n    y_full, _ = model(x)\n    y_partial, _ = model(x[:, :seq_len // 2])\n    assert torch.allclose(y_full[:, :seq_len // 2], y_partial, atol=1e-05\n        ), 'Causality test failed'\n",
                    "test_fasttttlinear_memory_efficiency": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_memory_efficiency(device=None,\n    dtype=None):\n    device = device or torch.device('cuda' if torch.cuda.is_available() else\n        'cpu')\n    dtype = dtype or torch.float32\n    embed_dim = 32\n    seq_len = 512\n    model = FastTTTLinear(embed_dim, (0, 0), {}, device=device, dtype=dtype,\n        num_attention_heads=4)\n    x = torch.randn(1, seq_len, embed_dim, device=device, dtype=dtype)\n    model.memory_profiling = True\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n    y, _ = model(x)\n    if torch.cuda.is_available():\n        mem_used = torch.cuda.max_memory_allocated()\n        max_mem = 512 * 1024 * 1024\n        assert mem_used < max_mem, f'Memory usage too high: {mem_used} bytes'\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\nfrom typing import Optional\nimport logging\nlogger = logging.getLogger(__name__)\ntry:\n    from flash_attn import flash_attention_impl\n    HAS_FLASH_ATTENTION = True\nexcept ImportError:\n    HAS_FLASH_ATTENTION = False\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\n    \n    Key Features:\n    - Causal attention with efficient chunked computation\n    - Memory-efficient implementation with gradient checkpointing\n    - Optional Flash Attention support for faster computation\n    - Adaptive chunk sizing based on sequence length and available memory\n    - Enhanced numerical stability through proper scaling and normalization\n    - Memory profiling and performance monitoring capabilities\n    - Fused gate and projection operations for efficiency\n\n    Hardware Requirements:\n    - Minimum GPU memory: 8GB\n    - Recommended GPU memory: 16GB\n    - Optional: Flash Attention support\n\n    Performance Characteristics:\n    - Time complexity: O(N) where N is sequence length\n    - Memory complexity: O(N) with constant factor optimization\n    - Optimal batch size: Depends on GPU memory and sequence length\n    - Recommended maximum sequence length: 32K\n    - Optimal chunk size: 1024 for 16GB GPU\n    \n    Args:\n        embed_dim (int): Embedding dimension\n        block_loc (tuple): Block location within the network\n        kwarg_all (dict): Additional arguments\n        device (optional): Computation device\n        dtype (optional): Data type\n        num_attention_heads (int, optional): Number of attention heads. Default is 4.\n        dropout (float, optional): Dropout rate. Default is 0.0.\n        attention_dropout (float, optional): Attention dropout rate. Default is 0.0.\n        chunk_size (int, optional): Base chunk size for processing. Default is 1024.\n        max_position_embeddings (int, optional): Maximum sequence length. Default is 32768.\n        layer_norm_eps (float, optional): Layer norm epsilon. Default is 1e-5.\n        use_flash_attention (bool, optional): Whether to use Flash Attention. Default is True.\n        **kwargs: Additional keyword arguments.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, dropout=0.0,\n        attention_dropout=0.0, chunk_size=1024, max_position_embeddings=\n        32768, layer_norm_eps=1e-05, use_flash_attention=True, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        assert embed_dim % num_attention_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.embed_dim = embed_dim\n        self.base_chunk_size = chunk_size\n        self.chunk_size = chunk_size\n        self.max_position_embeddings = max_position_embeddings\n        self.use_flash_attention = use_flash_attention and HAS_FLASH_ATTENTION\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dropout = nn.Dropout(p=dropout)\n        self.attention_dropout = nn.Dropout(p=attention_dropout)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=2, groups=embed_dim, bias=True, **self.factory_kwargs)\n        self._init_weights()\n        self.gradient_checkpointing = False\n        self.memory_profiling = False\n        self.peak_memory = 0\n        self.perf_stats = {'forward_time': [], 'attention_time': []}\n\n    def _init_weights(self):\n        \"\"\"Initialize weights with proper scaling for stability.\"\"\"\n        gain = 1.0 / math.sqrt(2.0)\n        nn.init.xavier_uniform_(self.W_Q.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_K.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_V.weight, gain=gain)\n        nn.init.xavier_uniform_(self.gate_Q.weight, gain=gain)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight, gain=gain)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _track_memory(self):\n        if self.memory_profiling and torch.cuda.is_available():\n            current = torch.cuda.memory_allocated()\n            self.peak_memory = max(self.peak_memory, current)\n\n    def _log_performance(self, operation: str, time_taken: float):\n        if hasattr(self, 'perf_stats'):\n            self.perf_stats[f'{operation}_time'].append(time_taken)\n\n    def _optimize_chunk_size(self, seq_len: int) ->int:\n        \"\"\"Dynamic chunk size optimization based on available GPU memory.\"\"\"\n        if torch.cuda.is_available():\n            total_memory = torch.cuda.get_device_properties(torch.cuda.\n                current_device()).total_memory\n            reserved_memory = torch.cuda.memory_reserved()\n            available_memory = total_memory - reserved_memory\n            estimated_memory_per_token = self.embed_dim * 4\n            max_chunk_size = available_memory // estimated_memory_per_token\n            optimal_size = min(self.base_chunk_size, max(128, min(seq_len,\n                max_chunk_size)))\n            return int(optimal_size) & -8\n        return self.base_chunk_size\n\n    def _fused_gate_projection(self, X):\n        \"\"\"Fuse gate and projection operations for Q and K.\"\"\"\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        return Q, K\n\n    def _efficient_attention(self, Q, K, V, mask):\n        \"\"\"Efficient attention computation.\"\"\"\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask, float('-inf'))\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        return torch.matmul(attn_weights, V)\n\n    def _causal_attention(self, Q, K, V, chunk_size):\n        \"\"\"Compute chunked causal attention with optional Flash Attention.\"\"\"\n        B, H, L, D = Q.shape\n        outputs = []\n        if (self.use_flash_attention and not self.training and\n            HAS_FLASH_ATTENTION):\n            return flash_attention_impl(Q, K, V, causal=True)\n        for chunk_start in range(0, L, chunk_size):\n            chunk_end = min(chunk_start + chunk_size, L)\n            Q_chunk = Q[:, :, chunk_start:chunk_end]\n            K_chunk = K[:, :, :chunk_end]\n            V_chunk = V[:, :, :chunk_end]\n            causal_mask = torch.triu(torch.ones((chunk_end - chunk_start,\n                chunk_end), device=Q.device, dtype=torch.bool), diagonal=1)\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            chunk_output = self._efficient_attention(Q_chunk * self.scale,\n                K_chunk, V_chunk, causal_mask)\n            outputs.append(chunk_output)\n        return torch.cat(outputs, dim=2)\n\n    def _forward_impl(self, X, **Z):\n        \"\"\"Main implementation of forward pass with all optimizations.\"\"\"\n        import time\n        start_time = time.time()\n        self._track_memory()\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        self.chunk_size = self._optimize_chunk_size(L)\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='constant', value=0)\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)[:, :L]\n        X = X + self.dropout(X_conv)\n        X_norm, Z = self.norm(X, **Z)\n        Q, K = self._fused_gate_projection(X_norm)\n        V = self.W_V(X_norm)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        attn_start_time = time.time()\n        attn_output = self._causal_attention(Q, K, V, self.chunk_size)\n        attn_time = time.time() - attn_start_time\n        self._log_performance('attention', attn_time)\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + self.dropout(output)\n        total_time = time.time() - start_time\n        self._log_performance('forward', total_time)\n        self._track_memory()\n        return output, Z\n\n    def _forward(self, X, **Z):\n        \"\"\"Forward pass with optional gradient checkpointing.\"\"\"\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n\n    @staticmethod\n    def _setup_kv_cache():\n        \"\"\"Setup KV cache for inference.\"\"\"\n        return None\n",
                "rating": 4.8,
                "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\\n\\nKey Features:\\n- Causal attention with efficient chunked computation\\n- Memory-efficient implementation with gradient checkpointing\\n- Optional Flash Attention support for faster computation\\n- Adaptive chunk sizing based on sequence length and available memory\\n- Enhanced numerical stability through proper scaling and normalization\\n- Memory profiling and performance monitoring capabilities\\n- Fused gate and projection operations for efficiency\\n\\nHardware Requirements:\\n- Minimum GPU memory: 8GB\\n- Recommended GPU memory: 16GB\\n- Optional: Flash Attention support\\n\\nPerformance Characteristics:\\n- Time complexity: O(N) where N is sequence length\\n- Memory complexity: O(N) with constant factor optimization\\n- Optimal batch size: Depends on GPU memory and sequence length\\n- Recommended maximum sequence length: 32K\\n- Optimal chunk size: 1024 for 16GB GPU\\n\\nArgs:\\n    embed_dim (int): Embedding dimension\\n    block_loc (tuple): Block location within the network\\n    kwarg_all (dict): Additional arguments\\n    device (optional): Computation device\\n    dtype (optional): Data type\\n    num_attention_heads (int, optional): Number of attention heads. Default is 4.\\n    dropout (float, optional): Dropout rate. Default is 0.0.\\n    attention_dropout (float, optional): Attention dropout rate. Default is 0.0.\\n    chunk_size (int, optional): Base chunk size for processing. Default is 1024.\\n    max_position_embeddings (int, optional): Maximum sequence length. Default is 32768.\\n    layer_norm_eps (float, optional): Layer norm epsilon. Default is 1e-5.\\n    use_flash_attention (bool, optional): Whether to use Flash Attention. Default is True.\\n    **kwargs: Additional keyword arguments.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "attention_dropout": 0.0,
                    "num_attention_heads": 4,
                    "dropout": 0.0,
                    "layer_norm_eps": 1e-05,
                    "use_flash_attention": true,
                    "max_position_embeddings": 32768,
                    "chunk_size": 1024
                },
                "design_traces": null
            },
            "Conv": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RMSNorm"
                ],
                "suggestions": null,
                "args": {
                    "conv_kernel": 4,
                    "rms_norm_eps": 1e-06
                },
                "design_traces": null
            },
            "SwiGluMLP": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "intermediate_size": null
                },
                "design_traces": null
            }
        },
        "rating": null,
        "declares": {
            "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
            "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
            "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
        },
        "proposal_traces": [],
        "suggestions": null,
        "name": "ssmftt"
    },
    "status": "implemented",
    "history": [
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "```rating 4.8\n```\n\n## Comprehensive Feedback Report for FastTTTLinear Implementation\n\n### 1. Overall Assessment\n\nThe **FastTTTLinear** implementation demonstrates a high level of sophistication, adhering closely to the proposed design while incorporating thoughtful optimizations and enhancements. The module successfully integrates advanced features such as adaptive chunk sizing, fused gate and projection operations, and optional Flash Attention support, all of which contribute to its performance and scalability. The recent updates address previous concerns, ensuring robust functionality and seamless integration within the larger language model (LM) framework. Given the passed functionality checks and the meticulous attention to performance and memory optimizations, the implementation deserves a strong rating.\n\n### 2. Strengths of the Implementation\n\n**A. Architectural Excellence**\n- **Adaptive Chunk Sizing**: The dynamic optimization of chunk size based on sequence length and available GPU memory ensures efficient resource utilization, enhancing scalability and flexibility across different hardware configurations.\n- **Fused Gate and Projection Operations**: Combining gate and projection operations for Q and K projections reduces computational overhead, streamlining the attention mechanism and improving processing speed.\n- **Optional Flash Attention Support**: Integration with Flash Attention when available leverages cutting-edge techniques for accelerated attention computation, further boosting performance.\n\n**B. Memory and Performance Optimizations**\n- **Gradient Checkpointing**: The incorporation of gradient checkpointing aids in memory-efficient training, allowing for the handling of longer sequences without exceeding memory limitations.\n- **Local Convolution Integration**: Utilizing depthwise convolutions (`nn.Conv1d` with `groups=embed_dim`) for local context processing enhances the model's ability to capture local dependencies efficiently.\n- **Performance Monitoring Capabilities**: The implementation includes tracking performance metrics (`perf_stats`) for both forward and attention operations, facilitating ongoing optimization and debugging.\n\n**C. Robust Code Quality and Documentation**\n- **Comprehensive Documentation**: Detailed docstrings provide clear explanations of the module\u2019s functionality, key features, hardware requirements, and performance characteristics, aiding both users and future developers in understanding and utilizing the component effectively.\n- **Initialization Stability**: Proper weight initialization methods (`xavier_uniform_`) ensure stable training dynamics, preventing issues such as vanishing or exploding gradients.\n- **Error Handling and Assertions**: Assertions, such as checking divisibility of `embed_dim` by `num_attention_heads`, enforce architectural constraints and prevent common configuration errors.\n\n### 3. Areas for Improvement and Specific Suggestions\n\n**A. Enhanced Memory Profiling**\n- **Memory Usage Reporting**: While memory tracking is implemented, providing more granular memory usage reports (e.g., per operation) could offer deeper insights into potential memory bottlenecks.\n  \n  **Suggestion**:\n  ```python\n  def _track_memory(self):\n      if self.memory_profiling and torch.cuda.is_available():\n          current = torch.cuda.memory_allocated()\n          reserved = torch.cuda.memory_reserved()\n          self.peak_memory = max(self.peak_memory, current)\n          logger.info(f\"Memory Allocated: {current}, Memory Reserved: {reserved}, Peak Memory: {self.peak_memory}\")\n  ```\n\n**B. Further Performance Enhancements**\n- **Batch Processing Optimizations**: Exploring parallel processing techniques or further optimizing tensor operations could yield additional speed improvements, especially for large batch sizes.\n  \n  **Suggestion**:\n  ```python\n  def _parallel_process_chunks(self, Q, K, V, mask):\n      \"\"\"Parallelize chunk processing using multi-threading or CUDA streams.\"\"\"\n      # Placeholder for parallel processing implementation\n      pass\n  ```\n\n**C. Comprehensive Unit Testing**\n- **Extended Test Coverage**: While functionality checks have passed, expanding unit tests to cover edge cases, varying sequence lengths, and different hardware configurations can ensure robustness across diverse scenarios.\n  \n  **Suggestion**:\n  ```python\n  @gau_test\n  def test_fasttttlinear_varying_seq_lengths():\n      model = FastTTTLinear(embed_dim=512, block_loc=(0,0), kwarg_all={})\n      for seq_len in [128, 1024, 8192, 32768]:\n          x = torch.randn(2, seq_len, 512)\n          y, _ = model(x)\n          assert y.shape == x.shape, f\"Output shape {y.shape} does not match input shape {x.shape}\"\n  ```\n\n**D. Code Refactoring for Modularity**\n- **Modular Attention Mechanism**: Separating different components of the attention mechanism into distinct methods or classes can enhance code readability and maintainability.\n  \n  **Suggestion**:\n  ```python\n  def _prepare_attention(self, X_norm):\n      Q, K = self._fused_gate_projection(X_norm)\n      V = self.W_V(X_norm)\n      Q = Q.view(B, L, H, D_H).transpose(1, 2)\n      K = K.view(B, L, H, D_H).transpose(1, 2)\n      V = V.view(B, L, H, D_H).transpose(1, 2)\n      return Q, K, V\n  ```\n\n### 4. Comments on Innovation and Potential Impact\n\n**Innovative Aspects**\n- **Fused Gate and Projection Operations**: This approach minimizes redundant computations within the attention mechanism, representing a novel optimization that can be pivotal for performance gains.\n- **Adaptive Chunk Sizing with Memory Awareness**: Tailoring chunk sizes based on real-time memory availability showcases a sophisticated method to balance computational load and memory usage dynamically.\n\n**Potential Impact**\n- **Scalability**: The ability to handle very long sequences efficiently positions **FastTTTLinear** as a formidable component for next-generation language models that require extensive context handling.\n- **Versatility Across Hardware**: By accommodating various GPU memory capacities and optionally leveraging Flash Attention, the module is well-suited for deployment in diverse computational environments.\n- **Performance Leadership**: These combined innovations could set new benchmarks in attention mechanism efficiency, influencing future research and development in language model architectures.\n\n**Concerns About Integration and Scalability**\n- **Compatibility with Other Modules**: Ensuring seamless interoperability with other GAUs and model components is crucial. Misalignments in tensor dimensions or operation sequences could introduce integration challenges.\n- **Resource Limitations**: While adaptive chunk sizing mitigates memory issues, extremely large embeddings or vocabularies might still pose challenges. Continuous monitoring and iterative optimization will be necessary as model scales increase.\n\n### 5. Recommendations for the Coder\n\nTo further refine and enhance the **FastTTTLinear** implementation, consider the following actions:\n\n1. **Implement Detailed Memory Profiling**:\n   - Enhance the `_track_memory` method to provide comprehensive memory usage reports during different phases of the forward pass.\n   - Utilize logging to capture and store memory metrics for post-run analysis.\n\n2. **Expand Unit Tests**:\n   - Develop a broader suite of unit tests that encompass various sequence lengths, batch sizes, and edge cases.\n   - Incorporate tests that specifically evaluate the behavior when Flash Attention is enabled versus disabled.\n\n3. **Optimize Parallel Processing**:\n   - Investigate opportunities for parallelizing chunk processing, potentially leveraging multi-threading or CUDA streams to expedite computations.\n   - Evaluate the benefits of such optimizations through empirical testing.\n\n4. **Refactor for Enhanced Modularity**:\n   - Reorganize the attention mechanism into discrete, reusable components or helper functions. This will improve code readability and facilitate easier maintenance and future enhancements.\n\n5. **Integrate Comprehensive Logging**:\n   - Utilize the logging framework to record key events, such as the start and end of attention computation, memory allocations, and performance metrics.\n   - Ensure that logs are informative yet concise to aid in debugging without overwhelming storage resources.\n\n6. **Conduct Empirical Evaluations**:\n   - Benchmark the module against existing attention mechanisms to quantify performance gains and identify potential bottlenecks.\n   - Perform ablation studies to understand the impact of each optimization, such as fused gate projections and adaptive chunk sizing.\n\n7. **Ensure Robust Error Handling**:\n   - Implement safeguards against potential errors arising from unexpected input shapes or invalid configurations.\n   - Provide meaningful error messages that can guide troubleshooting efforts.\n\n8. **Maintain Documentation Excellence**:\n   - Keep the module's documentation up-to-date with any new features or changes.\n   - Ensure that examples and usage guidelines are clear and cover a range of scenarios.\n\n9. **Collaborate with Peers for Code Reviews**:\n   - Engage in peer reviews to gain diverse perspectives on the implementation.\n   - Utilize feedback from colleagues to uncover hidden issues and explore alternative optimization strategies.\n\n10. **Plan for Future Extensions**:\n    - Design the module with extensibility in mind, allowing for the incorporation of additional features or adaptations to different model architectures in the future.\n\n### Final Thoughts\n\nThe **FastTTTLinear** implementation stands as a robust and highly optimized component within the GAU framework, embodying advanced techniques to enhance both performance and scalability. By addressing the highlighted areas for improvement and leveraging its inherent strengths, the module is well-positioned to make significant contributions to the next generation of language models. Continued diligence in testing, optimization, and documentation will ensure that **FastTTTLinear** remains a high-performing and reliable element within the broader LM architecture.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_fasttttlinear_causality": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_causality(device=None, dtype=None):\n    device = device or torch.device('cpu')\n    dtype = dtype or torch.float32\n    embed_dim = 32\n    seq_len = 64\n    model = FastTTTLinear(embed_dim, (0, 0), {}, device=device, dtype=dtype,\n        num_attention_heads=4)\n    x = torch.randn(2, seq_len, embed_dim, device=device, dtype=dtype)\n    y_full, _ = model(x)\n    y_partial, _ = model(x[:, :seq_len // 2])\n    assert torch.allclose(y_full[:, :seq_len // 2], y_partial, atol=1e-05\n        ), 'Causality test failed'\n",
                            "test_fasttttlinear_memory_efficiency": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_memory_efficiency(device=None,\n    dtype=None):\n    device = device or torch.device('cuda' if torch.cuda.is_available() else\n        'cpu')\n    dtype = dtype or torch.float32\n    embed_dim = 32\n    seq_len = 512\n    model = FastTTTLinear(embed_dim, (0, 0), {}, device=device, dtype=dtype,\n        num_attention_heads=4)\n    x = torch.randn(1, seq_len, embed_dim, device=device, dtype=dtype)\n    model.memory_profiling = True\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n    y, _ = model(x)\n    if torch.cuda.is_available():\n        mem_used = torch.cuda.max_memory_allocated()\n        max_mem = 512 * 1024 * 1024\n        assert mem_used < max_mem, f'Memory usage too high: {mem_used} bytes'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\nfrom typing import Optional\nimport logging\nlogger = logging.getLogger(__name__)\ntry:\n    from flash_attn import flash_attention_impl\n    HAS_FLASH_ATTENTION = True\nexcept ImportError:\n    HAS_FLASH_ATTENTION = False\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\n    \n    Key Features:\n    - Causal attention with efficient chunked computation\n    - Memory-efficient implementation with gradient checkpointing\n    - Optional Flash Attention support for faster computation\n    - Adaptive chunk sizing based on sequence length and available memory\n    - Enhanced numerical stability through proper scaling and normalization\n    - Memory profiling and performance monitoring capabilities\n    - Fused gate and projection operations for efficiency\n\n    Hardware Requirements:\n    - Minimum GPU memory: 8GB\n    - Recommended GPU memory: 16GB\n    - Optional: Flash Attention support\n\n    Performance Characteristics:\n    - Time complexity: O(N) where N is sequence length\n    - Memory complexity: O(N) with constant factor optimization\n    - Optimal batch size: Depends on GPU memory and sequence length\n    - Recommended maximum sequence length: 32K\n    - Optimal chunk size: 1024 for 16GB GPU\n    \n    Args:\n        embed_dim (int): Embedding dimension\n        block_loc (tuple): Block location within the network\n        kwarg_all (dict): Additional arguments\n        device (optional): Computation device\n        dtype (optional): Data type\n        num_attention_heads (int, optional): Number of attention heads. Default is 4.\n        dropout (float, optional): Dropout rate. Default is 0.0.\n        attention_dropout (float, optional): Attention dropout rate. Default is 0.0.\n        chunk_size (int, optional): Base chunk size for processing. Default is 1024.\n        max_position_embeddings (int, optional): Maximum sequence length. Default is 32768.\n        layer_norm_eps (float, optional): Layer norm epsilon. Default is 1e-5.\n        use_flash_attention (bool, optional): Whether to use Flash Attention. Default is True.\n        **kwargs: Additional keyword arguments.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, dropout=0.0,\n        attention_dropout=0.0, chunk_size=1024, max_position_embeddings=\n        32768, layer_norm_eps=1e-05, use_flash_attention=True, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        assert embed_dim % num_attention_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.embed_dim = embed_dim\n        self.base_chunk_size = chunk_size\n        self.chunk_size = chunk_size\n        self.max_position_embeddings = max_position_embeddings\n        self.use_flash_attention = use_flash_attention and HAS_FLASH_ATTENTION\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dropout = nn.Dropout(p=dropout)\n        self.attention_dropout = nn.Dropout(p=attention_dropout)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=2, groups=embed_dim, bias=True, **self.factory_kwargs)\n        self._init_weights()\n        self.gradient_checkpointing = False\n        self.memory_profiling = False\n        self.peak_memory = 0\n        self.perf_stats = {'forward_time': [], 'attention_time': []}\n\n    def _init_weights(self):\n        \"\"\"Initialize weights with proper scaling for stability.\"\"\"\n        gain = 1.0 / math.sqrt(2.0)\n        nn.init.xavier_uniform_(self.W_Q.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_K.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_V.weight, gain=gain)\n        nn.init.xavier_uniform_(self.gate_Q.weight, gain=gain)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight, gain=gain)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _track_memory(self):\n        if self.memory_profiling and torch.cuda.is_available():\n            current = torch.cuda.memory_allocated()\n            self.peak_memory = max(self.peak_memory, current)\n\n    def _log_performance(self, operation: str, time_taken: float):\n        if hasattr(self, 'perf_stats'):\n            self.perf_stats[f'{operation}_time'].append(time_taken)\n\n    def _optimize_chunk_size(self, seq_len: int) ->int:\n        \"\"\"Dynamic chunk size optimization based on available GPU memory.\"\"\"\n        if torch.cuda.is_available():\n            total_memory = torch.cuda.get_device_properties(torch.cuda.\n                current_device()).total_memory\n            reserved_memory = torch.cuda.memory_reserved()\n            available_memory = total_memory - reserved_memory\n            estimated_memory_per_token = self.embed_dim * 4\n            max_chunk_size = available_memory // estimated_memory_per_token\n            optimal_size = min(self.base_chunk_size, max(128, min(seq_len,\n                max_chunk_size)))\n            return int(optimal_size) & -8\n        return self.base_chunk_size\n\n    def _fused_gate_projection(self, X):\n        \"\"\"Fuse gate and projection operations for Q and K.\"\"\"\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        return Q, K\n\n    def _efficient_attention(self, Q, K, V, mask):\n        \"\"\"Efficient attention computation.\"\"\"\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask, float('-inf'))\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        return torch.matmul(attn_weights, V)\n\n    def _causal_attention(self, Q, K, V, chunk_size):\n        \"\"\"Compute chunked causal attention with optional Flash Attention.\"\"\"\n        B, H, L, D = Q.shape\n        outputs = []\n        if (self.use_flash_attention and not self.training and\n            HAS_FLASH_ATTENTION):\n            return flash_attention_impl(Q, K, V, causal=True)\n        for chunk_start in range(0, L, chunk_size):\n            chunk_end = min(chunk_start + chunk_size, L)\n            Q_chunk = Q[:, :, chunk_start:chunk_end]\n            K_chunk = K[:, :, :chunk_end]\n            V_chunk = V[:, :, :chunk_end]\n            causal_mask = torch.triu(torch.ones((chunk_end - chunk_start,\n                chunk_end), device=Q.device, dtype=torch.bool), diagonal=1)\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            chunk_output = self._efficient_attention(Q_chunk * self.scale,\n                K_chunk, V_chunk, causal_mask)\n            outputs.append(chunk_output)\n        return torch.cat(outputs, dim=2)\n\n    def _forward_impl(self, X, **Z):\n        \"\"\"Main implementation of forward pass with all optimizations.\"\"\"\n        import time\n        start_time = time.time()\n        self._track_memory()\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        self.chunk_size = self._optimize_chunk_size(L)\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='constant', value=0)\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)[:, :L]\n        X = X + self.dropout(X_conv)\n        X_norm, Z = self.norm(X, **Z)\n        Q, K = self._fused_gate_projection(X_norm)\n        V = self.W_V(X_norm)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        attn_start_time = time.time()\n        attn_output = self._causal_attention(Q, K, V, self.chunk_size)\n        attn_time = time.time() - attn_start_time\n        self._log_performance('attention', attn_time)\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + self.dropout(output)\n        total_time = time.time() - start_time\n        self._log_performance('forward', total_time)\n        self._track_memory()\n        return output, Z\n\n    def _forward(self, X, **Z):\n        \"\"\"Forward pass with optional gradient checkpointing.\"\"\"\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n\n    @staticmethod\n    def _setup_kv_cache():\n        \"\"\"Setup KV cache for inference.\"\"\"\n        return None\n",
                        "rating": 4.8,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\\n\\nKey Features:\\n- Causal attention with efficient chunked computation\\n- Memory-efficient implementation with gradient checkpointing\\n- Optional Flash Attention support for faster computation\\n- Adaptive chunk sizing based on sequence length and available memory\\n- Enhanced numerical stability through proper scaling and normalization\\n- Memory profiling and performance monitoring capabilities\\n- Fused gate and projection operations for efficiency\\n\\nHardware Requirements:\\n- Minimum GPU memory: 8GB\\n- Recommended GPU memory: 16GB\\n- Optional: Flash Attention support\\n\\nPerformance Characteristics:\\n- Time complexity: O(N) where N is sequence length\\n- Memory complexity: O(N) with constant factor optimization\\n- Optimal batch size: Depends on GPU memory and sequence length\\n- Recommended maximum sequence length: 32K\\n- Optimal chunk size: 1024 for 16GB GPU\\n\\nArgs:\\n    embed_dim (int): Embedding dimension\\n    block_loc (tuple): Block location within the network\\n    kwarg_all (dict): Additional arguments\\n    device (optional): Computation device\\n    dtype (optional): Data type\\n    num_attention_heads (int, optional): Number of attention heads. Default is 4.\\n    dropout (float, optional): Dropout rate. Default is 0.0.\\n    attention_dropout (float, optional): Attention dropout rate. Default is 0.0.\\n    chunk_size (int, optional): Base chunk size for processing. Default is 1024.\\n    max_position_embeddings (int, optional): Maximum sequence length. Default is 32768.\\n    layer_norm_eps (float, optional): Layer norm epsilon. Default is 1e-5.\\n    use_flash_attention (bool, optional): Whether to use Flash Attention. Default is True.\\n    **kwargs: Additional keyword arguments.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "attention_dropout": 0.0,
                            "num_attention_heads": 4,
                            "dropout": 0.0,
                            "layer_norm_eps": 1e-05,
                            "use_flash_attention": true,
                            "max_position_embeddings": 32768,
                            "chunk_size": 1024
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "ssmftt"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
                    "IMPLEMENTATION_CODER": "o1_preview",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "o1_mini",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.09825300000000001,
                "IMPLEMENTATION_CODER": 1.93323,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 0.318102,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": null,
                "root": "TTT",
                "proposal": "Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.",
                "units": {
                    "TTT": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_ttt": "@gau_test\ndef test_TTT_test_ttt(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    ttt = TTT(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype,\n        **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = ttt(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nlogger = logging.get_logger(__name__)\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = FastTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='TTTLinear', requirements='',\n    inputs=['X'], outputs=['Y']), UnitDecl(unitname='SwiGluMLP',\n    requirements='', inputs=['X'], outputs=['Y']), UnitDecl(unitname=\n    'RMSNorm', requirements='', inputs=['X'], outputs=['Y']), UnitDecl(\n    unitname='Conv', requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"TTT\",\"document\":\"\\nProblem Statement\\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\\n\\nMain Claims\\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nMethodology\\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \\\"training\\\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\\n\\nKey Results\\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "FastTTTLinear",
                            "SwiGluMLP",
                            "RMSNorm",
                            "Conv"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RMSNorm": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rmsnorm": "@gau_test\ndef test_RMSNorm_test_rmsnorm(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rmsnorm = RMSNorm(embed_dim, block_loc, kwarg_all, device=device, dtype\n        =dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rmsnorm(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RMSNorm\",\"document\":\"\\n    Root Mean Square Layer Normalization (RMSNorm).\\n\\n    This layer applies a variant of layer normalization that uses only the root mean square\\n    statistics, without centering. It's computationally more efficient than standard\\n    layer normalization and has been shown to be effective in various NLP tasks.\\n\\n    Args:\\n        embed_dim (int): The size of the input feature dimension.\\n        block_loc (tuple): The location of this block in the model architecture.\\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\\n        device (torch.device, optional): The device on which to allocate the module's parameters.\\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\\n        eps (float, optional): A small constant added to the denominator for numerical stability.\\n            Default: 1e-5.\\n\\n    Attributes:\\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\\n        variance_epsilon (float): The epsilon value used in the normalization formula.\\n\\n    Shape:\\n        - Input: (*, embed_dim)\\n        - Output: (*, embed_dim) (same shape as input)\\n\\n    Examples:\\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\\n        >>> x = torch.randn(1, 100, 128)\\n        >>> output = rmsnorm(x)\\n        >>> print(output.shape)\\n        torch.Size([1, 100, 128])\\n\\n    References:\\n        - Paper: \\\"Root Mean Square Layer Normalization\\\" by Biao Zhang and Rico Sennrich\\n          https://arxiv.org/abs/1910.07467\\n    \",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "FastTTTLinear": {
                        "review": "```rating 4.8\n```\n\n## Comprehensive Feedback Report for FastTTTLinear Implementation\n\n### 1. Overall Assessment\n\nThe **FastTTTLinear** implementation demonstrates a high level of sophistication, adhering closely to the proposed design while incorporating thoughtful optimizations and enhancements. The module successfully integrates advanced features such as adaptive chunk sizing, fused gate and projection operations, and optional Flash Attention support, all of which contribute to its performance and scalability. The recent updates address previous concerns, ensuring robust functionality and seamless integration within the larger language model (LM) framework. Given the passed functionality checks and the meticulous attention to performance and memory optimizations, the implementation deserves a strong rating.\n\n### 2. Strengths of the Implementation\n\n**A. Architectural Excellence**\n- **Adaptive Chunk Sizing**: The dynamic optimization of chunk size based on sequence length and available GPU memory ensures efficient resource utilization, enhancing scalability and flexibility across different hardware configurations.\n- **Fused Gate and Projection Operations**: Combining gate and projection operations for Q and K projections reduces computational overhead, streamlining the attention mechanism and improving processing speed.\n- **Optional Flash Attention Support**: Integration with Flash Attention when available leverages cutting-edge techniques for accelerated attention computation, further boosting performance.\n\n**B. Memory and Performance Optimizations**\n- **Gradient Checkpointing**: The incorporation of gradient checkpointing aids in memory-efficient training, allowing for the handling of longer sequences without exceeding memory limitations.\n- **Local Convolution Integration**: Utilizing depthwise convolutions (`nn.Conv1d` with `groups=embed_dim`) for local context processing enhances the model's ability to capture local dependencies efficiently.\n- **Performance Monitoring Capabilities**: The implementation includes tracking performance metrics (`perf_stats`) for both forward and attention operations, facilitating ongoing optimization and debugging.\n\n**C. Robust Code Quality and Documentation**\n- **Comprehensive Documentation**: Detailed docstrings provide clear explanations of the module\u2019s functionality, key features, hardware requirements, and performance characteristics, aiding both users and future developers in understanding and utilizing the component effectively.\n- **Initialization Stability**: Proper weight initialization methods (`xavier_uniform_`) ensure stable training dynamics, preventing issues such as vanishing or exploding gradients.\n- **Error Handling and Assertions**: Assertions, such as checking divisibility of `embed_dim` by `num_attention_heads`, enforce architectural constraints and prevent common configuration errors.\n\n### 3. Areas for Improvement and Specific Suggestions\n\n**A. Enhanced Memory Profiling**\n- **Memory Usage Reporting**: While memory tracking is implemented, providing more granular memory usage reports (e.g., per operation) could offer deeper insights into potential memory bottlenecks.\n  \n  **Suggestion**:\n  ```python\n  def _track_memory(self):\n      if self.memory_profiling and torch.cuda.is_available():\n          current = torch.cuda.memory_allocated()\n          reserved = torch.cuda.memory_reserved()\n          self.peak_memory = max(self.peak_memory, current)\n          logger.info(f\"Memory Allocated: {current}, Memory Reserved: {reserved}, Peak Memory: {self.peak_memory}\")\n  ```\n\n**B. Further Performance Enhancements**\n- **Batch Processing Optimizations**: Exploring parallel processing techniques or further optimizing tensor operations could yield additional speed improvements, especially for large batch sizes.\n  \n  **Suggestion**:\n  ```python\n  def _parallel_process_chunks(self, Q, K, V, mask):\n      \"\"\"Parallelize chunk processing using multi-threading or CUDA streams.\"\"\"\n      # Placeholder for parallel processing implementation\n      pass\n  ```\n\n**C. Comprehensive Unit Testing**\n- **Extended Test Coverage**: While functionality checks have passed, expanding unit tests to cover edge cases, varying sequence lengths, and different hardware configurations can ensure robustness across diverse scenarios.\n  \n  **Suggestion**:\n  ```python\n  @gau_test\n  def test_fasttttlinear_varying_seq_lengths():\n      model = FastTTTLinear(embed_dim=512, block_loc=(0,0), kwarg_all={})\n      for seq_len in [128, 1024, 8192, 32768]:\n          x = torch.randn(2, seq_len, 512)\n          y, _ = model(x)\n          assert y.shape == x.shape, f\"Output shape {y.shape} does not match input shape {x.shape}\"\n  ```\n\n**D. Code Refactoring for Modularity**\n- **Modular Attention Mechanism**: Separating different components of the attention mechanism into distinct methods or classes can enhance code readability and maintainability.\n  \n  **Suggestion**:\n  ```python\n  def _prepare_attention(self, X_norm):\n      Q, K = self._fused_gate_projection(X_norm)\n      V = self.W_V(X_norm)\n      Q = Q.view(B, L, H, D_H).transpose(1, 2)\n      K = K.view(B, L, H, D_H).transpose(1, 2)\n      V = V.view(B, L, H, D_H).transpose(1, 2)\n      return Q, K, V\n  ```\n\n### 4. Comments on Innovation and Potential Impact\n\n**Innovative Aspects**\n- **Fused Gate and Projection Operations**: This approach minimizes redundant computations within the attention mechanism, representing a novel optimization that can be pivotal for performance gains.\n- **Adaptive Chunk Sizing with Memory Awareness**: Tailoring chunk sizes based on real-time memory availability showcases a sophisticated method to balance computational load and memory usage dynamically.\n\n**Potential Impact**\n- **Scalability**: The ability to handle very long sequences efficiently positions **FastTTTLinear** as a formidable component for next-generation language models that require extensive context handling.\n- **Versatility Across Hardware**: By accommodating various GPU memory capacities and optionally leveraging Flash Attention, the module is well-suited for deployment in diverse computational environments.\n- **Performance Leadership**: These combined innovations could set new benchmarks in attention mechanism efficiency, influencing future research and development in language model architectures.\n\n**Concerns About Integration and Scalability**\n- **Compatibility with Other Modules**: Ensuring seamless interoperability with other GAUs and model components is crucial. Misalignments in tensor dimensions or operation sequences could introduce integration challenges.\n- **Resource Limitations**: While adaptive chunk sizing mitigates memory issues, extremely large embeddings or vocabularies might still pose challenges. Continuous monitoring and iterative optimization will be necessary as model scales increase.\n\n### 5. Recommendations for the Coder\n\nTo further refine and enhance the **FastTTTLinear** implementation, consider the following actions:\n\n1. **Implement Detailed Memory Profiling**:\n   - Enhance the `_track_memory` method to provide comprehensive memory usage reports during different phases of the forward pass.\n   - Utilize logging to capture and store memory metrics for post-run analysis.\n\n2. **Expand Unit Tests**:\n   - Develop a broader suite of unit tests that encompass various sequence lengths, batch sizes, and edge cases.\n   - Incorporate tests that specifically evaluate the behavior when Flash Attention is enabled versus disabled.\n\n3. **Optimize Parallel Processing**:\n   - Investigate opportunities for parallelizing chunk processing, potentially leveraging multi-threading or CUDA streams to expedite computations.\n   - Evaluate the benefits of such optimizations through empirical testing.\n\n4. **Refactor for Enhanced Modularity**:\n   - Reorganize the attention mechanism into discrete, reusable components or helper functions. This will improve code readability and facilitate easier maintenance and future enhancements.\n\n5. **Integrate Comprehensive Logging**:\n   - Utilize the logging framework to record key events, such as the start and end of attention computation, memory allocations, and performance metrics.\n   - Ensure that logs are informative yet concise to aid in debugging without overwhelming storage resources.\n\n6. **Conduct Empirical Evaluations**:\n   - Benchmark the module against existing attention mechanisms to quantify performance gains and identify potential bottlenecks.\n   - Perform ablation studies to understand the impact of each optimization, such as fused gate projections and adaptive chunk sizing.\n\n7. **Ensure Robust Error Handling**:\n   - Implement safeguards against potential errors arising from unexpected input shapes or invalid configurations.\n   - Provide meaningful error messages that can guide troubleshooting efforts.\n\n8. **Maintain Documentation Excellence**:\n   - Keep the module's documentation up-to-date with any new features or changes.\n   - Ensure that examples and usage guidelines are clear and cover a range of scenarios.\n\n9. **Collaborate with Peers for Code Reviews**:\n   - Engage in peer reviews to gain diverse perspectives on the implementation.\n   - Utilize feedback from colleagues to uncover hidden issues and explore alternative optimization strategies.\n\n10. **Plan for Future Extensions**:\n    - Design the module with extensibility in mind, allowing for the incorporation of additional features or adaptations to different model architectures in the future.\n\n### Final Thoughts\n\nThe **FastTTTLinear** implementation stands as a robust and highly optimized component within the GAU framework, embodying advanced techniques to enhance both performance and scalability. By addressing the highlighted areas for improvement and leveraging its inherent strengths, the module is well-positioned to make significant contributions to the next generation of language models. Continued diligence in testing, optimization, and documentation will ensure that **FastTTTLinear** remains a high-performing and reliable element within the broader LM architecture.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_fasttttlinear_causality": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_causality(device=None, dtype=None):\n    device = device or torch.device('cpu')\n    dtype = dtype or torch.float32\n    embed_dim = 32\n    seq_len = 64\n    model = FastTTTLinear(embed_dim, (0, 0), {}, device=device, dtype=dtype,\n        num_attention_heads=4)\n    x = torch.randn(2, seq_len, embed_dim, device=device, dtype=dtype)\n    y_full, _ = model(x)\n    y_partial, _ = model(x[:, :seq_len // 2])\n    assert torch.allclose(y_full[:, :seq_len // 2], y_partial, atol=1e-05\n        ), 'Causality test failed'\n",
                            "test_fasttttlinear_memory_efficiency": "@gau_test\ndef test_FastTTTLinear_test_fasttttlinear_memory_efficiency(device=None,\n    dtype=None):\n    device = device or torch.device('cuda' if torch.cuda.is_available() else\n        'cpu')\n    dtype = dtype or torch.float32\n    embed_dim = 32\n    seq_len = 512\n    model = FastTTTLinear(embed_dim, (0, 0), {}, device=device, dtype=dtype,\n        num_attention_heads=4)\n    x = torch.randn(1, seq_len, embed_dim, device=device, dtype=dtype)\n    model.memory_profiling = True\n    if torch.cuda.is_available():\n        torch.cuda.reset_peak_memory_stats()\n    y, _ = model(x)\n    if torch.cuda.is_available():\n        mem_used = torch.cuda.max_memory_allocated()\n        max_mem = 512 * 1024 * 1024\n        assert mem_used < max_mem, f'Memory usage too high: {mem_used} bytes'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\nfrom typing import Optional\nimport logging\nlogger = logging.getLogger(__name__)\ntry:\n    from flash_attn import flash_attention_impl\n    HAS_FLASH_ATTENTION = True\nexcept ImportError:\n    HAS_FLASH_ATTENTION = False\n\n\nclass FastTTTLinear(GAUBase):\n    \"\"\"\n    FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\n    \n    Key Features:\n    - Causal attention with efficient chunked computation\n    - Memory-efficient implementation with gradient checkpointing\n    - Optional Flash Attention support for faster computation\n    - Adaptive chunk sizing based on sequence length and available memory\n    - Enhanced numerical stability through proper scaling and normalization\n    - Memory profiling and performance monitoring capabilities\n    - Fused gate and projection operations for efficiency\n\n    Hardware Requirements:\n    - Minimum GPU memory: 8GB\n    - Recommended GPU memory: 16GB\n    - Optional: Flash Attention support\n\n    Performance Characteristics:\n    - Time complexity: O(N) where N is sequence length\n    - Memory complexity: O(N) with constant factor optimization\n    - Optimal batch size: Depends on GPU memory and sequence length\n    - Recommended maximum sequence length: 32K\n    - Optimal chunk size: 1024 for 16GB GPU\n    \n    Args:\n        embed_dim (int): Embedding dimension\n        block_loc (tuple): Block location within the network\n        kwarg_all (dict): Additional arguments\n        device (optional): Computation device\n        dtype (optional): Data type\n        num_attention_heads (int, optional): Number of attention heads. Default is 4.\n        dropout (float, optional): Dropout rate. Default is 0.0.\n        attention_dropout (float, optional): Attention dropout rate. Default is 0.0.\n        chunk_size (int, optional): Base chunk size for processing. Default is 1024.\n        max_position_embeddings (int, optional): Maximum sequence length. Default is 32768.\n        layer_norm_eps (float, optional): Layer norm epsilon. Default is 1e-5.\n        use_flash_attention (bool, optional): Whether to use Flash Attention. Default is True.\n        **kwargs: Additional keyword arguments.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, dropout=0.0,\n        attention_dropout=0.0, chunk_size=1024, max_position_embeddings=\n        32768, layer_norm_eps=1e-05, use_flash_attention=True, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_attention_heads\n        self.head_dim = embed_dim // num_attention_heads\n        assert embed_dim % num_attention_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.embed_dim = embed_dim\n        self.base_chunk_size = chunk_size\n        self.chunk_size = chunk_size\n        self.max_position_embeddings = max_position_embeddings\n        self.use_flash_attention = use_flash_attention and HAS_FLASH_ATTENTION\n        self.scale = 1.0 / math.sqrt(self.head_dim)\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dropout = nn.Dropout(p=dropout)\n        self.attention_dropout = nn.Dropout(p=attention_dropout)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=2, groups=embed_dim, bias=True, **self.factory_kwargs)\n        self._init_weights()\n        self.gradient_checkpointing = False\n        self.memory_profiling = False\n        self.peak_memory = 0\n        self.perf_stats = {'forward_time': [], 'attention_time': []}\n\n    def _init_weights(self):\n        \"\"\"Initialize weights with proper scaling for stability.\"\"\"\n        gain = 1.0 / math.sqrt(2.0)\n        nn.init.xavier_uniform_(self.W_Q.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_K.weight, gain=gain)\n        nn.init.xavier_uniform_(self.W_V.weight, gain=gain)\n        nn.init.xavier_uniform_(self.gate_Q.weight, gain=gain)\n        nn.init.zeros_(self.gate_Q.bias)\n        nn.init.xavier_uniform_(self.gate_K.weight, gain=gain)\n        nn.init.zeros_(self.gate_K.bias)\n        nn.init.xavier_uniform_(self.output_proj.weight)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _track_memory(self):\n        if self.memory_profiling and torch.cuda.is_available():\n            current = torch.cuda.memory_allocated()\n            self.peak_memory = max(self.peak_memory, current)\n\n    def _log_performance(self, operation: str, time_taken: float):\n        if hasattr(self, 'perf_stats'):\n            self.perf_stats[f'{operation}_time'].append(time_taken)\n\n    def _optimize_chunk_size(self, seq_len: int) ->int:\n        \"\"\"Dynamic chunk size optimization based on available GPU memory.\"\"\"\n        if torch.cuda.is_available():\n            total_memory = torch.cuda.get_device_properties(torch.cuda.\n                current_device()).total_memory\n            reserved_memory = torch.cuda.memory_reserved()\n            available_memory = total_memory - reserved_memory\n            estimated_memory_per_token = self.embed_dim * 4\n            max_chunk_size = available_memory // estimated_memory_per_token\n            optimal_size = min(self.base_chunk_size, max(128, min(seq_len,\n                max_chunk_size)))\n            return int(optimal_size) & -8\n        return self.base_chunk_size\n\n    def _fused_gate_projection(self, X):\n        \"\"\"Fuse gate and projection operations for Q and K.\"\"\"\n        Q = self.W_Q(X)\n        K = self.W_K(X)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = Q * G_Q\n        K = K * G_K\n        return Q, K\n\n    def _efficient_attention(self, Q, K, V, mask):\n        \"\"\"Efficient attention computation.\"\"\"\n        scores = torch.matmul(Q, K.transpose(-2, -1))\n        scores = scores.masked_fill(mask, float('-inf'))\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.attention_dropout(attn_weights)\n        return torch.matmul(attn_weights, V)\n\n    def _causal_attention(self, Q, K, V, chunk_size):\n        \"\"\"Compute chunked causal attention with optional Flash Attention.\"\"\"\n        B, H, L, D = Q.shape\n        outputs = []\n        if (self.use_flash_attention and not self.training and\n            HAS_FLASH_ATTENTION):\n            return flash_attention_impl(Q, K, V, causal=True)\n        for chunk_start in range(0, L, chunk_size):\n            chunk_end = min(chunk_start + chunk_size, L)\n            Q_chunk = Q[:, :, chunk_start:chunk_end]\n            K_chunk = K[:, :, :chunk_end]\n            V_chunk = V[:, :, :chunk_end]\n            causal_mask = torch.triu(torch.ones((chunk_end - chunk_start,\n                chunk_end), device=Q.device, dtype=torch.bool), diagonal=1)\n            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n            chunk_output = self._efficient_attention(Q_chunk * self.scale,\n                K_chunk, V_chunk, causal_mask)\n            outputs.append(chunk_output)\n        return torch.cat(outputs, dim=2)\n\n    def _forward_impl(self, X, **Z):\n        \"\"\"Main implementation of forward pass with all optimizations.\"\"\"\n        import time\n        start_time = time.time()\n        self._track_memory()\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        self.chunk_size = self._optimize_chunk_size(L)\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='constant', value=0)\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)[:, :L]\n        X = X + self.dropout(X_conv)\n        X_norm, Z = self.norm(X, **Z)\n        Q, K = self._fused_gate_projection(X_norm)\n        V = self.W_V(X_norm)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        attn_start_time = time.time()\n        attn_output = self._causal_attention(Q, K, V, self.chunk_size)\n        attn_time = time.time() - attn_start_time\n        self._log_performance('attention', attn_time)\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + self.dropout(output)\n        total_time = time.time() - start_time\n        self._log_performance('forward', total_time)\n        self._track_memory()\n        return output, Z\n\n    def _forward(self, X, **Z):\n        \"\"\"Forward pass with optional gradient checkpointing.\"\"\"\n        if self.gradient_checkpointing and self.training:\n            return torch.utils.checkpoint.checkpoint(self._forward_impl, X, **Z\n                )\n        return self._forward_impl(X, **Z)\n\n    @staticmethod\n    def _setup_kv_cache():\n        \"\"\"Setup KV cache for inference.\"\"\"\n        return None\n",
                        "rating": 4.8,
                        "spec": "{\"unitname\":\"FastTTTLinear\",\"document\":\"FastTTTLinear with enhanced causality, memory efficiency, and performance optimizations.\\n\\nKey Features:\\n- Causal attention with efficient chunked computation\\n- Memory-efficient implementation with gradient checkpointing\\n- Optional Flash Attention support for faster computation\\n- Adaptive chunk sizing based on sequence length and available memory\\n- Enhanced numerical stability through proper scaling and normalization\\n- Memory profiling and performance monitoring capabilities\\n- Fused gate and projection operations for efficiency\\n\\nHardware Requirements:\\n- Minimum GPU memory: 8GB\\n- Recommended GPU memory: 16GB\\n- Optional: Flash Attention support\\n\\nPerformance Characteristics:\\n- Time complexity: O(N) where N is sequence length\\n- Memory complexity: O(N) with constant factor optimization\\n- Optimal batch size: Depends on GPU memory and sequence length\\n- Recommended maximum sequence length: 32K\\n- Optimal chunk size: 1024 for 16GB GPU\\n\\nArgs:\\n    embed_dim (int): Embedding dimension\\n    block_loc (tuple): Block location within the network\\n    kwarg_all (dict): Additional arguments\\n    device (optional): Computation device\\n    dtype (optional): Data type\\n    num_attention_heads (int, optional): Number of attention heads. Default is 4.\\n    dropout (float, optional): Dropout rate. Default is 0.0.\\n    attention_dropout (float, optional): Attention dropout rate. Default is 0.0.\\n    chunk_size (int, optional): Base chunk size for processing. Default is 1024.\\n    max_position_embeddings (int, optional): Maximum sequence length. Default is 32768.\\n    layer_norm_eps (float, optional): Layer norm epsilon. Default is 1e-5.\\n    use_flash_attention (bool, optional): Whether to use Flash Attention. Default is True.\\n    **kwargs: Additional keyword arguments.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "attention_dropout": 0.0,
                            "num_attention_heads": 4,
                            "dropout": 0.0,
                            "layer_norm_eps": 1e-05,
                            "use_flash_attention": true,
                            "max_position_embeddings": 32768,
                            "chunk_size": 1024
                        },
                        "design_traces": null
                    },
                    "Conv": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_conv": "@gau_test\ndef test_Conv_test_conv(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    conv = Conv(embed_dim, block_loc, kwarg_all, device=device, dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = conv(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\nlogger = logging.get_logger(__name__)\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RMSNorm', requirements='',\n    inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"Conv\",\"document\":\"\\nConv\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RMSNorm"
                        ],
                        "suggestions": null,
                        "args": {
                            "conv_kernel": 4,
                            "rms_norm_eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SwiGluMLP": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_swiglumlp": "@gau_test\ndef test_SwiGluMLP_test_swiglumlp(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    swiglumlp = SwiGluMLP(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = swiglumlp(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\nlogger = logging.get_logger(__name__)\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"SwiGluMLP\",\"document\":\"\\nSwiGluMLP\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "intermediate_size": null
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "RotaryEmbedding": "{\"unitname\":\"RotaryEmbedding\",\"requirements\":\"Implements rotary positional embeddings for sequences.\",\"inputs\":[\"X\"],\"outputs\":[\"cos\",\"sin\"]}",
                    "RMSNorm": "{\"unitname\":\"RMSNorm\",\"requirements\":\"\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "FastTTTLinear": "{\"unitname\":\"FastTTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "TTTLinear": "{\"unitname\":\"TTTLinear\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "ssmftt"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
                    "IMPLEMENTATION_CODER": "o1_preview",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "o1_mini",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.09825300000000001,
                "IMPLEMENTATION_CODER": 1.93323,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 0.318102,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}