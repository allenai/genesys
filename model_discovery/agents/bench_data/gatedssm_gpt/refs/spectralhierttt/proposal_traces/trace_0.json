{
    "variantname": "SpectralHierRMSNorm",
    "review": "### Comprehensive Review of SpectralHierRMSNorm Proposal\n\n#### 1. Clarity\nThe proposal is clearly articulated with well-defined objectives. It outlines the motivation for integrating spectral filtering and selective state space mechanisms into hierarchical normalization. The design plan is detailed, with clear mathematical formulations and implementation guidelines. However, some sections could benefit from additional examples or visual aids to enhance understanding.\n\n#### 2. Innovation\nThe proposal introduces innovative concepts by combining spectral filtering with selective state space mechanisms within a hierarchical normalization framework. This integration is novel and not extensively covered in existing research, making it a valuable contribution. The use of spectral filtering to improve feature representation and stability is particularly noteworthy.\n\n#### 3. Feasibility\nThe proposed design appears feasible within the given framework. The use of spectral filtering and selective state space mechanisms is supported by existing research, and the implementation guidelines are detailed enough to guide development. However, the complexity of integrating these components may pose challenges, particularly in ensuring stability and efficient computation.\n\n#### 4. Scalability\nThe design is poised to scale efficiently with larger models and more data. Spectral filtering and selective state space mechanisms are expected to reduce computational overhead and improve memory utilization, enabling the model to handle longer sequences effectively. The emphasis on hardware-aware implementation further supports scalability.\n\n#### 5. Accuracy and Robustness\nThe proposed changes are likely to enhance model performance by improving feature representation and stability. Spectral filtering can help capture long-range dependencies, while selective state space mechanisms allow for adaptive processing. These improvements should enhance the model's ability to handle diverse inputs and maintain robustness.\n\n#### 6. Efficiency\nThe design offers significant potential improvements in computational efficiency and memory usage. By reducing memory requirements through adaptive compression and optimizing state management, the proposal addresses key efficiency challenges associated with hierarchical normalization.\n\n### Strengths and Concerns\n\n**Strengths:**\n- **Innovative Integration:** The proposal combines spectral filtering with selective state space mechanisms, offering a novel approach to hierarchical normalization.\n- **Efficiency Gains:** The design addresses critical challenges in computational efficiency and memory usage.\n- **Scalability:** The hierarchical structure and hardware-aware implementation support efficient scaling with larger models and datasets.\n\n**Concerns:**\n- **Implementation Complexity:** The integration of spectral filtering and selective state space mechanisms may introduce complexity, potentially affecting training stability and implementation feasibility.\n- **Validation Needs:** Empirical validation is crucial to substantiate the theoretical benefits and ensure that the proposed modifications lead to tangible improvements in performance.\n\n### Suggestions for Improvement\n1. **Empirical Validation:** Conduct comprehensive experiments to validate the proposed enhancements, including benchmarks comparing performance, memory usage, and efficiency against baseline models.\n2. **Implementation Details:** Provide more detailed guidelines for integrating spectral filtering and selective state space mechanisms, including potential challenges and solutions.\n3. **Visualization:** Include diagrams or visual aids to illustrate the hierarchical normalization process and the integration of spectral filtering.\n\n### Final Rating\nBased on the comprehensive analysis, the proposal is well-conceived, addresses significant challenges in hierarchical normalization, and introduces novel contributions through spectral filtering and selective state space mechanisms. The potential benefits in efficiency, scalability, and robustness are substantial, aligning with current research trends. However, empirical validation and careful implementation are crucial to assess the practical impact and address potential concerns.\n\n**Rating: 4.2 out of 5**",
    "search_stack": [
        {
            "ready": false,
            "query": "structured state space models hierarchical memory",
            "detail": "Analyze implementations and architectures of structured state space models and hierarchical memory systems in language models, focusing on efficiency, scalability, and information flow between hierarchical levels.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations and architectures of structured state space models and hierarchical memory systems in language models, focusing on efficiency, scalability, and information flow between hierarchical levels.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.95)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 1/27 (Score: 0.95)*\n\n```\n# State Space Models as Foundation Models: A Control Theoretic Overview \n\nCarmen Amo Alonso*, Jerome Sieber*, and Melanie N. Zeilinger\n\n\n#### Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences. Index Terms-Machine learning, Linear systems, Timevarying systems. ## I. INTRODUCTION\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models' capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window ${ }^{1}$ and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the\n\n[^0]attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inferencein contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section $\\Pi$, we provide an overview of the essential components and considerations in SSMs. In Section III we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. ## II. State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. ## A. Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\n$$\ny(k)=f(u(k), \\ldots, u(k-T) ; \\theta)\n$$\n\nwhere at each time $k$, the output $y(k)$ is produced after evaluating an input signal of length $k-T$, i.e., $u(k), \\ldots, u(k-$ $T)$, and a set of parameters $\\theta$. The parameters $\\theta$ are task dependent, and can be fine-tuned accordingly. Since the search space of general $f(\\cdot ; \\theta)$ is too broad, different parameterizations of $f(\\cdot ; \\theta)$ can be used to render the problem tractable. For instance, the model $f(\\cdot ; \\theta)$ can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of $f(\\cdot ; \\theta)$ is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model $f(\\cdot ; \\theta)$ is presented with input-output pairs $(u(k), y(k)) \\forall k$, where $\\theta$ represents the parameters. The parameters $\\theta$ are then iteratively updated to minimize a loss function $\\mathscr{L}(\\cdot)$, i.e., iteratively solving the following optimization problem\n\n$$\n\\min _{\\theta} \\mathscr{L}(y-f(u ; \\theta))\n$$\n\nFor a language model the inputs $u$ are tokenized ${ }^{2}$ sentences and the outputs $y$ are a shifted version of the same inputs, i.e., an auto-regressive setup. ## B. Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\n$$\n\\begin{aligned}\n& \\dot{x}(t)=A x(t)+B u(t) \\\\\n& y(t)=C x(t)+D u(t)\n\\end{aligned}\n$$\n\nwhere $x \\in \\mathbb{C}^{p}$ represents the complex-valued state, $u, y \\in \\mathbb{R}^{q}$ are the input and the output, respectively, and $t$ denotes the continuous-time index. We note that the input fed into the system denoted as $u$ is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. $A, B, C, D$ are complex-valued matrices of appropriate dimensions and in representation (3), these matrices\n\n[^1]are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., $A_{t}, B_{t}, C_{t}, D_{t}$. In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input $u(t)$ represents a signal or input token fed into the model at a given time $t$. The state $x(t)$ represents the hidden state that stores the relevant information about the current and previous inputs up to time $t$, and $y(t)$ is the output of the model at time $t$. In a learning setup, the matrices $A, B, C, D$ are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix $A$ is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption 2.1: The dynamic matrix in dynamics (3) has a diagonal structure, i.e., $A=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)$ with $\\lambda_{i} \\in \\mathbb{C} \\forall i$. Although initial proposals [11], [12] deviate slightly from Assumption 2.1, most of the Structured SSMs literature assumes a diagonal $A$ matrix. Specific choices will be discussed in Section III\n\n## C. Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\n$$\n\\begin{aligned}\nx(k+1) & =\\bar{A} x(k)+\\bar{B} u(k) \\\\\ny(k) & =\\bar{C} x(k)+\\bar{D} u(k)\n\\end{aligned}\n$$\n\nwhere $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ are the discrete-time dynamic matrices discretized with time-step $\\Delta \\in \\mathbb{R}$, possibly with complexvalued components, and $k$ denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III\nWe note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. ## D. Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix $A$ have a significant impact on the performance after training: on a simple classification task, performance increases from $67 \\%$ when $A$ is randomly initialized, to $80 \\%$ when $A$ is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state $x(k)$ being able to capture the recent history of the inputs $u(k), \\ldots, u(k-T)$ for some time horizon $T$. This property is referred to as memory in the standard SSM literature. As is well-known in control\ntheory, the memory of system (4) is directly linked to the eigenvalues of matrix $A$. Lemma 2.2: (Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of $A$ are inside the unit circle and very close to the unit circumference, i.e. $|\\operatorname{eig}(A)| \\leq 1$ and $|\\operatorname{eig}(A)| \\approx 1 \\forall \\operatorname{eig}(A)$. Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned $A$ matrix is approximately equal to (but not bigger than) 1 . For the initialization of the other matrices, i.e., $B, C$, and $D$, standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of $B, C$, and $D$ in the following and refer the reader to the original papers [14], [15]. ## E. Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation $u(1)$ and zero initial state $x(1)=0$. In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a noncausal representation since input-output pairs $(u(k), y(k))$ are available for all $k$. Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\n$$\ny(k)=\\sum_{\\tau=0}^{k} \\bar{C} \\bar{A}^{k-\\tau} \\bar{B} u(\\tau)\n$$\n\nThis convolutional representation allows for faster learning because the complete input sequence $u(k) \\forall k$ can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. ## F. Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input $u$ and postprocessing of the output $y$ is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding\nA\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-3.jpg?height=776&width=874&top_left_y=211&top_left_x=1078)\n\nFig. 1: A. General scaffolding of a SSM. The dynamical model (4) is represented in green. The input to the SSM is pre-processed and forked off in a skip connection (lower signal). The nature of the pre-processing map (linear or nonlinear) depends on the specific scaffolding. The output of the recursion is then post-processed with a nonlinear gate. B. Overall architecture of a SSM. Each of the SSMs including its scaffolding (Fig. 1.A.) is structured in a layered fashion, where the output from one layer is the input to the next. the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition 2.3. In general, a linear or nonlinear map is performed on the input $\\bar{u}$ before it is fed into system (4). Once the output $y$ has been computed, a gating operation is generally performed to control the flow of information from the input $\\tilde{u}$ to the output $\\tilde{y}$. Intuitively, the gate $g(\\tilde{y}, \\tilde{u})$ controls which outputs $\\tilde{y}$ are set to zero based on the inputs $\\tilde{u}$ via the softmax operation. Definition 2.3: Given two vectors $x_{1}, x_{2} \\in \\mathbb{R}^{p}$, a gating operation is defined as $g\\left(x_{1}, x_{2}\\right):=x_{1} \\odot \\sigma\\left(W x_{2}\\right)$, where $W \\in \\mathbb{R}^{p \\times p}$, $\\odot$ is the element-wise multiplication, and $\\sigma$ is the softmax operation ${ }^{3}$\n\nAs is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input $y$ and output $u$ are of the same dimension $\\mathbb{R}^{q}$. For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matri-\n\n[^2]ces), and the size of the systems ranges in $p \\in[64,512], q \\in$ [32, 1024]. For language modelling the number of layers and system size can be significantly larger.\n```\n\n#### 2. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.78)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.78)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.59)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.59)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. Spectral State Space Models (Avg. Score: 0.54)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.54)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 5. Efficient Beam Tree Recursion (Avg. Score: 0.40)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** These proposals standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n**Abstract:** Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n##### *Relevant Chunk: No. 19/50 (Score: 0.40)*\n\n```\nIn Proceedings of International Conference on Neural Networks (ICNN'96), volume 1, pages 347-352 vol.1, 1996. doi: 10.1109/ICNN.1996.548916. [25] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016. URL http://arxiv.org/abs/1603.08983\n[26] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [27] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022. [28] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-2017. URL https://aclanthology.org/N18-2017. [29] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11\n[30] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908-15919, 2021. [31] Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1118-1128, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1115. URLhttps://aclanthology org/N19-1115\n[32] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 908-921, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.74. URL https://aclanthology.org/2021 acl-long. 74\n[33] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735\n[34] Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, and Gerard de Melo. R2D2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: structured state space models hierarchical memory\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: structured state space models hierarchical memory*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 2. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: structured state space models hierarchical memory*\n\n*Paul Mattes, Rainer Schlosser, R. Herbrich*\n\n**TL;DR:** Hieros is a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space that allows for more efficient training than RNN- based world models and more efficient imagination than Transformer-based world models.\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: structured state space models hierarchical memory*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 4. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: structured state space models hierarchical memory*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 5. StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization\n\n*From Search Query: structured state space models hierarchical memory*\n\n*Shida Wang, Qianxiao Li*\n\n**TL;DR:** This paper introduces a class of reparameterization techniques for SSMs that effectively lift its memory limitations and illustrates that a principled choice of reparameterization scheme can also enhance optimization stability.\n\n**Abstract:** In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this\"curse of memory\"as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets, language models and image classifications.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n### 2 related papers from Papers with Code\n\n#### 1. Autoregressive Search Engines: Generating Substrings as Document Identifiers\n\n*From Search Query: structured state space models hierarchical memory*\n\n*Fabio Petroni, Sebastian Riedel, Wen-tau Yih, Patrick Lewis, Giuseppe Ottaviano, Michele Bevilacqua*\n\n**Abstract:** Knowledge-intensive language tasks require NLP systems to both provide the correct answer and retrieve supporting evidence for it in a given corpus. Autoregressive language models are emerging as the de-facto standard for generating answers, with newer and more powerful systems emerging at an astonishing pace. In this paper we argue that all this (and future) progress can be directly applied to the retrieval problem with minimal intervention to the models' architecture. Previous work has explored ways to partition the search space into hierarchical structures and retrieve documents by autoregressively generating their unique identifier. In this work we propose an alternative that doesn't force any structure in the search space: using all ngrams in a passage as its possible identifiers. This setup allows us to use an autoregressive model to generate and score distinctive ngrams, that are then mapped to full passages through an efficient data structure. Empirically, we show this not only outperforms prior autoregressive approaches but also leads to an average improvement of at least 10 points over more established retrieval solutions for passage-level retrieval on the KILT benchmark, establishing new state-of-the-art downstream performance on some datasets, while using a considerably lighter memory footprint than competing systems. Code and pre-trained models at https://github.com/facebookresearch/SEAL.\n\n**Published:** 2022-04-22\n\n\n\n#### 2. HIDA: A Hierarchical Dataflow Compiler for High-Level Synthesis\n\n*From Search Query: structured state space models hierarchical memory*\n\n*Anonymous*\n\n**Abstract:** Dataflow architectures are growing in popularity due to their potential to mitigate the challenges posed by the memory wall inherent to the Von Neumann architecture. At the same time, high-level synthesis (HLS) has demonstrated its efficacy as a design methodology for generating efficient dataflow architectures within a short development cycle. However, existing HLS tools rely on developers to explore the vast dataflow design space, ultimately leading to suboptimal designs. This phenomenon is especially concerning as the size of the HLS design grows. To tackle these challenges, we introduce HIDA, a new scalable and hierarchical HLS framework that can systematically convert an algorithmic description into a dataflow implementation on hardware. We first propose a collection of efficient and versatile dataflow representations for modeling the hierarchical dataflow structure. Capitalizing on these representations, we develop an automated optimizer that decomposes the dataflow optimization problem into multiple levels based on the inherent dataflow hierarchy. Using FPGAs as an evaluation platform, working with a set of neural networks modeled in PyTorch, HIDA achieves up to 8.54$\\times$ higher throughput compared to the state-of-the-art (SOTA) HLS optimization tool. Furthermore, despite being fully automated and able to handle various applications, HIDA achieves 1.29$\\times$ higher throughput over the SOTA RTL-based neural network accelerators on an FPGA.\n\n**Published:** 2023-11-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using structured state space models and hierarchical memory systems, here are some key insights and relevant information from the provided sources and additional context:\n\n## Structured State Space Models\n\n### Efficiency and Scalability\nState Space Models (SSMs) have shown promise in handling large sequences efficiently, which is crucial for language models. The SC-MAMBA2 model, for instance, integrates SSMs with the MAMBA framework, enabling the efficient modeling of large gene sequences and, by extension, potentially long text sequences. This integration allows for the handling of sequences up to 60,530 genes, demonstrating scalability.\n\nThe Mamba model and its variants use a discrete version of continuous SSMs, incorporating a timescale parameter and zero-order hold (ZOH) technique for discretization. This approach facilitates the transformation of continuous parameters into discrete forms, making it more suitable for computational frameworks.\n\n### Information Flow and Hierarchical Processing\nSSMs can be designed to capture hierarchical structures by iteratively updating a memory state matrix. The Longhorn model, for example, uses an SSM block for sequence mixing, updating the memory state matrix with a linear recurrence. This approach allows for the integration of hierarchical information flow, as the model updates the state based on previous inputs and current inputs.\n\nThe use of state space duality in models like Mamba2 simplifies the matrix operations, making it more efficient for hierarchical processing. This duality can be applied in both linear and quadratic forms, enhancing the model's ability to handle complex sequences.\n\n## Hierarchical Memory Systems\n\n### Hierarchical Compositionality\nHierarchical memory systems can be designed to reflect the hierarchical compositionality of data. For instance, the concept of hierarchical data structures explored through diffusion models shows how high-level features can undergo sharp transitions during a noising-denoising process, while low-level features recombine into new data. This behavior can be leveraged to create hierarchical models that capture non-trivial data structures.\n\n### Ordered Memory and Structured State Updates\nThe Ordered Memory model, which performs on par with state-of-the-art tree-type models, demonstrates that structured memory organization can improve efficiency. Combining ordered memory principles with hierarchical processing could enhance information retention across different scales. This structured approach can ensure better state representation and updates, addressing the current limitations in memory utilization across different scales.\n\n### Combining Local and Global Processing\nTo balance local and global information flow, models like the Segmented Recurrent Transformer (SRformer) can be insightful. SRformer leverages Recurrent Accumulate-and-Fire (RAF) neurons to update the cumulative product of keys and values, providing an efficient way to process sequences while retaining both local and global context.\n\n## Novel Approaches and Future Directions\n\n### Token Reduction and Residual Connections\nThe intra-layer token reduction design for SSMs, as discussed in the recent advancements, focuses on preserving essential information while eliminating redundancy. This strategy enhances the informational flow by removing tokens with minimal contextual relevance and preserving residual connections, which is crucial for maintaining the integrity of information from the last layer.\n\n### Online Learning Perspective\nTreating SSMs from an online learning perspective can streamline the design process. This approach involves updating the memory state dynamically, which can be particularly useful for test-time adaptation and multi-scale processing. The Longhorn model's ability to extrapolate to longer context lengths at inference time without significant perplexity drop is a promising example of this approach.\n\nBy integrating these concepts\u2014structured state space models, hierarchical memory systems, efficient token reduction, and dynamic adaptation mechanisms\u2014researchers can develop more efficient, scalable, and robust autoregressive language models that better capture hierarchical structures and improve overall performance.\n",
            "analysis": "Analysis Note - Round 1\n\n1. Summary of Analysis:\nThe seed design HierTTT proposes a hierarchical test-time training approach with multi-scale linear attention. From analyzing the seed design and initial references, several key areas emerge for potential innovation:\n\na) Memory and State Management:\n- Current designs struggle with efficient memory utilization across different scales\n- Need for better state representation and updates\n- Opportunity to improve information flow between hierarchical levels\n\nb) Attention Mechanisms:\n- Linear attention shows promise but has limitations\n- Toeplitz-based approaches offer interesting efficiency properties\n- Opportunity to combine with ordered memory concepts\n\nc) Adaptive Processing:\n- Test-time adaptation is valuable but could be enhanced\n- Multi-scale processing needs better integration\n- Potential for more dynamic adaptation mechanisms\n\n2. Useful References with Excerpts:\n\nFrom Toeplitz Neural Network paper:\n\"A lightweight sub-network called relative position encoder is proposed to generate relative position coefficients with a fixed budget of parameters, enabling the proposed Toeplitz neural network to deal with varying sequence lengths.\"\n- Key insight: Efficient position encoding with fixed parameter budget\n\nFrom Ordered Memory paper:\n\"The Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.\"\n- Key insight: Structured memory organization can improve efficiency\n\nFrom Segmented Recurrent Transformer:\n\"SRformer leverages Recurrent Accumulate-and-Fire (RAF) neurons' inherent memory to update the cumulative product of keys and values.\"\n- Key insight: Efficient sequential processing with accumulated memory\n\n3. Key Insights and Analysis:\n\na) Memory Organization:\n- Ordered memory principles could be combined with hierarchical processing\n- Potential for more structured state updates\n- Opportunity to improve information retention across scales\n\nb) Efficiency Mechanisms:\n- Toeplitz structure offers computational benefits\n- Segmented processing can reduce complexity\n- Need to balance local and global information flow\n\nc) Adaptation Strategies:\n- Test-time training could be enhanced with structured memory\n- Multi-scale processing needs better coordination\n- Potential for more dynamic parameter updates\n\n4. Future Search Plan:\nNeed to investigate:\n1. Recent advances in structured state space models\n2. Efficient implementations of hierarchical memory\n3. Novel approaches to combining local and global processing\n\n5. References:\n\n[1] Qin, Z., Han, X., Sun, W., He, B., Li, D., Li, D., ... & Zhong, Y. (2023). Toeplitz Neural Network for Sequence Modeling. International Conference on Learning Representations.\n\n[2] Borisov, D., D'Iorio, M., & Hyacinthe, J. (2023). Ordered Memory. arXiv preprint.\n\n[3] Long, Y., Chowdhury, S. S., & Roy, K. (2023). Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. Conference on Empirical Methods in Natural Language Processing.\n\n[4] Shao, N., Xiao, S., Liu, Z., & Zhang, P. (2024). Extensible Embedding: A Flexible Multipler For LLM's Context Length. arXiv preprint.\n\nBased on this initial analysis, I believe we need another round of search focusing on structured state space models and hierarchical memory implementations to develop a more comprehensive understanding before formulating the proposal."
        },
        {
            "ready": false,
            "query": "attention mechanisms adaptive processing neural networks",
            "detail": "Analyze implementations and architectures combining attention mechanisms with adaptive processing in neural networks, focusing on efficiency, scalability, and dynamic parameter adjustment.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations and architectures combining attention mechanisms with adaptive processing in neural networks, focusing on efficiency, scalability, and dynamic parameter adjustment.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Perceiver IO: A General Architecture for Structured Inputs & Outputs (Avg. Score: 0.80)\n\n*Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Andrew Brock, Evan Shelhamer, Olivier J. H'enaff, M. Botvinick, Andrew Zisserman, O. Vinyals, Jo\u00e3o Carreira*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 458  (*Influential: 46*)\n\n**TL;DR:** This work proposes Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs and augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.\n\n**Abstract:** A central goal of machine learning is the development of systems that can solve many problems in as many data domains as possible. Current architectures, however, cannot be applied beyond a small set of stereotyped settings, as they bake in domain&task assumptions or scale poorly to large inputs or outputs. In this work, we propose Perceiver IO, a general-purpose architecture that handles data from arbitrary settings while scaling linearly with the size of inputs and outputs. Our model augments the Perceiver with a flexible querying mechanism that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering. The same architecture achieves strong results on tasks spanning natural language and visual understanding, multi-task and multi-modal reasoning, and StarCraft II. As highlights, Perceiver IO outperforms a Transformer-based BERT baseline on the GLUE language benchmark despite removing input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation with no explicit mechanisms for multiscale correspondence.\n\n##### *Relevant Chunk: No. 2/54 (Score: 0.80)*\n\n```\n4.1), estimate optical flow without relying on explicit architectural features for multiscale correspondence (Sec. 4.2, learn joint representations of video, audio, and labels without separate network trunks (Sec. 4.3), and perform image classification with no information about the 2D structure of images ( Sec . A. ## 2 RELATEd WORK\n\nNeural network research has long sought architectures that can handle large, arbitrarily structured inputs and outputs. Autoencoding (Hinton \\& Zemel, 1994) was among the first attempts to build\nrepresentation which could encode and reproduce high-dimensional inputs like images. As hardware grew more powerful, neural nets led to breakthroughs in image understanding (Krizhevsky et al., 2012, Zeiler \\& Fergus, 2014; Szegedy et al., 2015) and interest intensified: autoregressive models that could process and complete samples of handwriting were developed (Graves 2013), and new convolutional network designs led to good results in structured output spaces like semantic segmentation (Farabet et al., 2012, Long et al., 2015, Ronneberger et al., 2015), pose estimation (Toshev \\& Szegedy, 2014), detection (Sermanet et al., 2014), captioning (You et al., 2016), and optical flow (Fischer et al. 2015). At the same time, natural language applications research has made extensive progressive in capturing the structured nature of language, typically via autoregressive models (Collobert et al. 2011; Sutskever et al., 2014, Vaswani et al., 2017, Radford et al., 2019, Brown et al, 2020) or context prediction (Mikolov et al., 2013; Pennington et al., 2014, Devlin et al., 2019). Similar to our work, several groups have proposed to solve tasks in multiple domains (e.g. Kaiser et al. 2017, Alayrac et al. 2020; Akbari et al. 2021, but typically across a fixed and predefined set of modalities by means of domain-specific networks. Although single-task specialist networks remain dominant in vision, multi-task learning has become popular (Misra et al., 2016, Doersch \\& Zisserman, 2017, Kokkinos, 2017, Zamir et al., 2018) and individual models achieve generality in a restricted domain: e.g. Mask-RCNN (He et al., 2017) handles object detection, segmentation, and pose estimation. In language, training or evaluation on multiple tasks has also become common (Collobert \\& Weston, 2008, Luong et al., 2016, Devlin et al., 2019, Liu et al., 2019, Raffel et al., 2020). Several groups have demonstrated that Transformers (originally designed for language) can be used or adapted to non-language tasks (e.g. Chen et al. 2020, Lu et al. 2021), but the limited scalability of Transformers limits their usefulness as general-purpose architectures. Several groups have proposed to use attention to manipulate the size of arrays or to introduce bottlenecks in processing. Set Transformers and related work (Lee et al., 2019, Goyal et al., 2022) use a learned query (\"inducing points\") to induce local bottlenecks by mapping a set back and forth from a set with fewer elements and learned decoder queries (\"seed vectors\") to map to outputs (\"pooling by multiheaded attention'). Each layer of these networks has complexity linear in the input size, while Perceivers use a deep latent network with complexity independent of the input and output. Our work uses attention over inputs and outputs of different sizes in part to produce an efficient attention architecture, and several other efficient attention architectures have been proposed, largely for language or small-scale problems (e.g.\n```\n\n#### 2. The Devil in Linear Transformer  (Avg. Score: 0.71)\n\n*Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, Yiran Zhong*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2022)\t**Cited by** 39  (*Influential: 3*)\n\n**TL;DR:** This paper proposes a new linear attention that replaces the scaling of attention matrices with a normalization to stabilize gradients, and proposes a diagonal attention to confine attention to only neighbouring tokens in early layers.\n\n**Abstract:** Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .\n\n##### *Relevant Chunk: No. 4/25 (Score: 0.71)*\n\n```\nThe overall architecture is shown in Fig. 3. ### 4.1 The overall architecture\n\nVanilla attention suffers less in attention dilution while linear attention is more efficient and scalable on longer sequences. This motivate us to design a method that exploits the best of the both worlds by using these mechanisms in combined. Specifically, our network consists of two types of attention: DiagAttention for the early stage of the model and NORMATTENTION for the later stage. The former addresses the attention dilution issue and the later aims to stabilize training gradients. Note that by properly reshaping the inputs, the diagonal attention can be efficiently computed in linear space-time, thus preserving the overall linear complexity. ### 4.2 NormAtTEntion\n\nTable 1: Ablation of linear attention with scaling operation.\n```\n\n#### 3. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.69)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.69)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.60)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.60)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.39)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.39)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: attention mechanisms adaptive processing neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Demystifying Oversmoothing in Attention-Based Graph Neural Networks\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Xinyi Wu, A. Ajorlou, Zihui Wu, A. Jadbabaie*\n\n**TL;DR:** It is established that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially.\n\n**Abstract:** Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models, including random walk GCNs, Graph Attention Networks (GATs) and (graph) transformers. In particular, our analysis accounts for asymmetric, state-dependent and time-varying aggregation operators and a wide range of common nonlinear activation functions, such as ReLU, LeakyReLU, GELU and SiLU.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 3*)\n\n#### 2. Towards Deep Attention in Graph Neural Networks: Problems and Remedies\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Soo Yong Lee, Fanchen Bu, Jaemin Yoo, Kijung Shin*\n\n**TL;DR:** AERO-GNN provably mitigates the proposed problems of deep graph attention, which is further empirically demonstrated with (a) its adaptive and less smooth attention functions and (b) higher performance at deep layers (up to 64).\n\n**Abstract:** Graph neural networks (GNNs) learn the representation of graph-structured data, and their expressiveness can be further enhanced by inferring node relations for propagation. Attention-based GNNs infer neighbor importance to manipulate the weight of its propagation. Despite their popularity, the discussion on deep graph attention and its unique challenges has been limited. In this work, we investigate some problematic phenomena related to deep graph attention, including vulnerability to over-smoothed features and smooth cumulative attention. Through theoretical and empirical analyses, we show that various attention-based GNNs suffer from these problems. Motivated by our findings, we propose AEROGNN, a novel GNN architecture designed for deep graph attention. AERO-GNN provably mitigates the proposed problems of deep graph attention, which is further empirically demonstrated with (a) its adaptive and less smooth attention functions and (b) higher performance at deep layers (up to 64). On 9 out of 12 node classification benchmarks, AERO-GNN outperforms the baseline GNNs, highlighting the advantages of deep graph attention. Our code is available at https://github.com/syleeheal/AERO-GNN.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 3. Unraveling Feature Extraction Mechanisms in Neural Networks\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Xiaobing Sun, Jiaxi Li, Wei Lu*\n\n**TL;DR:** Considering the infinite network width, it is hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening insights into their internal mechanisms and aiding the broader understanding of these complex systems.\n\n**Abstract:** The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the \\textit{ReLU} activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to excel in this area. We verify these theoretical findings through experiments and find that they can be applied to analyze language modeling tasks, which can be regarded as a special variant of classification. Our contributions offer insights into the roles and capacities of fundamental components within large language models, thereby aiding the broader understanding of these complex systems.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 4. Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Qi Xu, Yuyuan Gao, Jiangrong Shen, Yaxin Li, Xuming Ran, Huajin Tang, Gang Pan*\n\n**TL;DR:** This paper develops a recurrent spiking neural network model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns.\n\n**Abstract:** Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS). Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points. In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns. It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination. The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets. The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 5. Representing Long-Range Context for Graph Neural Networks with Global Attention\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Zhanghao Wu, Paras Jain, Matthew A. Wright, Azalia Mirhoseini, Joseph E. Gonzalez, Ion Stoica*\n\n**TL;DR:** This work proposes the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel readout mechanism to obtain a global graph embedding, and suggests that purely-learning-based approaches without graph structure may be suitable for learning high-level, long- range relationships on graphs.\n\n**Abstract:** Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel\"readout\"mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 205  (*Influential: 30*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Visual Attention Network\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Shi-Min Hu, Ming-Ming Cheng, Zheng-Ning Liu, Cheng-Ze Lu, Meng-Hao Guo*\n\n**Abstract:** While originally designed for natural language processing tasks, the self-attention mechanism has recently taken various computer vision areas by storm. However, the 2D nature of images brings three challenges for applying self-attention in computer vision. (1) Treating images as 1D sequences neglects their 2D structures. (2) The quadratic complexity is too expensive for high-resolution images. (3) It only captures spatial adaptability but ignores channel adaptability. In this paper, we propose a novel linear attention named large kernel attention (LKA) to enable self-adaptive and long-range correlations in self-attention while avoiding its shortcomings. Furthermore, we present a neural network based on LKA, namely Visual Attention Network (VAN). While extremely simple, VAN surpasses similar size vision transformers(ViTs) and convolutional neural networks(CNNs) in various tasks, including image classification, object detection, semantic segmentation, panoptic segmentation, pose estimation, etc. For example, VAN-B6 achieves 87.8% accuracy on ImageNet benchmark and set new state-of-the-art performance (58.2 PQ) for panoptic segmentation. Besides, VAN-B2 surpasses Swin-T 4% mIoU (50.1 vs. 46.1) for semantic segmentation on ADE20K benchmark, 2.6% AP (48.8 vs. 46.2) for object detection on COCO dataset. It provides a novel method and a simple yet strong baseline for the community. Code is available at https://github.com/Visual-Attention-Network.\n\n**Published:** 2022-02-20\n\n\n\n#### 2. Residual Attention Network for Image Classification\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Fei Wang, Honggang Zhang, Chen Qian, Mengqing Jiang, Xiaoou Tang, Xiaogang Wang, Shuo Yang, Cheng Li*\n\n**Abstract:** In this work, we propose \"Residual Attention Network\", a convolutional neural\nnetwork using attention mechanism which can incorporate with state-of-art feed\nforward network architecture in an end-to-end training fashion. Our Residual\nAttention Network is built by stacking Attention Modules which generate\nattention-aware features. The attention-aware features from different modules\nchange adaptively as layers going deeper. Inside each Attention Module,\nbottom-up top-down feedforward structure is used to unfold the feedforward and\nfeedback attention process into a single feedforward process. Importantly, we\npropose attention residual learning to train very deep Residual Attention\nNetworks which can be easily scaled up to hundreds of layers. Extensive\nanalyses are conducted on CIFAR-10 and CIFAR-100 datasets to verify the\neffectiveness of every module mentioned above. Our Residual Attention Network\nachieves state-of-the-art object recognition performance on three benchmark\ndatasets including CIFAR-10 (3.90% error), CIFAR-100 (20.45% error) and\nImageNet (4.8% single model and single crop, top-5 error). Note that, our\nmethod achieves 0.6% top-1 accuracy improvement with 46% trunk depth and 69%\nforward FLOPs comparing to ResNet-200. The experiment also demonstrates that\nour network is robust against noisy labels.\n\n**Conference:** residual-attention-network-for-image-1\n\n**Published:** 2017-04-23\n\n\n\n#### 3. CDSA: Cross-Dimensional Self-Attention for Multivariate, Geo-tagged Time Series Imputation\n\n*From Search Query: attention mechanisms adaptive processing neural networks*\n\n*Shih-Fu Chang, Anthony Vetro, Zheng Shou, Hassan Mansour, Jiawei Ma, Alireza Zareian*\n\n**Abstract:** Many real-world applications involve multivariate, geo-tagged time series data: at each location, multiple sensors record corresponding measurements. For example, air quality monitoring system records PM2.5, CO, etc. The resulting time-series data often has missing values due to device outages or communication errors. In order to impute the missing values, state-of-the-art methods are built on Recurrent Neural Networks (RNN), which process each time stamp sequentially, prohibiting the direct modeling of the relationship between distant time stamps. Recently, the self-attention mechanism has been proposed for sequence modeling tasks such as machine translation, significantly outperforming RNN because the relationship between each two time stamps can be modeled explicitly. In this paper, we are the first to adapt the self-attention mechanism for multivariate, geo-tagged time series data. In order to jointly capture the self-attention across multiple dimensions, including time, location and the sensor measurements, while maintain low computational complexity, we propose a novel approach called Cross-Dimensional Self-Attention (CDSA) to process each dimension sequentially, yet in an order-independent manner. Our extensive experiments on four real-world datasets, including three standard benchmarks and our newly collected NYC-traffic dataset, demonstrate that our approach outperforms the state-of-the-art imputation and forecasting methods. A detailed systematic analysis confirms the effectiveness of our design choices.\n\n**Published:** 2019-05-23\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by integrating attention mechanisms with adaptive processing, several key aspects need to be considered, focusing on efficiency, scalability, and dynamic parameter adjustment.\n\n### Attention Mechanisms\n\n1. **Multi-Head Attention**:\n   Multi-head attention mechanisms allow the model to capture different aspects of the input data by computing multiple attention representations in parallel. This is particularly effective in autoregressive models as it enables the model to focus on various parts of the input sequence simultaneously. The optimal number of attention heads and the dimensionality of each head are crucial parameters to tune for efficiency and performance.\n\n2. **Feature-Based Attention**:\n   Feature-based attention, as described in the context of visual models, can be adapted for language models. This involves modulating the neural activity based on the relevance of each feature to the task at hand. For language models, this could mean adjusting the attention weights based on the semantic importance of different words or phrases in the input sequence.\n\n### Adaptive Processing\n\n1. **Dynamic State Dimensionality**:\n   Adaptive state dimensionality in state space models (SSMs) can be extended to autoregressive language models. This involves adjusting the state dimension based on the input complexity, which can help in balancing memory efficiency and expressiveness. Attention mechanisms can be integrated with SSMs to dynamically adjust the state space dimensions, ensuring that the model captures long-range dependencies efficiently.\n\n2. **Efficient Parallel Processing**:\n   To enhance efficiency, careful design of the model architecture is necessary to enable efficient parallel processing. This can be achieved by structuring the attention mechanisms in a way that allows for parallel computation of attention weights and subsequent transformations. Multi-head attention, for instance, can be computed in parallel, reducing computational overhead.\n\n### Integration of Attention and Adaptive Processing\n\n1. **Hierarchical Attention**:\n   Hierarchical attention mechanisms can be integrated with state space models to create a temporal hierarchy. This approach allows the model to capture long-range dependencies and adapt to different scales of input data. By stacking structured state-space models with attention mechanisms, the model can improve its performance on continuous sequential prediction tasks.\n\n2. **Attention-Based Feature Weighting**:\n   Attention-based feature weighting layers can be used to dynamically adjust the importance of different features in the input sequence. This approach ensures that the model focuses on the most relevant features and suppresses irrelevant ones, which is crucial for adaptive processing. The use of multiple attention heads and aggregation layers helps in maintaining stability and preventing overfitting.\n\n### Scalability and Efficiency\n\n1. **Sparse Attention Mechanisms**:\n   Implementing sparse attention mechanisms can improve the efficiency and scalability of the model. Sparse nets with attention mechanisms, as seen in tabular data models, can outperform traditional models by identifying the most relevant features and reducing computational complexity. This approach can be adapted for autoregressive language models to enhance their performance on large datasets.\n\n2. **Regularization Techniques**:\n   To prevent overfitting and ensure the model's generalization capability, regularization techniques such as L2 regularization and dropout layers can be employed. These techniques help in maintaining the model's stability and efficiency, especially when dealing with large and complex datasets.\n\nBy combining these elements, researchers can develop an autoregressive language model that leverages attention mechanisms for adaptive processing, ensuring efficiency, scalability, and dynamic parameter adjustment. This approach can help in achieving low perplexity, high accuracy in downstream tasks, robustness to variant inputs, and better overall performance with larger models and more data.\n",
            "analysis": "Analysis Note - Round 2\n\n1. Summary of Analysis:\nFrom analyzing the search results, several key themes emerge around state space models (SSMs) and hierarchical memory systems:\n\na) State Space Model Architecture:\n- SSMs offer efficient sequence processing with linear complexity\n- Can be enhanced through hierarchical structuring and spectral filtering\n- Need careful consideration of stability and memory decay properties\n\nb) Memory Management:\n- Hierarchical approaches show promise for long-range dependencies\n- Structured state updates improve information retention\n- Trade-off between memory efficiency and expressiveness needs balancing\n\nc) Integration Opportunities:\n- Potential to combine SSM efficiency with hierarchical processing\n- Opportunities for adaptive state dimensionality\n- Need for better information flow between hierarchical levels\n\n2. Useful References with Excerpts:\n\nFrom \"State Space Models as Foundation Models\":\n\"SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas.\"\n- Key insight: SSMs provide efficient sequence modeling with control theoretic foundations\n\nFrom \"Hierarchical State Space Models\":\n\"HiSS stacks structured state-space models on top of each other to create a temporal hierarchy.\"\n- Key insight: Hierarchical stacking improves performance on continuous sequential prediction\n\nFrom \"StableSSM\":\n\"State-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory.\"\n- Key insight: Need for careful reparameterization to address memory limitations\n\n3. Key Insights and Analysis:\n\na) Architectural Design:\n- Hierarchical stacking of SSMs shows promise for improved performance\n- Need to address stability issues through proper parameterization\n- Opportunity to combine spectral filtering with hierarchical processing\n\nb) Memory Enhancement:\n- Reparameterization techniques can help overcome memory limitations\n- Hierarchical structure allows better information retention\n- Need for balance between stability and expressiveness\n\nc) Implementation Considerations:\n- Efficient parallel processing possible through careful design\n- State dimensionality can be adapted based on input complexity\n- Memory-computation trade-offs need careful consideration\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for combining attention mechanisms with SSMs\n2. Methods for adaptive state dimensionality\n3. Efficient implementations of hierarchical processing in neural networks\n\n5. References:\n\n[1] Alonso, C. A., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint.\n\n[2] Bhirangi, R. M., Wang, C., Pattabiraman, V., Majidi, C., Gupta, A., Hellebrekers, T., & Pinto, L. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. International Conference on Machine Learning.\n\n[3] Wang, S., & Li, Q. (2023). StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization. International Conference on Machine Learning.\n\n[4] Wang, S., & Xue, B. (2023). State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. Neural Information Processing Systems.\n\n[5] Lu, C. X., et al. (2023). Structured State Space Models for In-Context Reinforcement Learning. Neural Information Processing Systems.\n\nBased on this analysis, we need another round of search focusing on attention mechanisms and adaptive processing to develop a more comprehensive understanding before formulating the proposal."
        },
        {
            "ready": false,
            "query": "state transitions hierarchical attention neural networks",
            "detail": "Analyze implementations and architectures combining state transitions with hierarchical attention in neural networks, focusing on efficiency, stability, and information flow between levels.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations and architectures combining state transitions with hierarchical attention in neural networks, focusing on efficiency, stability, and information flow between levels.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.66)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.66)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.57)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.84)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n##### *Relevant Chunk: No. 4/29 (Score: 0.30)*\n\n```\nHere we discuss the most prominent one. S6. The first selective SSM parametrization (S6) was introduced together with the Mamba architecture Gu and Dao, 2023]. The S6 block parametrizes the recurrence as\n\n$$\nA_{i}=e^{-\\Delta_{i} A}, \\quad B_{i}=\\Delta_{i} W_{B} u_{i}, \\quad C_{i}=W_{C} u_{i}, \\quad D_{i}=W_{D} u_{i}\n$$\n\nwith $\\Delta_{i}=\\operatorname{softplus}\\left(W_{\\Delta}\\left(W_{u} u_{i}\\right)+b_{\\Delta}\\right)$ for every $i, W_{\\Delta}, W_{u}, W_{B}, W_{C}, W_{D}, A$ are learnt matrices of appropriate dimensions, and $b_{\\Delta}$ is a learnt bias. While SSM models allow for complex-valued matrices $A_{i}, B_{i}, C_{i}, D_{i}$, here we restrict ourselves to real-valued matrices as in Gu and Dao, 2023]. ### 2.3 Recurrent Neural Networks\n\nSimilar to SSMs, RNNs also parameterize the input-output relationship via a recurrent computation, commonly given by the long short-term memory (LSTM) [Hochreiter and Schmidhuber, 1997], at each time step $i$\n\n$$\n\\begin{aligned}\nx_{i} & =f_{i} \\odot x_{i-1}+i_{i} \\odot \\bar{u}_{i} \\\\\ny_{i} & =o_{i} \\odot \\tanh \\left(x_{i}\\right)\n\\end{aligned}\n$$\n\nwhere $\\bar{u}_{i}$ represents the pre-processed raw input $u_{i}$, i.e.,\n\n$$\n\\bar{u}_{i}=\\tanh \\left(W_{u} u_{i}+U_{u} y_{i-1}\\right)\n$$\n\nand $f_{i}, i_{i}$, and $o_{i}$ are the forget gate, the input gate, and the output gate, respectively,\n\n$$\nf_{i}=\\sigma\\left(W_{f} u_{i}+U_{f} y_{i-1}\\right), \\quad i_{i}=\\sigma\\left(W_{i} u_{i}+U_{i} y_{i-1}\\right), \\quad o_{i}=\\sigma\\left(W_{o} u_{i}+U_{o} y_{i-1}\\right)\n$$\n\nwhere $W_{f}, W_{i}, W_{o}$ and $U_{f}, U_{i}, U_{o}$ are the learnt gate parameters. In this paper, we focus on two variants: quasi LSTMs (qLSTM) [Stani\u0107 et al. 2023], which removes the output dependence of the gates, and RG-LRU [De et al. 2024], which attempts to integrate ideas from SSMs into RNNs. qLSTM. The qLSTM model is parameterized by recurrence (5) with pre-processed input $\\bar{u}_{i}$ and gates $f_{i}, i_{i}, o_{i}$ :\n\n$$\n\\bar{u}_{i}=\\tanh \\left(W_{u} u_{i}\\right), \\quad f_{i}=\\sigma\\left(W_{f} u_{i}\\right), \\quad i_{i}=\\sigma\\left(W_{i} u_{i}\\right), \\quad o_{i}=\\sigma\\left(W_{o} u_{i}\\right)\n$$\n\nRG-LRU. The RG-LRU model presents a hybrid between a qLSTM and a SSM. The recurrence is given as\n\n$$\n\\begin{aligned}\n& x_{i}=a_{i} \\odot x_{i-1}+\\sqrt{1-a_{i}^{2}} \\odot\\left(i_{i} \\odot u_{i}\\right) \\\\\n& y_{i}=x_{i}\n\\end{aligned}\n$$\n\nwith the following gates and no pre-processing of $u_{i}$ :\n\n$$\nr_{i}=\\sigma\\left(W_{a} u_{i}\\right), \\quad i_{i}=\\sigma\\left(W_{u} u_{i}\\right), \\quad a_{i}=e^{-c r_{i} \\odot \\operatorname{softplus}(\\Lambda)}\n$$\n\n## 3 Dynamical Systems Framework for Architecture Comparison\n\nIn this section, we introduce the Dynamical Systems Framework (DSF) that allows in-depth analysis of the architectural features of attention, SSMs, and RNNs from a dynamical systems perspective. We use this to rewrite the parametrizations in a common framework and provide detailed comparisons. ### 3.1 Dynamical Systems Framework (DSF)\n\nThe DSF relies on a dynamical systems representation of the architectures. As is standard in RNN and SSM literature, we choose a recurrent state space representation. In particular, a linear structured time-varying (LTV) dynamical system is defined by the recurrence\n\n$$\n\\begin{aligned}\nh_{i} & =\\Lambda_{i} h_{i-1}+B_{i} u_{i} \\\\\ny_{i} & =C_{i} h_{i}+D_{i} u_{i}\n\\end{aligned}\n$$\n\nwhere $h_{i} \\in \\mathbb{R}^{N}$ is the hidden state initialized with $h_{-1}=0, \\Lambda_{i} \\in \\mathbb{R}^{N \\times N}$ is the diagonal state transition matrix, $B_{i} \\in \\mathbb{R}^{N \\times d}$ and $C_{i} \\in \\mathbb{R}^{d \\times N}$ are the input and output matrices, respectively, and $D_{i} \\in \\mathbb{R}^{d \\times d}$ is a scaled skip connection. Dynamical system (11) can alternatively be written in its convolutional representation, i.e., $\\mathbf{y}=\\boldsymbol{\\Phi u}$, where the convolutional kernel $\\boldsymbol{\\Phi}$ is defined as\n\n$$\n\\boldsymbol{\\Phi}=\\left[\\begin{array}{cccc}\nC_{0} B_{0}+D_{0} & & & \\\\\nC_{1} \\Lambda_{1} B_{0} & C_{1} B_{1}+D_{1} & & \\\\\n\\vdots & \\ddots & \\ddots & \\\\\nC_{L} \\prod_{k=1}^{L} \\Lambda_{k} B_{0} & \\ldots & C_{L} \\Lambda_{L} B_{L-1} & C_{L} B_{L}+D_{L}\n\\end{array}\\right]\n$$\n\nNote that the convolution kernel $\\boldsymbol{\\Phi} \\in \\mathbb{R}^{L \\times L}$ is of the same dimension as the attention matrix $\\zeta\\left(\\mathbf{q} \\mathbf{k}^{\\top}\\right)$ and that these matrices are equivalent, up to the scaling factor $W_{V}$ used in self-attention.\n```\n\n#### 3. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.46)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 42/46 (Score: 0.46)*\n\n```\n[N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n## Appendix\n\n## 6 Related Work\n\nHierarchical or Multiscale Recurrent neural networks. This work takes inspiration from a wide array of work on introducing multiple scales of processing into recurrent neural networks (Chung et al. 2016; Hihi \\& Bengio, 1995; Mozer, 1991, Schmidhuber, 1991, Koutn\u00edk et al., 2014). These works divide the processing into multiple streams each operating at a different temporal granularity. While these works mainly focus on recurrent neural networks and their application is mainly on natural language tasks, we focus on introducing multiple streams of processing and a hierarchical structure into Transformers while also focusing on a broader range of domains beyond natural language. Transformers. Some of the components we describe in the proposed model have been used previously in various Transformer models. Transformer XL (Dai et al., 2019) also divides the input into segments. Each segment considers the tokens from the current segment and the previous segment for attention without passing gradients into the previous segments. A number of previous works (Zhang et al., 2021; Liu et al., 2021b, Wu et al., 2021, Yuan et al., 2021, Wang et al., 2021; Yang et al., 2021) have worked on introducing a hierarchical structure in Transformers mainly in the domain of vision. The main goal of these works has been to introduce convolution-like hierarchies into Vision Transformers (Dosovitskiy et al. 2020). While these works progressively reduce the spatial resolution of the inputs in order to introduce hierarchies, we introduce hierarchies by adding another slow stream of information processing and without reducing the spatial resolution of the inputs. We also provision for the higher level of the hierarchy (i.e. the slow stream) to provide information to the lower levels as top-down conditioning which is not possible in any of the previous works. Top-Down Conditioning. Top-down information is information propagated from higher to lower levels of the network. It represents the models beliefs of the world and provides context for interpreting perceptual information.\n```\n\n#### 4. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.20)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.20)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state transitions hierarchical attention neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n#### 2. Document-Level Neural Machine Translation with Hierarchical Attention Networks\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, James Henderson*\n\n**TL;DR:** Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.\n\n**Abstract:** Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model\u2019s own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 258  (*Influential: 59*)\n\n#### 3. The expressive power of pooling in Graph Neural Networks\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*F. Bianchi, Veronica Lachi*\n\n**TL;DR:** This work derives sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it, which serve as a universal and theoretically grounded criterion for choosing among existing pooling operators or designing new ones.\n\n**Abstract:** In Graph Neural Networks (GNNs), hierarchical pooling operators generate local summaries of the data by coarsening the graph structure and the vertex features. While considerable attention has been devoted to analyzing the expressive power of message-passing (MP) layers in GNNs, a study on how graph pooling affects the expressiveness of a GNN is still lacking. Additionally, despite the recent advances in the design of pooling operators, there is not a principled criterion to compare them. In this work, we derive sufficient conditions for a pooling operator to fully preserve the expressive power of the MP layers before it. These conditions serve as a universal and theoretically grounded criterion for choosing among existing pooling operators or designing new ones. Based on our theoretical findings, we analyze several existing pooling operators and identify those that fail to satisfy the expressiveness conditions. Finally, we introduce an experimental setup to verify empirically the expressive power of a GNN equipped with pooling layers, in terms of its capability to perform a graph isomorphism test.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 21  (*Influential: 3*)\n\n#### 4. Representing Long-Range Context for Graph Neural Networks with Global Attention\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*Zhanghao Wu, Paras Jain, Matthew A. Wright, Azalia Mirhoseini, Joseph E. Gonzalez, Ion Stoica*\n\n**TL;DR:** This work proposes the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel readout mechanism to obtain a global graph embedding, and suggests that purely-learning-based approaches without graph structure may be suitable for learning high-level, long- range relationships on graphs.\n\n**Abstract:** Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel\"readout\"mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 205  (*Influential: 30*)\n\n#### 5. DAG Matters! GFlowNets Enhanced Explainer For Graph Neural Networks\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*Wenqian Li, Yinchuan Li, Zhigang Li, Jianye Hao, Yan Pang*\n\n**TL;DR:** The GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its' reward, and proposes a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making the approach applicable in a large-scale setting.\n\n**Abstract:** Uncovering rationales behind predictions of graph neural networks (GNNs) has received increasing attention over the years. Existing literature mainly focus on selecting a subgraph, through combinatorial optimization, to provide faithful explanations. However, the exponential size of candidate subgraphs limits the applicability of state-of-the-art methods to large-scale GNNs. We enhance on this through a different approach: by proposing a generative structure -- GFlowNets-based GNN Explainer (GFlowExplainer), we turn the optimization problem into a step-by-step generative problem. Our GFlowExplainer aims to learn a policy that generates a distribution of subgraphs for which the probability of a subgraph is proportional to its' reward. The proposed approach eliminates the influence of node sequence and thus does not need any pre-training strategies. We also propose a new cut vertex matrix to efficiently explore parent states for GFlowNets structure, thus making our approach applicable in a large-scale setting. We conduct extensive experiments on both synthetic and real datasets, and both qualitative and quantitative results show the superiority of our GFlowExplainer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. DeepVS: A Deep Learning Based Video Saliency Prediction Approach\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*Minglang Qiao, Mai Xu, Zulin Wang, Tie Liu, Lai Jiang*\n\n**Abstract:** In this paper, we propose a novel deep learning based video saliency prediction method, named DeepVS. Specifically, we establish a large-scale eye-tracking database of videos (LEDOV), which includes 32 subjects' fixations on 538 videos. We find from LEDOV that human attention is more likely to be attracted by objects, particularly the moving objects or the moving parts of objects. Hence, an object-to-motion convolutional neural network (OM-CNN) is developed to predict the intra-frame saliency for DeepVS, which is composed of the objectness and motion subnets. In OM-CNN, cross-net mask and hierarchical feature normalization are proposed to combine the spatial features of the objectness subnet and the temporal features of the motion subnet. We further find from our database that there exists a temporal correlation of human attention with a smooth saliency transition across video frames. We thus propose saliency-structured convolutional long short-term memory (SS-ConvLSTM) network, using the extracted features from OM-CNN as the input. Consequently, the inter-frame saliency maps of a video can be generated, which consider both structured output with center-bias and cross-frame transitions of human attention maps. Finally, the experimental results show that DeepVS advances the state-of-the-art in video saliency prediction.\n\n**Proceeding:** eccv-2018-9\n\n**Published:** 2018-09-01\n\n\n\n#### 2. MATNet: Motion-Attentive Transition Network for Zero-Shot Video Object Segmentation\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*Ran; Shen, Shunzhou; Tao, Jianwu; Wang, Tianfei; Li, Zhou, Jianbing*\n\n**Abstract:** In this paper, we present a novel end-to-end learning neural network, i.e., MATNet, for zero-shot video object segmentation (ZVOS). Motivated by the human visual attention behavior, MATNet leverages motion cues as a bottom-up signal to guide the perception of object appearance. To achieve this, an asymmetric attention block, named Motion-Attentive Transition (MAT), is proposed within a two-stream encoder network to firstly identify moving regions and then attend appearance learning to capture the full extent of objects. Putting MATs in different convolutional layers, our encoder becomes deeply interleaved, allowing for close hierarchical interactions between object apperance and motion. Such a biologically-inspired design is proven to be superb to conventional two-stream structures, which treat motion and appearance independently in separate streams and often suffer severe overfitting to object appearance. Moreover, we introduce a bridge network to modulate multi-scale spatiotemporal features into more compact, discriminative and scale-sensitive representations, which are subsequently fed into a boundary-aware decoder network to produce accurate segmentation with crisp boundaries. We perform extensive quantitative and qualitative experiments on four challenging public benchmarks, i.e., DAVIS16, DAVIS17, FBMS and YouTube-Objects. Results show that our method achieves compelling performance against current state-of-the-art ZVOS methods. To further demonstrate the generalization ability of our spatiotemporal learning framework, we extend MATNet to another relevant task: dynamic visual attention prediction (DVAP). The experiments on two popular datasets (i.e., Hollywood-2 and UCF-Sports) further verify the superiority of our model.\n\n**Proceeding:** ieee-transactions-on-image-processing-2020-8\n\n**Published:** 2020-08-20\n\n\n\n#### 3. Image Captioning with Deep Bidirectional LSTMs\n\n*From Search Query: state transitions hierarchical attention neural networks*\n\n*Christoph Meinel, Christian Bartz, Haojin Yang, Cheng Wang*\n\n**Abstract:** This work presents an end-to-end trainable deep bidirectional LSTM\n(Long-Short Term Memory) model for image captioning. Our model builds on a deep\nconvolutional neural network (CNN) and two separate LSTM networks. It is\ncapable of learning long term visual-language interactions by making use of\nhistory and future context information at high level semantic space. Two novel\ndeep bidirectional variant models, in which we increase the depth of\nnonlinearity transition in different way, are proposed to learn hierarchical\nvisual-language embeddings. Data augmentation techniques such as multi-crop,\nmulti-scale and vertical mirror are proposed to prevent overfitting in training\ndeep models. We visualize the evolution of bidirectional LSTM internal states\nover time and qualitatively analyze how our models \"translate\" image to\nsentence. Our proposed models are evaluated on caption generation and\nimage-sentence retrieval tasks with three benchmark datasets: Flickr8K,\nFlickr30K and MSCOCO datasets. We demonstrate that bidirectional LSTM models\nachieve highly competitive performance to the state-of-the-art results on\ncaption generation even without integrating additional mechanism (e.g. object\ndetection, attention model etc.) and significantly outperform recent methods on\nretrieval task.\n\n**Published:** 2016-04-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating state transitions and hierarchical attention, here are some key points and references that can guide the researchers:\n\n## Hierarchical Attention Mechanisms\n\n### Multi-Level Attention Processing\nHierarchical attention mechanisms can be designed to process information at multiple levels, similar to the hierarchical policy networks described in the reinforcement learning context. This approach involves breaking down complex tasks into simpler subtasks, each handled by a different level of attention.\n\n- **Layered Attention**: Implementing different types of attention at various layers of the model can help address different processing needs. For example, using diagonal attention for local structure preservation in early layers and normalized attention for stability in later layers.\n\n### Efficient Attention Computation\nTo improve efficiency, block-based processing can be employed. This involves handling intra-block and inter-block components separately in linear attention calculations, which can reduce computational complexity and improve resource utilization.\n\n## State Transitions and Management\n\n### Dynamic State Sizing\nDynamic state sizing based on input complexity can enhance the adaptability of the model. This involves adjusting the state size dynamically to handle varying input lengths and complexities, ensuring efficient parallel processing[Analysis Note - Round 3].\n\n### Integration with Memory Mechanisms\nIntegrating memory mechanisms with attention can facilitate better state transitions. Recurrent neural networks (RNNs), particularly those with long short-term memory (LSTM) or other gated architectures, are well-suited for managing state transitions and maintaining context over long sequences.\n\n- **LSTM and Gated Mechanisms**: These mechanisms prevent the vanishing gradient problem and allow the model to learn from past inputs, incorporating that knowledge into current processing. This is crucial for maintaining stable and efficient state transitions.\n\n## Stability and Information Flow\n\n### Normalization and Stabilization\nNormalization techniques, such as those used in linear transformers, can help stabilize training gradients and improve the overall stability of the model. This is particularly important when combining different attention types at various stages of the model.\n\n### Hierarchical Processing\nHierarchical processing can enhance information flow between levels by ensuring that each level of attention processes relevant information before passing it to the next level. This hierarchical structure can be inspired by the hierarchical policy networks, where each level decomposes tasks into simpler subtasks.\n\n## Implementation Considerations\n\n### Hardware-Aware Design\nHardware-aware implementations are crucial for optimizing performance. Techniques such as tiling and sparse attention patterns can improve resource utilization and reduce computational overhead[Analysis Note - Round 3].\n\n### Combining Hierarchical and Linear Attention\nCombining hierarchical attention with linear attention mechanisms can provide a balanced approach. Hierarchical attention can handle long-range dependencies and complex structures, while linear attention can efficiently process local and sequential information.\n\nBy integrating these concepts, the researchers can develop an autoregressive language model that leverages hierarchical attention for better information flow, dynamic state management for adaptability, and efficient computational strategies for stability and performance.\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\nAfter analyzing attention mechanisms and adaptive processing approaches, several promising directions emerge for improving the seed design:\n\na) Attention Integration:\n- Linear attention with normalization shows better stability\n- Diagonal attention helps with local structure preservation\n- Adaptive attention patterns can improve efficiency\n\nb) State Management:\n- Dynamic state sizing based on input complexity\n- Efficient parallel processing through careful structuring\n- Integration of memory mechanisms with attention\n\nc) Scalability Solutions:\n- Sparse attention patterns for efficiency\n- Hardware-aware implementations\n- Tiling techniques for better resource utilization\n\n2. Useful References with Excerpts:\n\nFrom \"The Devil in Linear Transformer\":\n\"Our network consists of two types of attention: DiagAttention for the early stage of the model and NORMATTENTION for the later stage. The former addresses the attention dilution issue and the later aims to stabilize training gradients.\"\n- Key insight: Combining different attention types for different processing stages\n\nFrom \"Lightning Attention-2\":\n\"We leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation.\"\n- Key insight: Efficient attention computation through block-based processing\n\nFrom \"Understanding the differences in Foundation Models\":\n\"The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models.\"\n- Key insight: Framework for comparing different architectural approaches\n\n3. Key Insights and Analysis:\n\na) Attention Structure:\n- Multi-stage attention processing can address different needs\n- Normalization helps stabilize gradients\n- Block-based processing improves efficiency\n\nb) Memory Integration:\n- Dynamic state management improves adaptability\n- Hierarchical processing helps with information flow\n- Need for efficient state transitions\n\nc) Implementation Considerations:\n- Hardware-aware design is crucial for performance\n- Tiling strategies can improve resource utilization\n- Need to balance complexity with efficiency\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for efficient state transitions\n2. Methods for combining hierarchical and linear attention\n3. Approaches for dynamic parameter adaptation\n\n5. References:\n\n[1] Qin, Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N., & Zhong, Y. (2022). The Devil in Linear Transformer. Conference on Empirical Methods in Natural Language Processing.\n\n[2] Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n[3] Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint.\n\n[4] Wu, X., Ajorlou, A., Wu, Z., & Jadbabaie, A. (2023). Demystifying Oversmoothing in Attention-Based Graph Neural Networks. Neural Information Processing Systems.\n\n[5] Lee, S. Y., Bu, F., Yoo, J., & Shin, K. (2023). Towards Deep Attention in Graph Neural Networks: Problems and Remedies. International Conference on Machine Learning.\n\nBased on this analysis, we need another round of search focusing on state transitions and hierarchical attention integration to develop a more comprehensive understanding before formulating the proposal."
        },
        {
            "ready": false,
            "query": "selective state space models hierarchical attention integration",
            "detail": "Analyze implementations and architectures combining selective state space models with hierarchical attention, focusing on efficiency, stability, and information flow between processing levels.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations and architectures combining selective state space models with hierarchical attention, focusing on efficiency, stability, and information flow between processing levels.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.95)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.97)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.94)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.78)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.78)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.70)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.70)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.68)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 3/29 (Score: 0.68)*\n\n```\nWe use superscripts, e.g. . ${ }^{d}$, to denote the elements or block-elements of a matrix and a block-matrix. We use subscripts, e.g. ${ }_{i}$, to denote the time index (or input dependency). Specifically, $v_{i}$ represents the value of vector $v$ at time $i$. We use bold notation to indicate sequences, i.e., $\\mathbf{v}_{i}=\\left[v_{1}, \\ldots, v_{i}\\right]$. We use $\\sigma(\\cdot)$ to denote is the sigmoid function. The products $\\odot$ and $\\otimes$ denote the Hadamard (element-wise) product and the Kronecker (block-wise) product, respectively. $\\mathbb{I}_{n}$ denotes the identity matrix of size $\\mathbb{R}^{n \\times n}$. Generally, we omit stating the bias term for weight matrices unless stating the bias term helps with clarity. ## 2 Preliminaries\n\nIn this section, we introduce the key architectural components studied in this work: attention, SSMs, and RNNs. We remark that these components are often the central block - considered to be the backbone - within a complex architecture composed of other blocks and skip connections (see for instance Touvron et al., 2023]). In what follows, we review exclusively the backbone block, which\nwe denote as $f(\\cdot)$ in $\\mathbf{y}=f(\\mathbf{u})$, where $\\mathbf{u} \\in \\mathbb{R}^{L \\times d}$ and $\\mathbf{y} \\in \\mathbb{R}^{L \\times d}$ are the input and output sequences, respectively. ### 2.1 Attention\n\nThe standard self-attention block [Vaswani et al. 2017] consists of three matrices: $W_{Q}, W_{K}$, and $W_{V}$, which are the learnt parameters of the model. These matrices, when multiplied with the input $\\mathbf{u}$, yield the queries $\\mathbf{q} \\in \\mathbb{R}^{d_{k}}$, keys $\\mathbf{k} \\in \\mathbb{R}^{d_{k}}$, and values $\\mathbf{v} \\in \\mathbb{R}^{d_{v}}$, respectively:\n\n$$\n\\mathbf{q}=\\mathbf{u} W_{Q}, \\quad \\mathbf{k}=\\mathbf{u} W_{K}, \\quad \\mathbf{v}=\\mathbf{u} W_{V}\n$$\n\nKeys, queries, and values are then combined in the attention block to produce the output\n\n$$\n\\mathbf{y}=\\zeta\\left(\\frac{\\mathbf{q} \\mathbf{k}^{\\top}}{\\sqrt{d_{k}}}\\right) \\mathbf{v}\n$$\n\nwhere $\\zeta(\\cdot)$ is a map $\\mathbb{R}^{L} \\rightarrow \\mathbb{R}^{L}$ and is applied row-wise. In the standard version of attention - softmax attention $-\\zeta(\\cdot):=\\operatorname{softmax}(\\cdot)$. However, given the limitations of the softmax function, alternative formulations have been proposed. In this work, we consider two formulations of attention: softmax attention (2) and linear attention [Katharopoulos et al. 2020]. We focus on the masked attention formulations, i.e., the attention matrix $\\zeta\\left(\\mathbf{q k}^{\\top}\\right)$ has a lower-triangular structure. Furthermore, in order to simplify the derivations, we drop the scaling factor $\\sqrt{d_{k}}$. ### 2.2 State Space Models\n\nArchitectures based on a state space parametrization compute the output y through a dynamic recurrence of input signals at each time step $i$,\n\n$$\n\\begin{aligned}\nh_{i} & =A_{i} h_{i-1}+B_{i} u_{i} \\\\\ny_{i} & =C_{i} h_{i}+D_{i} u_{i}\n\\end{aligned}\n$$\n\nwhere $h_{i}$ is the hidden state of the system, and the dynamic matrices of appropriate dimensions $A_{i}, B_{i}, C_{i}, D_{i}$ are the learnt parameters of the model. Different time-varying and time-invariant parameterizations for $A_{i}, B_{i}, C_{i}, D_{i}$ have been proposed in the literature (an overview is given in (Amo Alonso et al., 2024]).\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective state space models hierarchical attention integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: selective state space models hierarchical attention integration*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: selective state space models hierarchical attention integration*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n#### 3. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space models hierarchical attention integration*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Generation and Extraction Combined Dialogue State Tracking with Hierarchical Ontology Integration\n\n*From Search Query: selective state space models hierarchical attention integration*\n\n*Xinmeng Li, Qian Li, Wansen Wu, Quanjun Yin*\n\n**TL;DR:** In state value decoding stage, the model explores the hierarchical semantic of ontology and enhances the interrelation between slots with masked hierarchical attention and solves the out-of-vocabulary problem by combining generation method and extraction method together.\n\n**Abstract:** Recently, the focus of dialogue state tracking has expanded from single domain to multiple domains. The task is characterized by the shared slots between domains. As the scenario gets more complex, the out-of-vocabulary problem also becomes severer. Current models are not satisfactory for solving the challenges of ontology integration between domains and out-of-vocabulary problems. To address the problem, we explore the hierarchical semantic of ontology and enhance the interrelation between slots with masked hierarchical attention. In state value decoding stage, we solve the out-of-vocabulary problem by combining generation method and extraction method together. We evaluate the performance of our model on two representative datasets, MultiWOZ in English and CrossWOZ in Chinese. The results show that our model yields a significant performance gain over current state-of-the-art state tracking model and it is more robust to out-of-vocabulary problem compared with other methods.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 5. Hierarchical Integration Diffusion Model for Realistic Image Deblurring\n\n*From Search Query: selective state space models hierarchical attention integration*\n\n*Zheng Chen, Yulun Zhang, Ding Liu, Bin Xia, Jinjin Gu, L. Kong, X. Yuan*\n\n**TL;DR:** The Hierarchical Integration Diffusion Model (HI-Diff) is proposed, which designs the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios.\n\n**Abstract:** Diffusion models (DMs) have recently been introduced in image deblurring and exhibited promising performance, particularly in terms of details reconstruction. However, the diffusion model requires a large number of inference iterations to recover the clean image from pure Gaussian noise, which consumes massive computational resources. Moreover, the distribution synthesized by the diffusion model is often misaligned with the target results, leading to restrictions in distortion-based metrics. To address the above issues, we propose the Hierarchical Integration Diffusion Model (HI-Diff), for realistic image deblurring. Specifically, we perform the DM in a highly compacted latent space to generate the prior feature for the deblurring process. The deblurring process is implemented by a regression-based method to obtain better distortion accuracy. Meanwhile, the highly compact latent space ensures the efficiency of the DM. Furthermore, we design the hierarchical integration module to fuse the prior into the regression-based model from multiple scales, enabling better generalization in complex blurry scenarios. Comprehensive experiments on synthetic and real-world blur datasets demonstrate that our HI-Diff outperforms state-of-the-art methods. Code and trained models are available at https://github.com/zhengchen1999/HI-Diff.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 39  (*Influential: 6*)\n\n### 1 related papers from Papers with Code\n\n#### 1. Mamba-Spike: Enhancing the Mamba Architecture with a Spiking Front-End for Efficient Temporal Data Processing\n\n*From Search Query: selective state space models hierarchical attention integration*\n\n*Feng Liu, Jiahao Qin*\n\n**Abstract:** The field of neuromorphic computing has gained significant attention in recent years, aiming to bridge the gap between the efficiency of biological neural networks and the performance of artificial intelligence systems. This paper introduces Mamba-Spike, a novel neuromorphic architecture that integrates a spiking front-end with the Mamba backbone to achieve efficient and robust temporal data processing. The proposed approach leverages the event-driven nature of spiking neural networks (SNNs) to capture and process asynchronous, time-varying inputs, while harnessing the power of the Mamba backbone's selective state spaces and linear-time sequence modeling capabilities to model complex temporal dependencies effectively. The spiking front-end of Mamba-Spike employs biologically inspired neuron models, along with adaptive threshold and synaptic dynamics. These components enable efficient spatiotemporal feature extraction and encoding of the input data. The Mamba backbone, on the other hand, utilizes a hierarchical structure with gated recurrent units and attention mechanisms to capture long-term dependencies and selectively process relevant information. To evaluate the efficacy of the proposed architecture, a comprehensive empirical study is conducted on both neuromorphic datasets, including DVS Gesture and TIDIGITS, and standard datasets, such as Sequential MNIST and CIFAR10-DVS. The results demonstrate that Mamba-Spike consistently outperforms state-of-the-art baselines, achieving higher accuracy, lower latency, and improved energy efficiency. Moreover, the model exhibits robustness to various input perturbations and noise levels, highlighting its potential for real-world applications. The code will be available at https://github.com/ECNU-Cross-Innovation-Lab/Mamba-Spike.\n\n**Published:** 2024-08-04\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by combining selective state space models (SSMs) with hierarchical attention, focusing on efficiency, stability, and information flow between processing levels, here are some key insights and relevant information:\n\n## Combining SSMs with Hierarchical Attention\n\n### Spatial-Mamba and Hierarchical Processing\nThe Spatial-Mamba model, as described in, integrates selective state space models with a structure-aware state fusion mechanism. This approach can be adapted for hierarchical attention by:\n- Using the state transition equations of SSMs to process sequential data.\n- Employing a structure-aware state fusion equation, which leverages dilated convolutions to capture spatial or contextual dependencies, similar to how hierarchical attention captures multi-level dependencies.\n- This method enhances the flow of contextual information, which is crucial for efficient information flow between processing levels.\n\n### Multi-Level Attention Integration\nThe concept of Hierarchical Attention (HAT) introduced in FasterViT can be integrated with SSMs. HAT decomposes global self-attention into multi-level attention, reducing computational costs. By combining this with SSMs:\n- Different attention types can be used at different stages of the hierarchical processing, improving content-based reasoning and reducing computational complexity.\n- This multi-level attention can be applied in conjunction with the selective mechanisms of SSMs to enhance the model's ability to capture long-range dependencies efficiently.\n\n## Efficient State Transitions and Dynamic Adaptation\n\n### State Space Duality and Parameter Adaptation\nThe Mamba2 model introduces the concept of State Space Duality (SSD), simplifying the matrix representation to a scalar, which improves adaptability and performance.\n- This simplification can facilitate dynamic parameter adaptation based on input complexity, allowing the model to adjust its state dimensionality and memory mechanisms efficiently.\n- Integrating this with hierarchical attention can ensure that the model adapts its attention mechanisms at different processing levels based on the input.\n\n### Efficient Processing and Information Flow\nThe SC-MAMBA2 model, which leverages SSMs for single-cell ultra-long transcriptome modeling, demonstrates efficient batch integration and processing.\n- Similar techniques can be applied to autoregressive language models to ensure efficient state transitions and information flow between different levels of hierarchical attention.\n- The use of convolutional kernels and discrete state space formulations can help in achieving this efficiency while maintaining the model's stability.\n\n## Implementation Strategy\n\n### Hardware-Aware Design and Parallel Processing\nTo achieve better performance, a hardware-aware design is crucial. By structuring the model to take advantage of parallel processing opportunities, the computational costs can be significantly reduced.\n- Implementing SSMs with hierarchical attention in a way that allows for efficient parallel processing can enhance the overall efficiency and scalability of the model.\n\n### Balance Between Complexity and Efficiency\nThe balance between model complexity and efficiency is critical. Techniques such as selective state space mechanisms and multi-level attention decomposition help in achieving this balance by reducing computational costs while maintaining or improving performance.\n\nIn summary, combining selective state space models with hierarchical attention involves:\n- Integrating structure-aware state fusion mechanisms with multi-level attention to enhance contextual information flow.\n- Utilizing State Space Duality and dynamic parameter adaptation to improve adaptability and efficiency.\n- Ensuring efficient state transitions and information flow through convolutional and discrete state space formulations.\n- Implementing hardware-aware designs to leverage parallel processing opportunities.\n\nThese approaches can help in developing a more comprehensive and efficient autoregressive language model.\n",
            "analysis": "Analysis Note - Round 4\n\n1. Summary of Analysis:\nAfter analyzing the latest search results, several key insights emerge for improving the seed design:\n\na) State Space Integration:\n- SSMs offer efficient sequence processing with linear complexity\n- Can be enhanced through selective mechanisms and spectral filtering\n- Need careful consideration of state transitions and memory management\n\nb) Hierarchical Processing:\n- Multi-level attention processing improves information flow\n- Different attention types can be used at different stages\n- Need for efficient integration between levels\n\nc) Dynamic Adaptation:\n- State dimensionality can adapt based on input complexity\n- Memory mechanisms can be integrated with attention\n- Need for efficient parallel processing strategies\n\n2. Useful References with Excerpts:\n\nFrom \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\"\n- Key insight: Selective SSMs improve content-based reasoning\n\nFrom \"Understanding the differences in Foundation Models\":\n\"The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models.\"\n- Key insight: Framework for comparing different architectural approaches\n\nFrom \"FasterViT\":\n\"Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs.\"\n- Key insight: Multi-level attention decomposition improves efficiency\n\n3. Key Insights and Analysis:\n\na) State Space Design:\n- Selective SSMs provide better content-based reasoning\n- Need for careful parameter adaptation based on input\n- Opportunity to combine with hierarchical processing\n\nb) Attention Integration:\n- Multi-level attention can reduce computational costs\n- Different attention types for different processing stages\n- Need for efficient information flow between levels\n\nc) Implementation Strategy:\n- Hardware-aware design crucial for performance\n- Parallel processing opportunities through careful structuring\n- Balance between complexity and efficiency\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for combining SSMs with hierarchical attention\n2. Methods for efficient state transitions in multi-level processing\n3. Approaches for dynamic parameter adaptation\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[2] Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint.\n\n[3] Hatamizadeh, A., Heinrich, G., Yin, H., Tao, A., \u00c1lvarez, J., Kautz, J., & Molchanov, P. (2023). FasterViT: Fast Vision Transformers with Hierarchical Attention. International Conference on Learning Representations.\n\n[4] Wu, Z., Jain, P., Wright, M. A., Mirhoseini, A., Gonzalez, J. E., & Stoica, I. (2022). Representing Long-Range Context for Graph Neural Networks with Global Attention. Neural Information Processing Systems.\n\n[5] Didolkar, A., Gupta, K., Goyal, A., Lamb, A., Ke, N. R., & Bengio, Y. (2022). Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning. Neural Information Processing Systems.\n\nBased on this analysis, we need another round of search focusing on combining SSMs with hierarchical attention and efficient state transitions to develop a more comprehensive understanding before formulating the proposal."
        },
        {
            "ready": false,
            "query": "Spectral filtering neural networks",
            "detail": "Spectral filtering autoregressive models feature representation",
            "search_ret": "\n---\n## Found 1 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSpectral filtering autoregressive models feature representation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 1 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.63)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 14/31 (Score: 0.86)*\n\n```\nWe provide further details about this study after this section. Spectral filtering. The technique of spectral filtering for learning linear dynamical systems was put forth in [HSZ17]. This work studies online prediction of the sequence of observations $y_{t}$, and the goal is to predict as well as the best symmetric LDS using past inputs and observations. Directly\nlearning the dynamics is a non-convex optimization problem, and spectral filtering is developed as an improper learning technique with an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in $\\mathrm{HLS}^{+} 18$. Convolutional Models for Sequence Modeling Exploiting the connnection between Linear dynamical systems and convolutions (as highlighted by [GGR21]) various convolutional models have been proposed for sequence modelling. [FEN ${ }^{+}$23] employ direct learning of convolutional kernels directly to sequence modelling but find that they underperform SSMs. They find the non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations to the kernels to match performance on the Long Range Arena benchmark. The proposed model still contains significantly large number of parameters growing with the sequence length. [ $\\left.\\mathrm{LCZ}^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. They achieve this via a specific form of the kernel derived by repeating and scaling the kernel in a dyadic fashion. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1] by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left\\lfloor\\mathrm{LCZ}^{+} 22\\right\\rfloor$ and $\\left[\\mathrm{FEN}^{+}\\right.$23]. ## A. 1 Ablations performed by OSG $^{+}$23]\n\nMotivated by the success of SSMs, $\\left[\\mathrm{OSG}^{+} 23\\right]$ revisit the RNN model (under the same deep stacked structure as SSMs) to investigate their efficiency. They begin from a simple linear RNN (a directly parameterized LDS) and add multiple components inspired from the SSM literature to ensure numerical stability and trainability of the model especially as the sequences grow larger. Overall they demonstrate that carefully designed parameterizations and initializations of LDS parameters as well as specifically designed normalizations are all necessary for model to learn consistently over the LRA dataset and in particularly over the 16 K context length task PathX. These interventions are driven by specific intuitions such as an inductive bias towards larger memory or controlling the loss blowup at initialization under long contexts but as such come with no theoretical guarantees towards alleviating the problem. We provide some quick details towards what these interventions are and refer the reader to $\\left[\\mathrm{OSG}^{+} 23\\right]$ to understand the motivations behind them and comparisons with similar ideas existing in previous SSM literature. The LRU model considered by $\\left[\\mathrm{OSG}^{+} 23\\right]$ is given by\n\n$$\ny_{k}=\\operatorname{diag}(\\lambda) y_{k-1}+\\gamma \\odot B u_{k}\n$$\n\nIn the above the learned parameters are $\\lambda$ and $B$ and note that $\\operatorname{diag}(\\lambda)$ corresponds to a diagonal $A$. $\\gamma$ is a specific normalization technique they develop to control the loss blowup under long-context detailed below. They perform the following interventions towards stable training\n\n- Stable Exponential Parameterization: They parameterize $\\lambda$ as\n\n$$\n\\lambda_{j}=\\underbrace{\\exp \\left(-\\exp \\left(\\nu_{j}^{\\log }\\right)\\right.}_{\\text {magnitude }}+i \\underbrace{\\exp \\left(\\theta_{j}^{\\log }\\right)}_{\\text {phase }})\n$$\n\nThe above is done to ensure a bound on the magnitude of eigenvalues of the effective A matrix as well as to ensure more resolution in the parameter space closer to the value of 1 . - Ring Initialization: They initialize the $\\lambda_{j}$ in the complex annulus [min_rad, max_rad]. This ensures that at initialization the magnitude of $\\lambda_{j}$ chosen randomly lies in $\\in$ [min_rad, max_rad] and the phase is chosen randomly. When not applying this intervention min_rad and max_rad are chosen to be 0,1 respectively. When applying this intervention these values are chosen to be closer to 1 , e.g. $0.9,0.999$ respectively. - $\\gamma$-Normalization: They set $\\gamma_{j}=\\sqrt{1-\\left|\\lambda_{j}\\right|^{2}}$\n- Restricting Phase at initialization: Instead of drawing a random phase at initialization the authors recommend selecting the initial phase from $[0, \\pi / 10]$. The authors claim that uniform phase inherently biases the network towards learning spurious features in the input sequence. $\\left[\\mathrm{OSG}^{+} 23\\right]$ provide the following ablation in the paper. In particular we see that all the above interventions are necessary to make the model get to non-trivial accuracy on PathX. On the contrary, as we show the STU model achieves comparable accuracy without requiring any specific initialization or normalization. | Model | Specification | sCIFAR | ListOps | Pathfinder | PathX |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| LRU | Dense A | 72.2 | 50.4 | $\\boldsymbol{X}$ | $\\boldsymbol{X}$ |\n|  | $\\Lambda$ Exp.\n```\n\n##### *Relevant Chunk: No. 3/31 (Score: 0.71)*\n\n```\nWe discuss the details of this ablation in the appendix (Section A.1). Spectral filtering. The technique of spectral filtering [HSZ17] was developed as a convex improper learning alternative to directly parameterizing an LDS (as in the case of SSMs) leading to an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods (eg. SSMs) that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in [HLS ${ }^{+}$18]. Convolutional Models for Sequence Modeling. Exploiting the connnection between LDS and convolutions [GGR21], various convolutional models have been proposed for sequence modelling. $\\left[\\mathrm{FEN}^{+}\\right.$23] employ direct learning of convolutional kernels but find that they underperform SSMs, identifying non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations. [LCZ $\\left.{ }^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1) by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left[\\mathrm{LCZ}^{+} 22\\right]$ and $\\left[\\mathrm{FEN}^{+} 23\\right]$. ## 2 Preliminaries\n\nSequence prediction. We treat sequence prediction as a game between a predictor/learner and nature in which iteratively at every time $t \\in[L]$, the learner is presented an input $u_{t} \\in \\mathbb{R}^{d_{\\text {in }}}$. The learner $A$ then produces a candidate output $\\hat{y}_{t}=\\hat{y}_{t}(A)$, and nature reveals the $t^{t h}$ element of a target sequence $y_{t} \\in \\mathbb{R}^{d_{\\text {out }}}$. The learner then suffers an instantaneous loss of $\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}$. The task of the learner is to minimize regret over a benchmark set of learning algorithms $\\mathcal{A}$, defined as follows\n\n$$\n\\text { Regret }=\\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}-\\min _{A \\in \\mathcal{A}} \\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}(A)\\right\\|^{2}\n$$\n\nLinear Dynamical Systems (LDS): An example benchmark set of methods is that of a linear dynamical system, which has four matrix parameters, $A \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\text {in }}}, C \\in \\mathbb{R}^{d_{\\text {out }} \\times N}, D \\in$ $\\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$. The system evolves and generates outputs according to the following equations\n\n$$\nx_{t} \\triangleq A x_{t-1}+B u_{t}, \\quad \\hat{y}_{t} \\triangleq C x_{t}+D u_{t}\n$$\n\nThus, an example class of benchmark algorithms $\\mathcal{A}$ are all predictors that generate $\\hat{y}_{t}$ according to these rules, for a fixed set of matrices $A, B, C, D$.\n```\n\n##### *Relevant Chunk: No. 7/31 (Score: 0.68)*\n\n```\nIn our experiments we search over two values of $k_{y}=\\{2,32\\}$. For non-image tasks, ListOps, Text and Retrieval, we find that setting $k_{y}=2$ is sufficient to get optimal results. For the image tasks, CIFAR, Pathfinder and PathX, we found that $k_{y}=32$ led to significant performance gains. A performance ablation over this parameter can be found in the appendix (Table 2]. Overall we find that the STU model provides improvements over baselines such as S4 and LRU on 4 out of the 6 tasks and performs comparably to the best baseline on the others. Remarkably, the STU layers come with provable guarantees and thus performs well out of the box without the need for specific initializations, discretizations or normalizations. We initialize all parameters $M_{i}^{y}, M_{i}^{u}, M_{k}^{\\phi+}, M_{k}^{\\phi-}$ with 0 . We provide details of the experimental setup, including hyperparameter tuning in the appendix (Section E). ## 6 Conclusion\n\nInsprired by the success of SSMs, we present a new theoretically-founded deep neural network architecture, Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for\nlearning Linear Dynamical Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz. robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark. We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without the need for designing specific initializations, discretizations or normalizations which were necessary for existing SSMs to learn in such settings. While spectral SSMs only model symmetric A, our presented set of experiments on the LRA benchmark suggest that the gap between symmetric and general A is potentially small in real world tasks. Indeed more recent SSM models like [GD23, $\\mathrm{DSF}^{+}$24] work with real diagonals (i.e. symmetric case) as they do not find evidence that adding complex eigenvalues help. Spectral filtering has been extended in certain settings to asymmetric A [HLS ${ }^{+}$18] and a similar extension to our proposal is straightforward but comes with efficiency losses and we leave it to future work. ## References\n\n[ASB16] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120-1128. PMLR, 2016. [Ble89] Guy E Blelloch. Scans as primitive parallel operations. IEEE Transactions on computers, 38(11):1526-1538, 1989. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [BSF94] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994. [CVMG ${ }^{+}$14] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. $\\left[\\mathrm{DBK}^{+}\\right.$20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.56)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 16/31 (Score: 0.32)*\n\n```\n+ Phase Init. | 89.0 | 60.2 | 95.1 | 94.2 |\n\n## B Computational complexity and comparison to other methods. Using the STU method to make a sequence of $L$ predictions, the features $U^{+}, U^{-} \\in \\mathbb{R}^{L \\times d_{\\text {in }} \\times K}$ can be computed in time $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\log (L)\\right)$ using the Discrete Fast Fourier Transform, where $K$ is the number of filters and $L$ is the context length. The linear prediction part (i.e. spectral component) takes $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$ time, and the autoregressive part can be implemented in total time $O\\left(L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$. Therefore the overall runtime is $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot\\left(\\log (L)+d_{\\text {out }}\\right)\\right) \\cdot{ }^{4}$\nFor comparison, consider LRU and transformers. The same computation carried out by LRU w. diagonal system matrices is dominated by the hidden dimension, i.e. $O\\left(L \\cdot d_{\\text {hidden }} \\cdot\\left(d_{\\text {in }}+d_{\\text {out }}\\right)\\right)$. Thus, the number of filters is replaced by $d_{\\text {hiden }}$, which is usually an order of magnitude larger, although STU has another $O(\\log L)$ overhead. A transformer model with full attention runs in time $O\\left(L^{2} d_{\\text {in }} d_{\\text {out }}\\right)$, which is significantly more costly than both LRU and STU. This is consistent with the motivation of SSM as more efficient models for sequences. ## C Proof of Theorem 3.1\n\nWe begin by observing that without loss of generality we can assume that $A$ is a real-diagonal matrix. This can be ensured by performing a spectral decomposition of $A=U \\Sigma U^{\\top}$ and absorbing the $U, U^{\\top}$ by redefining the system. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define the following vector for any $\\alpha \\in \\mathbb{R}, \\mu(\\alpha) \\in \\mathbb{R}^{L}$, with $\\mu(\\alpha)(i)=(\\alpha-1) \\alpha^{i-1}$. Further define the Hankel matrix $H$ as\n\n$$\nZ \\triangleq \\int_{0}^{1} \\mu(\\alpha) \\mu(\\alpha)^{\\top} d \\alpha\n$$\n\nAs the following lemma shows the Hankel matrix $Z$ above is the same Hankel matrix defined in the definition of STU (3). Lemma C.1. $Z$ is a Hankel matrix with entries given as\n\n$$\nZ(i, j)=\\frac{2}{(i+j)^{3}-(i+j)}\n$$\n\n[^3]Lemma C.2. We have that the following statements hold regarding $\\mu(\\alpha)$ for any $\\alpha \\in[0,1]$,\n\n- $|\\mu(\\alpha)|^{2} \\leq 1$\n- For any $\\alpha \\in[0,1]$ and any unit vector $v$ we have that\n\n$$\n\\left(\\mu(\\alpha)^{\\top} v\\right)^{2} \\leq 12\\left(v^{\\top} H v\\right)\n$$\n\nLemma C.3. For any $\\alpha \\in[0,1]$, let $\\tilde{\\mu}(\\alpha)$ be the projection of $\\mu(\\alpha)$ on the subspace spanned by top $k$ eigenvectors of $Z$, then we have that\n\n$$\n\\|\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\|^{2} \\leq 12 \\sum_{i=k+1}^{L} \\sigma_{i}\n$$\n\nFinally the following lemma from HSZ17] shows that the spectrum of the matrix $Z$ decays exponentially.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Spectral filtering neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Specformer: Spectral Graph Neural Networks Meet Transformers\n\n*From Search Query: Spectral filtering neural networks*\n\n*Deyu Bo, Chuan Shi, Lele Wang, Renjie Liao*\n\n**TL;DR:** This work introduces Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter and design a decoder with learnable bases to enable non-local graph convolution.\n\n**Abstract:** Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 57  (*Influential: 10*)\n\n#### 2. How Powerful are Spectral Graph Neural Networks\n\n*From Search Query: Spectral filtering neural networks*\n\n*Xiyuan Wang, Muhan Zhang*\n\n**TL;DR:** It is proved that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality, and JacobiConv is proposed, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions.\n\n**Abstract:** Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 144  (*Influential: 29*)\n\n#### 3. The Spectral Bias of Polynomial Neural Networks\n\n*From Search Query: Spectral filtering neural networks*\n\n*Moulik Choraria, L. Dadi, Grigorios G. Chrysos, J. Mairal, V. Cevher*\n\n**TL;DR:** A spectral analysis of the Neural Tangent Kernel (NTK) of PNNs finds that the Pi-Net family, i.e., a recently proposed parametrization of Pnns, speeds up the learning of the higher frequencies.\n\n**Abstract:** Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\\textit{spectral bias}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 4. Spectral evolution and invariance in linear-width neural networks\n\n*From Search Query: Spectral filtering neural networks*\n\n*Zhichao Wang, A. Engel, A. Sarwate, Ioana Dumitriu, Tony Chiang*\n\n**TL;DR:** The results show that monitoring the evolution of the spectra during training is an important step toward understanding the training dynamics and feature learning.\n\n**Abstract:** We investigate the spectral properties of linear-width feed-forward neural networks, where the sample size is asymptotically proportional to network width. Empirically, we show that the weight spectra in this high dimensional regime are invariant when trained by gradient descent for small constant learning rates and the changes in both operator and Frobenius norm are $\\Theta(1)$ in the limit. This implies the bulk spectra for both the conjugate and neural tangent kernels are also invariant. We demonstrate similar characteristics for models trained with mini-batch (stochastic) gradient descent with small learning rates and provide a theoretical justification for this special scenario. When the learning rate is large, we show empirically that an outlier emerges with its corresponding eigenvector aligned to the training data structure. We also show that after adaptive gradient training, where we have a lower test error and feature learning emerges, both the weight and kernel matrices exhibit heavy tail behavior. Different spectral properties such as invariant bulk, spike, and heavy-tailed distribution correlate to how far the kernels deviate from initialization. To understand this phenomenon better, we focus on a toy model, a two-layer network on synthetic data, which exhibits different spectral properties for different training strategies. Analogous phenomena also appear when we train conventional neural networks with real-world data. Our results show that monitoring the evolution of the spectra during training is an important step toward understanding the training dynamics and feature learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 12  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: Spectral filtering neural networks*\n\n*Micha\u00ebl Defferrard, Pierre Vandergheynst, Xavier Bresson*\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n\n**Conference:** convolutional-neural-networks-on-graphs-with-1\n\n**Published:** 2016-06-30\n\n\n\n#### 2. Deep Convolutional Networks on Graph-Structured Data\n\n*From Search Query: Spectral filtering neural networks*\n\n*Yann Lecun, Joan Bruna, Mikael Henaff*\n\n**Abstract:** Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate.\n\n**Published:** 2015-06-16\n\n\n\n#### 3. Spectral Networks and Locally Connected Networks on Graphs\n\n*From Search Query: Spectral filtering neural networks*\n\n*Yann Lecun, Joan Bruna, Arthur Szlam, Wojciech Zaremba*\n\n**Abstract:** Convolutional Neural Networks are extremely efficient architectures in image\nand audio recognition tasks, thanks to their ability to exploit the local\ntranslational invariance of signal classes over their domain. In this paper we\nconsider possible generalizations of CNNs to signals defined on more general\ndomains without the action of a translation group. In particular, we propose\ntwo constructions, one based upon a hierarchical clustering of the domain, and\nanother based on the spectrum of the graph Laplacian. We show through\nexperiments that for low-dimensional graphs it is possible to learn\nconvolutional layers with a number of parameters independent of the input size,\nresulting in efficient deep architectures.\n\n**Published:** 2013-12-21\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using spectral filtering, here are some key insights and relevant information from the provided sources and additional context:\n\n## Spectral Filtering in Neural Networks\n\n### Robustness and Efficiency\nSpectral filtering has been shown to be robust and efficient in handling long-range dependencies, which is crucial for autoregressive language models. For instance, spectral state space models are known for their provable robustness properties, as they do not depend on the spectrum of the underlying dynamics or the dimensionality of the problem.\n\n### Application in Autoregressive Models\nThe integration of spectral filtering in autoregressive models can enhance feature representation by capturing both global and local frequency domain information. A study on deep autoregressive models with spectral attention highlights how this approach can identify global trends and seasonality patterns in time series data, which can be analogous to capturing long-range dependencies in language models.\n\n### Spectral Attention Mechanism\nThe use of a Spectral Attention module, as proposed in \"Deep Autoregressive Models with Spectral Attention,\" can merge global and local frequency domain information in the model's embedded space. This mechanism can be adapted for language models to better capture contextual relationships and long-range dependencies.\n\n## Spectral Filtering in Graph Neural Networks\n\nWhile the context is different, the application of spectral filtering in graph neural networks (GNNs) provides insights into how spectral methods can be used to capture global patterns. The \"Specformer\" model, which combines spectral graph neural networks with transformers, effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain. This approach can be inspirational for designing spectral filtering mechanisms in autoregressive language models to capture global patterns in the data.\n\n## Addressing High-Frequency Components\n\nThe frequency principle (F-Principle) or spectral bias in deep neural networks indicates that these models tend to fit low-frequency components first and struggle with high-frequency components. To overcome this, techniques like phase shift DNNs or adaptive activation functions can be employed to convert high-frequency components to a lower frequency spectrum for easier learning. This concept can be applied to autoregressive models to ensure they capture both low and high-frequency patterns in the data.\n\n## Hardware-Aware Implementation\n\nFor scalability, it is crucial to consider hardware-aware design strategies. Optimizing models for specific hardware architectures can reduce computational overhead. This involves designing models that are efficient in both training and inference, which is a key aspect of the proposal. Investigating how spectral filtering can be implemented efficiently on various hardware platforms will be essential.\n\n## Future Directions\n\n### Integration with Selective State Space Mechanisms\nIntegrating spectral filtering with selective state space mechanisms can enhance the adaptive processing and efficient resource allocation in autoregressive models. This combination can help in better handling long-range dependencies and improving the overall performance and scalability of the model.\n\n### Empirical Evaluations\nConducting extensive empirical evaluations on real-world datasets will be necessary to validate the effectiveness of spectral filtering in autoregressive language models. This includes comparing the performance of models with and without spectral filtering, as well as evaluating their robustness to variant inputs and efficiency in training and inference.\n\nBy focusing on these areas, researchers can leverage the strengths of spectral filtering to improve the feature representation, robustness, and scalability of autoregressive language models.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for SpectralHierRMSNorm focuses on enhancing hierarchical normalization by integrating spectral filtering and selective state space mechanisms. The search results provide insights into spectral filtering's application in autoregressive models and state space models, highlighting its robustness and efficiency in handling long-range dependencies. Additionally, the concept of selective state space mechanisms and hardware-aware implementations are explored, providing a comprehensive understanding of the proposal's potential impact on feature representation, robustness, and scalability.\n\n### Useful References and Excerpts\n\n1. **Spectral State Space Models (Agarwal et al., 2023)**\n   - Spectral filtering is used in state space models to handle long-range dependencies efficiently. It provides robustness and does not depend on the spectrum of the underlying dynamics, making it suitable for tasks requiring long-range memory.\n   - \"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n\n2. **Specformer: Spectral Graph Neural Networks Meet Transformers (Bo et al., 2023)**\n   - Introduces spectral filtering in graph neural networks, emphasizing the importance of learnable spectral filters for capturing global patterns in the spectrum.\n   - \"Specformer effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter.\"\n\n3. **Spectral Filtering for General Linear Dynamical Systems (Hazan et al., 2018)**\n   - Discusses the extension of spectral filtering techniques to systems with non-symmetric transition matrices, highlighting the versatility of spectral methods.\n   - \"The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix.\"\n\n4. **Deep Autoregressive Models with Spectral Attention (Art\u00e9s-Rodr\u00edguez et al., 2021)**\n   - Proposes a forecasting architecture that combines deep autoregressive models with a Spectral Attention module, which merges global and local frequency domain information in the model's embedded space.\n   - \"By characterizing in the spectral domain the embedding of the time series as occurrences of a random process, our method can identify global trends and seasonality patterns.\"\n\n### Key Insights and Analysis\n\n- **Spectral Filtering**: The use of spectral filtering in neural networks is well-supported by existing research. It offers robustness and efficiency, which aligns with the proposal's goals of improving feature representation and stability.\n\n- **Selective State Space Mechanisms**: These mechanisms are crucial for adaptive processing and efficient resource allocation. The integration of selective state spaces in models like Mamba demonstrates their potential for enhancing model performance and scalability.\n\n- **Hardware-Aware Implementation**: Emphasizing hardware efficiency is critical for scalability. Current trends involve optimizing models for specific hardware architectures to reduce computational overhead, which is a key aspect of the proposal.\n\n### Future Search Plan\n\n1. Conduct further searches on the application of spectral filtering in autoregressive models and its impact on feature representation.\n2. Explore more literature on integrating selective state space models into hierarchical normalization frameworks.\n3. Investigate hardware-aware design strategies in neural networks to assess their applicability to the proposal.\n\n### List of References\n- Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv.org.\n- Bo, D., Shi, C., Wang, L., & Liao, R. (2023). Specformer: Spectral Graph Neural Networks Meet Transformers. International Conference on Learning Representations.\n- Hazan, E., Lee, H., Singh, K., Zhang, C., & Zhang, Y. (2018). Spectral Filtering for General Linear Dynamical Systems. Neural Information Processing Systems.\n- Art\u00e9s-Rodr\u00edguez, A., Olmos, P. M., & Moreno-Pino, F. (2021). Deep Autoregressive Models with Spectral Attention. arXiv.org."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "Spectral filtering hierarchical",
            "detail": "SpectralHierRMSNorm spectral filtering selective state space hardware-aware",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSpectralHierRMSNorm spectral filtering selective state space hardware-aware\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.21)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.42)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.00)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.04)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.04)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 3. Theoretical Foundations of Deep Selective State-Space Models (Avg. Score: 0.00)\n\n*Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, C. Salvi, Terry Lyons*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** Theoretical grounding is given to this recent finding that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.\n\n**Abstract:** Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.\n\n##### *Relevant Chunk: No. 1/45 (Score: 0.00)*\n\n```\n# Theoretical Foundations of Deep Selective State-Space Models \n\nNicola Muca Cirone ${ }^{1}$ Antonio Orvieto ${ }^{2}$ Benjamin Walker ${ }^{3}$ Cristopher Salvi ${ }^{1}$ Terry Lyons ${ }^{3}$\n\n\n#### Abstract\n\nStructured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data.\n```\n\n#### 4. Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation (Avg. Score: 0.00)\n\n*Xilin Jiang, Cong Han, N. Mesgarani*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work replaces transformers with Mamba, a selective state space model, for speech separation, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces.\n\n**Abstract:** Transformers have been the most successful architecture for various speech modeling tasks, including speech separation. However, the self-attention mechanism in transformers with quadratic complexity is inefficient in computation and memory. Recent models incorporate new layers and modules along with transformers for better performance but also introduce extra model complexity. In this work, we replace transformers with Mamba, a selective state space model, for speech separation. We propose dual-path Mamba, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces. Our experimental results on the WSJ0-2mix data show that our dual-path Mamba models of comparably smaller sizes outperform state-of-the-art RNN model DPRNN, CNN model WaveSplit, and transformer model Sepformer. Code: https://github.com/xi-j/Mamba-TasNet\n\n##### *Relevant Chunk: No. 12/15 (Score: 0.00)*\n\n```\nInterspeech 2022, 2022, pp. 5353-5357. [10] Rudolph Emil Kalman, \"A new approach to linear filtering and prediction problems,\" 1960. [11] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9, \"Combining recurrent, convolutional, and continuous-time models with linear state space layers,\" Advances in neural information processing systems, vol. 34, pp. 572-585, 2021. [12] Albert Gu, Karan Goel, and Christopher R\u00e9, \"Efficiently modeling long sequences with structured state spaces,\" in The International Conference on Learning Representations (ICLR), 2022. [13] Albert Gu and Tri Dao, \"Mamba: Linear-time sequence modeling with selective state spaces,\" arXiv preprint arXiv:2312.00752, 2023. [14] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang, \"Vision mamba: Efficient visual representation learning with bidirectional state space model,\" arXiv preprint arXiv:2401.09417, 2024. [15] John R. Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, \"Deep clustering: Discriminative embeddings for segmentation and separation,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 31-35. [16] Neil Zeghidour and David Grangier, \"Wavesplit: End-to-end speech separation by speaker clustering,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 2840-2849, 2021. [17] Eliya Nachmani, Yossi Adi, and Lior Wolf, \"Voice separation with an unknown number of multiple speakers,\" in International Conference on Machine Learning. PMLR, 2020, pp. 7164-7175. [18] Jingjing Chen, Qirong Mao, and Dong Liu, \"Dual-Path Transformer Network: Direct Context-Aware Modeling for End-toEnd Monaural Speech Separation,\" in Proc. Interspeech 2020, 2020, pp. 2642-2646. [19] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis, \"End-to-end source separation with adaptive front-ends,\" in 2018 52nd asilomar conference on signals, systems, and computers. IEEE, 2018, pp. 684-688. [20] Yi Luo and Nima Mesgarani, \"Tasnet: time-domain audio separation network for real-time, single-channel speech separation,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 696700. [21] Efthymios Tzinis, Zhepei Wang, and Paris Smaragdis, \"Sudo rm-rf: Efficient networks for universal audio source separation,\" in 2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2020, pp. 1-6. [22] Efthymios Tzinis, Zhepei Wang, Xilin Jiang, and Paris Smaragdis, \"Compute and memory efficient universal sound source separation,\" Journal of Signal Processing Systems, vol.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Spectral filtering hierarchical\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: Spectral filtering hierarchical*\n\n*M. Defferrard, X. Bresson, P. Vandergheynst*\n\n**TL;DR:** This work presents a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs.\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 7132  (*Influential: 767*)\n\n#### 2. Spectral Filtering for General Linear Dynamical Systems\n\n*From Search Query: Spectral filtering hierarchical*\n\n*Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, Yi Zhang*\n\n**TL;DR:** The recently introduced technique of spectral filtering is extended, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Abstract:** We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 91  (*Influential: 4*)\n\n#### 3. Spectral Smoothing Unveils Phase Transitions in Hierarchical Variational Autoencoders\n\n*From Search Query: Spectral filtering hierarchical*\n\n*Adeel Pervez, E. Gavves*\n\n**TL;DR:** This work generalizes VAE neural networks by incorporating a smoothing parameter motivated by Gaussian analysis to reduce higher frequency components and consequently the variance in parameterizing functions and shows that under such smoothing the VAE loss exhibits a phase transition.\n\n**Abstract:** Variational autoencoders with deep stochastic hierarchies are known to suffer from the problem of posterior collapse, where the top layers fall back to the prior and become independent of input. We suggest that the hierarchical VAE objective explicitly includes the variance of the function pa-rameterizing the mean and variance of the latent Gaussian distribution which itself is often a high variance function. Building on this we generalize VAE neural networks by incorporating a smoothing parameter motivated by Gaussian analysis to reduce higher frequency components and consequently the variance in parameterizing functions. We show this helps to solve the problem of posterior collapse. We further show that under such smoothing the VAE loss exhibits a phase transition, where the top layer KL divergence sharply drops to zero at a critical value of the smoothing parameter that is similar for the same model across datasets. We validate the phenomenon across model con\ufb01gurations and datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Learning Linear Dynamical Systems via Spectral Filtering\n\n*From Search Query: Spectral filtering hierarchical*\n\n*Elad Hazan, Karan Singh, Cyril Zhang*\n\n**TL;DR:** A polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning, based on a novel filtering technique, which may be of independent interest.\n\n**Abstract:** We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 92  (*Influential: 2*)\n\n#### 5. Solving High-Dimensional PDEs with Latent Spectral Models\n\n*From Search Query: Spectral filtering hierarchical*\n\n*Haixu Wu, Tengge Hu, Huakun Luo, Jianmin Wang, Mingsheng Long*\n\n**TL;DR:** Inspired by classical spectral methods in numerical analysis, this work designs a neural spectral block to solve PDEs in the latent space that approximates complex input-output mappings via learning multiple basis operators, enjoying nice theoretical guarantees for convergence and approximation.\n\n**Abstract:** Deep models have achieved impressive progress in solving partial differential equations (PDEs). A burgeoning paradigm is learning neural operators to approximate the input-output mappings of PDEs. While previous deep models have explored the multiscale architectures and various operator designs, they are limited to learning the operators as a whole in the coordinate space. In real physical science problems, PDEs are complex coupled equations with numerical solvers relying on discretization into high-dimensional coordinate space, which cannot be precisely approximated by a single operator nor efficiently learned due to the curse of dimensionality. We present Latent Spectral Models (LSM) toward an efficient and precise solver for high-dimensional PDEs. Going beyond the coordinate space, LSM enables an attention-based hierarchical projection network to reduce the high-dimensional data into a compact latent space in linear time. Inspired by classical spectral methods in numerical analysis, we design a neural spectral block to solve PDEs in the latent space that approximates complex input-output mappings via learning multiple basis operators, enjoying nice theoretical guarantees for convergence and approximation. Experimentally, LSM achieves consistent state-of-the-art and yields a relative gain of 11.5% averaged on seven benchmarks covering both solid and fluid physics. Code is available at https://github.com/thuml/Latent-Spectral-Models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 4*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Deep Shells: Unsupervised Shape Correspondence with Optimal Transport\n\n*From Search Query: Spectral filtering hierarchical*\n\n*Daniel Cremers, Laura Leal-Taix\u00e9, Aysim Toker, Marvin Eisenberger*\n\n**Abstract:** We propose a novel unsupervised learning approach to 3D shape correspondence that builds a multiscale matching pipeline into a deep neural network. This approach is based on smooth shells, the current state-of-the-art axiomatic correspondence method, which requires an a priori stochastic search over the space of initial poses. Our goal is to replace this costly preprocessing step by directly learning good initializations from the input surfaces. To that end, we systematically derive a fully differentiable, hierarchical matching pipeline from entropy regularized optimal transport. This allows us to combine it with a local feature extractor based on smooth, truncated spectral convolution filters. Finally, we show that the proposed unsupervised method significantly improves over the state-of-the-art on multiple datasets, even in comparison to the most recent supervised methods. Moreover, we demonstrate compelling generalization results by applying our learned filters to examples that significantly deviate from the training set.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-28\n\n\n\n#### 2. Adaptive Morphological Reconstruction for Seeded Image Segmentation\n\n*From Search Query: Spectral filtering hierarchical*\n\n*Hongy-ing Meng, Tongliang Liu, Tao Lei, Shigang Liu, Xiaohong Jia, Asoke K. Nandi*\n\n**Abstract:** Morphological reconstruction (MR) is often employed by seeded image\nsegmentation algorithms such as watershed transform and power watershed as it\nis able to filter seeds (regional minima) to reduce over-segmentation. However,\nMR might mistakenly filter meaningful seeds that are required for generating\naccurate segmentation and it is also sensitive to the scale because a\nsingle-scale structuring element is employed. In this paper, a novel adaptive\nmorphological reconstruction (AMR) operation is proposed that has three\nadvantages. Firstly, AMR can adaptively filter useless seeds while preserving\nmeaningful ones. Secondly, AMR is insensitive to the scale of structuring\nelements because multiscale structuring elements are employed. Finally, AMR has\ntwo attractive properties: monotonic increasingness and convergence that help\nseeded segmentation algorithms to achieve a hierarchical segmentation.\nExperiments clearly demonstrate that AMR is useful for improving algorithms of\nseeded image segmentation and seed-based spectral segmentation. Compared to\nseveral state-of-the-art algorithms, the proposed algorithms provide better\nsegmentation results requiring less computing time. Source code is available at\nhttps://github.com/SUST-reynole/AMR.\n\n**Published:** 2019-04-08\n\n\n\n#### 3. Mesh Convolutional Autoencoder for Semi-Regular Meshes of Different Sizes\n\n*From Search Query: Spectral filtering hierarchical*\n\n*Jochen Garcke, Sara Hahner*\n\n**Abstract:** The analysis of deforming 3D surface meshes is accelerated by autoencoders since the low-dimensional embeddings can be used to visualize underlying dynamics. But, state-of-the-art mesh convolutional autoencoders require a fixed connectivity of all input meshes handled by the autoencoder. This is due to either the use of spectral convolutional layers or mesh dependent pooling operations. Therefore, the types of datasets that one can study are limited and the learned knowledge cannot be transferred to other datasets that exhibit similar behavior. To address this, we transform the discretization of the surfaces to semi-regular meshes that have a locally regular connectivity and whose meshing is hierarchical. This allows us to apply the same spatial convolutional filters to the local neighborhoods and to define a pooling operator that can be applied to every semi-regular mesh. We apply the same mesh autoencoder to different datasets and our reconstruction error is more than 50% lower than the error from state-of-the-art models, which have to be trained for every mesh separately. Additionally, we visualize the underlying dynamics of unseen mesh sequences with an autoencoder trained on different classes of meshes.\n\n**Published:** 2021-10-18\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using the concepts outlined in the proposal for SpectralHierRMSNorm, here are some key points and relevant information from the given context and additional insights:\n\n### Spectral Filtering in Neural Networks\n\n- **Spectral Methods**: While the provided sources do not directly discuss spectral filtering in neural networks, the concept of spectral methods is widely used in signal processing. For instance, the use of spectral estimation methods like Welch and multitaper in EEG data analysis suggests that spectral techniques can be adapted for feature extraction and representation in neural networks. In the context of hierarchical models, spectral filtering could help in decomposing and processing features at different frequency bands, potentially enhancing the model's ability to capture diverse patterns.\n\n### Selective State Space Mechanisms\n\n- **Selective State Space Models**: These models are not explicitly discussed in the provided sources, but the idea of selective state space mechanisms involves dynamically choosing which states or features to retain or compress. This concept is analogous to attention mechanisms in neural networks, which selectively focus on certain parts of the input data. Integrating such mechanisms into hierarchical normalization could help in adaptive state compression and better information flow between hierarchical levels.\n\n### Hardware-Aware Implementation\n\n- **Hardware Efficiency**: The emphasis on hardware-aware design is crucial for scalability and efficiency. Current trends in hardware-aware neural network design involve optimizing models for specific hardware architectures, such as GPUs or TPUs, to reduce computational overhead and improve training and inference times. This can be achieved through techniques like model pruning, quantization, and efficient tensor operations.\n\n### Comparison and Potential Trade-offs\n\n- **Comparison with Existing Designs**: To assess the novelty and advantages of SpectralHierRMSNorm, comparing it with other hierarchical normalization techniques like SparseStateGAU and VariaHierAT is essential. This comparison should focus on aspects such as feature representation, computational efficiency, and training stability.\n- **Potential Trade-offs**: The introduction of spectral filtering and selective state space mechanisms may increase the model's complexity, potentially leading to higher computational overhead and training instability. Investigating these trade-offs through experiments and simulations will be crucial to ensure that the benefits outweigh the costs.\n\n### Additional Insights\n\n- **Hierarchical Processing**: Hierarchical processing in neural networks often involves multi-scale feature extraction and representation. Spectral filtering can be integrated into this framework to process features at different scales or frequencies, enhancing the model's ability to capture complex patterns.\n- **Adaptive State Compression**: The use of selective state space mechanisms can help in adaptively compressing or retaining states based on their importance, which can improve the model's efficiency and stability.\n\n### Recommendations for Further Investigation\n\n1. **Literature Review**:\n   - Conduct a thorough review of existing literature on spectral methods in neural networks, focusing on their application in hierarchical models.\n   - Explore selective state space models and their integration into neural architectures to understand their benefits and limitations.\n\n2. **Hardware-Aware Design**:\n   - Investigate current best practices in hardware-aware neural network design to optimize the SpectralHierRMSNorm for specific hardware architectures.\n\n3. **Experimental Evaluation**:\n   - Implement and evaluate the SpectralHierRMSNorm against existing hierarchical normalization techniques to assess its novelty and advantages.\n   - Analyze the trade-offs in terms of computational overhead, training stability, and feature representation.\n\nBy following these steps, the researchers can effectively integrate spectral filtering and selective state space mechanisms into a hierarchical normalization framework, potentially leading to improved performance, efficiency, and scalability in autoregressive language models.\n",
            "analysis": "### Initial Analysis\n\nThe proposal for SpectralHierRMSNorm introduces a novel approach to hierarchical normalization by integrating spectral filtering and selective state space mechanisms. The design aims to improve information flow between hierarchical levels through spectral decomposition and adaptive state compression, while maintaining computational efficiency through hardware-aware implementation. This approach combines insights from selective state space models with hierarchical processing to achieve better feature representation and stability.\n\n### Key Aspects for Further Investigation\n\n1. **Spectral Filtering**: The proposal suggests using spectral filtering to enhance feature representation. Investigating existing literature on spectral methods in neural networks, especially in the context of hierarchical models, will be crucial.\n\n2. **Selective State Space Mechanisms**: Understanding how selective state space models have been used in other contexts can provide insights into their potential benefits and limitations in this proposal.\n\n3. **Hardware-Aware Implementation**: The proposal emphasizes hardware efficiency. Exploring current trends in hardware-aware neural network design will help assess the feasibility and potential impact of this aspect.\n\n4. **Comparison with Existing Designs**: The proposal should be compared with similar designs, such as SparseStateGAU and VariaHierAT, to evaluate its novelty and potential advantages.\n\n5. **Potential Trade-offs**: The proposal introduces additional complexity with spectral filtering and selective state integration. Investigating potential trade-offs in terms of computational overhead and training stability will be important.\n\n### Plan for Investigations\n\n1. Conduct a broad search on spectral filtering in neural networks and its application in hierarchical models.\n2. Explore literature on selective state space models and their integration into neural architectures.\n3. Investigate hardware-aware implementations in neural networks to understand current best practices.\n4. Compare the proposal with similar designs identified in the previous design proposals to assess novelty and potential advantages.\n\n### High-Level Query for External Searches\n- \"Spectral filtering hierarchical models\"\n- \"Selective state space neural networks\"\n- \"Hardware-aware neural network design\"\n\n### Detailed Query for Internal Vector Store\n- \"SpectralHierRMSNorm spectral filtering selective state space hardware-aware\"\n\n### Novelty Assessment\nThe proposal appears to be novel in its integration of spectral filtering and selective state space mechanisms within a hierarchical normalization framework. While similar concepts have been explored in other contexts, the specific combination and application in SpectralHierRMSNorm seem unique compared to previous design proposals and existing research. Further investigation is needed to confirm this assessment."
        },
        {
            "ready": false,
            "query": "Spectral filtering neural networks",
            "detail": "Spectral filtering hierarchical models feature representation",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSpectral filtering hierarchical models feature representation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.47)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 3/31 (Score: 0.81)*\n\n```\nWe discuss the details of this ablation in the appendix (Section A.1). Spectral filtering. The technique of spectral filtering [HSZ17] was developed as a convex improper learning alternative to directly parameterizing an LDS (as in the case of SSMs) leading to an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods (eg. SSMs) that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in [HLS ${ }^{+}$18]. Convolutional Models for Sequence Modeling. Exploiting the connnection between LDS and convolutions [GGR21], various convolutional models have been proposed for sequence modelling. $\\left[\\mathrm{FEN}^{+}\\right.$23] employ direct learning of convolutional kernels but find that they underperform SSMs, identifying non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations. [LCZ $\\left.{ }^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1) by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left[\\mathrm{LCZ}^{+} 22\\right]$ and $\\left[\\mathrm{FEN}^{+} 23\\right]$. ## 2 Preliminaries\n\nSequence prediction. We treat sequence prediction as a game between a predictor/learner and nature in which iteratively at every time $t \\in[L]$, the learner is presented an input $u_{t} \\in \\mathbb{R}^{d_{\\text {in }}}$. The learner $A$ then produces a candidate output $\\hat{y}_{t}=\\hat{y}_{t}(A)$, and nature reveals the $t^{t h}$ element of a target sequence $y_{t} \\in \\mathbb{R}^{d_{\\text {out }}}$. The learner then suffers an instantaneous loss of $\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}$. The task of the learner is to minimize regret over a benchmark set of learning algorithms $\\mathcal{A}$, defined as follows\n\n$$\n\\text { Regret }=\\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}-\\min _{A \\in \\mathcal{A}} \\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}(A)\\right\\|^{2}\n$$\n\nLinear Dynamical Systems (LDS): An example benchmark set of methods is that of a linear dynamical system, which has four matrix parameters, $A \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\text {in }}}, C \\in \\mathbb{R}^{d_{\\text {out }} \\times N}, D \\in$ $\\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$. The system evolves and generates outputs according to the following equations\n\n$$\nx_{t} \\triangleq A x_{t-1}+B u_{t}, \\quad \\hat{y}_{t} \\triangleq C x_{t}+D u_{t}\n$$\n\nThus, an example class of benchmark algorithms $\\mathcal{A}$ are all predictors that generate $\\hat{y}_{t}$ according to these rules, for a fixed set of matrices $A, B, C, D$.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.57)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 7/31 (Score: 0.52)*\n\n```\nIn our experiments we search over two values of $k_{y}=\\{2,32\\}$. For non-image tasks, ListOps, Text and Retrieval, we find that setting $k_{y}=2$ is sufficient to get optimal results. For the image tasks, CIFAR, Pathfinder and PathX, we found that $k_{y}=32$ led to significant performance gains. A performance ablation over this parameter can be found in the appendix (Table 2]. Overall we find that the STU model provides improvements over baselines such as S4 and LRU on 4 out of the 6 tasks and performs comparably to the best baseline on the others. Remarkably, the STU layers come with provable guarantees and thus performs well out of the box without the need for specific initializations, discretizations or normalizations. We initialize all parameters $M_{i}^{y}, M_{i}^{u}, M_{k}^{\\phi+}, M_{k}^{\\phi-}$ with 0 . We provide details of the experimental setup, including hyperparameter tuning in the appendix (Section E). ## 6 Conclusion\n\nInsprired by the success of SSMs, we present a new theoretically-founded deep neural network architecture, Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for\nlearning Linear Dynamical Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz. robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark. We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without the need for designing specific initializations, discretizations or normalizations which were necessary for existing SSMs to learn in such settings. While spectral SSMs only model symmetric A, our presented set of experiments on the LRA benchmark suggest that the gap between symmetric and general A is potentially small in real world tasks. Indeed more recent SSM models like [GD23, $\\mathrm{DSF}^{+}$24] work with real diagonals (i.e. symmetric case) as they do not find evidence that adding complex eigenvalues help. Spectral filtering has been extended in certain settings to asymmetric A [HLS ${ }^{+}$18] and a similar extension to our proposal is straightforward but comes with efficiency losses and we leave it to future work. ## References\n\n[ASB16] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120-1128. PMLR, 2016. [Ble89] Guy E Blelloch. Scans as primitive parallel operations. IEEE Transactions on computers, 38(11):1526-1538, 1989. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [BSF94] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994. [CVMG ${ }^{+}$14] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. $\\left[\\mathrm{DBK}^{+}\\right.$20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.\n```\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.00)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n#### 2. Sequence Modeling with Multiresolution Convolutional Memory  (Avg. Score: 0.00)\n\n*Jiaxin Shi, Ke Alexander Wang, E. Fox*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 6  (*Influential: 1*)\n\n**TL;DR:** The key component of the model is the multiresolution convolution, capturing multiscale trends in the input sequence, and can be implemented with shared filters across a dilated causal convolution tree, garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions.\n\n**Abstract:** Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -- poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. Our MultiresLayer is straightforward to implement, requires significantly fewer parameters, and maintains at most a $\\mathcal{O}(N\\log N)$ memory footprint for a length $N$ sequence. Yet, by stacking such layers, our model yields state-of-the-art performance on a number of sequence classification and autoregressive density estimation tasks using CIFAR-10, ListOps, and PTB-XL datasets.\n\n##### *Relevant Chunk: No. 12/37 (Score: 0.00)*\n\n```\n1 5}$ |\n| Liquid-S4 (Hasani et al., 2022) | $\\mathbf{6 2 . 7 5}$ |\n| Convolution: |  |\n| CDIL (Cheng et al., 2023) | 44.05 |\n| SGConv* (Li et al., 2022) | 61.45 |\n| MuLTIRESNET (Ours) | $\\mathbf{6 2 . 7 5}$ |\n\nwith length-2 filters shared between tree levels, and levels connected by linear links. In particular, our best model uses 10 MultiresBlocks, each containing a MultiresConV layer that amounts to 10 layers of tied-weight dilated causal convolutions. Together, that amounts to 100 layers of dilated causal convolutions, but the total number of parameters is only 1.4 M . In contrast, the best S 4 model uses around 7.9 M parameters (Gu et al., 2021), but still underperforms our 5x smaller model (and likewise relies on fancy initializations). ### 5.2. Hierarchical reasoning on symbolic sequences\n\nIn order to test our model's capability of reasoning about hierarchical structures, we conduct experiments on the long ListOps dataset from Tay et al. (2021). The dataset consists of sequences that represent a composition of multiple list operations including MAX, MEAN, MED (median) and SM (sum and mod). For example, the input can be\n\n## [MAX 1 [MAX 20 3 ] 56 [MIN 7 7 8 ] ].\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Spectral filtering neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Specformer: Spectral Graph Neural Networks Meet Transformers\n\n*From Search Query: Spectral filtering neural networks*\n\n*Deyu Bo, Chuan Shi, Lele Wang, Renjie Liao*\n\n**TL;DR:** This work introduces Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter and design a decoder with learnable bases to enable non-local graph convolution.\n\n**Abstract:** Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 57  (*Influential: 10*)\n\n#### 2. How Powerful are Spectral Graph Neural Networks\n\n*From Search Query: Spectral filtering neural networks*\n\n*Xiyuan Wang, Muhan Zhang*\n\n**TL;DR:** It is proved that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality, and JacobiConv is proposed, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions.\n\n**Abstract:** Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 144  (*Influential: 29*)\n\n#### 3. The Spectral Bias of Polynomial Neural Networks\n\n*From Search Query: Spectral filtering neural networks*\n\n*Moulik Choraria, L. Dadi, Grigorios G. Chrysos, J. Mairal, V. Cevher*\n\n**TL;DR:** A spectral analysis of the Neural Tangent Kernel (NTK) of PNNs finds that the Pi-Net family, i.e., a recently proposed parametrization of Pnns, speeds up the learning of the higher frequencies.\n\n**Abstract:** Polynomial neural networks (PNNs) have been recently shown to be particularly effective at image generation and face recognition, where high-frequency information is critical. Previous studies have revealed that neural networks demonstrate a $\\textit{spectral bias}$ towards low-frequency functions, which yields faster learning of low-frequency components during training. Inspired by such studies, we conduct a spectral analysis of the Neural Tangent Kernel (NTK) of PNNs. We find that the $\\Pi$-Net family, i.e., a recently proposed parametrization of PNNs, speeds up the learning of the higher frequencies. We verify the theoretical bias through extensive experiments. We expect our analysis to provide novel insights into designing architectures and learning frameworks by incorporating multiplicative interactions via polynomials.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 4. Spectral evolution and invariance in linear-width neural networks\n\n*From Search Query: Spectral filtering neural networks*\n\n*Zhichao Wang, A. Engel, A. Sarwate, Ioana Dumitriu, Tony Chiang*\n\n**TL;DR:** The results show that monitoring the evolution of the spectra during training is an important step toward understanding the training dynamics and feature learning.\n\n**Abstract:** We investigate the spectral properties of linear-width feed-forward neural networks, where the sample size is asymptotically proportional to network width. Empirically, we show that the weight spectra in this high dimensional regime are invariant when trained by gradient descent for small constant learning rates and the changes in both operator and Frobenius norm are $\\Theta(1)$ in the limit. This implies the bulk spectra for both the conjugate and neural tangent kernels are also invariant. We demonstrate similar characteristics for models trained with mini-batch (stochastic) gradient descent with small learning rates and provide a theoretical justification for this special scenario. When the learning rate is large, we show empirically that an outlier emerges with its corresponding eigenvector aligned to the training data structure. We also show that after adaptive gradient training, where we have a lower test error and feature learning emerges, both the weight and kernel matrices exhibit heavy tail behavior. Different spectral properties such as invariant bulk, spike, and heavy-tailed distribution correlate to how far the kernels deviate from initialization. To understand this phenomenon better, we focus on a toy model, a two-layer network on synthetic data, which exhibits different spectral properties for different training strategies. Analogous phenomena also appear when we train conventional neural networks with real-world data. Our results show that monitoring the evolution of the spectra during training is an important step toward understanding the training dynamics and feature learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 12  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: Spectral filtering neural networks*\n\n*Micha\u00ebl Defferrard, Pierre Vandergheynst, Xavier Bresson*\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n\n**Conference:** convolutional-neural-networks-on-graphs-with-1\n\n**Published:** 2016-06-30\n\n\n\n#### 2. Deep Convolutional Networks on Graph-Structured Data\n\n*From Search Query: Spectral filtering neural networks*\n\n*Yann Lecun, Joan Bruna, Mikael Henaff*\n\n**Abstract:** Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate.\n\n**Published:** 2015-06-16\n\n\n\n#### 3. Spectral Networks and Locally Connected Networks on Graphs\n\n*From Search Query: Spectral filtering neural networks*\n\n*Yann Lecun, Joan Bruna, Arthur Szlam, Wojciech Zaremba*\n\n**Abstract:** Convolutional Neural Networks are extremely efficient architectures in image\nand audio recognition tasks, thanks to their ability to exploit the local\ntranslational invariance of signal classes over their domain. In this paper we\nconsider possible generalizations of CNNs to signals defined on more general\ndomains without the action of a translation group. In particular, we propose\ntwo constructions, one based upon a hierarchical clustering of the domain, and\nanother based on the spectrum of the graph Laplacian. We show through\nexperiments that for low-dimensional graphs it is possible to learn\nconvolutional layers with a number of parameters independent of the input size,\nresulting in efficient deep architectures.\n\n**Published:** 2013-12-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using spectral filtering and hierarchical feature representation, here are some key insights and relevant information from the provided sources and additional context:\n\n## Spectral Filtering in Neural Networks\n\n- **Robustness and Efficiency**: Spectral filtering has been shown to be effective in handling long-range dependencies and providing robustness in state space models. This technique does not depend on the spectrum of the underlying dynamics, making it suitable for tasks that require long-range memory, such as language modeling[3,.\n\n## Application in Hierarchical Models\n\n- **Hierarchical Normalization**: Integrating spectral filtering into hierarchical normalization frameworks can enhance feature representation and improve information flow. This approach can be particularly beneficial in autoregressive models where maintaining contextual information across different layers is crucial.\n- **Example from Graph Neural Networks**: The concept of using spectral filtering in hierarchical models can be drawn from graph neural networks. For instance, the Heterogeneous Heterophilic Spectral Graph Neural Network (H2SGNN) employs local independent filtering and global hybrid filtering to capture interactions across different subgraphs, which can be analogous to handling hierarchical feature representations in language models.\n\n## Selective State Space Mechanisms\n\n- **Efficient Sequence Modeling**: Selective state space models, as seen in architectures like Mamba, can be integrated into neural networks to emphasize efficient sequence modeling. This can help in adaptive processing and resource allocation, which is critical for autoregressive language models that need to process sequences of varying lengths efficiently[3,.\n\n## Feature Representation and Information Flow\n\n- **Frequency Principle**: The frequency principle or spectral bias in deep neural networks suggests that these models tend to fit low-frequency components of the target function before high-frequency components. This principle can be leveraged to design models that prioritize learning low-frequency features first, which can be beneficial for maintaining contextual information in autoregressive models.\n- **Multi-Stage Neural Networks**: The use of multi-stage neural networks, where sequential neural networks are optimized to fit the residuals from previous networks, can help in boosting approximation accuracy and addressing spectral bias. This approach can be adapted to improve the feature representation in hierarchical autoregressive models.\n\n## Hardware-Aware Implementation\n\n- **Efficiency and Scalability**: Optimizing models for specific hardware architectures is crucial for scalability. Ensuring that the spectral filtering and selective state space mechanisms are hardware-aware can help in reducing computational overhead and improving the overall efficiency of the model[3,.\n\n### Future Search Directions\n\n1. **Spectral Filtering in Autoregressive Models**:\n   - Investigate how spectral filtering can be specifically applied to autoregressive language models to enhance feature representation and stability.\n   - Look into existing literature on the application of spectral methods in sequence modeling tasks.\n\n2. **Integration with Selective State Spaces**:\n   - Explore more literature on integrating selective state space models into hierarchical normalization frameworks, particularly in the context of autoregressive language models.\n   - Analyze how these mechanisms can be optimized for efficient sequence processing.\n\n3. **Hardware-Aware Design**:\n   - Investigate hardware-aware design strategies that can be applied to autoregressive language models to ensure computational efficiency and scalability.\n   - Look into case studies or benchmarks that demonstrate the impact of hardware-aware optimizations on model performance.\n\nBy focusing on these areas, researchers can develop a novel autoregressive language model that leverages spectral filtering and selective state space mechanisms to improve feature representation, information flow, and overall performance.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for SpectralHierRMSNorm is centered around integrating spectral filtering and selective state space mechanisms into hierarchical normalization. This approach aims to enhance feature representation, improve information flow, and maintain computational efficiency. The search results provide insights into spectral filtering, selective state space models, and hardware-aware implementations, which are crucial for evaluating the proposal's novelty and potential impact.\n\n### Useful References and Excerpts\n\n1. **Spectral State Space Models (Agarwal et al., 2023)**\n   - Spectral filtering is used in state space models to handle long-range dependencies efficiently. It provides robustness and does not depend on the spectrum of the underlying dynamics, making it suitable for tasks requiring long-range memory.\n   - \"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n\n2. **Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Gu & Dao, 2023)**\n   - Selective state space models are integrated into a simplified neural network architecture, emphasizing the importance of selective mechanisms for efficient sequence modeling.\n   - \"We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\"\n\n3. **Spectral Filtering for General Linear Dynamical Systems (Hazan et al., 2018)**\n   - Discusses the extension of spectral filtering techniques to systems with non-symmetric transition matrices, highlighting the versatility of spectral methods.\n   - \"The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix.\"\n\n### Key Insights and Analysis\n\n- **Spectral Filtering**: The use of spectral filtering in neural networks, particularly in state space models, is well-supported by existing research. It offers robustness and efficiency, which aligns with the proposal's goals of improving feature representation and stability.\n\n- **Selective State Space Mechanisms**: These mechanisms are crucial for adaptive processing and efficient resource allocation. The integration of selective state spaces in models like Mamba demonstrates their potential for enhancing model performance and scalability.\n\n- **Hardware-Aware Implementation**: Emphasizing hardware efficiency is critical for scalability. Current trends involve optimizing models for specific hardware architectures to reduce computational overhead, which is a key aspect of the proposal.\n\n### Future Search Plan\n\n1. Conduct further searches on the application of spectral filtering in hierarchical models and its impact on feature representation.\n2. Explore more literature on selective state space models and their integration into hierarchical normalization frameworks.\n3. Investigate hardware-aware design strategies in neural networks to assess their applicability to the proposal.\n\n### List of References\n- Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv.org.\n- Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.org.\n- Hazan, E., Lee, H., Singh, K., Zhang, C., & Zhang, Y. (2018). Spectral Filtering for General Linear Dynamical Systems. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "Spectral filtering autoregressive models",
            "detail": "Spectral filtering autoregressive models feature representation",
            "search_ret": "\n---\n## Found 1 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSpectral filtering autoregressive models feature representation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 1 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.63)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 14/31 (Score: 0.86)*\n\n```\nWe provide further details about this study after this section. Spectral filtering. The technique of spectral filtering for learning linear dynamical systems was put forth in [HSZ17]. This work studies online prediction of the sequence of observations $y_{t}$, and the goal is to predict as well as the best symmetric LDS using past inputs and observations. Directly\nlearning the dynamics is a non-convex optimization problem, and spectral filtering is developed as an improper learning technique with an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in $\\mathrm{HLS}^{+} 18$. Convolutional Models for Sequence Modeling Exploiting the connnection between Linear dynamical systems and convolutions (as highlighted by [GGR21]) various convolutional models have been proposed for sequence modelling. [FEN ${ }^{+}$23] employ direct learning of convolutional kernels directly to sequence modelling but find that they underperform SSMs. They find the non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations to the kernels to match performance on the Long Range Arena benchmark. The proposed model still contains significantly large number of parameters growing with the sequence length. [ $\\left.\\mathrm{LCZ}^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. They achieve this via a specific form of the kernel derived by repeating and scaling the kernel in a dyadic fashion. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1] by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left\\lfloor\\mathrm{LCZ}^{+} 22\\right\\rfloor$ and $\\left[\\mathrm{FEN}^{+}\\right.$23]. ## A. 1 Ablations performed by OSG $^{+}$23]\n\nMotivated by the success of SSMs, $\\left[\\mathrm{OSG}^{+} 23\\right]$ revisit the RNN model (under the same deep stacked structure as SSMs) to investigate their efficiency. They begin from a simple linear RNN (a directly parameterized LDS) and add multiple components inspired from the SSM literature to ensure numerical stability and trainability of the model especially as the sequences grow larger. Overall they demonstrate that carefully designed parameterizations and initializations of LDS parameters as well as specifically designed normalizations are all necessary for model to learn consistently over the LRA dataset and in particularly over the 16 K context length task PathX. These interventions are driven by specific intuitions such as an inductive bias towards larger memory or controlling the loss blowup at initialization under long contexts but as such come with no theoretical guarantees towards alleviating the problem. We provide some quick details towards what these interventions are and refer the reader to $\\left[\\mathrm{OSG}^{+} 23\\right]$ to understand the motivations behind them and comparisons with similar ideas existing in previous SSM literature. The LRU model considered by $\\left[\\mathrm{OSG}^{+} 23\\right]$ is given by\n\n$$\ny_{k}=\\operatorname{diag}(\\lambda) y_{k-1}+\\gamma \\odot B u_{k}\n$$\n\nIn the above the learned parameters are $\\lambda$ and $B$ and note that $\\operatorname{diag}(\\lambda)$ corresponds to a diagonal $A$. $\\gamma$ is a specific normalization technique they develop to control the loss blowup under long-context detailed below. They perform the following interventions towards stable training\n\n- Stable Exponential Parameterization: They parameterize $\\lambda$ as\n\n$$\n\\lambda_{j}=\\underbrace{\\exp \\left(-\\exp \\left(\\nu_{j}^{\\log }\\right)\\right.}_{\\text {magnitude }}+i \\underbrace{\\exp \\left(\\theta_{j}^{\\log }\\right)}_{\\text {phase }})\n$$\n\nThe above is done to ensure a bound on the magnitude of eigenvalues of the effective A matrix as well as to ensure more resolution in the parameter space closer to the value of 1 . - Ring Initialization: They initialize the $\\lambda_{j}$ in the complex annulus [min_rad, max_rad]. This ensures that at initialization the magnitude of $\\lambda_{j}$ chosen randomly lies in $\\in$ [min_rad, max_rad] and the phase is chosen randomly. When not applying this intervention min_rad and max_rad are chosen to be 0,1 respectively. When applying this intervention these values are chosen to be closer to 1 , e.g. $0.9,0.999$ respectively. - $\\gamma$-Normalization: They set $\\gamma_{j}=\\sqrt{1-\\left|\\lambda_{j}\\right|^{2}}$\n- Restricting Phase at initialization: Instead of drawing a random phase at initialization the authors recommend selecting the initial phase from $[0, \\pi / 10]$. The authors claim that uniform phase inherently biases the network towards learning spurious features in the input sequence. $\\left[\\mathrm{OSG}^{+} 23\\right]$ provide the following ablation in the paper. In particular we see that all the above interventions are necessary to make the model get to non-trivial accuracy on PathX. On the contrary, as we show the STU model achieves comparable accuracy without requiring any specific initialization or normalization. | Model | Specification | sCIFAR | ListOps | Pathfinder | PathX |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| LRU | Dense A | 72.2 | 50.4 | $\\boldsymbol{X}$ | $\\boldsymbol{X}$ |\n|  | $\\Lambda$ Exp.\n```\n\n##### *Relevant Chunk: No. 3/31 (Score: 0.71)*\n\n```\nWe discuss the details of this ablation in the appendix (Section A.1). Spectral filtering. The technique of spectral filtering [HSZ17] was developed as a convex improper learning alternative to directly parameterizing an LDS (as in the case of SSMs) leading to an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods (eg. SSMs) that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in [HLS ${ }^{+}$18]. Convolutional Models for Sequence Modeling. Exploiting the connnection between LDS and convolutions [GGR21], various convolutional models have been proposed for sequence modelling. $\\left[\\mathrm{FEN}^{+}\\right.$23] employ direct learning of convolutional kernels but find that they underperform SSMs, identifying non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations. [LCZ $\\left.{ }^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1) by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left[\\mathrm{LCZ}^{+} 22\\right]$ and $\\left[\\mathrm{FEN}^{+} 23\\right]$. ## 2 Preliminaries\n\nSequence prediction. We treat sequence prediction as a game between a predictor/learner and nature in which iteratively at every time $t \\in[L]$, the learner is presented an input $u_{t} \\in \\mathbb{R}^{d_{\\text {in }}}$. The learner $A$ then produces a candidate output $\\hat{y}_{t}=\\hat{y}_{t}(A)$, and nature reveals the $t^{t h}$ element of a target sequence $y_{t} \\in \\mathbb{R}^{d_{\\text {out }}}$. The learner then suffers an instantaneous loss of $\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}$. The task of the learner is to minimize regret over a benchmark set of learning algorithms $\\mathcal{A}$, defined as follows\n\n$$\n\\text { Regret }=\\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}\\right\\|^{2}-\\min _{A \\in \\mathcal{A}} \\sum_{t=1}^{L}\\left\\|y_{t}-\\hat{y}_{t}(A)\\right\\|^{2}\n$$\n\nLinear Dynamical Systems (LDS): An example benchmark set of methods is that of a linear dynamical system, which has four matrix parameters, $A \\in \\mathbb{R}^{N \\times N}, B \\in \\mathbb{R}^{N \\times d_{\\text {in }}}, C \\in \\mathbb{R}^{d_{\\text {out }} \\times N}, D \\in$ $\\mathbb{R}^{d_{\\text {out }} \\times d_{\\text {in }}}$. The system evolves and generates outputs according to the following equations\n\n$$\nx_{t} \\triangleq A x_{t-1}+B u_{t}, \\quad \\hat{y}_{t} \\triangleq C x_{t}+D u_{t}\n$$\n\nThus, an example class of benchmark algorithms $\\mathcal{A}$ are all predictors that generate $\\hat{y}_{t}$ according to these rules, for a fixed set of matrices $A, B, C, D$.\n```\n\n##### *Relevant Chunk: No. 7/31 (Score: 0.68)*\n\n```\nIn our experiments we search over two values of $k_{y}=\\{2,32\\}$. For non-image tasks, ListOps, Text and Retrieval, we find that setting $k_{y}=2$ is sufficient to get optimal results. For the image tasks, CIFAR, Pathfinder and PathX, we found that $k_{y}=32$ led to significant performance gains. A performance ablation over this parameter can be found in the appendix (Table 2]. Overall we find that the STU model provides improvements over baselines such as S4 and LRU on 4 out of the 6 tasks and performs comparably to the best baseline on the others. Remarkably, the STU layers come with provable guarantees and thus performs well out of the box without the need for specific initializations, discretizations or normalizations. We initialize all parameters $M_{i}^{y}, M_{i}^{u}, M_{k}^{\\phi+}, M_{k}^{\\phi-}$ with 0 . We provide details of the experimental setup, including hyperparameter tuning in the appendix (Section E). ## 6 Conclusion\n\nInsprired by the success of SSMs, we present a new theoretically-founded deep neural network architecture, Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for\nlearning Linear Dynamical Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz. robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark. We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without the need for designing specific initializations, discretizations or normalizations which were necessary for existing SSMs to learn in such settings. While spectral SSMs only model symmetric A, our presented set of experiments on the LRA benchmark suggest that the gap between symmetric and general A is potentially small in real world tasks. Indeed more recent SSM models like [GD23, $\\mathrm{DSF}^{+}$24] work with real diagonals (i.e. symmetric case) as they do not find evidence that adding complex eigenvalues help. Spectral filtering has been extended in certain settings to asymmetric A [HLS ${ }^{+}$18] and a similar extension to our proposal is straightforward but comes with efficiency losses and we leave it to future work. ## References\n\n[ASB16] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120-1128. PMLR, 2016. [Ble89] Guy E Blelloch. Scans as primitive parallel operations. IEEE Transactions on computers, 38(11):1526-1538, 1989. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [BSF94] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994. [CVMG ${ }^{+}$14] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. $\\left[\\mathrm{DBK}^{+}\\right.$20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.56)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 16/31 (Score: 0.32)*\n\n```\n+ Phase Init. | 89.0 | 60.2 | 95.1 | 94.2 |\n\n## B Computational complexity and comparison to other methods. Using the STU method to make a sequence of $L$ predictions, the features $U^{+}, U^{-} \\in \\mathbb{R}^{L \\times d_{\\text {in }} \\times K}$ can be computed in time $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\log (L)\\right)$ using the Discrete Fast Fourier Transform, where $K$ is the number of filters and $L$ is the context length. The linear prediction part (i.e. spectral component) takes $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$ time, and the autoregressive part can be implemented in total time $O\\left(L \\cdot d_{\\text {in }} \\cdot d_{\\text {out }}\\right)$. Therefore the overall runtime is $O\\left(K \\cdot L \\cdot d_{\\text {in }} \\cdot\\left(\\log (L)+d_{\\text {out }}\\right)\\right) \\cdot{ }^{4}$\nFor comparison, consider LRU and transformers. The same computation carried out by LRU w. diagonal system matrices is dominated by the hidden dimension, i.e. $O\\left(L \\cdot d_{\\text {hidden }} \\cdot\\left(d_{\\text {in }}+d_{\\text {out }}\\right)\\right)$. Thus, the number of filters is replaced by $d_{\\text {hiden }}$, which is usually an order of magnitude larger, although STU has another $O(\\log L)$ overhead. A transformer model with full attention runs in time $O\\left(L^{2} d_{\\text {in }} d_{\\text {out }}\\right)$, which is significantly more costly than both LRU and STU. This is consistent with the motivation of SSM as more efficient models for sequences. ## C Proof of Theorem 3.1\n\nWe begin by observing that without loss of generality we can assume that $A$ is a real-diagonal matrix. This can be ensured by performing a spectral decomposition of $A=U \\Sigma U^{\\top}$ and absorbing the $U, U^{\\top}$ by redefining the system. Before continuing with the proof, we will provide some requisite definitions and lemmas. Define the following vector for any $\\alpha \\in \\mathbb{R}, \\mu(\\alpha) \\in \\mathbb{R}^{L}$, with $\\mu(\\alpha)(i)=(\\alpha-1) \\alpha^{i-1}$. Further define the Hankel matrix $H$ as\n\n$$\nZ \\triangleq \\int_{0}^{1} \\mu(\\alpha) \\mu(\\alpha)^{\\top} d \\alpha\n$$\n\nAs the following lemma shows the Hankel matrix $Z$ above is the same Hankel matrix defined in the definition of STU (3). Lemma C.1. $Z$ is a Hankel matrix with entries given as\n\n$$\nZ(i, j)=\\frac{2}{(i+j)^{3}-(i+j)}\n$$\n\n[^3]Lemma C.2. We have that the following statements hold regarding $\\mu(\\alpha)$ for any $\\alpha \\in[0,1]$,\n\n- $|\\mu(\\alpha)|^{2} \\leq 1$\n- For any $\\alpha \\in[0,1]$ and any unit vector $v$ we have that\n\n$$\n\\left(\\mu(\\alpha)^{\\top} v\\right)^{2} \\leq 12\\left(v^{\\top} H v\\right)\n$$\n\nLemma C.3. For any $\\alpha \\in[0,1]$, let $\\tilde{\\mu}(\\alpha)$ be the projection of $\\mu(\\alpha)$ on the subspace spanned by top $k$ eigenvectors of $Z$, then we have that\n\n$$\n\\|\\mu(\\alpha)-\\tilde{\\mu}(\\alpha)\\|^{2} \\leq 12 \\sum_{i=k+1}^{L} \\sigma_{i}\n$$\n\nFinally the following lemma from HSZ17] shows that the spectrum of the matrix $Z$ decays exponentially.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Spectral filtering autoregressive models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Estimating Structured Vector Autoregressive Models\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Igor Melnyk, A. Banerjee*\n\n**TL;DR:** In VAR setting with correlated noise, although there is strong dependence over time and covariates, this work establishes bounds on the non-asymptotic estimation error of structured VAR parameters.\n\n**Abstract:** While considerable advances have been made in estimating high-dimensional structured models from independent data using Lasso-type models, limited progress has been made for settings when the samples are dependent. We consider estimating structured VAR (vector auto-regressive models), where the structure can be captured by any suitable norm, e.g., Lasso, group Lasso, order weighted Lasso, sparse group Lasso, etc. In VAR setting with correlated noise, although there is strong dependence over time and covariates, we establish bounds on the non-asymptotic estimation error of structured VAR parameters. Surprisingly, the estimation error is of the same order as that of the corresponding Lasso-type estimator with independent samples, and the analysis holds for any norm. Our analysis relies on results in generic chaining, sub-exponential martingales, and spectral representation of VAR models. Experimental results on synthetic data with a variety of structures as well as real aviation data are presented, validating theoretical results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2016\n\n**Citations:** 51  (*Influential: 10*)\n\n#### 2. SPECTRE : Spectral Conditioning Helps to Overcome the Expressivity Limits of One-shot Graph Generators\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Karolis Martinkus, Andreas Loukas, Nathanael Perraudin, R. Wattenhofer*\n\n**TL;DR:** This novel GAN, called SPECTRE, enables the one- shot generation of much larger graphs than previously possible with one-shot models, and outperforms state-of-the-art deep autoregressive generators in terms of modeling fidelity, while also avoiding expensive sequential generation and dependence on node ordering.\n\n**Abstract:** We approach the graph generation problem from a spectral perspective by first generating the dominant parts of the graph Laplacian spectrum and then building a graph matching these eigenvalues and eigenvectors. Spectral conditioning allows for direct modeling of the global and local graph structure and helps to overcome the expressivity and mode collapse issues of one-shot graph generators. Our novel GAN, called SPECTRE, enables the one-shot generation of much larger graphs than previously possible with one-shot models. SPECTRE outperforms state-of-the-art deep autoregressive generators in terms of modeling fidelity, while also avoiding expensive sequential generation and dependence on node ordering. A case in point, in sizable synthetic and real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the best competitor that does not overfit and is 23-to-30 times faster than autoregressive generators.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 56  (*Influential: 11*)\n\n#### 3. Towards Better Graph Representation Learning with Parameterized Decomposition & Filtering\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Mingqi Yang, Wenjie Feng, Yanming Shen, Bryan Hooi*\n\n**TL;DR:** This work develops a novel and general framework which unifies many existing GNN models from the view of parameterized decomposition and filtering, and shows how it helps to enhance the flexibility of GNNs while alleviating the smoothness and amplification issues of existing models.\n\n**Abstract:** Proposing an effective and flexible matrix to represent a graph is a fundamental challenge that has been explored from multiple perspectives, e.g., filtering in Graph Fourier Transforms. In this work, we develop a novel and general framework which unifies many existing GNN models from the view of parameterized decomposition and filtering, and show how it helps to enhance the flexibility of GNNs while alleviating the smoothness and amplification issues of existing models. Essentially, we show that the extensively studied spectral graph convolutions with learnable polynomial filters are constrained variants of this formulation, and releasing these constraints enables our model to express the desired decomposition and filtering simultaneously. Based on this generalized framework, we develop models that are simple in implementation but achieve significant improvements and computational efficiency on a variety of graph learning tasks. Code is available at https://github.com/qslim/PDF.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. A Neural Autoregressive Approach to Collaborative Filtering\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Yin Zheng, Bangsheng Tang, Wenkui Ding, Hanning Zhou*\n\n**TL;DR:** Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.\n\n**Abstract:** This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2016\n\n**Citations:** 218  (*Influential: 27*)\n\n#### 5. A Spectral Algorithm for List-Decodable Covariance Estimation in Relative Frobenius Norm\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Ilias Diakonikolas, D. Kane, Jasper C. H. Lee, Ankit Pensia, Thanasis Pittas*\n\n**TL;DR:** A novel multi-filtering method for list-decodable covariance estimation of Gaussian mixture models and the first Sum-of-Squares-free algorithm for robustly learning GMMs is developed.\n\n**Abstract:** We study the problem of list-decodable Gaussian covariance estimation. Given a multiset $T$ of $n$ points in $\\mathbb R^d$ such that an unknown $\\alpha<1/2$ fraction of points in $T$ are i.i.d. samples from an unknown Gaussian $\\mathcal{N}(\\mu, \\Sigma)$, the goal is to output a list of $O(1/\\alpha)$ hypotheses at least one of which is close to $\\Sigma$ in relative Frobenius norm. Our main result is a $\\mathrm{poly}(d,1/\\alpha)$ sample and time algorithm for this task that guarantees relative Frobenius norm error of $\\mathrm{poly}(1/\\alpha)$. Importantly, our algorithm relies purely on spectral techniques. As a corollary, we obtain an efficient spectral algorithm for robust partial clustering of Gaussian mixture models (GMMs) -- a key ingredient in the recent work of [BDJ+22] on robustly learning arbitrary GMMs. Combined with the other components of [BDJ+22], our new method yields the first Sum-of-Squares-free algorithm for robustly learning GMMs. At the technical level, we develop a novel multi-filtering method for list-decodable covariance estimation that may be useful in other settings.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Speech waveform synthesis from MFCC sequences with generative adversarial networks\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Paavo Alku, Xin Wang, Bajibabu Bollepalli, Manu Airaksinen, Lauri Juvela, Junichi Yamagishi, Hirokazu Kameoka*\n\n**Abstract:** This paper proposes a method for generating speech from filterbank mel\nfrequency cepstral coefficients (MFCC), which are widely used in speech\napplications, such as ASR, but are generally considered unusable for speech\nsynthesis. First, we predict fundamental frequency and voicing information from\nMFCCs with an autoregressive recurrent neural net. Second, the spectral\nenvelope information contained in MFCCs is converted to all-pole filters, and a\npitch-synchronous excitation model matched to these filters is trained.\nFinally, we introduce a generative adversarial network -based noise model to\nadd a realistic high-frequency stochastic component to the modeled excitation\nsignal. The results show that high quality speech reconstruction can be\nobtained, given only MFCC information at test time.\n\n**Published:** 2018-04-03\n\n\n\n#### 2. Deep Autoregressive Models with Spectral Attention\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Antonio Art\u00e9s-Rodr\u00edguez, Pablo M. Olmos, Fernando Moreno-Pino*\n\n**Abstract:** Time series forecasting is an important problem across many domains, playing a crucial role in multiple real-world applications. In this paper, we propose a forecasting architecture that combines deep autoregressive models with a Spectral Attention (SA) module, which merges global and local frequency domain information in the model's embedded space. By characterizing in the spectral domain the embedding of the time series as occurrences of a random process, our method can identify global trends and seasonality patterns. Two spectral attention models, global and local to the time series, integrate this information within the forecast and perform spectral filtering to remove time series's noise. The proposed architecture has a number of useful properties: it can be effectively incorporated into well-know forecast architectures, requiring a low number of parameters and producing interpretable results that improve forecasting accuracy. We test the Spectral Attention Autoregressive Model (SAAM) on several well-know forecast datasets, consistently demonstrating that our model compares favorably to state-of-the-art approaches.\n\n**Published:** 2021-07-13\n\n\n\n#### 3. A study of two periodogram algorithms for improving the detection of small transiting planets\n\n*From Search Query: Spectral filtering autoregressive models*\n\n*Anonymous*\n\n**Abstract:** The sensitivities of two periodograms are compared for weak signal planet detection in transit surveys: the widely used Box-Least Squares (BLS) algorithm following light curve detrending and the Transit Comb Filter (TCF) algorithm following autoregressive ARIMA modeling. Small depth transits are injected into light curves with different simulated noise characteristics. Two measures of spectral peak significance are examined: the periodogram signal-to-noise ratio (SNR) and a False Alarm Probability (FAP) based on the generalized extreme value distribution. The relative performance of the BLS and TCF algorithms for small planet detection is examined for a range of light curve characteristics, including orbital period, transit duration, depth, number of transits, and type of noise. We find that the TCF periodogram applied to ARIMA fit residuals with the SNR detection metric is preferred when short-memory autocorrelation is present in the detrended light curve and even when the light curve noise had white Gaussian noise. BLS is more sensitive to small planets only under limited circumstances with the FAP metric. BLS periodogram characteristics are inferior when autocorrelated noise is present due to heteroscedastic noise and false period detection. Application of these methods to TESS light curves with known small exoplanets confirms our simulation results. The study ends with a decision tree that advises transit survey scientists on procedures to detect small planets most efficiently. The use of ARIMA detrending and TCF periodograms can significantly improve the sensitivity of any transit survey with regularly spaced cadence.\n\n**Published:** 2023-08-08\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using spectral filtering and autoregressive models, here are some key insights and relevant information from the provided sources and additional context:\n\n## Spectral Filtering in Autoregressive Models\n\n### Robustness and Efficiency\nSpectral filtering can be particularly beneficial in autoregressive models due to its robustness and efficiency in handling long-range dependencies. For instance, spectral state space models have been shown to have provable robustness properties that are independent of the spectrum of the underlying dynamics and the dimensionality of the problem.\n\n### Application in State Space Models\nIn the context of state space models, spectral filtering can be integrated to enhance the handling of long-range dependencies. This approach is highlighted in the use of spectral state space models, which do not depend on the spectrum of the underlying dynamics, making them suitable for tasks requiring long-range memory.\n\n### Learnable Spectral Filters\nThe concept of learnable spectral filters, as seen in models like Specformer, can be adapted to autoregressive language models. Specformer effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. This could be applied to capture global patterns in the spectral domain of language data.\n\n## Autoregressive Models and Feature Representation\n\n### AR Models in Signal Processing\nAutoregressive (AR) models in signal processing can be used to derive the power spectral density using techniques like the Yule-Walker equations. This can provide insights into how AR models can be optimized for feature representation in language models, by leveraging similar techniques to capture spectral properties of the data.\n\n### State Space Formulation\nFormulating AR models in state space form, as described in the statsmodels documentation, allows for flexible and efficient estimation, forecasting, and handling of missing observations. This formulation can be extended to incorporate spectral filtering, enhancing the model's ability to capture complex dependencies and improve feature representation.\n\n## Selective State Space Mechanisms\n\n### Adaptive Processing\nSelective state space mechanisms are crucial for adaptive processing and efficient resource allocation. Integrating these mechanisms into hierarchical normalization frameworks can enhance model performance and scalability. For example, models like Mamba demonstrate the potential of selective state spaces in improving model efficiency and adaptability.\n\n## Hardware-Aware Implementation\n\n### Optimizing for Hardware\nOptimizing models for specific hardware architectures is essential for scalability. By designing models that are hardware-aware, researchers can reduce computational overhead, which is a critical aspect of improving autoregressive language models. This involves considering the computational efficiency of spectral filtering and state space mechanisms in the context of available hardware resources.\n\n## Future Directions\n\n### Integration of Spectral Filtering\nConduct further research on integrating spectral filtering into autoregressive language models to enhance feature representation. This could involve adapting techniques from spectral state space models and Specformer to the language modeling domain.\n\n### Hierarchical Normalization Frameworks\nExplore more literature on integrating selective state space models into hierarchical normalization frameworks. This could involve combining spectral filtering with adaptive state space mechanisms to improve the stability and efficiency of the model.\n\n### Hardware-Aware Design\nInvestigate hardware-aware design strategies in neural networks to assess their applicability to autoregressive language models. This includes optimizing the computational overhead of spectral filtering and state space mechanisms for specific hardware architectures.\n\nBy focusing on these areas, researchers can develop novel autoregressive language models that leverage spectral filtering and selective state space mechanisms, leading to improved feature representation, robustness, and scalability.\n",
            "analysis": "### Summary of Analysis\n\nThe proposal for SpectralHierRMSNorm is focused on enhancing hierarchical normalization through spectral filtering and selective state space mechanisms. The search results provide valuable insights into the application of spectral filtering in neural networks, particularly in state space models and hierarchical frameworks. These insights are crucial for evaluating the proposal's novelty and potential impact on feature representation, information flow, and computational efficiency.\n\n### Useful References and Excerpts\n\n1. **Spectral State Space Models (Agarwal et al., 2023)**\n   - Spectral filtering is used in state space models to handle long-range dependencies efficiently. It provides robustness and does not depend on the spectrum of the underlying dynamics, making it suitable for tasks requiring long-range memory.\n   - \"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem.\"\n\n2. **Specformer: Spectral Graph Neural Networks Meet Transformers (Bo et al., 2023)**\n   - Introduces spectral filtering in graph neural networks, emphasizing the importance of learnable spectral filters for capturing global patterns in the spectrum.\n   - \"Specformer effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter.\"\n\n3. **Spectral Filtering for General Linear Dynamical Systems (Hazan et al., 2018)**\n   - Discusses the extension of spectral filtering techniques to systems with non-symmetric transition matrices, highlighting the versatility of spectral methods.\n   - \"The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix.\"\n\n### Key Insights and Analysis\n\n- **Spectral Filtering**: The use of spectral filtering in neural networks is well-supported by existing research. It offers robustness and efficiency, which aligns with the proposal's goals of improving feature representation and stability.\n\n- **Selective State Space Mechanisms**: These mechanisms are crucial for adaptive processing and efficient resource allocation. The integration of selective state spaces in models like Mamba demonstrates their potential for enhancing model performance and scalability.\n\n- **Hardware-Aware Implementation**: Emphasizing hardware efficiency is critical for scalability. Current trends involve optimizing models for specific hardware architectures to reduce computational overhead, which is a key aspect of the proposal.\n\n### Future Search Plan\n\n1. Conduct further searches on the application of spectral filtering in autoregressive models and its impact on feature representation.\n2. Explore more literature on integrating selective state space models into hierarchical normalization frameworks.\n3. Investigate hardware-aware design strategies in neural networks to assess their applicability to the proposal.\n\n### List of References\n- Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv.org.\n- Bo, D., Shi, C., Wang, L., & Liao, R. (2023). Specformer: Spectral Graph Neural Networks Meet Transformers. International Conference on Learning Representations.\n- Hazan, E., Lee, H., Singh, K., Zhang, C., & Zhang, Y. (2018). Spectral Filtering for General Linear Dynamical Systems. Neural Information Processing Systems."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.7635599999999999,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.5796775000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalRMSNorm",
    "proposal": "## Title: SpectralHierRMSNorm: Enhanced Hierarchical Normalization with Spectral Filtering and Selective State Integration\n\n### 1. Motivation\n\nCurrent hierarchical normalization approaches face several challenges:\n- Limited ability to capture long-range dependencies across scales\n- Inefficient information flow between hierarchical levels\n- Suboptimal balance between stability and adaptivity\n- High computational overhead in multi-scale processing\n\n### 2. Problem Analysis\n\n#### Current HierarchicalRMSNorm Limitations:\n1. Fixed scale processing without content-aware adaptation\n2. Limited interaction between different scales\n3. Inefficient memory utilization across scales\n4. Potential instability in gradient propagation\n\n#### Opportunities for Improvement:\n1. Integration of spectral filtering for better feature representation\n2. Selective state space mechanisms for adaptive processing\n3. Hardware-aware implementation for efficiency\n4. Enhanced stability through careful normalization design\n\n### 3. Core Idea and Philosophy\n\nSpectralHierRMSNorm enhances hierarchical normalization by:\n1. Incorporating spectral filtering for efficient feature processing\n2. Using selective state mechanisms for adaptive scale integration\n3. Implementing hardware-aware computation strategies\n4. Maintaining stability through careful parameter management\n\n### 4. Design Plan\n\n#### Mathematical Formulation:\n\n1. Spectral Decomposition:\n```python\ndef spectral_decomposition(x, scale):\n    # Transform to frequency domain\n    x_freq = torch.fft.rfft(x, dim=-1)\n    # Apply scale-specific filtering\n    x_filtered = x_freq * self.spectral_filters[scale]\n    return torch.fft.irfft(x_filtered, dim=-1)\n```\n\n2. Selective State Integration:\n```python\ndef selective_state_update(state, input, scale):\n    # Compute state update based on input complexity\n    complexity = compute_complexity(input)\n    gamma = torch.sigmoid(self.state_gates[scale](complexity))\n    return state * (1 - gamma) + input * gamma\n```\n\n3. Scale-Adaptive Normalization:\n```python\ndef scale_norm(x, scale):\n    # Compute scale-specific statistics\n    rms = torch.sqrt(torch.mean(x.pow(2), dim=-1, keepdim=True) + self.eps)\n    # Apply selective scaling\n    gamma = self.compute_adaptive_weight(x, scale)\n    return (x / rms) * gamma\n```\n\n#### Implementation:\n\n```python\nclass SpectralHierRMSNorm(nn.Module):\n    def __init__(self, embed_dim, scales=[1, 2, 4], eps=1e-5):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.scales = scales\n        self.eps = eps\n        \n        # Spectral filtering parameters\n        self.spectral_filters = nn.ParameterDict({\n            f's{s}': nn.Parameter(torch.ones(embed_dim // 2 + 1))\n            for s in scales\n        })\n        \n        # State integration parameters\n        self.state_gates = nn.ModuleDict({\n            f's{s}': nn.Linear(embed_dim, embed_dim)\n            for s in scales\n        })\n        \n        # Scale-specific parameters\n        self.scale_weights = nn.Parameter(torch.ones(len(scales)))\n        self.scale_biases = nn.Parameter(torch.zeros(len(scales)))\n        \n        # Adaptive compression\n        self.compression = nn.ModuleDict({\n            f's{s}': nn.Linear(embed_dim, embed_dim)\n            for s in scales\n        })\n\n    def forward(self, x):\n        # Process at different scales\n        scale_outputs = []\n        for s in self.scales:\n            # Spectral filtering\n            x_s = self.spectral_decomposition(x, s)\n            \n            # State integration\n            state = self.get_scale_state(s)\n            x_s = self.selective_state_update(state, x_s, s)\n            \n            # Scale-specific normalization\n            x_s = self.scale_norm(x_s, s)\n            \n            # Adaptive compression\n            x_s = self.compression[f's{s}'](x_s)\n            scale_outputs.append(x_s)\n        \n        # Combine scales with learned weights\n        weights = F.softmax(self.scale_weights, dim=0)\n        output = sum(w * out for w, out in zip(weights, scale_outputs))\n        \n        return output\n```\n\n### 5. Implementation Guidelines\n\n1. Initialization:\n- Initialize spectral filters with identity mapping\n- Use careful weight initialization for stability\n- Set initial scale weights to equal values\n\n2. Training Considerations:\n- Implement gradient checkpointing for memory efficiency\n- Use adaptive learning rates for different components\n- Monitor scale weights for potential collapse\n\n3. Optimization:\n- Fuse operations where possible\n- Implement efficient parallel processing\n- Use hardware-aware memory management\n\n### 6. Theoretical Analysis\n\n#### Complexity Analysis:\n- Time Complexity: O(n log n) per scale\n- Memory Complexity: O(n) per scale\n- Additional Parameters: O(d * s) for dimension d and scales s\n\n#### Stability Analysis:\n1. Gradient Flow:\n- Spectral filtering provides smooth gradient paths\n- Selective state updates prevent vanishing gradients\n- Scale integration maintains stable information flow\n\n2. Memory Efficiency:\n- Adaptive compression reduces memory requirements\n- Efficient state management through selective updates\n- Hardware-aware implementation minimizes overhead\n\n### 7. Expected Benefits\n\n1. Improved Feature Representation:\n- Better capture of long-range dependencies\n- Enhanced scale-specific information processing\n- More efficient information flow between scales\n\n2. Computational Efficiency:\n- Reduced memory requirements through compression\n- Efficient parallel processing capabilities\n- Hardware-optimized implementation\n\n3. Enhanced Stability:\n- Better gradient propagation\n- More stable scale integration\n- Improved convergence properties\n\n### 8. References\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\n\n[2] Sieber, J., et al. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks.\n\n[3] Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences.\n\n[4] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models.\n\n### 9. Conclusion\n\nSpectralHierRMSNorm represents a significant advancement in hierarchical normalization by combining spectral filtering with selective state space mechanisms. The design achieves better feature representation and stability while maintaining computational efficiency through careful hardware-aware implementation. The integration of adaptive compression and scale-specific processing enables more effective handling of long-range dependencies and improved information flow between hierarchical levels.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "gpt4o_0806",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "This proposal introduces SpectralHierRMSNorm, an enhanced variant of HierarchicalRMSNorm that incorporates spectral filtering and selective state space mechanisms. The design improves information flow between hierarchical levels through spectral decomposition and adaptive state compression, while maintaining computational efficiency through careful hardware-aware implementation. The approach combines insights from selective state space models with hierarchical processing to achieve better feature representation and stability.",
    "ideation": null,
    "modelname": "SpectralHierTTT",
    "suggestions": "1. **Empirical Validation:** Conduct comprehensive experiments to validate the proposed enhancements, including benchmarks comparing performance, memory usage, and efficiency against baseline models.\n2. **Implementation Details:** Provide more detailed guidelines for integrating spectral filtering and selective state space mechanisms, including potential challenges and solutions.\n3. **Visualization:** Include diagrams or visual aids to illustrate the hierarchical normalization process and the integration of spectral filtering.",
    "user_input": ""
}