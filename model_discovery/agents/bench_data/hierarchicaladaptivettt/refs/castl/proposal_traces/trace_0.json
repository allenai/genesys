{
    "variantname": "ContentAdaptiveSSMTTTLinear",
    "review": "The ContentAdaptiveSSMTTTLinear proposal presents an innovative approach to enhancing FastTTTLinear by integrating content-dependent state space models with adaptive compression and selective state updates. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nCLARITY (4.3/5):\nThe proposal is exceptionally well-structured and clearly articulated, with:\n+ Well-defined objectives and motivation\n+ Clear mathematical formulations\n+ Detailed implementation guidelines\n+ Comprehensive pseudo-code\nHowever:\n- Some aspects of state compression could be more explicitly detailed\n- Integration with test-time training needs more elaboration\n\nINNOVATION (4.2/5):\nThe proposal demonstrates significant innovation through:\n+ Novel integration of content-dependent SSMs with TTTLinear\n+ Unique approach to hierarchical state management\n+ Innovative selective computation mechanism\nHowever:\n- Some concepts overlap with Mamba's selective state spaces\n- Adaptive compression techniques share similarities with existing methods\n\nFEASIBILITY (3.9/5):\nImplementation appears feasible but with challenges:\n+ Clear mathematical foundation\n+ Detailed pseudo-code implementation\n+ Well-defined integration steps\nConcerns:\n- Complex state management system\n- Potential challenges in content-dependent updates\n- Need for careful parameter tuning\n\nSCALABILITY (4.1/5):\nThe design shows strong potential for scalability:\n+ Linear complexity through selective computation\n+ Efficient memory usage via adaptive compression\n+ Hierarchical processing enables handling of longer sequences\nHowever:\n- Memory overhead from maintaining hierarchical states needs careful management\n- Impact of content-dependent updates on large-scale training requires validation\n\nACCURACY AND ROBUSTNESS (4.0/5):\nThe proposal addresses performance considerations:\n+ Content-dependent processing should improve adaptability\n+ Hierarchical structure provides redundancy\n+ Selective updates maintain important information\nConcerns:\n- Impact of compression on representation quality needs validation\n- Stability under different input distributions requires testing\n- Complex update mechanisms might introduce instability\n\nEFFICIENCY (4.2/5):\nStrong emphasis on efficiency improvements:\n+ Multiple complementary approaches to reducing memory usage\n+ Efficient computation through selective updates\n+ Hardware-aware implementation considerations\nLimitations:\n- State management overhead\n- Content-dependent computation costs\n- Memory requirements for hierarchical states",
    "search_stack": [
        {
            "ready": false,
            "query": "state compression language models",
            "detail": "Find papers and technical documents discussing efficient state compression techniques in language models, particularly focusing on methods that maintain model expressiveness while reducing memory requirements.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and technical documents discussing efficient state compression techniques in language models, particularly focusing on methods that maintain model expressiveness while reducing memory requirements.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.97)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.97)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 2. Extensible Embedding: A Flexible Multipler For LLM's Context Length (Avg. Score: 0.96)\n\n*Ninglu Shao, Shitao Xiao, Zheng Liu, Peitian Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.\n\n**Abstract:** Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.\n\n##### *Relevant Chunk: No. 17/19 (Score: 0.96)*\n\n```\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736. Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Szymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b. 2023. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023a. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023b. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Retrieval meets long context large language models. CoRR, abs/2310.03025.\n```\n\n#### 3. PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation (Avg. Score: 0.96)\n\n*Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yunhe Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work presents a new efficient model architecture for establishing modern language models, namely, PanGu-$\\pi$, and develops an LLM named YunShan for practical application, which can surpass other models with similar scales on benchmarks.\n\n**Abstract:** The recent trend of large language models (LLMs) is to increase the scale of both model size (\\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama. However, large models often involve massive computational costs, and practical applications cannot afford such high prices. However, the method of constructing a strong model architecture for LLMs is rarely discussed. We first analyze the state-of-the-art language model architectures and observe the feature collapse problem. Based on the theoretical analysis, we propose that the nonlinearity is also very important for language models, which is usually studied in convolutional neural networks for vision tasks. The series informed activation function is then introduced with tiny calculations that can be ignored, and an augmented shortcut is further used to enhance the model nonlinearity. We then demonstrate that the proposed approach is significantly effective for enhancing the model nonlinearity through carefully designed ablations; thus, we present a new efficient model architecture for establishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using the same dataset and training strategy to compare PanGu-$\\pi$ with state-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a comparable performance to that of benchmarks with about 10\\% inference speed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms of accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the high-value domains of finance and law, developing an LLM named YunShan for practical application. The results show that YunShan can surpass other models with similar scales on benchmarks.\n\n##### *Relevant Chunk: No. 21/62 (Score: 0.96)*\n\n```\n[36] J. W. Rae et al. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [37] G. Xiao et al. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.\n```\n\n#### 4. Adapting Language Models to Compress Contexts (Avg. Score: 0.95)\n\n*Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 75  (*Influential: 11*)\n\n**TL;DR:** AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts and the benefits of pre-computing summary vectors for large corpora are explored.\n\n**Abstract:** Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts.\n\n##### *Relevant Chunk: No. 1/40 (Score: 0.95)*\n\n```\n# Adapting Language Models to Compress Contexts \n\nAlexis Chevalier* Alexander Wettig* Anirudh Ajith Danqi Chen<br>Department of Computer Science \\& Princeton Language and Intelligence<br>Princeton University<br>\\{achevalier, anirudh.ajith\\}@princeton.edu<br>\\{awettig, danqic\\}@cs.princeton.edu\n\n\n#### Abstract\n\nTransformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.\n```\n\n#### 5. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.95)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 21/38 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2001.04451, 2020. [33] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Haotong Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. Proceedings of the 29th Symposium on Operating Systems Principles, 2023. [34] Je-Yong Lee, Donghyun Lee, Genghan Zhang, Mo Tiwari, and Azalia Mirhoseini. Cats: Contextually-aware thresholding for sparsity in large language models. arXiv preprint arXiv:2404.08763, 2024. [35] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. How long can open-source llms truly promise on context length?, June 2023. [36] Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li, Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization for efficient llm deployment. NeurIPS Workshop, 2024. [37] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. arXiv preprint arXiv:2402.18158, 2024. [38] Xin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics, 2002. [39] Yuhong Li, Tianle Cai, Yi Zhang, De huai Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? ArXiv, abs/2210.09298, 2022. [40] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state compression language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Compression of Generative Pre-trained Language Models via Quantization\n\n*From Search Query: state compression language models*\n\n*Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong*\n\n**TL;DR:** This paper compress generative PLMs by quantization with comparable performance with the full-precision models, and proposes a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules.\n\n**Abstract:** The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 88  (*Influential: 6*)\n\n#### 2. History Compression via Language Models in Reinforcement Learning\n\n*From Search Query: state compression language models*\n\n*Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, S. Lehner, Hamid Eghbalzadeh, Sepp Hochreiter*\n\n**TL;DR:** This work proposes to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency, and introduces FrozenHopfield, which automatically associates observations with pretrained token embeddings.\n\n**Abstract:** In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with pretrained token embeddings. To form these associations, a modern Hopfield network stores these token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 2*)\n\n#### 3. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\n\n*From Search Query: state compression language models*\n\n*Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu*\n\n**TL;DR:** A coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models.\n\n**Abstract:** Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 62  (*Influential: 6*)\n\n#### 4. KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation\n\n*From Search Query: state compression language models*\n\n*Marzieh S. Tahaei, Ella Charlaix, V. Nia, A. Ghodsi, Mehdi Rezagholizadeh*\n\n**TL;DR:** The KroneckerBERT is a compressed version of the BERT_BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers, and is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples.\n\n**Abstract:** The development of over-parameterized pre-trained language models has made a significant contribution toward the success of natural language processing. While over-parameterization of these models is the key to their generalization power, it makes them unsuitable for deployment on low-capacity devices. We push the limits of state-of-the-art Transformer-based pre-trained language model compression using Kronecker decomposition. We present our KroneckerBERT, a compressed version of the BERT_BASE model obtained by compressing the embedding layer and the linear mappings in the multi-head attention, and the feed-forward network modules in the Transformer layers. Our KroneckerBERT is trained via a very efficient two-stage knowledge distillation scheme using far fewer data samples than state-of-the-art models like MobileBERT and TinyBERT. We evaluate the performance of KroneckerBERT on well-known NLP benchmarks. We show that our KroneckerBERT with compression factors of 7.7x and 21x outperforms state-of-the-art compression methods on the GLUE and SQuAD benchmarks. In particular, using only 13% of the teacher model parameters, it retain more than 99% of the accuracy on the majority of GLUE tasks.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 5. ZipLM: Inference-Aware Structured Pruning of Language Models\n\n*From Search Query: state compression language models*\n\n*Eldar Kurtic, Elias Frantar, Dan Alistarh*\n\n**TL;DR:** ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications.\n\n**Abstract:** The breakthrough performance of large language models (LLMs) comes with major computational footprints and high deployment costs. In this paper, we progress towards resolving this problem by proposing a novel structured compression approach for LLMs, called ZipLM. ZipLM achieves state-of-the-art accuracy-vs-speedup, while matching a set of desired target runtime speedups in any given inference environment. Specifically, given a model, a dataset, an inference environment, as well as a set of speedup targets, ZipLM iteratively identifies and removes components with the worst loss-runtime trade-off. Unlike prior methods that specialize in either the post-training/one-shot or the gradual compression setting, and only for specific families of models such as BERT (encoder) or GPT (decoder), ZipLM produces state-of-the-art compressed models across all these settings. Furthermore, ZipLM achieves superior results for a fraction of the computational cost relative to prior distillation and pruning techniques, making it a cost-effective approach for generating an entire family of smaller, faster, and highly accurate models, guaranteed to meet the desired inference specifications. In particular, ZipLM outperforms all prior BERT-base distillation and pruning techniques, such as CoFi, MiniLM, and TinyBERT. Moreover, it matches the performance of the heavily optimized MobileBERT model, obtained via extensive architecture search, by simply pruning the baseline BERT-large model. When compressing GPT2, ZipLM outperforms DistilGPT2 while being 60% smaller and 30% faster. Our code is available at: https://github.com/IST-DASLab/ZipLM.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n\n*From Search Query: state compression language models*\n\n*Carole-Jean Wu, Xing Liu, Bilge Acun, Chunxing Yin*\n\n**Abstract:** The memory capacity of embedding tables in deep learning recommendation models (DLRMs) is increasing dramatically from tens of GBs to TBs across the industry. Given the fast growth in DLRMs, novel solutions are urgently needed, in order to enable fast and efficient DLRM innovations. At the same time, this must be done without having to exponentially increase infrastructure capacity demands. In this paper, we demonstrate the promising potential of Tensor Train decomposition for DLRMs (TT-Rec), an important yet under-investigated context. We design and implement optimized kernels (TT-EmbeddingBag) to evaluate the proposed TT-Rec design. TT-EmbeddingBag is 3 times faster than the SOTA TT implementation. The performance of TT-Rec is further optimized with the batched matrix multiplication and caching strategies for embedding vector lookup operations. In addition, we present mathematically and empirically the effect of weight initialization distribution on DLRM accuracy and propose to initialize the tensor cores of TT-Rec following the sampled Gaussian distribution. We evaluate TT-Rec across three important design space dimensions -- memory capacity, accuracy, and timing performance -- by training MLPerf-DLRM with Criteo's Kaggle and Terabyte data sets. TT-Rec achieves 117 times and 112 times model size compression, for Kaggle and Terabyte, respectively. This impressive model size reduction can come with no accuracy nor training time overhead as compared to the uncompressed baseline.\n\n**Published:** 2021-01-25\n\n\n\n#### 2. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing\n\n*From Search Query: state compression language models*\n\n*Quoc V. Le, Zihang Dai, Yiming Yang, Guokun Lai*\n\n**Abstract:** With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at https://github.com/laiguokun/Funnel-Transformer.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-06-05\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: state compression language models*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient state compression techniques that maintain model expressiveness while reducing memory requirements, here are some key findings and methods from the provided sources:\n\n## SeedLM: Compression Using Pseudo-Random Generators\nThe paper on SeedLM introduces a novel post-training compression method that uses seeds of pseudo-random generators to encode and compress model weights. This approach involves generating random matrices using Linear Feedback Shift Registers (LFSRs) and combining them with compressed coefficients to reconstruct the weight blocks. This method is data-free, generalizes well across diverse tasks, and reduces memory access by trading compute for fewer memory accesses. SeedLM consistently outperforms state-of-the-art compression techniques, especially at higher compression levels, without needing calibration data.\n\n## ShortGPT: Layer Pruning Based on Block Influence\nThe ShortGPT method involves pruning layers in large language models based on a new metric called Block Influence (BI), which measures the impact of layer removal on hidden state transformations. This approach eliminates redundant layers without additional training, reducing inference costs. ShortGPT is orthogonal to quantization methods and can further reduce parameters and computation. This layer-wise pruning can help in maintaining model performance while reducing the memory footprint.\n\n## LLM-Streamline: Layer Pruning and Replacement\nLLM-Streamline is another layer pruning approach that identifies and removes less important layers based on their impact on hidden states. It also introduces a lightweight network to replace the pruned layers, mitigating performance loss. This method uses a new metric called stability to evaluate the consistency of predictions before and after pruning, ensuring that the compressed model maintains its expressiveness. LLM-Streamline outperforms previous pruning methods in both performance and training efficiency.\n\n## Semantic Text Compression\nWhile not directly focused on model weights, the semantic text compression approach by Bazaarvoice can indirectly help in reducing the input size to language models, thus affecting the state management. This method uses hierarchical clustering to compress input text without losing semantic information, which can reduce the context window size and subsequently the memory requirements for processing long sequences.\n\n### Key Insights for Improvement\n\n- **Efficient State Representation**: Methods like SeedLM and LLM-Streamline offer innovative ways to compress model weights and layers, respectively, which can significantly reduce memory requirements while maintaining model expressiveness.\n- **Layer Pruning**: Techniques such as ShortGPT and LLM-Streamline highlight the importance of identifying and pruning redundant layers, which can be crucial in streamlining the model architecture without compromising performance.\n- **Hybrid Approaches**: Combining different compression techniques, such as using pseudo-random generators for weight compression and layer pruning, could provide a balanced approach to state management and attention mechanisms.\n- **Position Encoding and Attention Mechanisms**: While the sources do not directly address position encoding and hybrid attention mechanisms, integrating innovations like functional interpolation for relative positions and retrospection mechanisms into these compression techniques could further enhance the model's ability to handle long sequences efficiently.\n\nBy leveraging these methods, researchers can develop more efficient autoregressive language models that balance expressiveness with reduced memory requirements, enhancing overall performance and scalability.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design Enhancement\n\n1. Summary of Analysis\n\nThe seed design FastTTTLinear presents an interesting approach combining Gated Linear Attention with TTTLinear for efficient long-sequence processing. From analyzing the seed design and initial references, several key areas emerge for potential improvement:\n\na) State Management:\n- Current designs struggle with efficient state representation for long sequences\n- Need for better mechanisms to compress and update states\n- Opportunity to improve information retention in state updates\n\nb) Attention Mechanisms:\n- Linear attention shows promise but has limitations in expressiveness\n- Need to balance efficiency with model capacity\n- Potential for hybrid approaches combining different attention types\n\nc) Position Encoding:\n- Current position encoding methods may limit model generalization\n- Opportunity to improve handling of relative positions\n- Need for more efficient position-aware processing\n\n2. Useful References with Excerpts\n\nFrom \"Functional Interpolation for Relative Positions\":\n> \"Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs.\"\n> \"FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.\"\n\nKey insight: Position encoding significantly impacts model generalization to longer sequences.\n\nFrom \"State Space Augmented Transformer\":\n> \"SSM augments global information, which complements the lack of long-range dependency issue in local attention methods\"\n> \"Experimental results demonstrate the effectiveness of the proposed method\"\n\nKey insight: Combining state space models with attention can enhance long-range dependency modeling.\n\nFrom \"The Expressive Capacity of State Space Models\":\n> \"SSMs and transformers have overlapping but distinct strengths\"\n> \"In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly\"\n\nKey insight: SSMs offer unique advantages in certain types of sequence modeling tasks.\n\nFrom \"RRWKV: Capturing Long-range Dependencies\":\n> \"RWKV fails to capture long-range dependencies because of its limitation on looking back at previous information\"\n> \"incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency\"\n\nKey insight: Retrospection mechanisms can enhance long-range dependency modeling while maintaining efficiency.\n\n3. Key Insights and Detailed Analysis\n\na) Position Encoding Innovation:\n- FIRE's functional interpolation approach shows promise for improving generalization\n- Could be integrated with gated mechanisms for more adaptive position processing\n- Potential to enhance FastTTTLinear's position handling\n\nb) State Space Integration:\n- SSMs offer complementary strengths to attention mechanisms\n- Could enhance FastTTTLinear's state representation\n- Need to carefully balance complexity with efficiency\n\nc) Retrospection Mechanisms:\n- RRWKV's retrospection concept could improve information retention\n- Potential to enhance FastTTTLinear's ability to access past information\n- Need to maintain computational efficiency\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in efficient state compression techniques\n2. Methods for combining different attention mechanisms\n3. Innovative approaches to position-aware processing in linear attention models\n\nKeywords for next search:\n- \"state compression language models\"\n- \"hybrid attention mechanisms\"\n- \"efficient position encoding\"\n\n5. References\n\n[1] Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S., Zaheer, M., ... & Bhojanapalli, S. (2023). Functional Interpolation for Relative Positions Improves Long Context Transformers. arXiv preprint.\n\n[2] Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint.\n\n[3] Sarrof, Y., Veitsman, Y., & Hahn, M. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv preprint.\n\n[4] Wang, L. (2023). RRWKV: Capturing Long-range Dependencies in RWKV. arXiv preprint."
        },
        {
            "ready": false,
            "query": "adaptive attention mechanisms language models",
            "detail": "Find papers and technical documents discussing adaptive attention mechanisms in language models, particularly focusing on methods that dynamically adjust attention patterns based on input characteristics.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and technical documents discussing adaptive attention mechanisms in language models, particularly focusing on methods that dynamically adjust attention patterns based on input characteristics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.98)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 27/35 (Score: 0.98)*\n\n```\nIn Proceedings of the 2013 Conference on\n\nEmpirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331-335, Florence, Italy. Association for Computational Linguistics. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10183-10192. PMLR. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021b. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. ArXiv preprint, abs/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.\n```\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.98)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 24/32 (Score: 0.98)*\n\n```\n[RSVG20b] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. International Conference On Topology, Algebra And Categories In Logic, 2020. $\\left[\\mathrm{RZW}^{+}\\right.$22] Liliang Ren, Zixuan Zhang, Han Wang, Clare Voss, ChengXiang Zhai, and Heng Ji. Language model pre-training with sparse latent typing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 14801494, Abu Dhabi, United Arab Emirates, dec 2022. Association for Computational Linguistics. [SGBJ19] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019. [SJP+ 21] Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur D.\n```\n\n#### 3. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.98)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.98)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 4. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.97)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 14/34 (Score: 0.97)*\n\n```\nZanchettin. 2019. Hierarchical attentional hybrid neural networks for document classification. ArXiv, abs/1901.06610. Joshua Ainslie, S. Onta\u00f1\u00f3n, C. Alberti, V. Cvicek, Zachary Kenneth Fisher, Philip Pham, Anirudh Ravula, S. Sanghai, Qifan Wang, and L. Yang. 2020. Etc: Encoding long and structured inputs in transformers. In EMNLP. Alexei Baevski and M. Auli. 2019. Adaptive input representations for neural language modeling. ArXiv, abs/1809.10853. I. Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. 2019. Attention augmented convolutional networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 3285-3294. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer.\n```\n\n#### 5. Compressive Transformers for Long-Range Sequence Modelling (Avg. Score: 0.96)\n\n*Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, T. Lillicrap*\n\n**Published in:** International Conference on Learning Representations (2019)\t**Cited by** 492  (*Influential: 64*)\n\n**TL;DR:** The Compressive Transformer is presented, an attentive sequence model which compresses past memories for long-range sequence learning and can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task.\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n##### *Relevant Chunk: No. 34/43 (Score: 0.96)*\n\n```\n2018. URL https://openreview.net/pdf?id=B1Yy1BxCZ\n\nS. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong. End-to-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739-8748, 2018. Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive attention mechanisms language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Improving Natural Language Understanding with Computation-Efficient Retrieval Representation Fusion\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Shangyu Wu, Ying Xiong, Yufei Cui, Xue Liu, Buzhou Tang, Tei-Wei Kuo, Chun Jason Xue*\n\n**TL;DR:** A new paradigm of RA named ReFusion is proposed, a computation-efficient Retrieval representation Fusion with bi-level optimization, which directly fuses the retrieval representations into the hidden states of models.\n\n**Abstract:** Retrieval-based augmentations (RA) incorporating knowledge from an external database into language models have greatly succeeded in various knowledge-intensive (KI) tasks. However, integrating retrievals in non-knowledge-intensive (NKI) tasks is still challenging. Existing works focus on concatenating retrievals with inputs to improve model performance. Unfortunately, the use of retrieval concatenation-based augmentations causes an increase in the input length, substantially raising the computational demands of attention mechanisms. This paper proposes a new paradigm of RA named \\textbf{ReFusion}, a computation-efficient Retrieval representation Fusion with bi-level optimization. Unlike previous works, ReFusion directly fuses the retrieval representations into the hidden states of models. Specifically, ReFusion leverages an adaptive retrieval integrator to seek the optimal combination of the proposed ranking schemes across different model layers. Experimental results demonstrate that the proposed ReFusion can achieve superior and robust performance in various NKI tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 2. Characterizing Mechanisms for Factual Recall in Language Models\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Qinan Yu, Jack Merullo, Ellie Pavlick*\n\n**TL;DR:** This work contributes to a body of evidence showing that the language model can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.\n\n**Abstract:** Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g.,\"The capital of Poland is London\") to overwrite what it learned in pretraining (\"Warsaw\"). On Pythia and GPT2, the training frequency of both the query country (\"Poland\") and the in-context city (\"London\") highly affect the models' likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88\\% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 0*)\n\n#### 3. REPLUG: Retrieval-Augmented Black-Box Language Models\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, M. Lewis, Luke Zettlemoyer, Wen-tau Yih*\n\n**TL;DR:** REPLUG is introduced, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model, and can be easily applied to any existing language models.\n\n**Abstract:** We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross-attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design can be easily applied to any existing language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%, as well as the performance of Codex on five-shot MMLU by 5.1%. Code is publicly released at github.com/swj0419/REPLUG.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 449  (*Influential: 35*)\n\n#### 4. AdaPlanner: Adaptive Planning from Feedback with Language Models\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang*\n\n**TL;DR:** A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\n\n**Abstract:** Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 85  (*Influential: 17*)\n\n#### 5. The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Ulme Wennberg, G. Henter*\n\n**TL;DR:** Analysis of position embeddings of existing language models finds strong evidence of translation invariance, which leads to translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embedDings.\n\n**Abstract:** Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Proof-of-concept experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 21  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Peng Gao, Chris Liu, Yu Qiao, Hongsheng Li, Pan Lu, Shilin Yan, Xiangfei Hu, Aojun Zhou, Jiaming Han, Renrui Zhang*\n\n**Abstract:** We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the word tokens at higher transformer layers. Then, a zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With our efficient training, LLaMA-Adapter can generate high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Besides language commands, our approach can be simply extended to multi-modal instructions for learning image-conditioned LLaMA model, which achieves superior reasoning performance on ScienceQA and COCO Caption benchmarks. Furthermore, we also evaluate the zero-initialized attention mechanism for fine-tuning other pre-trained models (ViT, RoBERTa) on traditional vision and language tasks, demonstrating the superior generalization capacity of our approach. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.\n\n**Published:** 2023-03-28\n\n\n\n#### 2. DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Heyang Qin, Yuxiong He, Samyam Rajbhandari, Ammar Ahmad Awan, Olatunji Ruwase, Minjia Zhang, Conglong Li, Xiaoxia Wu, Zhewei Yao*\n\n**Abstract:** Most of the existing multi-modal models, hindered by their incapacity to adeptly manage interleaved image-and-text inputs in multi-image, multi-round dialogues, face substantial constraints in resource allocation for training and data accessibility, impacting their adaptability and scalability across varied interaction realms. To address this, we present the DeepSpeed-VisualChat framework, designed to optimize Large Language Models (LLMs) by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs. Our framework is notable for (1) its open-source support for multi-round and multi-image dialogues, (2) introducing an innovative multi-modal causal attention mechanism, and (3) utilizing data blending techniques on existing datasets to assure seamless interactions in multi-round, multi-image conversations. Compared to existing frameworks, DeepSpeed-VisualChat shows superior scalability up to 70B parameter language model size, representing a significant advancement in multi-modal language models and setting a solid foundation for future explorations.\n\n**Published:** 2023-09-25\n\n\n\n#### 3. ImageBind-LLM: Multi-modality Instruction Tuning\n\n*From Search Query: adaptive attention mechanisms language models*\n\n*Yu Qiao, Hongsheng Li, Xiangyu Yue, Xiaoxin Chen, Yafei Wen, Shuai Ren, Xudong Lu, Ziyu Guo, Song Wen, Chris Liu, Kaipeng Zhang, Han Xiao, Peng Xu, Peng Gao, Wenqi Shao, Renrui Zhang, Jiaming Han*\n\n**Abstract:** We present ImageBind-LLM, a multi-modality instruction tuning method of large language models (LLMs) via ImageBind. Existing works mainly focus on language and image instruction tuning, different from which, our ImageBind-LLM can respond to multi-modality conditions, including audio, 3D point clouds, video, and their embedding-space arithmetic by only image-text alignment training. During training, we adopt a learnable bind network to align the embedding space between LLaMA and ImageBind's image encoder. Then, the image features transformed by the bind network are added to word tokens of all layers in LLaMA, which progressively injects visual instructions via an attention-free and zero-initialized gating mechanism. Aided by the joint embedding of ImageBind, the simple image-text training enables our model to exhibit superior multi-modality instruction-following capabilities. During inference, the multi-modality inputs are fed into the corresponding ImageBind encoders, and processed by a proposed visual cache model for further cross-modal embedding enhancement. The training-free cache model retrieves from three million image features extracted by ImageBind, which effectively mitigates the training-inference modality discrepancy. Notably, with our approach, ImageBind-LLM can respond to instructions of diverse modalities and demonstrate significant language generation quality. Code is released at https://github.com/OpenGVLab/LLaMA-Adapter.\n\n**Published:** 2023-09-07\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive attention mechanisms that dynamically adjust attention patterns based on input characteristics, here are some key findings and references:\n\n## Dynamic and Adaptive Attention Mechanisms\n\n### Multi-Head Density Adaptive Attention Mechanism (DAAM)\nThe DAAM, introduced in the paper \"Robust Parameter-Efficient Fine-Tuning Across Multiple Modalities,\" is a novel probabilistic attention framework that integrates learnable mean and variance into its attention mechanism. This approach allows the model to dynamically adjust attention based on input context, enhancing its responsiveness to deviations and improving its capability to interpret and process sequential and spatial data. DAAM's multi-head design enables each head to address different aspects of the data, making the model more adaptable to non-Gaussian traits.\n\n### Dynamic Sparse Attention and Adaptive KV-Cache Compression\nThe CASAK-V approach, detailed in the paper \"CASAK-V: Dynamic Sparse Attention and Adaptive KV-Cache Compression for Memory-Efficient Long-Context LLM Inference,\" uses a meta-learning framework to fine-tune a compact pre-trained vision-language encoder-decoder transformer. This method dynamically generates and applies head-specific sparse attention patterns, which are adjusted during token generation based on an attention map reconstruction heuristic. This approach reduces memory usage and maintains near-linear runtime complexity, making it efficient for long-context processing in resource-constrained environments.\n\n## Adaptive Attention in Transformers\n\n### Multi-Head Attention\nTraditional multi-head attention mechanisms, as discussed in the context of AI memory attention, allow models to attend to multiple aspects of input data simultaneously. This technique captures diverse features and integrates information from various sources, enhancing the robustness of AI models. However, recent advancements aim to make these mechanisms more adaptive and efficient. For instance, dynamic and adaptive attention mechanisms enable models to shift focus based on varying task demands and environmental cues, mirroring human brain's attentional capabilities.\n\n## Practical Applications and Enhancements\n\n### Context-Aware Attention\nThe CASAK-V method and similar approaches emphasize the importance of context-aware attention mechanisms. These mechanisms dynamically adjust attention patterns based on the input characteristics, ensuring that the model focuses on the most relevant information while minimizing computational and memory costs. This is particularly useful for long-context processing, where traditional attention mechanisms can be computationally expensive.\n\n### Memory-Augmented Neural Networks\nTechniques like Memory-Augmented Neural Networks, such as Neural Turing Machines (NTMs), incorporate external memory components for dynamic read and write operations. These enhancements allow models to store, recall, and update memories, adapting to user interactions over time. Integrating such memory systems with adaptive attention mechanisms can further improve the model's ability to handle long-range dependencies and varying input characteristics.\n\n## Key Insights for Improving Autoregressive Language Models\n\n- **Hybrid Memory Representation**: Combining different types of memory representations (eidetic and fading memory) can enhance model capabilities. This can be integrated with adaptive attention mechanisms to optimize memory usage and efficiency[Analysis Note].\n- **Learned Context Compression**: Using learned summary vectors to compress long contexts can reduce memory requirements and improve inference efficiency. This can be particularly beneficial when combined with adaptive attention mechanisms that dynamically adjust focus based on input characteristics[Analysis Note].\n- **Dynamic Attention Patterns**: Adaptive attention patterns, such as those in MoA (Mixture of Sparse Attention), can increase the effective context length and improve efficiency. These patterns can be learned during pre-training and fine-tuned for specific tasks[Analysis Note].\n\nBy incorporating these adaptive attention mechanisms and memory management techniques, researchers can significantly improve the performance, efficiency, and scalability of autoregressive language models.\n",
            "analysis": "Analysis Note: State Compression and Memory Management in LMs\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing FastTTTLinear, particularly in:\n\na) State Compression Techniques:\n- Novel approaches to compress model states while maintaining expressiveness\n- Methods for efficient memory management in long-sequence processing\n- Hybrid approaches combining different compression strategies\n\nb) Memory Management:\n- Techniques for reducing memory requirements through state compression\n- Methods for efficient handling of long-range dependencies\n- Approaches to balance memory usage and computational efficiency\n\nc) Model Architecture:\n- Integration of state space models with attention mechanisms\n- Efficient handling of position information\n- Novel approaches to memory compression and retrieval\n\n2. Useful References with Excerpts\n\nFrom \"B'MOJO: Hybrid State Space Realizations\":\n> \"Current architectures use such resources to represent data either eidetically over a finite span ('context' in Transformers), or fading over an infinite span (in State Space Models, or SSMs)\"\n> \"We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory\"\n\nKey insight: Combining different types of memory representations can enhance model capabilities.\n\nFrom \"Adapting Language Models to Compress Contexts\":\n> \"We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts.\"\n> \"Summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost.\"\n\nKey insight: Context compression through learned summary vectors can improve efficiency.\n\nFrom \"MoA: Mixture of Sparse Attention\":\n> \"MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts.\"\n> \"MoA increases the effective context length by 3.9\u00d7 with the same average attention span\"\n\nKey insight: Adaptive attention patterns can improve efficiency and effectiveness.\n\n3. Key Insights and Detailed Analysis\n\na) Memory Representation:\n- Combining eidetic and fading memory offers potential benefits\n- Learned compression of contexts can reduce memory requirements\n- Adaptive attention patterns can enhance efficiency\n\nb) State Management:\n- Summary vectors can effectively compress long contexts\n- Different attention heads can serve different purposes\n- Memory compression can be learned during pre-training\n\nc) Architecture Considerations:\n- Need to balance different types of memory\n- Importance of adaptive mechanisms\n- Potential for hybrid approaches\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in adaptive attention mechanisms\n2. Methods for combining different types of memory representations\n3. Techniques for efficient state updates in long sequences\n\nKeywords for next search:\n- \"adaptive attention mechanisms language models\"\n- \"hybrid memory representation transformers\"\n\n5. References\n\n[1] Zancato, L., et al. (2024). B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory. arXiv preprint.\n\n[2] Chevalier, A., et al. (2023). Adapting Language Models to Compress Contexts. Conference on Empirical Methods in Natural Language Processing.\n\n[3] Fu, T., et al. (2024). MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression. arXiv preprint.\n\n[4] Jiang, H., et al. (2023). LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. Conference on Empirical Methods in Natural Language Processing.\n\n[5] Kurtic, E., et al. (2023). ZipLM: Inference-Aware Structured Pruning of Language Models. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "state space models attention integration",
            "detail": "Find papers and technical documents discussing the integration of state space models with attention mechanisms, particularly focusing on methods that combine their strengths efficiently.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and technical documents discussing the integration of state space models with attention mechanisms, particularly focusing on methods that combine their strengths efficiently.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 1.00)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.99)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.99)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 4. The Illusion of State in State-Space Models (Avg. Score: 0.99)\n\n*William Merrill, Jackson Petty, Ashish Sabharwal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Analysis of state-space models reveals that SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.\n\n**Abstract:** State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill&Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the\"state\"in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.\n\n##### *Relevant Chunk: No. 13/39 (Score: 0.99)*\n\n```\narXiv:2312.00752. Gu, A., Johnson, I., Goel, K., Saab, K. K., Dao, T., Rudra, A., and Re, C. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In NeurIPS, 2021. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In ICLR, 2022a. Gu, A., Goel, K., Saab, K., and R\u00e9, C. Structured state spaces: Combining continuous-time, recurrent, and convolutional models, January 2022b. URL https://hazyresearch.stanford.edu/ blog/2022-01-14-s 4-3. Blog post accessed January $31,2024$. Hao, S., Angluin, D., and Frank, R. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. TACL, 10:800-810, 2022. Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D. Liquid structural state-space models.\n```\n\n#### 5. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.99)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.99)*\n\n```\nFinally, we provide analysis and ablation experiments to further demonstrate the effectiveness of the proposed method. Our code ${ }^{1}$ and pre-trained model checkpoints ${ }^{2}$ are publicly available. ## 2 Background\n\n### 2.1 Attention Mechanism\n\nSuppose the input to the layer is $\\mathbf{X} \\in \\mathbb{R}^{L \\times d}$, where $L$ is the sequence length and $d$ is the embedding dimension, then the attention mechanism outputs\n\n$$\n\\operatorname{Attn}(\\mathbf{X})=\\operatorname{softmax}\\left(\\frac{\\mathbf{Q K}^{\\top}}{\\sqrt{d}}\\right) \\mathbf{V}\n$$\n\nwhere $\\mathbf{Q}=\\mathbf{X} \\mathbf{W}_{q}, \\mathbf{K}=\\mathbf{X} \\mathbf{W}_{k}, \\mathbf{V}=\\mathbf{X} \\mathbf{W}_{v}$. Here $\\mathbf{W}_{q}, \\mathbf{W}_{k}, \\mathbf{W}_{v} \\in \\mathbb{R}^{d \\times d}$ are learnable weights. The attention mechanism can simultaneously compute the alignment between any pair of input tokens, such that it models long-range dependencies better than recurrent neural networks. Specifically, denote the attention score matrix $\\mathbf{A}=$ $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right) \\in \\mathbb{R}^{L \\times L}$. Then, $\\mathbf{A}_{i j}$ captures the alignment between the $i$-th and the $j$-th input tokens. ### 2.2 State Space Models\n\nContinuous time state space model. A continuous time latent space model maps a 1-dimensional input signal $u(t)$ to a $d_{s}$-dimensional latent state $x(t)$, after which $x(t)$ is mapped to a 1-dimensional output signal $y(t)$. Concretely,\n\n$$\nx^{\\prime}(t)=\\mathbf{A} x(t)+\\mathbf{B} u(t), \\quad y(t)=\\mathbf{C} x(t)\n$$\n\nHere, $\\mathbf{A} \\in \\mathbb{R}^{d_{s} \\times d_{s}}, \\mathbf{B} \\in \\mathbb{R}^{d_{s}}$ and $\\mathbf{C} \\in \\mathbb{R}^{d_{s}}$. Existing works leverage Eq. 2 to model long sequences. For example, Gu et al. (2020) claim that randomly initialized parameters $\\mathbf{A}, \\mathbf{B}$ and $\\mathbf{C}$\n\n[^1]cannot model long-range dependencies well. Subsequently, a class of matrices (termed HiPPO, highorder polynomial projection operators) are proposed to initialize A. The HiPPO matrices are designed such that the state $x(t)$ at time $t$ can memorize the history of the input $u(t)$ up to time $t$. Discrete time state space model. In practice, we often work with discrete sequences such as natural language inputs $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, where $L$ is the sequence length. To facilitate modeling discrete data, the model in Eq. 2 can be discretized (using the bilinear method) by a step size $\\Delta$, such that\n\n$$\n\\begin{aligned}\n& x_{k}=\\overline{\\mathbf{A}} x_{k-1}+\\overline{\\mathbf{B}} u_{k}, \\quad y_{k}=\\overline{\\mathbf{C}} x_{k} \\\\\n& \\text { where } \\overline{\\mathbf{A}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1}(\\mathbf{I}+\\Delta / 2 \\cdot \\mathbf{A}) \\\\\n& \\quad \\overline{\\mathbf{B}}=(\\mathbf{I}-\\Delta / 2 \\cdot \\mathbf{A})^{-1} \\Delta \\mathbf{B}, \\quad \\overline{\\mathbf{C}}=\\mathbf{C}\n\\end{aligned}\n$$\n\nWe unroll the above recurrent representation, after which we have\n\n$$\ny_{k}=\\overline{\\mathbf{C A}}^{k} \\overline{\\mathbf{B}} u_{0}+\\cdots+\\overline{\\mathbf{C A B}} u_{k-1}+\\overline{\\mathbf{C B}} u_{k}\n$$\n\nThis can be written as a convolutional representation $y=\\overline{\\mathbf{K}} * u$, where the convolution kernel\n\n$$\n\\overline{\\mathbf{K}} \\in \\mathbb{R}^{L}=\\left(\\overline{\\mathbf{C B}}, \\overline{\\mathbf{C A B}}, \\cdots, \\overline{\\mathbf{C A}}^{L-1} \\overline{\\mathbf{B}}\\right)\n$$\n\nHere, \" $*$ \" is the discrete convolution operator, $u$ represents the input sequence $\\left(u_{0}, u_{1}, \\cdots, u_{L}\\right)$, and $y$ represents the corresponding output sequence $\\left(y_{0}, y_{1}, \\cdots, y_{L}\\right)$.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space models attention integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: state space models attention integration*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: state space models attention integration*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 270  (*Influential: 21*)\n\n#### 3. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: state space models attention integration*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space models attention integration*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 156  (*Influential: 36*)\n\n#### 5. State-Free Inference of State-Space Models: The Transfer Function Approach\n\n*From Search Query: state space models attention integration*\n\n*Rom N. Parnichkun, Stefano Massaroli, Alessandro Moro, Jimmy T.H. Smith, Ramin M. Hasani, Mathias Lechner, Qi An, Christopher R'e, Hajime Asama, Stefano Ermon, Taiji Suzuki, Atsushi Yamashita, Michael Poli*\n\n**TL;DR:** This work uncovers a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size.\n\n**Abstract:** We approach designing a state-space model for deep learning applications through its dual representation, the transfer function, and uncover a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers -- parametrized in time-domain -- on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space models attention integration*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models\n\n*From Search Query: state space models attention integration*\n\n*Jingren Zhou, Fei Huang, Ji Zhang, Qi Qian, Ming Yan, Anwen Hu, Haowei Liu, Haiyang Xu, Jiabo Ye*\n\n**Abstract:** Multi-modal Large Language Models (MLLMs) have demonstrated remarkable capabilities in executing instructions for a variety of single-image tasks. Despite this progress, significant challenges remain in modeling long image sequences. In this work, we introduce the versatile multi-modal large language model, mPLUG-Owl3, which enhances the capability for long image-sequence understanding in scenarios that incorporate retrieved image-text knowledge, interleaved image-text, and lengthy videos. Specifically, we propose novel hyper attention blocks to efficiently integrate vision and language into a common language-guided semantic space, thereby facilitating the processing of extended multi-image scenarios. Extensive experimental results suggest that mPLUG-Owl3 achieves state-of-the-art performance among models with a similar size on single-image, multi-image, and video benchmarks. Moreover, we propose a challenging long visual sequence evaluation named Distractor Resistance to assess the ability of models to maintain focus amidst distractions. Finally, with the proposed architecture, mPLUG-Owl3 demonstrates outstanding performance on ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to the development of more efficient and powerful multimodal large language models.\n\n**Published:** 2024-08-09\n\n\n\n#### 3. xLSTM: Extended Long Short-Term Memory\n\n*From Search Query: state space models attention integration*\n\n*Sepp Hochreiter, Johannes Brandstetter, G\u00fcnter Klambauer, Michael Kopp, Oleksandra Prudnikova, Andreas Auer, Markus Spanring, Korbinian P\u00f6ppel, Maximilian Beck*\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Published:** 2024-05-07\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by integrating state space models (SSMs) with attention mechanisms, here are some key findings and references that align with your goals:\n\n## Integration of SSMs and Attention Mechanisms\n\n### Mamba and State Space Models\nThe Mamba framework, which integrates SSMs, has shown promising results in various domains. In the context of PDEs and other sequential data, Mamba Neural Operators (MNOs) combine the strengths of SSMs and neural operators, offering better scalability and efficiency in handling long-range dependencies.\n\n### Spatial-Mamba for Visual Tasks\nFor visual tasks, Spatial-Mamba extends the Mamba framework by incorporating structure-aware state fusion, which effectively captures spatial dependencies in images. This approach unifies the original Mamba and linear attention under the same matrix multiplication framework, enhancing the model's ability to handle 2D data efficiently.\n\n### State Space Duality and Efficient Modeling\nThe concept of State Space Duality (SSD) introduced in Mamba2 simplifies the parameter matrix to a scalar representation, making it feasible to explore larger and more expressive state spaces without sacrificing efficiency. This is particularly useful for modeling long sequences, such as those encountered in natural language processing and single-cell transcriptomics.\n\n## Adaptive Attention and Memory Management\n\n### Combining SSMs with Attention\nThe integration of SSMs with attention mechanisms can enhance global information processing. For instance, the SPADE approach augments global information using SSMs, which complements local attention methods and improves the handling of long-range dependencies[Analysis Note].\n\n### Efficient Position-Aware Processing\nTranslation-invariant self-attention mechanisms can improve efficiency and interpretability by accounting for relative token positions without conventional position embeddings. This approach can be combined with SSMs to maintain efficiency while processing long sequences[Analysis Note].\n\n### Dynamic Routing and Adaptive Attention\nDynamic routing between different attention mechanisms and selective activation of attention components can reduce computational overhead. The LLaMA-Adapter approach, which uses zero-initialized attention mechanisms, demonstrates how to adaptively inject new instructional cues while preserving pre-trained knowledge efficiently[Analysis Note].\n\n## Applications in NLP and Other Domains\n\n### NLP Tasks\nSSMs have been successfully applied in NLP to model complex dependencies and process data efficiently. The integration of SSMs with attention mechanisms can enhance the predictive capabilities of models, especially in tasks requiring efficient handling of long-range dependencies.\n\n### Biological Data\nIn the context of single-cell ultra-long transcriptome data, SC-MAMBA2 integrates SSMs with the MAMBA framework, enabling efficient and scalable modeling of extensive gene sequences. This approach overcomes the limitations of traditional Transformer-based architectures in handling large-scale biological data.\n\n## Future Directions\n\n- **Efficient State Compression and Update Mechanisms**: Investigate recent advances in state compression techniques to reduce memory usage while maintaining model performance.\n- **Combining SSMs with Adaptive Attention**: Explore methods that dynamically adjust attention patterns based on input characteristics and integrate these with SSMs for enhanced global information processing.\n- **Dynamic Routing in Attention Mechanisms**: Develop techniques for adaptive routing between different attention mechanisms to optimize computational efficiency and model performance.\n\nBy leveraging these insights, researchers can design autoregressive language models that efficiently integrate the strengths of SSMs and attention mechanisms, leading to improved performance, scalability, and efficiency.\n",
            "analysis": "Analysis Note: Adaptive Attention and Memory Management\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing FastTTTLinear, particularly in:\n\na) Adaptive Attention Mechanisms:\n- Dynamic adjustment of attention patterns based on input characteristics\n- Hybrid approaches combining different types of attention\n- Memory-efficient implementations for long sequences\n\nb) State Space Integration:\n- Complementary strengths of SSMs and attention mechanisms\n- Efficient handling of global information\n- Selective activation of attention components\n\nc) Position-Aware Processing:\n- Translation-invariant attention mechanisms\n- Efficient position encoding methods\n- Adaptive position representation\n\n2. Useful References with Excerpts\n\nFrom \"SPADE: State Space Augmented Transformer\":\n> \"SSM augments global information, which complements the lack of long-range dependency issue in local attention methods\"\n> \"Experimental results demonstrate the effectiveness of the proposed method\"\n\nKey insight: Combining SSMs with attention can enhance global information processing while maintaining efficiency.\n\nFrom \"Translation-Invariant Self-Attention\":\n> \"Analysis of position embeddings finds strong evidence of translation invariance\"\n> \"TISA accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings\"\n\nKey insight: Translation-invariant attention mechanisms can improve efficiency and interpretability.\n\nFrom \"LLaMA-Adapter\":\n> \"Zero-initialized attention mechanism with zero gating adaptively injects new instructional cues while effectively preserves pre-trained knowledge\"\n> \"Superior reasoning performance while maintaining computational efficiency\"\n\nKey insight: Zero-initialized attention mechanisms can enable efficient adaptation while preserving model capabilities.\n\nFrom \"DeepSpeed-VisualChat\":\n> \"Innovative multi-modal causal attention mechanism\"\n> \"Superior scalability up to 70B parameter language model size\"\n\nKey insight: Novel attention mechanisms can improve scalability and efficiency in large models.\n\n3. Key Insights and Detailed Analysis\n\na) Adaptive Attention Design:\n- Zero-initialized attention mechanisms show promise for efficient adaptation\n- Translation invariance can reduce positional encoding overhead\n- Multi-scale attention patterns can improve efficiency\n\nb) Memory Management:\n- Selective activation of attention components can reduce computation\n- Efficient state compression techniques\n- Dynamic memory allocation based on input characteristics\n\nc) Integration Strategies:\n- Complementary use of SSMs and attention\n- Adaptive routing between different attention mechanisms\n- Efficient position-aware processing\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in efficient state compression and update mechanisms\n2. Methods for combining SSMs with adaptive attention\n3. Techniques for dynamic routing in attention mechanisms\n\nKeywords for next search:\n- \"state space models attention integration\"\n- \"dynamic routing attention mechanisms\"\n\n5. References\n\n[1] Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint.\n\n[2] Wennberg, U., & Henter, G. (2021). The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models. Annual Meeting of the Association for Computational Linguistics.\n\n[3] Gao, P., Liu, C., Qiao, Y., Li, H., Lu, P., Yan, S., ... & Zhang, R. (2023). LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. arXiv preprint.\n\n[4] Qin, H., He, Y., Rajbhandari, S., Awan, A. A., Ruwase, O., Zhang, M., ... & Yao, Z. (2023). DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention. arXiv preprint.\n\n[5] Qiao, Y., Li, H., Yue, X., Chen, X., Wen, Y., Ren, S., ... & Han, J. (2023). ImageBind-LLM: Multi-modality Instruction Tuning. arXiv preprint."
        },
        {
            "ready": false,
            "query": "content dependent state space models",
            "detail": "Find papers and technical documents discussing content-dependent state space models, particularly focusing on methods that adapt model parameters based on input content while maintaining computational efficiency.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and technical documents discussing content-dependent state space models, particularly focusing on methods that adapt model parameters based on input content while maintaining computational efficiency.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.97)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.97)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n#### 2. MambaByte: Token-free Selective State Space Model (Avg. Score: 0.95)\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 18  (*Influential: 1*)\n\n**TL;DR:** This work proposes MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences, and develops an adaptation of speculative decoding with tokenized drafting and byte-level verification, establishing the viability of SSMs in enabling token-free language modeling.\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.\n\n##### *Relevant Chunk: No. 13/40 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2101.00027, 2020a. Yingqiang Gao, Nikola I Nikolov, Yuhuang Hu, and Richard HR Hahnloser. Character-Level Translation with Self-attention. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 1591-1604, 2020b. Karan Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. In International Conference on Machine Learning, pp. 7616-7633. PMLR, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396, 2021. Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HiPPO: State Space Models with Generalized Orthogonal Basis Projections. In International Conference on Learning Representations, 2023. URL https://openreview.net/f orum?id=k1K170Q3KB. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal State Spaces are as Effective as Structured State Spaces.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.94)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.94)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 4. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.94)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 1/20 (Score: 0.94)*\n\n```\n# State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory \n\nShida Wang<br>Department of Mathematics<br>National University of Singapore<br>e0622338@u.nus.edu\n\nBeichen Xue<br>Department of Statistics and Data Science<br>National University of Singapore<br>e0773769@u.nus.edu\n\n\n#### Abstract\n\nState-space models have gained popularity in sequence modelling due to their simple and efficient network structures.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.94)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 4/74 (Score: 0.94)*\n\n```\n2023), which may be easier to reason about. Computation. After the parameters have been transformed from $(\\Delta, A, B, C) \\mapsto(\\bar{A}, \\bar{B}, C)$, the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3). Commonly, the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where the inputs are seen one timestep at a time). Linear Time Invariance (LTI). An important property of equations (1) to (3) is that the model's dynamics are constant through time. In other words $(\\Delta, A, B, C)$, and consequently $(\\bar{A}, \\bar{B})$ as well, are fixed for all time-steps. This property is\ncalled linear time invariance ( $L T I$ ), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models. Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiency bottlenecks. Structure and Dimensions. Finally, we note that structured SSMs are so named because computing them efficiently also requires imposing structure on the $A$ matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use. In this case, the $\\boldsymbol{A} \\in \\mathbb{R}^{N \\times N}, \\boldsymbol{B} \\in \\mathbb{R}^{N \\times 1}, \\boldsymbol{C} \\in \\mathbb{R}^{1 \\times N}$ matrices can all be represented by $N$ numbers. To operate over an input sequence $x$ of batch size $B$ and length $L$ with $D$ channels, the SSM is applied independently to each channel. Note that in this case, the total hidden state has dimension $D N$ per input, and computing it over the sequence length requires $O(B L D N)$ time and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3. General State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in different disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning). Throughout this entire paper we use the term \"SSM\" to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: content dependent state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Liquid Structural State-Space Models\n\n*From Search Query: content dependent state space models*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 2. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: content dependent state space models*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 3. From Generalization Analysis to Optimization Designs for State Space Models\n\n*From Search Query: content dependent state space models*\n\n*Fusheng Liu, Qianxiao Li*\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 4. Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data\n\n*From Search Query: content dependent state space models*\n\n*Maximilian Karl, Maximilian S\u00f6lch, Justin Bayer, Patrick van der Smagt*\n\n**TL;DR:** Deep Variational Bayes Filters is introduced, a new method for unsupervised learning and identification of latent Markovian state space models that can overcome intractable inference distributions via variational inference and enables realistic long-term prediction.\n\n**Abstract:** We introduce Deep Variational Bayes Filters (DVBF), a new method for unsupervised learning and identification of latent Markovian state space models. Leveraging recent advances in Stochastic Gradient Variational Bayes, DVBF can overcome intractable inference distributions via variational inference. Thus, it can handle highly nonlinear input data with temporal and spatial dependencies such as image sequences without domain knowledge. Our experiments show that enabling backpropagation through transitions enforces state space assumptions and significantly improves information content of the latent embedding. This also enables realistic long-term prediction.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2016\n\n**Citations:** 351  (*Influential: 39*)\n\n#### 5. Discriminative State Space Models\n\n*From Search Query: content dependent state space models*\n\n*Vitaly Kuznetsov, M. Mohri*\n\n**TL;DR:** This paper provides data-dependent generalization guarantees for learning Discriminative State-Space Models for forecasting non-stationary time series based on the recently introduced notion of discrepancy and an in-depth analysis of the complexity of such models.\n\n**Abstract:** In this paper, we introduce and analyze Discriminative State-Space Models for forecasting non-stationary time series. We provide data-dependent generalization guarantees for learning these models based on the recently introduced notion of discrepancy. We provide an in-depth analysis of the complexity of such models. Finally, we also study the generalization guarantees for several structural risk minimization approaches to this problem and provide an efficient implementation for one of them which is based on a convex objective.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 3  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: content dependent state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Deep Bilateral Learning for Real-Time Image Enhancement\n\n*From Search Query: content dependent state space models*\n\n*Fr\u00e9do Durand, Samuel W. Hasinoff, Jonathan T. Barron, Micha\u00ebl Gharbi, Jiawen Chen*\n\n**Abstract:** Performance is a critical challenge in mobile image processing. Given a\nreference imaging pipeline, or even human-adjusted pairs of images, we seek to\nreproduce the enhancements and enable real-time evaluation. For this, we\nintroduce a new neural network architecture inspired by bilateral grid\nprocessing and local affine color transforms. Using pairs of input/output\nimages, we train a convolutional neural network to predict the coefficients of\na locally-affine model in bilateral space. Our architecture learns to make\nlocal, global, and content-dependent decisions to approximate the desired image\ntransformation. At runtime, the neural network consumes a low-resolution\nversion of the input image, produces a set of affine transformations in\nbilateral space, upsamples those transformations in an edge-preserving fashion\nusing a new slicing node, and then applies those upsampled transformations to\nthe full-resolution image. Our algorithm processes high-resolution images on a\nsmartphone in milliseconds, provides a real-time viewfinder at 1080p\nresolution, and matches the quality of state-of-the-art approximation\ntechniques on a large class of image operators. Unlike previous work, our model\nis trained off-line from data and therefore does not require access to the\noriginal operator at runtime. This allows our model to learn complex,\nscene-dependent transformations for which no reference implementation is\navailable, such as the photographic edits of a human retoucher.\n\n**Published:** 2017-07-10\n\n\n\n#### 3. Transition-Based Dependency Parsing with Stack Long Short-Term Memory\n\n*From Search Query: content dependent state space models*\n\n*Noah A. Smith, Miguel Ballesteros, Wang Ling, Chris Dyer, Austin Matthews*\n\n**Abstract:** We propose a technique for learning representations of parser states in\ntransition-based dependency parsers. Our primary innovation is a new control\nstructure for sequence-to-sequence neural networks---the stack LSTM. Like the\nconventional stack data structures used in transition-based parsing, elements\ncan be pushed to or popped from the top of the stack in constant time, but, in\naddition, an LSTM maintains a continuous space embedding of the stack contents.\nThis lets us formulate an efficient parsing model that captures three facets of\na parser's state: (i) unbounded look-ahead into the buffer of incoming words,\n(ii) the complete history of actions taken by the parser, and (iii) the\ncomplete contents of the stack of partially built tree fragments, including\ntheir internal structures. Standard backpropagation techniques are used for\ntraining and yield state-of-the-art parsing performance.\n\n**Conference:** transition-based-dependency-parsing-with-5\n\n**Published:** 2015-05-29\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using content-dependent state space models, the following resources and insights are particularly relevant:\n\n## Content-Dependent State Space Models\n\n### S7 Model\nThe S7 model, introduced in the paper \"Selective and Simplified State Space Layers for Sequence Modeling,\" is a significant advancement in this area. It incorporates input-dependent dynamics into the state space model, allowing the model to adapt state transitions based on the input content. This approach enhances the model's ability to capture long-range dependencies and perform content-based reasoning while maintaining computational efficiency.\n\nKey points:\n- The S7 model makes the transition matrices a function of the input, enabling dynamic adjustment of state transitions.\n- This model combines the efficiency of recurrent architectures with the adaptive processing capabilities of more complex models.\n- It ensures long-term stability and performance through stable reparameterization and careful design choices.\n\n### Mamba and State Space Duality\nThe Mamba framework, discussed in several sources, also addresses the need for content-dependent processing. Mamba introduces a method where the parameters of the state space model are input-independent but can be adapted to be input-dependent, improving the model's expressiveness and ability to capture long-term context.\n\nKey points:\n- Mamba employs an S4-based data-dependent selection technique, which can be extended to make parameters input-dependent.\n- The concept of State Space Duality (SSD) simplifies the matrix transformations, making it more suitable for implementation in modern computational frameworks.\n\n### Efficient Implementation and Theoretical Foundations\nThe integration of state space models with attention mechanisms and the theoretical foundations underlying these models are crucial for efficient implementation.\n\n- The paper \"Transformers are SSMs\" highlights the deep theoretical connections between SSMs and attention mechanisms, providing a framework for combining these approaches efficiently[Analysis Note].\n- The use of structured matrix decompositions and state space duality frameworks can enable hardware-aware optimizations and efficient algorithms[Analysis Note].\n\n## Key Insights for Improvement\n\n### Content-Dependent Processing\nMaking SSM parameters input-dependent is a key improvement, as it enhances the model's expressiveness and ability to perform content-based reasoning. This is demonstrated in the S7 model and can be applied to other SSM frameworks like Mamba.\n\n### Efficient State Management\nEfficient state compression techniques and adaptive memory mechanisms are essential for managing the state in content-dependent SSMs. This includes hierarchical memory structures and dynamic parameter generation, which can help balance expressiveness with computational efficiency[Analysis Note].\n\n### Hardware Efficiency\nHardware-efficient implementations are critical. Techniques such as quantization methods and parallel processing capabilities can ensure that these models are deployable across various hardware platforms without compromising performance.\n\nBy focusing on these aspects, researchers can develop autoregressive language models that leverage the strengths of content-dependent state space models, ensuring improved performance, efficiency, and scalability.\n",
            "analysis": "Analysis Note: Integration of SSMs, Attention, and Memory Management\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for enhancing FastTTTLinear, particularly in:\n\na) State Space and Attention Integration:\n- SSMs and attention mechanisms have complementary strengths\n- Novel frameworks for combining them efficiently\n- Opportunities for hardware-efficient implementations\n\nb) Memory and State Management:\n- Efficient state compression techniques\n- Dynamic parameter generation\n- Adaptive routing mechanisms\n\nc) Theoretical Foundations:\n- State space duality framework\n- Connections between SSMs and attention variants\n- Mathematical foundations for integration\n\n2. Useful References with Excerpts\n\nFrom \"Transformers are SSMs\":\n> \"We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices.\"\n\nKey insight: Deep theoretical connections exist between SSMs and attention mechanisms.\n\nFrom \"CHELA\":\n> \"To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern\"\n\nKey insight: Combining linear attention with SSMs can leverage benefits of both approaches.\n\nFrom \"Mamba\":\n> \"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n\nKey insight: Making SSM parameters content-dependent can improve model expressiveness.\n\nFrom \"The Illusion of State\":\n> \"SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems\"\n\nKey insight: Need to carefully consider limitations when designing hybrid approaches.\n\n3. Key Insights and Detailed Analysis\n\na) State Space Duality Framework:\n- Provides theoretical foundation for combining SSMs and attention\n- Enables efficient algorithms through structured matrix decompositions\n- Offers opportunities for hardware-aware optimizations\n\nb) Content-Dependent Processing:\n- Making SSM parameters input-dependent improves expressiveness\n- Need for efficient implementation of content-dependent mechanisms\n- Balance between expressiveness and computational efficiency\n\nc) Memory Management:\n- Importance of efficient state compression\n- Need for adaptive memory mechanisms\n- Potential for hierarchical memory structures\n\nd) Hardware Considerations:\n- Need for hardware-efficient implementations\n- Importance of parallel processing capabilities\n- Balance between theoretical benefits and practical efficiency\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Efficient implementations of content-dependent SSMs\n2. Hardware-aware optimization techniques\n3. Memory compression and management strategies\n\nKeywords for next search:\n- \"content dependent state space models\"\n- \"hardware efficient ssm implementation\"\n\n5. References\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. International Conference on Machine Learning.\n\n[2] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[4] Merrill, W., Petty, J., & Sabharwal, A. (2024). The Illusion of State in State-Space Models. arXiv preprint.\n\n[5] Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint."
        },
        {
            "ready": false,
            "query": "practical SSM implementation, memory efficient hierarchical models, hardware optimization neural networks",
            "detail": "Search for papers discussing:\n1. Practical implementations and optimizations of content-dependent state space models\n2. Memory-efficient hierarchical architectures for sequence modeling\n3. Hardware-aware design strategies for neural networks\n4. Real-world performance benchmarks of SSM architectures\n5. Efficient implementations of adaptive state updates",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Practical implementations and optimizations of content-dependent state space models\n2. Memory-efficient hierarchical architectures for sequence modeling\n3. Hardware-aware design strategies for neural networks\n4. Real-world performance benchmarks of SSM architectures\n5. Efficient implementations of adaptive state updates\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 1.00)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.99)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 5. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.98)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.98)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: practical SSM implementation, memory efficient hierarchical models, hardware optimization neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. On a Neural Implementation of Brenier's Polar Factorization\n\n*From Search Query: practical SSM implementation*\n\n*Nina Vesseron, Marco Cuturi*\n\n**TL;DR:** A practical implementation of Brenier's polar factorization theorem is proposed, and possible applications to non-convex optimization problems, as well as sampling of densities that are not log-concave are illustrated.\n\n**Abstract:** In 1991, Brenier proved a theorem that generalizes the polar decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of Brenier's polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. Making Scalable Meta Learning Practical\n\n*From Search Query: practical SSM implementation*\n\n*Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing*\n\n**TL;DR:** SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.\n\n**Abstract:** Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 3. AutoCoreset: An Automatic Practical Coreset Construction Framework\n\n*From Search Query: practical SSM implementation*\n\n*Alaa Maalouf, M. Tukan, V. Braverman, Daniela Rus*\n\n**TL;DR:** This work proposes an automatic practical framework for constructing coresets, which requires (only) the input data and the desired cost function from the user, without the need for any other task-related computation to be done by the user.\n\n**Abstract:** A coreset is a tiny weighted subset of an input set, that closely resembles the loss function, with respect to a certain set of queries. Coresets became prevalent in machine learning as they have shown to be advantageous for many applications. While coreset research is an active research area, unfortunately, coresets are constructed in a problem-dependent manner, where for each problem, a new coreset construction algorithm is usually suggested, a process that may take time or may be hard for new researchers in the field. Even the generic frameworks require additional (problem-dependent) computations or proofs to be done by the user. Besides, many problems do not have (provable) small coresets, limiting their applicability. To this end, we suggest an automatic practical framework for constructing coresets, which requires (only) the input data and the desired cost function from the user, without the need for any other task-related computation to be done by the user. To do so, we reduce the problem of approximating a loss function to an instance of vector summation approximation, where the vectors we aim to sum are loss vectors of a specific subset of the queries, such that we aim to approximate the image of the function on this subset. We show that while this set is limited, the coreset is quite general. An extensive experimental study on various machine learning applications is also conducted. Finally, we provide a ``plug and play\"style implementation, proposing a user-friendly system that can be easily used to apply coresets for many problems. Full open source code can be found at \\href{https://github.com/alaamaalouf/AutoCoreset}{\\text{https://github.com/alaamaalouf/AutoCoreset}}. We believe that these contributions enable future research and easier use and applications of coresets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. Parallel and Efficient Hierarchical k-Median Clustering\n\n*From Search Query: memory efficient hierarchical models*\n\n*Vincent Cohen-Addad, Silvio Lattanzi, A. Norouzi-Fard, C. Sohler, O. Svensson*\n\n**TL;DR:** This paper introduces a new parallel algorithm for the Euclidean hierarchical k -median problem that outputs a hierarchical clustering such that for every value of k the cost of the solution is at most an O (min { d, log n } log \u2206) factor larger in expectation than that of an optimal solution.\n\n**Abstract:** As a fundamental unsupervised learning task, hierarchical clustering has been extensively studied in the past decade. In particular, standard metric formulations as hierarchical k -center, k -means, and k -median received a lot of attention and the problems have been studied extensively in different models of computation. Despite all this interest, not many ef\ufb01cient parallel algorithms are known for these problems. In this paper we introduce a new parallel algorithm for the Euclidean hierarchical k -median problem that, when using machines with memory s (for s \u2208 \u2126(log 2 ( n + \u2206 + d )) ), outputs a hierarchical clustering such that for every \ufb01xed value of k the cost of the solution is at most an O (min { d, log n } log \u2206) factor larger in expectation than that of an optimal solution. Furthermore, we also get that in for all k simultanuously the cost of the solution is at most an expected O (min { d, log n } log \u2206 log(\u2206 dn )) factor bigger that the corresponding optimal solution. The algorithm requires in O (log s ( nd log( n + \u2206))) rounds. Here d is the dimension of the data set and \u2206 is the ratio between the maximum and minimum distance of two points in the input dataset. To the best of our knowledge, this is the \ufb01rst parallel algorithm for the hierarchical k -median problem with theoretical guarantees. We further complement our theoretical results with an empirical study of our algorithm that shows its effectiveness in practice.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 5. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences\n\n*From Search Query: memory efficient hierarchical models*\n\n*Zhenhai Zhu, Radu Soricut*\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 35  (*Influential: 7*)\n\n#### 6. Memory-Based Graph Networks\n\n*From Search Query: memory efficient hierarchical models*\n\n*Amir Hosein Khas Ahmadi, Kaveh Hassani, Parsa Moradi, Leo Lee, Q. Morris*\n\n**TL;DR:** An efficient memory layer for GNNs is introduced that can learn to jointly perform graph representation learning and graph pooling and two new networks based on this memory layer are introduced: Memory-Based Graph Neural Network (MemGNN) and Graph Memory Network (GMN) that can learning hierarchical graph representations by coarsening the graph throughout the layers of memory.\n\n**Abstract:** Graph Neural Networks (GNNs) are a class of deep models that operates on data with arbitrary topology and order-invariant structure represented as graphs. We introduce an efficient memory layer for GNNs that can learn to jointly perform graph representation learning and graph pooling. We also introduce two new networks based on our memory layer: Memory-Based Graph Neural Network (MemGNN) and Graph Memory Network (GMN) that can learn hierarchical graph representations by coarsening the graph throughout the layers of memory. The experimental results demonstrate that the proposed models achieve state-of-the-art results in six out of seven graph classification and regression benchmarks. We also show that the learned representations could correspond to chemical features in the molecule data.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 84  (*Influential: 16*)\n\n#### 7. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks\n\n*From Search Query: hardware optimization neural networks*\n\n*Y. Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi, Vikas Chandra, Yingyan Lin*\n\n**TL;DR:** This work opens up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy, and proposes a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient Dnns that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real- hardware efficiency.\n\n**Abstract:** Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 8. L2ight: Enabling On-Chip Learning for Optical Neural Networks via Efficient in-situ Subspace Optimization\n\n*From Search Query: hardware optimization neural networks*\n\n*Jiaqi Gu, Hanqing Zhu, Chenghao Feng, Zixuan Jiang, Ray T. Chen, D. Pan*\n\n**TL;DR:** This synergistic framework L2ight is the first scalable on-chip learning solution that pushes this emerging field from intractable to scalable and further to efficient for next-generation self-learnable photonic neural chips.\n\n**Abstract:** Silicon-photonics-based optical neural network (ONN) is a promising hardware platform that could represent a paradigm shift in efficient AI with its CMOS-compatibility, flexibility, ultra-low execution latency, and high energy efficiency. In-situ training on the online programmable photonic chips is appealing but still encounters challenging issues in on-chip implementability, scalability, and efficiency. In this work, we propose a closed-loop ONN on-chip learning framework L2ight to enable scalable ONN mapping and efficient in-situ learning. L2ight adopts a three-stage learning flow that first calibrates the complicated photonic circuit states under challenging physical constraints, then performs photonic core mapping via combined analytical solving and zeroth-order optimization. A subspace learning procedure with multi-level sparsity is integrated into L2ight to enable in-situ gradient evaluation and fast adaptation, unleashing the power of optics for real on-chip intelligence. Extensive experiments demonstrate our proposed L2ight outperforms prior ONN training protocols with 3-order-of-magnitude higher scalability and over 30X better efficiency, when benchmarked on various models and learning tasks. This synergistic framework is the first scalable on-chip learning solution that pushes this emerging field from intractable to scalable and further to efficient for next-generation self-learnable photonic neural chips. From a co-design perspective, L2ight also provides essential insights for hardware-restricted unitary subspace optimization and efficient sparse training. We open-source our framework at https://github.com/JeremieMelo/L2ight.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 9. Temporal Effective Batch Normalization in Spiking Neural Networks\n\n*From Search Query: hardware optimization neural networks*\n\n*Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, Tiejun Huang*\n\n**TL;DR:** Experimental results show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Abstract:** Spiking Neural Networks (SNNs) are promising in neuromorphic hardware owing to utilizing spatio-temporal information and sparse event-driven signal processing. However, it is challenging to train SNNs due to the non-differentiable nature of the binary firing function. The surrogate gradients alleviate the training problem and make SNNs obtain comparable performance as Artificial Neural Networks (ANNs) with the same structure. Unfortunately, batch normalization, contributing to the success of ANNs, does not play a prominent role in SNNs because of the additional temporal dimension. To this end, we propose an effective normalization method called temporal effective batch normalization (TEBN). By rescaling the presynaptic inputs with different weights at every time-step, temporal distributions become smoother and uniform. Theoretical analysis shows that TEBN can be viewed as a smoother of SNN\u2019s optimization landscape and could help stabilize the gradient norm. Experimental results on both static and neuromorphic datasets show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 63  (*Influential: 7*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Modular Tracking Framework: A Unified Approach to Registration based Tracking\n\n*From Search Query: practical SSM implementation*\n\n*Martin Jagersand, Abhineet Singh*\n\n**Abstract:** This paper presents a modular, extensible and highly efficient open source\nframework for registration based tracking called Modular Tracking Framework\n(MTF). Targeted at robotics applications, it is implemented entirely in C++ and\ndesigned from the ground up to easily integrate with systems that support any\nof several major vision and robotics libraries including OpenCV, ROS, ViSP and\nEigen. It implements more methods, is faster, and more precise than other\nexisting systems. Further, the theoretical basis for its design is a new way to\nconceptualize registration based trackers that decomposes them into three\nconstituent sub modules - Search Method (SM), Appearance Model (AM) and State\nSpace Model (SSM).\n  In the process, we integrate many important advances published after Baker \\&\nMatthews' landmark work in 2004. In addition to being a practical solution for\nfast and high precision tracking, MTF can also serve as a useful research tool\nby allowing existing and new methods for any of the sub modules to be studied\nbetter. When a new method is introduced for one of these, the breakdown can\nhelp to experimentally find the combination of methods for the others that is\noptimum for it. By extensive use of generic programming, MTF makes it easy to\nplug in a new method for any of the sub modules so that it can not only be\ntested comprehensively with existing methods but also become immediately\navailable for deployment in any project that uses the framework. With 16 AMs,\n11 SMs and 13 SSMs implemented already, MTF provides over 2000 distinct single\nlayer trackers. It also allows two or more of these to be combined together in\nseveral ways to create a practically unlimited variety of novel multi layer\ntrackers.\n\n**Published:** 2016-02-29\n\n\n\n#### 2. Flemme: A Flexible and Modular Learning Platform for Medical Images\n\n*From Search Query: practical SSM implementation*\n\n*Yang Li, Jingyun Yang, Guoqing Zhang*\n\n**Abstract:** As the rapid development of computer vision and the emergence of powerful network backbones and architectures, the application of deep learning in medical imaging has become increasingly significant. Unlike natural images, medical images lack huge volumes of data but feature more modalities, making it difficult to train a general model that has satisfactory performance across various datasets. In practice, practitioners often suffer from manually creating and testing models combining independent backbones and architectures, which is a laborious and time-consuming process. We propose Flemme, a FLExible and Modular learning platform for MEdical images. Our platform separates encoders from the model architectures so that different models can be constructed via various combinations of supported encoders and architectures. We construct encoders using building blocks based on convolution, transformer, and state-space model (SSM) to process both 2D and 3D image patches. A base architecture is implemented following an encoder-decoder style, with several derived architectures for image segmentation, reconstruction, and generation tasks. In addition, we propose a general hierarchical architecture incorporating a pyramid loss to optimize and fuse vertical features. Experiments demonstrate that this simple design leads to an average improvement of 5.60% in Dice score and 7.81% in mean interaction of units (mIoU) for segmentation models, as well as an enhancement of 5.57% in peak signal-to-noise ratio (PSNR) and 8.22% in structural similarity (SSIM) for reconstruction models. We further utilize Flemme as an analytical tool to assess the effectiveness and efficiency of various encoders across different tasks. Code is available at https://github.com/wlsdzyzl/flemme.\n\n**Published:** 2024-08-18\n\n\n\n#### 3. Neighborhood Attention Transformer\n\n*From Search Query: memory efficient hierarchical models*\n\n*Humphrey Shi, Shen Li, Jiachen Li, Steven Walton, Ali Hassani*\n\n**Abstract:** We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2022-04-14\n\n\n\n#### 4. SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing\n\n*From Search Query: memory efficient hierarchical models*\n\n*Jingfeng Zhang, Zhen Han, Yulin Pan, Chaojie Mao, Zeyinzi Jiang*\n\n**Abstract:** Image diffusion models have been utilized in various tasks, such as text-to-image generation and controllable image synthesis. Recent research has introduced tuning methods that make subtle adjustments to the original models, yielding promising results in specific adaptations of foundational generative diffusion models. Rather than modifying the main backbone of the diffusion model, we delve into the role of skip connection in U-Net and reveal that hierarchical features aggregating long-distance information across encoder and decoder make a significant impact on the content and quality of image generation. Based on the observation, we propose an efficient generative tuning framework, dubbed SCEdit, which integrates and edits Skip Connection using a lightweight tuning module named SC-Tuner. Furthermore, the proposed framework allows for straightforward extension to controllable image synthesis by injecting different conditions with Controllable SC-Tuner, simplifying and unifying the network design for multi-condition inputs. Our SCEdit substantially reduces training parameters, memory usage, and computational expense due to its lightweight tuners, with backward propagation only passing to the decoder blocks. Extensive experiments conducted on text-to-image generation and controllable image synthesis tasks demonstrate the superiority of our method in terms of efficiency and performance. Project page: \\url{https://scedit.github.io/}\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2023-12-18\n\n\n\n#### 5. Multi-Component Optimization and Efficient Deployment of Neural-Networks on Resource-Constrained IoT Hardware\n\n*From Search Query: hardware optimization neural networks*\n\n*Rajiv Ranjan, Albert Zomaya, Schahram Dustdar, Muhammad Intizar Ali, John G. Breslin, Pankesh Patel, Dineshkumar Sundaram, Bharath Sudharsan*\n\n**Abstract:** The majority of IoT devices like smartwatches, smart plugs, HVAC controllers, etc., are powered by hardware with a constrained specification (low memory, clock speed and processor) which is insufficient to accommodate and execute large, high-quality models. On such resource-constrained devices, manufacturers still manage to provide attractive functionalities (to boost sales) by following the traditional approach of programming IoT devices/products to collect and transmit data (image, audio, sensor readings, etc.) to their cloud-based ML analytics platforms. For decades, this online approach has been facing issues such as compromised data streams, non-real-time analytics due to latency, bandwidth constraints, costly subscriptions, recent privacy issues raised by users and the GDPR guidelines, etc. In this paper, to enable ultra-fast and accurate AI-based offline analytics on resource-constrained IoT devices, we present an end-to-end multi-component model optimization sequence and open-source its implementation. Researchers and developers can use our optimization sequence to optimize high memory, computation demanding models in multiple aspects in order to produce small size, low latency, low-power consuming models that can comfortably fit and execute on resource-constrained hardware. The experimental results show that our optimization components can produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our optimization sequence is generic and can be applied to any state-of-the-art models trained for anomaly detection, predictive maintenance, robotics, voice recognition, and machine vision.\n\n**Published:** 2022-04-20\n\n\n\n#### 6. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: hardware optimization neural networks*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on practical implementations of content-dependent State Space Models (SSMs), memory-efficient hierarchical architectures, hardware-aware design strategies, real-world performance benchmarks, and efficient implementations of adaptive state updates, here are some key findings and references:\n\n## Practical Implementations and Optimizations of Content-Dependent SSMs\n\n- The paper on \"Provable Benefits of Complex Parameterizations for Structured State Space Models\" discusses the practical learning of mappings using complex SSMs. It highlights that complex parameterizations significantly improve performance, especially in tasks where real parameterizations struggle. The introduction of selectivity, a new architectural feature, allows real parameterizations to achieve comparable or better performance in certain tasks.\n\n- The \"Mamba\" neural network architecture, discussed in the context of selective SSM models, implements a data-dependent selection mechanism that filters out irrelevant information. This approach enhances efficiency and performance by integrating the H3 SSM block design with the MLP block from transformer architecture, making the overall structure more efficient.\n\n## Memory-Efficient Hierarchical Architectures for Sequence Modeling\n\n- The \"Mamba\" architecture is notable for its memory-efficient design. It employs a recurrent computation method with a scan, avoiding memory access overhead and achieving linear performance that is three times faster than traditional methods. This architecture also uses a parallel scan algorithm to optimize GPU memory hierarchy usage.\n\n- The use of hierarchical approaches in sequence modeling is emphasized in the analysis note, where hierarchical structures are shown to enhance stability while maintaining expressiveness. This aligns with the benefits seen in models like Mamba, which simplify the overall architecture and enhance efficiency.\n\n## Hardware-Aware Design Strategies for Neural Networks\n\n- The Mamba architecture is designed with hardware-aware considerations, utilizing a parallel scan algorithm to optimally use GPU memory hierarchy. This involves executing some processes in fast SRAM and others in slower HBM, combined with kernel fusion and recomputation to achieve efficient implementation.\n\n- The blog post on \"Efficient Algorithm Compression Methods\" also touches on hardware-aware optimizations, such as quantization and low-rank decomposition, which can significantly reduce the memory footprint and accelerate inference times. These techniques are crucial for deploying models on limited hardware.\n\n## Real-World Performance Benchmarks of SSM Architectures\n\n- The experiments in the paper on \"Provable Benefits of Complex Parameterizations for Structured State Space Models\" provide real-world performance benchmarks. The results show that complex parameterizations significantly improve performance in certain tasks, while selectivity allows real parameterizations to achieve comparable or better performance in other tasks.\n\n- The performance of Mamba is evaluated on synthetic tasks such as copy and induction-head, demonstrating its state-of-the-art performance and efficiency. These benchmarks highlight the practical benefits of using selective SSM models in real-world scenarios.\n\n## Efficient Implementations of Adaptive State Updates\n\n- The \"Mamba\" architecture incorporates an adaptive learning mechanism through its data-dependent selection mechanism. This mechanism allows parameters to depend on the input through linear projections, enhancing the model's ability to adaptively update its state based on the input.\n\n- The analysis note emphasizes the importance of adaptive forgetting factors and gradient-based adaptation mechanisms for enhancing stability and tracking behavior in SSMs. These mechanisms are crucial for efficient and adaptive state updates[Analysis Note].\n\nIn summary, the Mamba architecture and the discussions around complex parameterizations and selectivity in SSMs provide valuable insights into practical implementations, memory-efficient designs, hardware-aware strategies, real-world performance, and efficient adaptive state updates for improving autoregressive language models.\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\nThe search results reveal important insights about content-dependent SSMs, training dynamics, and stability analysis. Key findings include:\n\na) Training Dynamics:\n- Content-dependent SSMs show promise for improved expressiveness and adaptability\n- Training stability can be achieved through careful parameterization and initialization\n- Hierarchical approaches demonstrate benefits for efficient state management\n\nb) Stability Considerations:\n- Lipschitz continuity and stable reparameterization are crucial for convergence\n- Adaptive learning mechanisms need careful balance between flexibility and stability\n- Hierarchical structures can enhance stability while maintaining expressiveness\n\n2. Useful References with Excerpts:\n\nFrom \"Scan and Snap: Understanding Training Dynamics\":\n\"We prove that self-attention acts as a discriminative scanning algorithm: starting from uniform attention, it gradually attends more to distinct key tokens [...] and pays less attention to common key tokens\"\n\nFrom \"Grokking as the Transition\":\n\"The key determinants of grokking are the rate of feature learning [...] and the alignment of the initial features with the target function\"\n\nFrom \"Adaptive Learning of Smoothing Functions\":\n\"The tracking behaviour is enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors\"\n\n3. Key Insights:\n\na) Content-Dependent Processing:\n- Need for careful balance between adaptivity and stability\n- Importance of initialization and parameterization\n- Role of gradient-based adaptation mechanisms\n\nb) Training Dynamics:\n- Progressive feature learning is key for stable adaptation\n- Initial alignment affects convergence behavior\n- Selective attention mechanisms emerge naturally\n\nc) Stability Considerations:\n- Adaptive forgetting factors can enhance stability\n- Hierarchical structures provide natural regularization\n- Gradient-based updates need careful control\n\n4. Future Search Plan:\n\nNeed additional information on:\na) Practical implementations of content-dependent SSMs\nb) Memory-efficient hierarchical architectures\nc) Real-world performance comparisons\nd) Hardware optimization strategies\n\n5. References:\n\n[1] Tian, Y., Wang, Y., Chen, B., & Du, S. (2023). Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. Neural Information Processing Systems.\n\n[2] Kumar, T., Bordelon, B., Gershman, S., & Pehlevan, C. (2023). Grokking as the Transition from Lazy to Rich Training Dynamics. International Conference on Learning Representations.\n\n[3] Ba, A., Sinn, M., Goude, Y., & Pompey, P. (2012). Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting. Neural Information Processing Systems.\n\n[4] Kwan, H. M., et al. (2023). HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation. Neural Information Processing Systems.\n\n[5] Minnen, D. C., Ball\u00e9, J., & Toderici, G. (2018). Joint Autoregressive and Hierarchical Priors for Learned Image Compression. Neural Information Processing Systems."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "content-dependent state space models, hierarchical compression neural networks, selective computation attention",
            "detail": "Search for papers discussing:\n1. Content-dependent or input-dependent state space models in neural networks\n2. Hierarchical state compression techniques for sequence models\n3. Theoretical analysis of selective computation in neural networks\n4. Memory-efficient implementations of state space models\n5. Stability analysis of adaptive state transitions",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Content-dependent or input-dependent state space models in neural networks\n2. Hierarchical state compression techniques for sequence models\n3. Theoretical analysis of selective computation in neural networks\n4. Memory-efficient implementations of state space models\n5. Stability analysis of adaptive state transitions\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.97)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.97)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.94)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 1/20 (Score: 0.94)*\n\n```\n# State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory \n\nShida Wang<br>Department of Mathematics<br>National University of Singapore<br>e0622338@u.nus.edu\n\nBeichen Xue<br>Department of Statistics and Data Science<br>National University of Singapore<br>e0773769@u.nus.edu\n\n\n#### Abstract\n\nState-space models have gained popularity in sequence modelling due to their simple and efficient network structures.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.91)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.91)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 5. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.85)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 23/31 (Score: 0.85)*\n\n```\n[27] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [28] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [29] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. [30] Shida Wang and Qianxiao Li. Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: content-dependent state space models, hierarchical compression neural networks, selective computation attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Liquid Structural State-Space Models\n\n*From Search Query: content-dependent state space models*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 2. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: content-dependent state space models*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 3. From Generalization Analysis to Optimization Designs for State Space Models\n\n*From Search Query: content-dependent state space models*\n\n*Fusheng Liu, Qianxiao Li*\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 4. Improving Inference for Neural Image Compression\n\n*From Search Query: hierarchical compression neural networks*\n\n*Yibo Yang, Robert Bamler, S. Mandt*\n\n**TL;DR:** This work identifies three approximation gaps which limit performance in the conventional approach to compression and proposes improvements to each based on ideas related to iterative inference, stochastic annealing for discrete optimization, and bits-back coding, resulting in the first application of bits- back coding to lossy compression.\n\n**Abstract:** We consider the problem of lossy image compression with deep latent variable models. State-of-the-art methods build on hierarchical variational autoencoders (VAEs) and learn inference networks to predict a compressible latent representation of each data point. Drawing on the variational inference perspective on compression, we identify three approximation gaps which limit performance in the conventional approach: (i) an amortization gap, (ii) a discretization gap, and (iii) a marginalization gap. We propose improvements to each of these three shortcomings based on ideas related to iterative inference, stochastic annealing for discrete optimization, and bits-back coding, resulting in the first application of bits-back coding to lossy compression. In our experiments, which include extensive baseline comparisons and ablation studies, we achieve new state-of-the-art performance on lossy image compression using an established VAE architecture, by changing only the inference method.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 107  (*Influential: 15*)\n\n#### 5. Learning Hierarchical Information Flow with Recurrent Neural Modules\n\n*From Search Query: hierarchical compression neural networks*\n\n*Danijar Hafner, A. Irpan, James Davidson, N. Heess*\n\n**TL;DR:** ThalNet, a deep learning model inspired by neocortical communication via the thalamus, is proposed, which consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps.\n\n**Abstract:** We propose ThalNet, a deep learning model inspired by neocortical communication via the thalamus. Our model consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps. We show that our model learns to route information hierarchically, processing input data by a chain of modules. We observe common architectures, such as feed forward neural networks and skip connections, emerging as special cases of our architecture, while novel connectivity patterns are learned for the text8 compression task. Our model outperforms standard recurrent neural networks on several sequential benchmarks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 6. C2FAR: Coarse-to-Fine Autoregressive Networks for Precise Probabilistic Forecasting\n\n*From Search Query: hierarchical compression neural networks*\n\n*S. Bergsma, Timothy J. Zeyl, J. R. Anaraki, Lei Guo*\n\n**TL;DR:** C2FAR is the first method to simultaneously handle discrete and continuous series of arbitrary scale and distribution shape, which enables a variety of time series use cases, including anomaly detection, interpolation, and compression.\n\n**Abstract:** We present coarse-to-fine autoregressive networks (C2FAR), a method for modeling the probability distribution of univariate, numeric random variables. C2FAR generates a hierarchical, coarse-to-fine discretization of a variable autoregressively; progressively finer intervals of support are generated from a sequence of binned distributions, where each distribution is conditioned on previously-generated coarser intervals. Unlike prior (flat) binned distributions, C2FAR can represent values with exponentially higher precision, for only a linear increase in complexity. We use C2FAR for probabilistic forecasting via a recurrent neural network, thus modeling time series autoregressively in both space and time. C2FAR is the first method to simultaneously handle discrete and continuous series of arbitrary scale and distribution shape. This flexibility enables a variety of time series use cases, including anomaly detection, interpolation, and compression. C2FAR achieves improvements over the state-of-the-art on several benchmark forecasting datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 7. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective computation attention*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 8. How to Capture Higher-order Correlations? Generalizing Matrix Softmax Attention to Kronecker Computation\n\n*From Search Query: selective computation attention*\n\n*Josh Alman, Zhao Song*\n\n**TL;DR:** This work studies a generalization of attention which captures triple-wise correlations, and shows that bounded entries are both necessary and sufficient for quickly performing generalized computations and yields a natural tradeoff between the boundedness of the entries, and order of the tensor one may use for more expressive, efficient attention computation.\n\n**Abstract:** In the classical transformer attention scheme, we are given three $n \\times d$ size matrices $Q, K, V$ (the query, key, and value tokens), and the goal is to compute a new $n \\times d$ size matrix $D^{-1} \\exp(QK^\\top) V$ where $D = \\mathrm{diag}( \\exp(QK^\\top) {\\bf 1}_n )$. In this work, we study a generalization of attention which captures triple-wise correlations. This generalization is able to solve problems about detecting triple-wise connections that were shown to be impossible for transformers. The potential downside of this generalization is that it appears as though computations are even more difficult, since the straightforward algorithm requires cubic time in $n$. However, we show that in the bounded-entry setting (which arises in practice, and which is well-studied in both theory and practice), there is actually a near-linear time algorithm. More precisely, we show that bounded entries are both necessary and sufficient for quickly performing generalized computations: $\\bullet$ On the positive side, if all entries of the input matrices are bounded above by $o(\\sqrt[3]{\\log n})$ then we show how to approximate the ``tensor-type'' attention matrix in $n^{1+o(1)}$ time. $\\bullet$ On the negative side, we show that if the entries of the input matrices may be as large as $\\Omega(\\sqrt[3]{\\log n})$, then there is no algorithm that runs faster than $n^{3-o(1)}$ (assuming the Strong Exponential Time Hypothesis from fine-grained complexity theory). We also show that our construction, algorithms, and lower bounds naturally generalize to higher-order tensors and correlations. Interestingly, the higher the order of the tensors, the lower the bound on the entries needs to be for an efficient algorithm. Our results thus yield a natural tradeoff between the boundedness of the entries, and order of the tensor one may use for more expressive, efficient attention computation.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 2*)\n\n#### 9. Treeformer: Dense Gradient Trees for Efficient Attention Computation\n\n*From Search Query: selective computation attention*\n\n*Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, Prateek Jain*\n\n**TL;DR:** This work views attention computation as that of nearest neighbor retrieval, and uses decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic.\n\n**Abstract:** Standard inference and training with transformer based architectures scale quadratically with input sequence length. This is prohibitively large for a variety of applications especially in web-page translation, query-answering etc. Consequently, several approaches have been developed recently to speedup attention computation by enforcing different attention structures such as sparsity, low-rank, approximating attention using kernels. In this work, we view attention computation as that of nearest neighbor retrieval, and use decision tree based hierarchical navigation to reduce the retrieval cost per query token from linear in sequence length to nearly logarithmic. Based on such hierarchical navigation, we design Treeformer which can use one of two efficient attention layers -- TF-Attention and TC-Attention. TF-Attention computes the attention in a fine-grained style, while TC-Attention is a coarse attention layer which also ensures that the gradients are\"dense\". To optimize such challenging discrete layers, we propose a two-level bootstrapped training method. Using extensive experiments on standard NLP benchmarks, especially for long-sequences, we demonstrate that our Treeformer architecture can be almost as accurate as baseline Transformer while using 30x lesser FLOPs in the attention layer. Compared to Linformer, the accuracy can be as much as 12% higher while using similar FLOPs in the attention layer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 1*)\n\n### 5 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: content-dependent state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. Deep Bilateral Learning for Real-Time Image Enhancement\n\n*From Search Query: content-dependent state space models*\n\n*Fr\u00e9do Durand, Samuel W. Hasinoff, Jonathan T. Barron, Micha\u00ebl Gharbi, Jiawen Chen*\n\n**Abstract:** Performance is a critical challenge in mobile image processing. Given a\nreference imaging pipeline, or even human-adjusted pairs of images, we seek to\nreproduce the enhancements and enable real-time evaluation. For this, we\nintroduce a new neural network architecture inspired by bilateral grid\nprocessing and local affine color transforms. Using pairs of input/output\nimages, we train a convolutional neural network to predict the coefficients of\na locally-affine model in bilateral space. Our architecture learns to make\nlocal, global, and content-dependent decisions to approximate the desired image\ntransformation. At runtime, the neural network consumes a low-resolution\nversion of the input image, produces a set of affine transformations in\nbilateral space, upsamples those transformations in an edge-preserving fashion\nusing a new slicing node, and then applies those upsampled transformations to\nthe full-resolution image. Our algorithm processes high-resolution images on a\nsmartphone in milliseconds, provides a real-time viewfinder at 1080p\nresolution, and matches the quality of state-of-the-art approximation\ntechniques on a large class of image operators. Unlike previous work, our model\nis trained off-line from data and therefore does not require access to the\noriginal operator at runtime. This allows our model to learn complex,\nscene-dependent transformations for which no reference implementation is\navailable, such as the photographic edits of a human retoucher.\n\n**Published:** 2017-07-10\n\n\n\n#### 3. LVAC: Learned Volumetric Attribute Compression for Point Clouds using Coordinate Based Networks\n\n*From Search Query: hierarchical compression neural networks*\n\n*George Toderici, Nick Johnston, Sung Jin Hwang, Philip A. Chou, Berivan Isik*\n\n**Abstract:** We consider the attributes of a point cloud as samples of a vector-valued volumetric function at discrete positions. To compress the attributes given the positions, we compress the parameters of the volumetric function. We model the volumetric function by tiling space into blocks, and representing the function over each block by shifts of a coordinate-based, or implicit, neural network. Inputs to the network include both spatial coordinates and a latent vector per block. We represent the latent vectors using coefficients of the region-adaptive hierarchical transform (RAHT) used in the MPEG geometry-based point cloud codec G-PCC. The coefficients, which are highly compressible, are rate-distortion optimized by back-propagation through a rate-distortion Lagrangian loss in an auto-decoder configuration. The result outperforms RAHT by 2--4 dB. This is the first work to compress volumetric functions represented by local coordinate-based neural networks. As such, we expect it to be applicable beyond point clouds, for example to compression of high-resolution neural radiance fields.\n\n**Published:** 2021-11-17\n\n\n\n#### 4. Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting\n\n*From Search Query: hierarchical compression neural networks*\n\n*Mohit Bansal, Yen-Chun Chen*\n\n**Abstract:** Inspired by how humans summarize long documents, we propose an accurate and\nfast summarization model that first selects salient sentences and then rewrites\nthem abstractively (i.e., compresses and paraphrases) to generate a concise\noverall summary. We use a novel sentence-level policy gradient method to bridge\nthe non-differentiable computation between these two neural networks in a\nhierarchical way, while maintaining language fluency. Empirically, we achieve\nthe new state-of-the-art on all metrics (including human evaluation) on the\nCNN/Daily Mail dataset, as well as significantly higher abstractiveness scores.\nMoreover, by first operating at the sentence-level and then the word-level, we\nenable parallel decoding of our neural generative model that results in\nsubstantially faster (10-20x) inference speed as well as 4x faster training\nconvergence than previous long-paragraph encoder-decoder models. We also\ndemonstrate the generalization of our model on the test-only DUC-2002 dataset,\nwhere we achieve higher scores than a state-of-the-art model.\n\n**Conference:** fast-abstractive-summarization-with-reinforce-1\n\n**Published:** 2018-05-28\n\n\n\n#### 5. ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\n\n*From Search Query: selective computation attention*\n\n*QinGhua Hu, WangMeng Zuo, Peihua Li, Qilong Wang, Pengfei Zhu, Banggu Wu*\n\n**Abstract:** Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via $1D$ convolution. Furthermore, we develop a method to adaptively select kernel size of $1D$ convolution, determining coverage of local cross-channel interaction. The proposed ECA module is efficient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.\n\n**Conference:** eca-net-efficient-channel-attention-for-deep-1\n\n**Published:** 2019-10-08\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, here are some key findings and references that align with the specified details:\n\n## Content-Dependent State Space Models\n\n- The paper \"Selective and Simplified State Space Layers for Sequence Modeling\" introduces the S7 model, which is a content-dependent State Space Model (SSM). This model dynamically adjusts state transitions based on the input content, combining the efficiency of recurrent architectures with the adaptive processing capabilities of more complex models. This approach is crucial for content-dependent state space models and their impact on model expressiveness and computational overhead.\n\n## Hierarchical State Compression Techniques\n\n- The concept of hierarchical state compression is touched upon in the discussion of State-Space Models (SSMs) and their efficiency. The Mamba framework, mentioned in the context of SSMs, employs an S4-based data-dependent selection technique that filters specific inputs and captures long-term context, which can be seen as a form of hierarchical compression. This technique allows for efficient handling of long sequences and maintains linear time efficiency.\n\n## Theoretical Analysis of Selective Computation\n\n- The paper on \"Selective and Simplified State Space Layers for Sequence Modeling\" also delves into selective computation by introducing importance-based updates. This approach ensures that the model performs content-based reasoning and adaptive filtering while preserving the simplicity and computational efficiency of recurrent models. The theoretical analysis here includes the stability and performance implications of such selective computation methods.\n\n## Memory-Efficient Implementations of State Space Models\n\n- State-Space Models (SSMs) are highlighted for their efficiency and scalability. The quantization methods proposed for SSMs aim to make them more deployable across resource-limited hardware platforms, addressing significant challenges in their widespread adoption. This includes the use of techniques like Mamba, which achieves linear time efficiency and is more resource-efficient compared to traditional Transformer models.\n\n## Stability Analysis of Adaptive State Transitions\n\n- The stability of content-dependent state transitions is a critical aspect. The S7 model introduces stable reparameterization and additional design choices to ensure long-term stability and performance across diverse tasks. This stability analysis is essential for understanding how adaptive state transitions affect the overall model performance and convergence.\n\n### Additional Insights\n\n- **State-Space Models in General**: These models are mathematically defined and can efficiently handle long-range dependencies, making them a compelling alternative to traditional architectures like Transformers. They can be computed efficiently, achieving linear or near-linear scaling in sequence length, which is crucial for real-time processing and analysis of sequential data.\n\n- **Comparison and Integration**: The proposal to integrate content-dependent state space models with hierarchical compression and selective computation builds on established research. For example, the Mamba framework and other SSM designs provide a foundation for efficient sequence modeling, while the introduction of content-dependent transitions and hierarchical compression adds a novel layer of complexity and efficiency.\n\nBy focusing on these aspects, the researchers can develop a more robust and efficient autoregressive language model that leverages content-dependent state transitions, hierarchical state compression, and selective computation, while ensuring stability and memory efficiency.\n",
            "analysis": "Initial Analysis of ContentAdaptiveSSMTTTLinear Proposal\n\n1. Key Components Requiring Investigation:\n\na) Content-Dependent State Space Models:\n- Novel integration with TTTLinear architecture\n- Impact on model expressiveness\n- Computational overhead vs. benefits\n- Stability of content-dependent transitions\n\nb) Adaptive State Management:\n- Efficiency of hierarchical compression\n- Information preservation in compressed states\n- Memory-computation tradeoffs\n- Scalability with sequence length\n\nc) Selective Computation:\n- Effectiveness of importance-based updates\n- Impact on model quality\n- Hardware efficiency considerations\n- Comparison with existing selective approaches\n\n2. Comparison with Previous Designs:\n\nThe proposal shares some elements with previous designs:\n- Similar to adaptivecompressedlm in using SSMs\n- Overlaps with ssmfasttttlinear in state management\n- Related to pixelsparselm in selective computation\n\nHowever, it introduces unique elements:\n- Content-dependent state transitions\n- Hierarchical state compression with selective updates\n- Integration of multiple efficiency techniques\n\n3. Research Questions:\n\na) Theoretical Foundations:\n- How does content-dependent SSM compare to fixed SSMs?\n- What are the theoretical bounds on compression efficiency?\n- How does selective computation affect model capacity?\n\nb) Implementation Concerns:\n- Stability of content-dependent updates\n- Memory efficiency of hierarchical states\n- Hardware optimization potential\n\nc) Performance Implications:\n- Impact on perplexity and accuracy\n- Scalability with sequence length\n- Training stability and convergence\n\n4. Investigation Plan:\n\nPhase 1: Theoretical Foundation\n- Research on content-dependent SSMs\n- Analysis of state compression techniques\n- Study of selective computation methods\n\nPhase 2: Implementation Feasibility\n- Hardware efficiency considerations\n- Memory management strategies\n- Stability analysis\n\nPhase 3: Performance Comparison\n- Benchmark against existing approaches\n- Scalability analysis\n- Resource utilization study\n\nInitial findings suggest this proposal introduces novel combinations of techniques while building on established research. The content-dependent SSM integration appears particularly innovative, though requiring careful investigation of stability and efficiency tradeoffs.\n\nReferences found so far:\n1. \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" - Relevant for SSM design\n2. \"RWKV: Reinventing RNNs for the Transformer Era\" - Background on efficient sequence modeling\n3. \"Extending Context Window of Large Language Models via Semantic Compression\" - Compression techniques\n4. \"The Expressive Capacity of State Space Models\" - Theoretical foundations\n\nNext steps will focus on gathering more specific information about content-dependent state transitions and hierarchical compression techniques."
        },
        {
            "ready": false,
            "query": "memory efficient state space models, compression techniques neural networks, hardware optimization SSM",
            "detail": "Search for papers discussing:\n1. Memory-efficient implementations of state space models and compression techniques\n2. Hardware optimization strategies for SSM architectures\n3. Training stability and convergence in adaptive neural networks\n4. Efficient state representation and compression methods\n5. Performance analysis of compressed state spaces",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory-efficient implementations of state space models and compression techniques\n2. Hardware optimization strategies for SSM architectures\n3. Training stability and convergence in adaptive neural networks\n4. Efficient state representation and compression methods\n5. Performance analysis of compressed state spaces\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.98)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.98)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.92)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.92)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n#### 4. Spectral State Space Models (Avg. Score: 0.91)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.91)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 5. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.74)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.74)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: memory efficient state space models, compression techniques neural networks, hardware optimization SSM\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: memory efficient state space models*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 2. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: memory efficient state space models*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 352  (*Influential: 57*)\n\n#### 3. State-Free Inference of State-Space Models: The Transfer Function Approach\n\n*From Search Query: memory efficient state space models*\n\n*Rom N. Parnichkun, Stefano Massaroli, Alessandro Moro, Jimmy T.H. Smith, Ramin M. Hasani, Mathias Lechner, Qi An, Christopher R'e, Hajime Asama, Stefano Ermon, Taiji Suzuki, Atsushi Yamashita, Michael Poli*\n\n**TL;DR:** This work uncovers a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size.\n\n**Abstract:** We approach designing a state-space model for deep learning applications through its dual representation, the transfer function, and uncover a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers -- parametrized in time-domain -- on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. Searching Lottery Tickets in Graph Neural Networks: A Dual Perspective\n\n*From Search Query: compression techniques neural networks*\n\n*Kun Wang, Yuxuan Liang, Pengkun Wang, Xu Wang, Pengfei Gu, Junfeng Fang, Yang Wang*\n\n**TL;DR:** This proposal helps achieve a triple-win situation of graph lottery tickets with high sparsity, admirable performance, and good explainability and rigorously proves that the model can eliminate noise and maintain reliable information in substructures using the graph information bottleneck theory.\n\n**Abstract:** Graph Neural Networks (GNNs) have shown great promise in various graph learning tasks. However, the computational overheads of fitting GNNs to large-scale graphs grow rapidly, posing obstacles to GNNs from scaling up to real-world applications. To tackle this issue, Graph Lottery Ticket (GLT) hypothesis articulates that there always exists a sparse subnetwork/subgraph with admirable performance in GNNs with random initialization. Such a pair of core subgraph and sparse subnetwork (called graph lottery tickets) can be uncovered by iteratively applying a novel sparsification method. While GLT provides new insights for GNN compression, it requires a full pretraining process to obtain graph lottery tickets, which is not universal and friendly to real-world applications. Moreover, the graph sparsification in GLT utilizes sampling techniques, which may result in massive information loss and aggregation failure. In this paper, we explore the searching of graph lottery tickets from a complementary perspective \u2013 transforming a random ticket into a graph lottery ticket, which allows us to more comprehensively explore the relationships between the original network/graph and their sparse counterpart. Compared to GLT, our proposal helps achieve a triple-win situation of graph lottery tickets with high sparsity, admirable performance, and good explainability. More importantly, we rigorously prove that our model can eliminate noise and maintain reliable information in substructures using the graph information bottleneck theory. Extensive experimental results on various graphrelated tasks validate the effectiveness of our framework.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 5. Adaptive Estimators Show Information Compression in Deep Neural Networks\n\n*From Search Query: compression techniques neural networks*\n\n*Ivan Chelombiev, Conor J. Houghton, Cian O\u2019Donnell*\n\n**TL;DR:** More robust mutual information estimation techniques are developed, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions, which explore compression in networks with a range of different activation functions.\n\n**Abstract:** To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 7*)\n\n#### 6. Probabilistic Connection Importance Inference and Lossless Compression of Deep Neural Networks\n\n*From Search Query: compression techniques neural networks*\n\n*Xin Xing, Long Sha, Pengyu Hong, Zuofeng Shang, Jun S. Liu*\n\n**TL;DR:** A probabilistic importance inference approach for pruning DNNs is proposed, which tests the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring test and keeps only those significant ones.\n\n**Abstract:** Deep neural networks (DNNs) can be huge in size, requiring a considerable a mount of energy and computational resources to operate, which limits their applications in numerous scenarios. It is thus of interest to compress DNNs while maintaining their performance levels. We here propose a probabilistic importance inference approach for pruning DNNs. Specifically, we test the significance of the relevance of a connection in a DNN to the DNN\u2019s outputs using a nonparemtric scoring testand keep only those significant ones. Experimental results show that the proposed approach achieves better lossless compression rates than existing techniques\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 7. Data-Driven Offline Optimization For Architecting Hardware Accelerators\n\n*From Search Query: hardware optimization SSM*\n\n*Aviral Kumar, A. Yazdanbakhsh, Milad Hashemi, Kevin Swersky, S. Levine*\n\n**TL;DR:** This paper develops a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization.\n\n**Abstract:** Industry has gradually moved towards application-specific hardware accelerators in order to attain higher efficiency. While such a paradigm shift is already starting to show promising results, designers need to spend considerable manual effort and perform a large number of time-consuming simulations to find accelerators that can accelerate multiple target applications while obeying design constraints. Moreover, such a\"simulation-driven\"approach must be re-run from scratch every time the set of target applications or design constraints change. An alternative paradigm is to use a\"data-driven\", offline approach that utilizes logged simulation data, to architect hardware accelerators, without needing any form of simulations. Such an approach not only alleviates the need to run time-consuming simulation, but also enables data reuse and applies even when set of target applications changes. In this paper, we develop such a data-driven offline optimization method for designing hardware accelerators, dubbed PRIME, that enjoys all of these properties. Our approach learns a conservative, robust estimate of the desired cost function, utilizes infeasible points, and optimizes the design against this estimate without any additional simulator queries during optimization. PRIME architects accelerators -- tailored towards both single and multiple applications -- improving performance upon state-of-the-art simulation-driven methods by about 1.54x and 1.20x, while considerably reducing the required total simulation time by 93% and 99%, respectively. In addition, PRIME also architects effective accelerators for unseen applications in a zero-shot setting, outperforming simulation-based methods by 1.26x.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 8. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: hardware optimization SSM*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 45  (*Influential: 4*)\n\n#### 9. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware optimization SSM*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n### 4 related papers from Papers with Code\n\n#### 1. MoViNets: Mobile Video Networks for Efficient Video Recognition\n\n*From Search Query: memory efficient state space models*\n\n*Boqing Gong, Matthew Brown, Mingxing Tan, Li Zhang, Yandong Li, Liangzhe Yuan, Dan Kondratyuk*\n\n**Abstract:** We present Mobile Video Networks (MoViNets), a family of computation and memory efficient video networks that can operate on streaming video for online inference. 3D convolutional neural networks (CNNs) are accurate at video recognition but require large computation and memory budgets and do not support online inference, making them difficult to work on mobile devices. We propose a three-step approach to improve computational efficiency while substantially reducing the peak memory usage of 3D CNNs. First, we design a video network search space and employ neural architecture search to generate efficient and diverse 3D CNN architectures. Second, we introduce the Stream Buffer technique that decouples memory from video clip duration, allowing 3D CNNs to embed arbitrary-length streaming video sequences for both training and inference with a small constant memory footprint. Third, we propose a simple ensembling technique to improve accuracy further without sacrificing efficiency. These three progressive techniques allow MoViNets to achieve state-of-the-art accuracy and efficiency on the Kinetics, Moments in Time, and Charades video action recognition datasets. For instance, MoViNet-A5-Stream achieves the same accuracy as X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less memory. Code will be made available at https://github.com/tensorflow/models/tree/master/official/vision.\n\n**Proceeding:** cvpr-2021-1\n\n**Published:** 2021-03-21\n\n\n\n#### 2. Linformer: Self-Attention with Linear Complexity\n\n*From Search Query: memory efficient state space models*\n\n*Madian Khabsa, Hao Ma, Sinong Wang, Belinda Z. Li, Han Fang*\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n**Published:** 2020-06-08\n\n\n\n#### 3. Distilling the Knowledge in a Neural Network\n\n*From Search Query: compression techniques neural networks*\n\n*Geoffrey Hinton, Oriol Vinyals, Jeff Dean*\n\n**Abstract:** A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.\n\n**Published:** 2015-03-09\n\n\n\n#### 4. An Embarrassingly Simple Approach for Knowledge Distillation\n\n*From Search Query: compression techniques neural networks*\n\n*Xiaoou Tang, Yujun Shen, Quanquan Li, Liang Wan, Chen Change Loy, Mengya Gao, Junjie Yan, Dahua Lin*\n\n**Abstract:** Knowledge Distillation (KD) aims at improving the performance of a low-capacity student model by inheriting knowledge from a high-capacity teacher model. Previous KD methods typically train a student by minimizing a task-related loss and the KD loss simultaneously, using a pre-defined loss weight to balance these two terms. In this work, we propose to first transfer the backbone knowledge from a teacher to the student, and then only learn the task-head of the student network. Such a decomposition of the training process circumvents the need of choosing an appropriate loss weight, which is often difficult in practice, and thus makes it easier to apply to different datasets and tasks. Importantly, the decomposition permits the core of our method, Stage-by-Stage Knowledge Distillation (SSKD), which facilitates progressive feature mimicking from teacher to student. Extensive experiments on CIFAR-100 and ImageNet suggest that SSKD significantly narrows down the performance gap between student and teacher, outperforming state-of-the-art approaches. We also demonstrate the generalization ability of SSKD on other challenging benchmarks, including face recognition on IJB-A dataset as well as object detection on COCO dataset.\n\n**Published:** 2018-12-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on memory-efficient implementations, compression techniques, hardware optimization, training stability, and efficient state representation, here are some key findings and techniques from the provided sources and additional insights:\n\n## Memory-Efficient Implementations and Compression Techniques\n\n### Compressed Activations\nThe paper on CompAct introduces a technique for compressing activations during the training of large language models (LLMs). By storing low-rank, compressed activations, CompAct reduces peak memory utilization by 25-30% for pretraining and 50% for fine-tuning, without significant impact on training throughput and performance. This method is particularly relevant for reducing memory usage in state space models.\n\n### Context Compression\nContext compression techniques, such as those discussed in the Restackio article, are crucial for sequence models. Methods like attention mechanisms, dimensionality reduction (e.g., PCA), and quantization can significantly reduce the computational burden and memory footprint. These techniques can be applied to compress the context in autoregressive language models, enhancing efficiency and reducing overfitting.\n\n### Quantization\nQuantization techniques, as described in the OpenVINO blog and the protein language model paper, involve reducing the precision of model weights. For example, 4-bit and 8-bit quantization can reduce memory usage substantially while maintaining comparable performance. These methods are effective for compressing model weights and can be adapted for state space models.\n\n## Hardware Optimization Strategies\n\n### Efficient Memory Management\nCompAct's approach to compressing activations and gradients also involves efficient memory management, which is critical for hardware optimization. By using random projection matrices, the method avoids additional memory overheads, making it scalable and applicable to various model sizes.\n\n### System Co-design for LLM Serving\nThe QServe paper discusses system co-design for efficient LLM serving, including quantization and system optimization. Techniques like W4A8KV4 quantization and progressive quantization of weights are designed to optimize memory usage and computational efficiency, which can be applied to optimize hardware performance for state space models.\n\n## Training Stability and Convergence\n\n### Stability in Deep SSM Architectures\nThe importance of maintaining stability in deep state-space models is highlighted in various studies. For instance, the paper \"State-space models with layer-wise nonlinearity\" emphasizes the need for stability in deep SSM architectures. Content-dependent state transitions, as in Mamba and Liquid-S4 models, require careful balancing to ensure stability and efficiency.\n\n### Adaptive Processing and Hierarchical Structures\nHierarchical state space models (HiSS) and liquid structural state-space models (Liquid-S4) demonstrate improved performance through adaptive processing and hierarchical structures. These approaches can help in maintaining stability while enhancing expressiveness and efficiency[Analysis Note].\n\n## Efficient State Representation and Compression Methods\n\n### Low-Rank Decomposition\nLow-rank decomposition methods, such as those mentioned in the Restackio article, can reduce the number of parameters by breaking down matrices or tensors into smaller components. Techniques like Singular Value Decomposition (SVD) are effective for compressing models while maintaining performance.\n\n### Knowledge Distillation\nKnowledge distillation involves training a smaller 'student' model using the outputs of a larger 'teacher' model. This method can be used to compress state representations in autoregressive language models, retaining accuracy while reducing the model size.\n\n## Performance Analysis of Compressed State Spaces\n\n### Performance Metrics\nEvaluations in the CompAct paper and the QServe paper show that compression techniques can achieve significant memory savings and performance improvements. For example, CompAct reduces memory usage by 25-30% for pretraining and 50% for fine-tuning with minimal impact on performance. Similarly, quantization methods in QServe achieve good compression ratios with minimal loss of accuracy.\n\nBy integrating these techniques\u2014compressed activations, context compression, quantization, efficient memory management, and hierarchical structures\u2014the researchers can develop more memory-efficient, stable, and performant autoregressive language models. These methods will help in optimizing state space models for better performance, reduced resource usage, and improved scalability.\n",
            "analysis": "Analysis Note for ContentAdaptiveSSMTTTLinear Proposal\n\n1. Summary of Analysis:\nThe initial search reveals several key aspects relevant to the proposal:\n- Content-dependent state transitions are emerging as a crucial innovation in SSMs\n- Hierarchical approaches show promise for efficient state management\n- Selective computation mechanisms are being actively researched\n- Stability and efficiency considerations are well-documented\n\n2. Useful References with Excerpts:\n\nFrom Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information.\"\n\nFrom Liquid Structural State-Space Models:\n\"The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\"\n\nFrom Hierarchical State Space Models:\n\"HiSS stacks structured state-space models on top of each other to create a temporal hierarchy... outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE.\"\n\n3. Key Insights:\n\na) Content-Dependent Processing:\n- Mamba's success demonstrates the value of input-dependent state transitions\n- Liquid-S4 shows benefits of adaptive processing based on input similarities\n- Need to carefully balance expressiveness with computational efficiency\n\nb) Hierarchical Organization:\n- HiSS demonstrates benefits of stacked SSM structures\n- Temporal hierarchies can improve performance significantly\n- Memory efficiency through hierarchical compression needs further investigation\n\nc) Stability Considerations:\n- From \"State-space models with layer-wise nonlinearity\" paper: importance of maintaining stability in deep SSM architectures\n- Need to investigate stability implications of content-dependent transitions\n\n4. Future Search Plan:\n\nNeed additional information on:\na) Memory efficiency techniques in SSMs\nb) Specific compression methods for state representations\nc) Hardware optimization strategies for SSM implementations\nd) Training stability in adaptive SSM architectures\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n[2] Hasani, R. M., et al. (2022). Liquid Structural State-Space Models. International Conference on Learning Representations.\n\n[3] Bhirangi, R. M., et al. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. International Conference on Machine Learning.\n\n[4] Wang, S., & Xue, B. (2023). State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Neural Information Processing Systems.\n\n[5] Liu, F., & Li, Q. (2024). From Generalization Analysis to Optimization Designs for State Space Models. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "training dynamics SSM, adaptive state stability, hierarchical compression theory",
            "detail": "Search for papers discussing:\n1. Training dynamics and convergence analysis of content-dependent state space models\n2. Stability analysis of adaptive state updates in neural networks\n3. Theoretical foundations of hierarchical compression in sequence models\n4. Test-time training integration with state space models\n5. Convergence guarantees for adaptive parameter updates",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Training dynamics and convergence analysis of content-dependent state space models\n2. Stability analysis of adaptive state updates in neural networks\n3. Theoretical foundations of hierarchical compression in sequence models\n4. Test-time training integration with state space models\n5. Convergence guarantees for adaptive parameter updates\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.30)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.30)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n#### 2. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.29)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 9/20 (Score: 0.29)*\n\n```\nIn International Conference on Learning Representations, January 2021. [7] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models. Advances in Neural Information Processing Systems, 35:35971-35983, December 2022. [8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [9] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections. In International Conference on Learning Representations, February 2023. [10] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [11] Eric Martin and Chris Cundy. Parallelizing Linear Recurrent Neural Nets Over Sequence Length. In International Conference on Learning Representations, February 2018. [12] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards Larger Convolutional Language Models. In International Conference on Machine Learning, June 2023. [13] Joshua Hanson, Maxim Raginsky, and Eduardo Sontag. Learning Recurrent Neural Net Models of Nonlinear Systems. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, pages 425-435. PMLR, May 2021. [14] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks. Journal of Machine Learning Research, 23(42):1-85, 2022. ISSN 1533-7928. [15] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, March 1994.\n```\n\n#### 3. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.17)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 23/31 (Score: 0.17)*\n\n```\n[27] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [28] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [29] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. [30] Shida Wang and Qianxiao Li. Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization.\n```\n\n#### 4. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.15)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.15)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 5. Spectral State Space Models (Avg. Score: 0.12)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.12)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: training dynamics SSM, adaptive state stability, hierarchical compression theory\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Rotational Equilibrium: How Weight Decay Balances Learning Across Neural Networks\n\n*From Search Query: training dynamics SSM*\n\n*Atli Kosson, Bettina Messmer, Martin Jaggi*\n\n**TL;DR:** This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning.\n\n**Abstract:** This study investigates how weight decay affects the update behavior of individual neurons in deep neural networks through a combination of applied analysis and experimentation. Weight decay can cause the expected magnitude and angular updates of a neuron's weight vector to converge to a steady state we call rotational equilibrium. These states can be highly homogeneous, effectively balancing the average rotation -- a proxy for the effective learning rate -- across different layers and neurons. Our work analyzes these dynamics across optimizers like Adam, Lion, and SGD with momentum, offering a new simple perspective on training that elucidates the efficacy of widely used but poorly understood methods in deep learning. We demonstrate how balanced rotation plays a key role in the effectiveness of normalization like Weight Standardization, as well as that of AdamW over Adam with L2-regularization. Finally, we show that explicitly controlling the rotation provides the benefits of weight decay while substantially reducing the need for learning rate warmup.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer\n\n*From Search Query: training dynamics SSM*\n\n*Yuandong Tian, Yiping Wang, Beidi Chen, S. Du*\n\n**TL;DR:** It is proved that self-attention acts as adiscriminative scanning algorithm: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens.\n\n**Abstract:** Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \\emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \\emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends more to distinct key tokens for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a \\emph{phase transition} that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this \\textbf{\\emph{scan and snap}} dynamics on synthetic and real-world data (WikiText).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 60  (*Influential: 5*)\n\n#### 3. Grokking as the Transition from Lazy to Rich Training Dynamics\n\n*From Search Query: training dynamics SSM*\n\n*Tanishq Kumar, Blake Bordelon, S. Gershman, C. Pehlevan*\n\n**TL;DR:** Evidence is concluded that this transition from lazy (linear model) to rich training (feature learning) can control grokking in more general settings, like on MNIST, one-layer Transformers, and student-teacher networks.\n\n**Abstract:** We propose that the grokking phenomenon, where the train loss of a neural network decreases much earlier than its test loss, can arise due to a neural network transitioning from lazy training dynamics to a rich, feature learning regime. To illustrate this mechanism, we study the simple setting of vanilla gradient descent on a polynomial regression problem with a two layer neural network which exhibits grokking without regularization in a way that cannot be explained by existing theories. We identify sufficient statistics for the test loss of such a network, and tracking these over training reveals that grokking arises in this setting when the network first attempts to fit a kernel regression solution with its initial features, followed by late-time feature learning where a generalizing solution is identified after train loss is already low. We find that the key determinants of grokking are the rate of feature learning -- which can be controlled precisely by parameters that scale the network output -- and the alignment of the initial features with the target function $y(x)$. We argue this delayed generalization arises when (1) the top eigenvectors of the initial neural tangent kernel and the task labels $y(x)$ are misaligned, but (2) the dataset size is large enough so that it is possible for the network to generalize eventually, but not so large that train loss perfectly tracks test loss at all epochs, and (3) the network begins training in the lazy regime so does not learn features immediately. We conclude with evidence that this transition from lazy (linear model) to rich training (feature learning) can control grokking in more general settings, like on MNIST, one-layer Transformers, and student-teacher networks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 1*)\n\n#### 4. Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model\n\n*From Search Query: adaptive state stability*\n\n*Amine Ouasfi, A. Boukhayma*\n\n**TL;DR:** A efficient mechanism to remedy generalization issues in forward generalizable models for implicit shape reconstruction from unoriented point cloud by combining the inter-shape data prior of the network with an intra-shape regularization prior of a Nystr\\\"om Kernel Ridge Regression.\n\n**Abstract:** Feedforward generalizable models for implicit shape reconstruction from unoriented point cloud present multiple advantages, including high performance and inference speed. However, they still suffer from generalization issues, ranging from underfitting the input point cloud, to misrepresenting samples outside of the training data distribution, or with toplogies unseen at training. We propose here an efficient mechanism to remedy some of these limitations at test time. We combine the inter-shape data prior of the network with an intra-shape regularization prior of a Nystr\\\"om Kernel Ridge Regression, that we further adapt by fitting its hyperprameters to the current shape. The resulting shape function defined in a shape specific Reproducing Kernel Hilbert Space benefits from desirable stability and efficiency properties and grants a shape adaptive expressiveness-robustness trade-off. We demonstrate the improvement obtained through our method with respect to baselines and the state-of-the-art using synthetic and real data.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 5. Adaptive Learning of Smoothing Functions: Application to Electricity Load Forecasting\n\n*From Search Query: adaptive state stability*\n\n*A. Ba, M. Sinn, Y. Goude, Pascal Pompey*\n\n**TL;DR:** This paper proposes an efficient online learning algorithm to track the smoothing functions of Additive Models with a Recursive Least Squares filter which achieves a superior performance in terms of model tracking and prediction accuracy.\n\n**Abstract:** This paper proposes an efficient online learning algorithm to track the smoothing functions of Additive Models. The key idea is to combine the linear representation of Additive Models with a Recursive Least Squares (RLS) filter. In order to quickly track changes in the model and put more weight on recent data, the RLS filter uses a forgetting factor which exponentially weights down observations by the order of their arrival. The tracking behaviour is further enhanced by using an adaptive forgetting factor which is updated based on the gradient of the a priori errors. Using results from Lyapunov stability theory, upper bounds for the learning rate are analyzed. The proposed algorithm is applied to 5 years of electricity load data provided by the French utility company Electricite de France (EDF). Compared to state-of-the-art methods, it achieves a superior performance in terms of model tracking and prediction accuracy.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2012\n\n**Citations:** 34  (*Influential: 3*)\n\n#### 6. Greedy-based Value Representation for Optimal Coordination in Multi-agent Reinforcement Learning\n\n*From Search Query: adaptive state stability*\n\n*Lipeng Wan, Zeyang Liu, Xingyu Chen, Han Wang, Xuguang Lan*\n\n**TL;DR:** The greedy-based value representation (GVR), which turns the optimal node into an STN via inferior target shaping and further eliminates the non-optimal STNs via superior experience replay and achieves an adaptive trade-off between optimality and stability.\n\n**Abstract:** Due to the representation limitation of the joint Q value function, multi-agent reinforcement learning methods with linear value decomposition (LVD) or monotonic value decomposition (MVD) suffer from relative overgeneralization. As a result, they can not ensure optimal consistency (i.e., the correspondence between individual greedy actions and the maximal true Q value). In this paper, we derive the expression of the joint Q value function of LVD and MVD. According to the expression, we draw a transition diagram, where each self-transition node (STN) is a possible convergence. To ensure optimal consistency, the optimal node is required to be the unique STN. Therefore, we propose the greedy-based value representation (GVR), which turns the optimal node into an STN via inferior target shaping and further eliminates the non-optimal STNs via superior experience replay. In addition, GVR achieves an adaptive trade-off between optimality and stability. Our method outperforms state-of-the-art baselines in experiments on various benchmarks. Theoretical proofs and empirical results on matrix games demonstrate that GVR ensures optimal consistency under sufficient exploration.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 7. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: hierarchical compression theory*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 23  (*Influential: 2*)\n\n#### 8. Joint Autoregressive and Hierarchical Priors for Learned Image Compression\n\n*From Search Query: hierarchical compression theory*\n\n*David C. Minnen, J. Ball\u00e9, G. Toderici*\n\n**TL;DR:** It is found that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models.\n\n**Abstract:** Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate\u2013distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1082  (*Influential: 275*)\n\n#### 9. HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression\n\n*From Search Query: hierarchical compression theory*\n\n*Chenhe Dong, Yaliang Li, Ying Shen, Minghui Qiu*\n\n**TL;DR:** To enhance the model capability and transferability, the idea of meta-learning is leveraged and set up domain-relational graphs to capture the relational information across different domains, and to dynamically select the most representative prototypes for each domain, a hierarchical compare-aggregate mechanism to capture hierarchical relationships.\n\n**Abstract:** On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https://github.com/cheneydon/hrkd.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 6  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections\n\n*From Search Query: training dynamics SSM*\n\n*Christopher R\u00e9, Atri Rudra, Aman Timalsina, Isys Johnson, Albert Gu*\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n**Published:** 2022-06-24\n\n\n\n#### 2. Vision Mamba: A Comprehensive Survey and Taxonomy\n\n*From Search Query: training dynamics SSM*\n\n*Lei Zhang, Chenxu Zhang, Xiao Liu*\n\n**Abstract:** State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems. This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning. In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding. By mapping sequence data to state space, long-term dependencies in the data can be better captured. In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity. Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference. Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer. Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain. To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study. This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains. Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.\n\n**Published:** 2024-05-07\n\n\n\n#### 3. Layer Normalization\n\n*From Search Query: adaptive state stability*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n#### 4. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: adaptive state stability*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 5. QARV: Quantization-Aware ResNet VAE for Lossy Image Compression\n\n*From Search Query: hierarchical compression theory*\n\n*Zhan Ma, Yuning Huang, Fengqing Zhu, Jack Ma, Ming Lu, Zhihao Duan*\n\n**Abstract:** This paper addresses the problem of lossy image compression, a fundamental problem in image processing and information theory that is involved in many real-world applications. We start by reviewing the framework of variational autoencoders (VAEs), a powerful class of generative probabilistic models that has a deep connection to lossy compression. Based on VAEs, we develop a novel scheme for lossy image compression, which we name quantization-aware ResNet VAE (QARV). Our method incorporates a hierarchical VAE architecture integrated with test-time quantization and quantization-aware training, without which efficient entropy coding would not be possible. In addition, we design the neural network architecture of QARV specifically for fast decoding and propose an adaptive normalization operation for variable-rate compression. Extensive experiments are conducted, and results show that QARV achieves variable-rate compression, high-speed decoding, and a better rate-distortion performance than existing baseline methods. The code of our method is publicly accessible at https://github.com/duanzhiihao/lossy-vae\n\n**Published:** 2023-02-16\n\n\n\n#### 6. Bayesian Convolutional Neural Networks for Compressed Sensing Restoration\n\n*From Search Query: hierarchical compression theory*\n\n*Anonymous*\n\n**Abstract:** Deep Neural Networks (DNNs) have aroused great attention in Compressed\nSensing (CS) restoration. However, the working mechanism of DNNs is not\nexplainable, thereby it is unclear that how to design an optimal DNNs for CS\nrestoration. In this paper, we propose a novel statistical framework to explain\nDNNs, which proves that the hidden layers of DNNs are equivalent to Gibbs\ndistributions and interprets DNNs as a Bayesian hierarchical model. The\nframework provides a Bayesian perspective to explain the working mechanism of\nDNNs, namely some hidden layers learn a prior distribution and other layers\nlearn a likelihood distribution. Moreover, the framework provides insights into\nDNNs and reveals two inherent limitations of DNNs for CS restoration. In\ncontrast to most previous works designing an end-to-end DNNs for CS\nrestoration, we propose a novel DNNs to model a prior distribution only, which\ncan circumvent the limitations of DNNs. Given the prior distribution generated\nfrom the DNNs, we design a Bayesian inference algorithm to realize CS\nrestoration in the framework of Bayesian Compressed Sensing. Finally, extensive\nsimulations validate the proposed theory of DNNs and demonstrate that the\nproposed algorithm outperforms the state-of-the-art CS restoration methods.\n\n**Published:** 2019-02-24\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models, particularly focusing on content-dependent state space models (SSMs), here are some key findings and references that address the specified areas of interest:\n\n## Training Dynamics and Convergence Analysis of Content-Dependent State Space Models\n\nThe paper introducing the S7 model provides significant insights into the training dynamics and convergence analysis of content-dependent SSMs. The S7 model incorporates input-dependent state transitions and stable reparameterization, which enhances its ability to capture long-range dependencies while maintaining computational efficiency.\n\n- The S7 model dynamically adjusts its state transitions based on input content, allowing for selective filtering and content-based reasoning. This approach ensures stability and convergence by using Lipschitz continuity and stability conditions, preventing uncontrolled behavior and ensuring smooth transitions over time.\n\n## Stability Analysis of Adaptive State Updates in Neural Networks\n\nThe stability analysis of the S7 model is a crucial aspect of its design. The model ensures stability through several key mechanisms:\n\n- The mappings involved in the state transition matrices are Lipschitz continuous, which ensures that small parameter changes lead to small changes in the state transition matrices, promoting stable learning and smooth transitions over time.\n- The eigenvalues of the state transition matrices have negative real parts, ensuring that the system remains uniformly asymptotically stable.\n\n## Theoretical Foundations of Hierarchical Compression in Sequence Models\n\nWhile the provided sources do not delve deeply into the theoretical foundations of hierarchical compression specifically, the concept of hierarchical approaches is mentioned in the context of state space models. The Mamba and Vision Mamba papers suggest that hierarchical compression and representation are important for efficient sequence modeling:\n\n- The Vision Mamba paper discusses representing visual data using bidirectional Mamba blocks, which involve compressing visual representations. This indicates a hierarchical approach to handling complex data[Analysis Note].\n\nFor a more detailed theoretical foundation on hierarchical compression, further research into papers specifically focused on compression techniques in sequence models would be necessary.\n\n## Test-Time Training Integration with State Space Models\n\nThe integration of test-time training with state space models is not explicitly discussed in the provided sources. However, the concept of adaptive state updates and content-dependent processing suggests that such models could be adapted for test-time training by leveraging their ability to adjust state transitions based on input content.\n\n- The S7 model's ability to dynamically adjust state transitions based on input content could be a foundation for integrating test-time training, as it allows the model to adapt to new inputs during inference.\n\n## Convergence Guarantees for Adaptive Parameter Updates\n\nThe S7 model provides convergence guarantees through its stability analysis and the use of stable reparameterization techniques:\n\n- The model ensures that the approximation error tends to zero, and the gradient norms are well-behaved during optimization, which guarantees stable and accurate approximations of target functionals. This ensures that the model can be trained to handle long sequences with complex dependencies.\n\nIn summary, the S7 model and related research on state space models offer significant advancements in training dynamics, stability analysis, and the potential for hierarchical compression and test-time training integration, all of which are crucial for improving autoregressive language model design.\n",
            "analysis": "Analysis Note for ContentAdaptiveSSMTTTLinear Proposal - Round 2\n\n1. Summary of Analysis:\nThe search results reveal important insights about state space models (SSMs), memory efficiency, and hardware optimization. Key findings include:\n- Content-dependent SSMs show promise for improved expressiveness\n- Hierarchical approaches and adaptive compression are emerging trends\n- Hardware-aware implementations are crucial for practical efficiency\n- Memory management and state compression require careful balance\n\n2. Useful References with Excerpts:\n\nFrom Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning... letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information.\"\n\nFrom Vision Mamba:\n\"Representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context... bidirectional Mamba blocks (Vim) marks the image sequences with position embeddings and compresses the visual representation.\"\n\nFrom State-Free Inference paper:\n\"Unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size... using properties of the proposed frequency domain transfer function parameterization.\"\n\nFrom Gated Linear Attention:\n\"Linear attention generally underperforms ordinary softmax attention... We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates.\"\n\n3. Key Insights:\n\na) Content-Dependent Processing:\n- SSMs benefit from input-dependent parameter adaptation\n- Selective propagation improves information flow\n- Balance needed between adaptivity and stability\n\nb) Memory Efficiency:\n- State-free approaches offer promising direction\n- Transfer function parameterization reduces memory costs\n- Hierarchical compression shows potential\n\nc) Hardware Considerations:\n- IO-aware algorithms crucial for performance\n- Memory access patterns affect efficiency\n- Trade-offs between parallelizability and memory usage\n\n4. Future Search Plan:\n\nNeed additional information on:\na) Training dynamics of content-dependent SSMs\nb) Stability analysis of adaptive state updates\nc) Theoretical foundations of hierarchical compression\nd) Integration strategies for test-time training\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n[2] Zhu, L., et al. (2024). Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model. International Conference on Machine Learning.\n\n[3] Parnichkun, R.N., et al. (2024). State-Free Inference of State-Space Models: The Transfer Function Approach. International Conference on Machine Learning.\n\n[4] Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n[5] Fu, D.Y., et al. (2023). Simple Hardware-Efficient Long Convolutions for Sequence Modeling. International Conference on Machine Learning."
        }
    ],
    "rating": 4.1,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.780846,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.841914,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "# ContentAdaptiveSSMTTTLinear: Enhancing FastTTTLinear with Content-Dependent State Space Models and Adaptive State Management\n\n## Motivation\n\nModern language models face significant challenges in efficiently processing long sequences while maintaining expressiveness and adaptability. While FastTTTLinear addresses some efficiency concerns through Gated Linear Attention, it can be further enhanced by incorporating content-dependent state space models (SSMs) and adaptive state management techniques. The motivation behind ContentAdaptiveSSMTTTLinear is to improve FastTTTLinear by:\n\n1. Making state transitions content-dependent for better expressiveness\n2. Introducing adaptive state compression for efficient memory usage\n3. Implementing selective state updates for computational efficiency\n4. Maintaining test-time training capabilities while improving scalability\n\n## Problem Analysis\n\n### Current Limitations of FastTTTLinear\n\n1. **Fixed State Transitions**:\n   - State transitions are input-independent\n   - Limited ability to adapt to content characteristics\n   - Reduced expressiveness for complex patterns\n\n2. **Memory Management**:\n   - Inefficient state representation\n   - High memory requirements for long sequences\n   - Limited ability to compress and decompress information\n\n3. **Computational Efficiency**:\n   - Uniform computation regardless of content importance\n   - Inefficient handling of varying sequence lengths\n   - Limited ability to selectively update states\n\n### Proposed Solutions\n\n1. **Content-Dependent State Space Models**:\n   - Make state transitions input-dependent\n   - Enable adaptive processing based on content\n   - Maintain computational efficiency through selective updates\n\n2. **Adaptive State Compression**:\n   - Implement hierarchical state representation\n   - Enable dynamic compression based on content importance\n   - Maintain information fidelity while reducing memory usage\n\n3. **Selective Update Mechanisms**:\n   - Implement importance-based state updates\n   - Reduce unnecessary computations\n   - Preserve test-time training capabilities\n\n## Core Idea and Philosophy\n\nContentAdaptiveSSMTTTLinear introduces three key innovations:\n\n1. **Content-Dependent State Transitions**:\n   - State transition matrices are functions of input content\n   - Enables dynamic adaptation to sequence characteristics\n   - Maintains stability through careful parameterization\n\n2. **Hierarchical State Management**:\n   - Multi-scale state representation\n   - Adaptive compression based on content importance\n   - Efficient memory usage through selective retention\n\n3. **Selective Computation**:\n   - Importance-based update mechanisms\n   - Efficient parallel processing\n   - Hardware-aware optimizations\n\n## Design Plan\n\n### Selected GAU to Modify\n```selection FastTTTLinear```\n\n### Modification Overview\n\n1. **Content-Dependent State Space Integration**:\n   - Replace fixed state transitions with content-dependent ones\n   - Implement selective state update mechanism\n   - Maintain test-time training capabilities\n\n2. **Adaptive State Management**:\n   - Introduce hierarchical state representation\n   - Implement dynamic compression\n   - Enable efficient memory access patterns\n\n3. **Computational Optimization**:\n   - Hardware-aware parallel processing\n   - Selective computation based on importance\n   - Efficient parameter sharing\n\n### Detailed Modifications\n\n#### 1. Content-Dependent State Space Module\n\n```python\nclass ContentDependentSSM(nn.Module):\n    def __init__(self, dim, state_dim):\n        super().__init__()\n        self.state_proj = nn.Linear(dim, state_dim * 4)\n        self.gate_proj = nn.Linear(dim, state_dim)\n        \n    def forward(self, x, state):\n        # Generate content-dependent parameters\n        delta, A, B, C = self.state_proj(x).chunk(4, dim=-1)\n        \n        # Compute importance gates\n        gates = torch.sigmoid(self.gate_proj(x))\n        \n        # Apply selective updates\n        new_state = (state * (1 - gates) + \n                    (torch.tanh(A) * state + B * x) * gates)\n        \n        # Generate output\n        output = C * new_state\n        return output, new_state\n```\n\n#### 2. Hierarchical State Management\n\n```python\nclass HierarchicalStateManager(nn.Module):\n    def __init__(self, dim, levels=3):\n        super().__init__()\n        self.levels = levels\n        self.compressors = nn.ModuleList([\n            nn.Linear(dim, dim // (2**i))\n            for i in range(levels)\n        ])\n        self.decompressors = nn.ModuleList([\n            nn.Linear(dim // (2**i), dim)\n            for i in range(levels)\n        ])\n        \n    def compress(self, x):\n        states = []\n        current = x\n        for comp in self.compressors:\n            current = comp(current)\n            states.append(current)\n        return states\n        \n    def decompress(self, states):\n        output = sum(decomp(state) \n                    for decomp, state in zip(self.decompressors, states))\n        return output\n```\n\n#### 3. Selective Computation Module\n\n```python\nclass SelectiveComputation(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.importance_proj = nn.Linear(dim, 1)\n        self.threshold = nn.Parameter(torch.zeros(1))\n        \n    def forward(self, x):\n        # Compute importance scores\n        importance = torch.sigmoid(self.importance_proj(x))\n        \n        # Generate computation mask\n        mask = (importance > self.threshold).float()\n        \n        # Apply selective computation\n        output = x * mask\n        return output, mask\n```\n\n### Implementation Guidelines\n\n```python\nclass ContentAdaptiveSSMTTTLinear(nn.Module):\n    def __init__(self, embed_dim, num_heads, state_dim):\n        super().__init__()\n        self.ssm = ContentDependentSSM(embed_dim, state_dim)\n        self.state_manager = HierarchicalStateManager(embed_dim)\n        self.selective_comp = SelectiveComputation(embed_dim)\n        \n        # FastTTTLinear components\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, x, state=None):\n        # Compute importance and apply selective computation\n        x_selective, mask = self.selective_comp(x)\n        \n        # Process through SSM\n        ssm_out, new_state = self.ssm(x_selective, state)\n        \n        # Manage state hierarchy\n        states = self.state_manager.compress(new_state)\n        state_out = self.state_manager.decompress(states)\n        \n        # FastTTTLinear attention\n        q = self.q_proj(x)\n        k = self.k_proj(state_out)\n        v = self.v_proj(state_out)\n        \n        # Compute attention with state information\n        attn = torch.softmax(q @ k.transpose(-2, -1), dim=-1)\n        out = attn @ v\n        \n        # Combine outputs\n        output = self.o_proj(out + ssm_out)\n        return output, new_state\n```\n\n## Theoretical Analysis\n\n1. **Computational Complexity**:\n   - O(N) for sequence length N through selective computation\n   - Reduced memory complexity through hierarchical compression\n   - Efficient parallel processing capabilities\n\n2. **Memory Usage**:\n   - Adaptive compression reduces memory requirements\n   - Hierarchical state representation enables efficient storage\n   - Selective updates minimize memory bandwidth\n\n3. **Model Capacity**:\n   - Enhanced expressiveness through content-dependent transitions\n   - Maintained test-time training capabilities\n   - Improved long-range dependency modeling\n\n## Research Summary\n\nKey findings from research:\n1. Content-dependent SSMs improve expressiveness\n2. Hierarchical state management reduces memory usage\n3. Selective computation enhances efficiency\n4. Hardware-aware implementation crucial for performance\n\n## Evolution of Design\n\nThe design evolved through several stages:\n1. Initial integration of content-dependent SSMs\n2. Addition of hierarchical state management\n3. Implementation of selective computation\n4. Optimization for hardware efficiency\n\n## Advantages and Trade-offs\n\nAdvantages:\n- Improved expressiveness through content-dependent processing\n- Reduced memory usage through hierarchical compression\n- Enhanced computational efficiency through selective updates\n- Maintained test-time training capabilities\n\nTrade-offs:\n- Increased implementation complexity\n- Additional overhead from state management\n- Need for careful parameter tuning\n\n## References\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n2. Hasani, R. M., et al. (2022). Liquid Structural State-Space Models. International Conference on Learning Representations.\n\n3. Bhirangi, R. M., et al. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. International Conference on Machine Learning.\n\n4. Liu, F., & Li, Q. (2024). From Generalization Analysis to Optimization Designs for State Space Models. International Conference on Machine Learning.\n\n5. Wang, S., & Xue, B. (2023). State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory. Neural Information Processing Systems.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "An enhancement of FastTTTLinear that integrates content-dependent state space models with adaptive compression and selective state updates, enabling efficient processing of long sequences while maintaining model expressiveness through dynamic parameter generation and hierarchical state management.",
    "ideation": null,
    "modelname": "CASTL",
    "suggestions": "1. State Management:\n- Provide more detailed analysis of memory-computation tradeoffs\n- Include specific guidelines for compression ratio selection\n- Add failure mode analysis and mitigation strategies\n\n2. Stability Analysis:\n- Include theoretical guarantees for content-dependent updates\n- Add analysis of convergence properties\n- Provide guidelines for parameter initialization\n\n3. Implementation Details:\n- Expand on hardware optimization strategies\n- Include more specific memory management techniques\n- Add concrete parallelization strategies\n\n4. Integration Guidelines:\n- Provide more detailed guidance on test-time training integration\n- Include analysis of different hierarchical architectures\n- Add concrete benchmarking strategies\n\n5. Theoretical Foundation:\n- Strengthen mathematical analysis of content-dependent updates\n- Include complexity analysis for all components\n- Provide proofs for key theoretical claims\n\n6. Empirical Validation Plan:\n- Add specific evaluation metrics\n- Include ablation study design\n- Outline comparison methodology with existing approaches",
    "user_input": ""
}