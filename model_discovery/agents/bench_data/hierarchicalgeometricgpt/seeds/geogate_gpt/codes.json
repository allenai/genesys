{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = GeometricGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\nfrom typing import Optional, Tuple\n\n\nclass GeometricGatedMHA(GAUBase):\n    \"\"\"\n    Memory-efficient Geometric Gated Multi-Head Attention with hierarchical attention routing.\n    \n    This implementation combines:\n    1. Geometric Attention Patterns with memory-efficient computation\n    2. Adaptive Copy Gates with stable gradients\n    3. Dynamic Head Routing with chunked processing\n    \n    The implementation uses chunked attention computation and stable softmax to handle\n    long sequences efficiently while maintaining numerical stability.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: Optional[int]=None,\n        head_dim: Optional[int]=None, qkv_proj_bias: bool=True,\n        out_proj_bias: bool=True, softmax_scale: Optional[float]=None,\n        rotary_emb_base: float=10000.0, chunk_size: int=1024, device=None,\n        dtype=None, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.causal = causal\n        self.chunk_size = chunk_size\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n            self.head_dim = embed_dim // n_heads\n        else:\n            self.head_dim = head_dim\n        self.qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        self.out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, self.qkv_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        self.out_proj = nn.Linear(self.out_dim, embed_dim, bias=\n            out_proj_bias, **self.factory_kwargs)\n        self.geo_proj = nn.Linear(self.head_dim, self.head_dim, **self.\n            factory_kwargs)\n        self.geo_scale = nn.Parameter(torch.ones(self.num_heads, 1, 1, **\n            self.factory_kwargs) * 0.1)\n        self.copy_gate = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.head_router = nn.Linear(embed_dim, self.num_heads, **self.\n            factory_kwargs)\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.scale = (self.head_dim ** -0.5 if softmax_scale is None else\n            softmax_scale)\n        self.reset_parameters()\n\n    def chunk_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Compute attention scores in chunks to save memory\"\"\"\n        batch_size, num_heads, seq_len, head_dim = q.shape\n        out = torch.zeros_like(q)\n        for i in range(0, seq_len, self.chunk_size):\n            chunk_end = min(i + self.chunk_size, seq_len)\n            q_chunk = q[:, :, i:chunk_end]\n            attn_weights = torch.matmul(q_chunk, k.transpose(-2, -1)\n                ) * self.scale\n            q_geo = self.geo_proj(q_chunk)\n            k_geo = self.geo_proj(k)\n            geo_bias = torch.matmul(q_geo, k_geo.transpose(-2, -1)\n                ) * self.geo_scale\n            attn_weights = attn_weights + geo_bias\n            if self.causal:\n                causal_mask = torch.triu(torch.ones(chunk_end - i, seq_len,\n                    dtype=torch.bool, device=q.device), diagonal=i + 1)\n                attn_weights.masked_fill_(causal_mask[None, None], float(\n                    '-inf'))\n            attn_max = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n            exp_weights = torch.exp(attn_weights - attn_max)\n            attn_weights = exp_weights / (torch.sum(exp_weights, dim=-1,\n                keepdim=True) + 1e-06)\n            out[:, :, i:chunk_end] = torch.matmul(attn_weights, v)\n        return out\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, dict]:\n        batch_size, seq_len = X.shape[:2]\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        if self.num_heads > self.num_heads_kv:\n            k = torch.repeat_interleave(k, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n            v = torch.repeat_interleave(v, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n        context = self.chunk_attention(q, k, v)\n        head_weights = torch.sigmoid(self.head_router(X))\n        head_weights = rearrange(head_weights, 'b s h -> b h s 1')\n        context = context * head_weights\n        context = rearrange(context, 'b h s d -> b s (h d)')\n        output = self.out_proj(context)\n        gate = torch.sigmoid(self.copy_gate(X).clamp(-5, 5))\n        output = gate * output + (1 - gate) * X\n        return output, Z\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with stable values\"\"\"\n        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.geo_proj.weight, gain=0.1)\n        if self.qkv_proj.bias is not None:\n            nn.init.zeros_(self.qkv_proj.bias)\n        if self.out_proj.bias is not None:\n            nn.init.zeros_(self.out_proj.bias)\n        nn.init.zeros_(self.copy_gate.weight)\n        if self.copy_gate.bias is not None:\n            nn.init.constant_(self.copy_gate.bias, 1.0)\n        nn.init.xavier_uniform_(self.head_router.weight, gain=0.1)\n        if self.head_router.bias is not None:\n            nn.init.zeros_(self.head_router.bias)\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'qkv_proj_bias': True,\n    'out_proj_bias': True, 'softmax_scale': None, 'chunk_size': 1024}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = GeometricGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\nfrom typing import Optional, Tuple\n\n\nclass GeometricGatedMHA(GAUBase):\n    \"\"\"\n    Memory-efficient Geometric Gated Multi-Head Attention with hierarchical attention routing.\n    \n    This implementation combines:\n    1. Geometric Attention Patterns with memory-efficient computation\n    2. Adaptive Copy Gates with stable gradients\n    3. Dynamic Head Routing with chunked processing\n    \n    The implementation uses chunked attention computation and stable softmax to handle\n    long sequences efficiently while maintaining numerical stability.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: Optional[int]=None,\n        head_dim: Optional[int]=None, qkv_proj_bias: bool=True,\n        out_proj_bias: bool=True, softmax_scale: Optional[float]=None,\n        rotary_emb_base: float=10000.0, chunk_size: int=1024, device=None,\n        dtype=None, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.causal = causal\n        self.chunk_size = chunk_size\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n            self.head_dim = embed_dim // n_heads\n        else:\n            self.head_dim = head_dim\n        self.qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        self.out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, self.qkv_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        self.out_proj = nn.Linear(self.out_dim, embed_dim, bias=\n            out_proj_bias, **self.factory_kwargs)\n        self.geo_proj = nn.Linear(self.head_dim, self.head_dim, **self.\n            factory_kwargs)\n        self.geo_scale = nn.Parameter(torch.ones(self.num_heads, 1, 1, **\n            self.factory_kwargs) * 0.1)\n        self.copy_gate = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.head_router = nn.Linear(embed_dim, self.num_heads, **self.\n            factory_kwargs)\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.scale = (self.head_dim ** -0.5 if softmax_scale is None else\n            softmax_scale)\n        self.reset_parameters()\n\n    def chunk_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Compute attention scores in chunks to save memory\"\"\"\n        batch_size, num_heads, seq_len, head_dim = q.shape\n        out = torch.zeros_like(q)\n        for i in range(0, seq_len, self.chunk_size):\n            chunk_end = min(i + self.chunk_size, seq_len)\n            q_chunk = q[:, :, i:chunk_end]\n            attn_weights = torch.matmul(q_chunk, k.transpose(-2, -1)\n                ) * self.scale\n            q_geo = self.geo_proj(q_chunk)\n            k_geo = self.geo_proj(k)\n            geo_bias = torch.matmul(q_geo, k_geo.transpose(-2, -1)\n                ) * self.geo_scale\n            attn_weights = attn_weights + geo_bias\n            if self.causal:\n                causal_mask = torch.triu(torch.ones(chunk_end - i, seq_len,\n                    dtype=torch.bool, device=q.device), diagonal=i + 1)\n                attn_weights.masked_fill_(causal_mask[None, None], float(\n                    '-inf'))\n            attn_max = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n            exp_weights = torch.exp(attn_weights - attn_max)\n            attn_weights = exp_weights / (torch.sum(exp_weights, dim=-1,\n                keepdim=True) + 1e-06)\n            out[:, :, i:chunk_end] = torch.matmul(attn_weights, v)\n        return out\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, dict]:\n        batch_size, seq_len = X.shape[:2]\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        if self.num_heads > self.num_heads_kv:\n            k = torch.repeat_interleave(k, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n            v = torch.repeat_interleave(v, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n        context = self.chunk_attention(q, k, v)\n        head_weights = torch.sigmoid(self.head_router(X))\n        head_weights = rearrange(head_weights, 'b s h -> b h s 1')\n        context = context * head_weights\n        context = rearrange(context, 'b h s d -> b s (h d)')\n        output = self.out_proj(context)\n        gate = torch.sigmoid(self.copy_gate(X).clamp(-5, 5))\n        output = gate * output + (1 - gate) * X\n        return output, Z\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with stable values\"\"\"\n        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.geo_proj.weight, gain=0.1)\n        if self.qkv_proj.bias is not None:\n            nn.init.zeros_(self.qkv_proj.bias)\n        if self.out_proj.bias is not None:\n            nn.init.zeros_(self.out_proj.bias)\n        nn.init.zeros_(self.copy_gate.weight)\n        if self.copy_gate.bias is not None:\n            nn.init.constant_(self.copy_gate.bias, 1.0)\n        nn.init.xavier_uniform_(self.head_router.weight, gain=0.1)\n        if self.head_router.bias is not None:\n            nn.init.zeros_(self.head_router.bias)\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'qkv_proj_bias': True,\n    'out_proj_bias': True, 'softmax_scale': None, 'chunk_size': 1024}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = GeometricGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\nfrom typing import Optional, Tuple\n\n\nclass GeometricGatedMHA(GAUBase):\n    \"\"\"\n    Memory-efficient Geometric Gated Multi-Head Attention with hierarchical attention routing.\n    \n    This implementation combines:\n    1. Geometric Attention Patterns with memory-efficient computation\n    2. Adaptive Copy Gates with stable gradients\n    3. Dynamic Head Routing with chunked processing\n    \n    The implementation uses chunked attention computation and stable softmax to handle\n    long sequences efficiently while maintaining numerical stability.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: Optional[int]=None,\n        head_dim: Optional[int]=None, qkv_proj_bias: bool=True,\n        out_proj_bias: bool=True, softmax_scale: Optional[float]=None,\n        rotary_emb_base: float=10000.0, chunk_size: int=1024, device=None,\n        dtype=None, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.causal = causal\n        self.chunk_size = chunk_size\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n            self.head_dim = embed_dim // n_heads\n        else:\n            self.head_dim = head_dim\n        self.qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        self.out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, self.qkv_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        self.out_proj = nn.Linear(self.out_dim, embed_dim, bias=\n            out_proj_bias, **self.factory_kwargs)\n        self.geo_proj = nn.Linear(self.head_dim, self.head_dim, **self.\n            factory_kwargs)\n        self.geo_scale = nn.Parameter(torch.ones(self.num_heads, 1, 1, **\n            self.factory_kwargs) * 0.1)\n        self.copy_gate = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.head_router = nn.Linear(embed_dim, self.num_heads, **self.\n            factory_kwargs)\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.scale = (self.head_dim ** -0.5 if softmax_scale is None else\n            softmax_scale)\n        self.reset_parameters()\n\n    def chunk_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Compute attention scores in chunks to save memory\"\"\"\n        batch_size, num_heads, seq_len, head_dim = q.shape\n        out = torch.zeros_like(q)\n        for i in range(0, seq_len, self.chunk_size):\n            chunk_end = min(i + self.chunk_size, seq_len)\n            q_chunk = q[:, :, i:chunk_end]\n            attn_weights = torch.matmul(q_chunk, k.transpose(-2, -1)\n                ) * self.scale\n            q_geo = self.geo_proj(q_chunk)\n            k_geo = self.geo_proj(k)\n            geo_bias = torch.matmul(q_geo, k_geo.transpose(-2, -1)\n                ) * self.geo_scale\n            attn_weights = attn_weights + geo_bias\n            if self.causal:\n                causal_mask = torch.triu(torch.ones(chunk_end - i, seq_len,\n                    dtype=torch.bool, device=q.device), diagonal=i + 1)\n                attn_weights.masked_fill_(causal_mask[None, None], float(\n                    '-inf'))\n            attn_max = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n            exp_weights = torch.exp(attn_weights - attn_max)\n            attn_weights = exp_weights / (torch.sum(exp_weights, dim=-1,\n                keepdim=True) + 1e-06)\n            out[:, :, i:chunk_end] = torch.matmul(attn_weights, v)\n        return out\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, dict]:\n        batch_size, seq_len = X.shape[:2]\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        if self.num_heads > self.num_heads_kv:\n            k = torch.repeat_interleave(k, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n            v = torch.repeat_interleave(v, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n        context = self.chunk_attention(q, k, v)\n        head_weights = torch.sigmoid(self.head_router(X))\n        head_weights = rearrange(head_weights, 'b s h -> b h s 1')\n        context = context * head_weights\n        context = rearrange(context, 'b h s d -> b s (h d)')\n        output = self.out_proj(context)\n        gate = torch.sigmoid(self.copy_gate(X).clamp(-5, 5))\n        output = gate * output + (1 - gate) * X\n        return output, Z\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with stable values\"\"\"\n        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.geo_proj.weight, gain=0.1)\n        if self.qkv_proj.bias is not None:\n            nn.init.zeros_(self.qkv_proj.bias)\n        if self.out_proj.bias is not None:\n            nn.init.zeros_(self.out_proj.bias)\n        nn.init.zeros_(self.copy_gate.weight)\n        if self.copy_gate.bias is not None:\n            nn.init.constant_(self.copy_gate.bias, 1.0)\n        nn.init.xavier_uniform_(self.head_router.weight, gain=0.1)\n        if self.head_router.bias is not None:\n            nn.init.zeros_(self.head_router.bias)\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'qkv_proj_bias': True,\n    'out_proj_bias': True, 'softmax_scale': None, 'chunk_size': 1024}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = GeometricGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\nfrom typing import Optional, Tuple\n\n\nclass GeometricGatedMHA(GAUBase):\n    \"\"\"\n    Memory-efficient Geometric Gated Multi-Head Attention with hierarchical attention routing.\n    \n    This implementation combines:\n    1. Geometric Attention Patterns with memory-efficient computation\n    2. Adaptive Copy Gates with stable gradients\n    3. Dynamic Head Routing with chunked processing\n    \n    The implementation uses chunked attention computation and stable softmax to handle\n    long sequences efficiently while maintaining numerical stability.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: Optional[int]=None,\n        head_dim: Optional[int]=None, qkv_proj_bias: bool=True,\n        out_proj_bias: bool=True, softmax_scale: Optional[float]=None,\n        rotary_emb_base: float=10000.0, chunk_size: int=1024, device=None,\n        dtype=None, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.causal = causal\n        self.chunk_size = chunk_size\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n            self.head_dim = embed_dim // n_heads\n        else:\n            self.head_dim = head_dim\n        self.qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        self.out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, self.qkv_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        self.out_proj = nn.Linear(self.out_dim, embed_dim, bias=\n            out_proj_bias, **self.factory_kwargs)\n        self.geo_proj = nn.Linear(self.head_dim, self.head_dim, **self.\n            factory_kwargs)\n        self.geo_scale = nn.Parameter(torch.ones(self.num_heads, 1, 1, **\n            self.factory_kwargs) * 0.1)\n        self.copy_gate = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.head_router = nn.Linear(embed_dim, self.num_heads, **self.\n            factory_kwargs)\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.scale = (self.head_dim ** -0.5 if softmax_scale is None else\n            softmax_scale)\n        self.reset_parameters()\n\n    def chunk_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Compute attention scores in chunks to save memory\"\"\"\n        batch_size, num_heads, seq_len, head_dim = q.shape\n        out = torch.zeros_like(q)\n        for i in range(0, seq_len, self.chunk_size):\n            chunk_end = min(i + self.chunk_size, seq_len)\n            q_chunk = q[:, :, i:chunk_end]\n            attn_weights = torch.matmul(q_chunk, k.transpose(-2, -1)\n                ) * self.scale\n            q_geo = self.geo_proj(q_chunk)\n            k_geo = self.geo_proj(k)\n            geo_bias = torch.matmul(q_geo, k_geo.transpose(-2, -1)\n                ) * self.geo_scale\n            attn_weights = attn_weights + geo_bias\n            if self.causal:\n                causal_mask = torch.triu(torch.ones(chunk_end - i, seq_len,\n                    dtype=torch.bool, device=q.device), diagonal=i + 1)\n                attn_weights.masked_fill_(causal_mask[None, None], float(\n                    '-inf'))\n            attn_max = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n            exp_weights = torch.exp(attn_weights - attn_max)\n            attn_weights = exp_weights / (torch.sum(exp_weights, dim=-1,\n                keepdim=True) + 1e-06)\n            out[:, :, i:chunk_end] = torch.matmul(attn_weights, v)\n        return out\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, dict]:\n        batch_size, seq_len = X.shape[:2]\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        if self.num_heads > self.num_heads_kv:\n            k = torch.repeat_interleave(k, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n            v = torch.repeat_interleave(v, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n        context = self.chunk_attention(q, k, v)\n        head_weights = torch.sigmoid(self.head_router(X))\n        head_weights = rearrange(head_weights, 'b s h -> b h s 1')\n        context = context * head_weights\n        context = rearrange(context, 'b h s d -> b s (h d)')\n        output = self.out_proj(context)\n        gate = torch.sigmoid(self.copy_gate(X).clamp(-5, 5))\n        output = gate * output + (1 - gate) * X\n        return output, Z\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with stable values\"\"\"\n        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.geo_proj.weight, gain=0.1)\n        if self.qkv_proj.bias is not None:\n            nn.init.zeros_(self.qkv_proj.bias)\n        if self.out_proj.bias is not None:\n            nn.init.zeros_(self.out_proj.bias)\n        nn.init.zeros_(self.copy_gate.weight)\n        if self.copy_gate.bias is not None:\n            nn.init.constant_(self.copy_gate.bias, 1.0)\n        nn.init.xavier_uniform_(self.head_router.weight, gain=0.1)\n        if self.head_router.bias is not None:\n            nn.init.zeros_(self.head_router.bias)\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'qkv_proj_bias': True,\n    'out_proj_bias': True, 'softmax_scale': None, 'chunk_size': 1024}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = GeometricGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\nfrom typing import Optional, Tuple\n\n\nclass GeometricGatedMHA(GAUBase):\n    \"\"\"\n    Memory-efficient Geometric Gated Multi-Head Attention with hierarchical attention routing.\n    \n    This implementation combines:\n    1. Geometric Attention Patterns with memory-efficient computation\n    2. Adaptive Copy Gates with stable gradients\n    3. Dynamic Head Routing with chunked processing\n    \n    The implementation uses chunked attention computation and stable softmax to handle\n    long sequences efficiently while maintaining numerical stability.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: Optional[int]=None,\n        head_dim: Optional[int]=None, qkv_proj_bias: bool=True,\n        out_proj_bias: bool=True, softmax_scale: Optional[float]=None,\n        rotary_emb_base: float=10000.0, chunk_size: int=1024, device=None,\n        dtype=None, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.causal = causal\n        self.chunk_size = chunk_size\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n            self.head_dim = embed_dim // n_heads\n        else:\n            self.head_dim = head_dim\n        self.qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        self.out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, self.qkv_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        self.out_proj = nn.Linear(self.out_dim, embed_dim, bias=\n            out_proj_bias, **self.factory_kwargs)\n        self.geo_proj = nn.Linear(self.head_dim, self.head_dim, **self.\n            factory_kwargs)\n        self.geo_scale = nn.Parameter(torch.ones(self.num_heads, 1, 1, **\n            self.factory_kwargs) * 0.1)\n        self.copy_gate = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.head_router = nn.Linear(embed_dim, self.num_heads, **self.\n            factory_kwargs)\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.scale = (self.head_dim ** -0.5 if softmax_scale is None else\n            softmax_scale)\n        self.reset_parameters()\n\n    def chunk_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Compute attention scores in chunks to save memory\"\"\"\n        batch_size, num_heads, seq_len, head_dim = q.shape\n        out = torch.zeros_like(q)\n        for i in range(0, seq_len, self.chunk_size):\n            chunk_end = min(i + self.chunk_size, seq_len)\n            q_chunk = q[:, :, i:chunk_end]\n            attn_weights = torch.matmul(q_chunk, k.transpose(-2, -1)\n                ) * self.scale\n            q_geo = self.geo_proj(q_chunk)\n            k_geo = self.geo_proj(k)\n            geo_bias = torch.matmul(q_geo, k_geo.transpose(-2, -1)\n                ) * self.geo_scale\n            attn_weights = attn_weights + geo_bias\n            if self.causal:\n                causal_mask = torch.triu(torch.ones(chunk_end - i, seq_len,\n                    dtype=torch.bool, device=q.device), diagonal=i + 1)\n                attn_weights.masked_fill_(causal_mask[None, None], float(\n                    '-inf'))\n            attn_max = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n            exp_weights = torch.exp(attn_weights - attn_max)\n            attn_weights = exp_weights / (torch.sum(exp_weights, dim=-1,\n                keepdim=True) + 1e-06)\n            out[:, :, i:chunk_end] = torch.matmul(attn_weights, v)\n        return out\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, dict]:\n        batch_size, seq_len = X.shape[:2]\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        if self.num_heads > self.num_heads_kv:\n            k = torch.repeat_interleave(k, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n            v = torch.repeat_interleave(v, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n        context = self.chunk_attention(q, k, v)\n        head_weights = torch.sigmoid(self.head_router(X))\n        head_weights = rearrange(head_weights, 'b s h -> b h s 1')\n        context = context * head_weights\n        context = rearrange(context, 'b h s d -> b s (h d)')\n        output = self.out_proj(context)\n        gate = torch.sigmoid(self.copy_gate(X).clamp(-5, 5))\n        output = gate * output + (1 - gate) * X\n        return output, Z\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with stable values\"\"\"\n        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.geo_proj.weight, gain=0.1)\n        if self.qkv_proj.bias is not None:\n            nn.init.zeros_(self.qkv_proj.bias)\n        if self.out_proj.bias is not None:\n            nn.init.zeros_(self.out_proj.bias)\n        nn.init.zeros_(self.copy_gate.weight)\n        if self.copy_gate.bias is not None:\n            nn.init.constant_(self.copy_gate.bias, 1.0)\n        nn.init.xavier_uniform_(self.head_router.weight, gain=0.1)\n        if self.head_router.bias is not None:\n            nn.init.zeros_(self.head_router.bias)\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'qkv_proj_bias': True,\n    'out_proj_bias': True, 'softmax_scale': None, 'chunk_size': 1024}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = GeometricGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\nfrom typing import Optional, Tuple\n\n\nclass GeometricGatedMHA(GAUBase):\n    \"\"\"\n    Memory-efficient Geometric Gated Multi-Head Attention with hierarchical attention routing.\n    \n    This implementation combines:\n    1. Geometric Attention Patterns with memory-efficient computation\n    2. Adaptive Copy Gates with stable gradients\n    3. Dynamic Head Routing with chunked processing\n    \n    The implementation uses chunked attention computation and stable softmax to handle\n    long sequences efficiently while maintaining numerical stability.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: Optional[int]=None,\n        head_dim: Optional[int]=None, qkv_proj_bias: bool=True,\n        out_proj_bias: bool=True, softmax_scale: Optional[float]=None,\n        rotary_emb_base: float=10000.0, chunk_size: int=1024, device=None,\n        dtype=None, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.causal = causal\n        self.chunk_size = chunk_size\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n            self.head_dim = embed_dim // n_heads\n        else:\n            self.head_dim = head_dim\n        self.qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        self.out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, self.qkv_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        self.out_proj = nn.Linear(self.out_dim, embed_dim, bias=\n            out_proj_bias, **self.factory_kwargs)\n        self.geo_proj = nn.Linear(self.head_dim, self.head_dim, **self.\n            factory_kwargs)\n        self.geo_scale = nn.Parameter(torch.ones(self.num_heads, 1, 1, **\n            self.factory_kwargs) * 0.1)\n        self.copy_gate = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.head_router = nn.Linear(embed_dim, self.num_heads, **self.\n            factory_kwargs)\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.scale = (self.head_dim ** -0.5 if softmax_scale is None else\n            softmax_scale)\n        self.reset_parameters()\n\n    def chunk_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Compute attention scores in chunks to save memory\"\"\"\n        batch_size, num_heads, seq_len, head_dim = q.shape\n        out = torch.zeros_like(q)\n        for i in range(0, seq_len, self.chunk_size):\n            chunk_end = min(i + self.chunk_size, seq_len)\n            q_chunk = q[:, :, i:chunk_end]\n            attn_weights = torch.matmul(q_chunk, k.transpose(-2, -1)\n                ) * self.scale\n            q_geo = self.geo_proj(q_chunk)\n            k_geo = self.geo_proj(k)\n            geo_bias = torch.matmul(q_geo, k_geo.transpose(-2, -1)\n                ) * self.geo_scale\n            attn_weights = attn_weights + geo_bias\n            if self.causal:\n                causal_mask = torch.triu(torch.ones(chunk_end - i, seq_len,\n                    dtype=torch.bool, device=q.device), diagonal=i + 1)\n                attn_weights.masked_fill_(causal_mask[None, None], float(\n                    '-inf'))\n            attn_max = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n            exp_weights = torch.exp(attn_weights - attn_max)\n            attn_weights = exp_weights / (torch.sum(exp_weights, dim=-1,\n                keepdim=True) + 1e-06)\n            out[:, :, i:chunk_end] = torch.matmul(attn_weights, v)\n        return out\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, dict]:\n        batch_size, seq_len = X.shape[:2]\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        if self.num_heads > self.num_heads_kv:\n            k = torch.repeat_interleave(k, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n            v = torch.repeat_interleave(v, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n        context = self.chunk_attention(q, k, v)\n        head_weights = torch.sigmoid(self.head_router(X))\n        head_weights = rearrange(head_weights, 'b s h -> b h s 1')\n        context = context * head_weights\n        context = rearrange(context, 'b h s d -> b s (h d)')\n        output = self.out_proj(context)\n        gate = torch.sigmoid(self.copy_gate(X).clamp(-5, 5))\n        output = gate * output + (1 - gate) * X\n        return output, Z\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with stable values\"\"\"\n        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.geo_proj.weight, gain=0.1)\n        if self.qkv_proj.bias is not None:\n            nn.init.zeros_(self.qkv_proj.bias)\n        if self.out_proj.bias is not None:\n            nn.init.zeros_(self.out_proj.bias)\n        nn.init.zeros_(self.copy_gate.weight)\n        if self.copy_gate.bias is not None:\n            nn.init.constant_(self.copy_gate.bias, 1.0)\n        nn.init.xavier_uniform_(self.head_router.weight, gain=0.1)\n        if self.head_router.bias is not None:\n            nn.init.zeros_(self.head_router.bias)\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'qkv_proj_bias': True,\n    'out_proj_bias': True, 'softmax_scale': None, 'chunk_size': 1024}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = GeometricGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\nfrom typing import Optional, Tuple\n\n\nclass GeometricGatedMHA(GAUBase):\n    \"\"\"\n    Memory-efficient Geometric Gated Multi-Head Attention with hierarchical attention routing.\n    \n    This implementation combines:\n    1. Geometric Attention Patterns with memory-efficient computation\n    2. Adaptive Copy Gates with stable gradients\n    3. Dynamic Head Routing with chunked processing\n    \n    The implementation uses chunked attention computation and stable softmax to handle\n    long sequences efficiently while maintaining numerical stability.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: Optional[int]=None,\n        head_dim: Optional[int]=None, qkv_proj_bias: bool=True,\n        out_proj_bias: bool=True, softmax_scale: Optional[float]=None,\n        rotary_emb_base: float=10000.0, chunk_size: int=1024, device=None,\n        dtype=None, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.causal = causal\n        self.chunk_size = chunk_size\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n            self.head_dim = embed_dim // n_heads\n        else:\n            self.head_dim = head_dim\n        self.qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        self.out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, self.qkv_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        self.out_proj = nn.Linear(self.out_dim, embed_dim, bias=\n            out_proj_bias, **self.factory_kwargs)\n        self.geo_proj = nn.Linear(self.head_dim, self.head_dim, **self.\n            factory_kwargs)\n        self.geo_scale = nn.Parameter(torch.ones(self.num_heads, 1, 1, **\n            self.factory_kwargs) * 0.1)\n        self.copy_gate = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.head_router = nn.Linear(embed_dim, self.num_heads, **self.\n            factory_kwargs)\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.scale = (self.head_dim ** -0.5 if softmax_scale is None else\n            softmax_scale)\n        self.reset_parameters()\n\n    def chunk_attention(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Compute attention scores in chunks to save memory\"\"\"\n        batch_size, num_heads, seq_len, head_dim = q.shape\n        out = torch.zeros_like(q)\n        for i in range(0, seq_len, self.chunk_size):\n            chunk_end = min(i + self.chunk_size, seq_len)\n            q_chunk = q[:, :, i:chunk_end]\n            attn_weights = torch.matmul(q_chunk, k.transpose(-2, -1)\n                ) * self.scale\n            q_geo = self.geo_proj(q_chunk)\n            k_geo = self.geo_proj(k)\n            geo_bias = torch.matmul(q_geo, k_geo.transpose(-2, -1)\n                ) * self.geo_scale\n            attn_weights = attn_weights + geo_bias\n            if self.causal:\n                causal_mask = torch.triu(torch.ones(chunk_end - i, seq_len,\n                    dtype=torch.bool, device=q.device), diagonal=i + 1)\n                attn_weights.masked_fill_(causal_mask[None, None], float(\n                    '-inf'))\n            attn_max = torch.max(attn_weights, dim=-1, keepdim=True)[0]\n            exp_weights = torch.exp(attn_weights - attn_max)\n            attn_weights = exp_weights / (torch.sum(exp_weights, dim=-1,\n                keepdim=True) + 1e-06)\n            out[:, :, i:chunk_end] = torch.matmul(attn_weights, v)\n        return out\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, dict]:\n        batch_size, seq_len = X.shape[:2]\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b s (h d) -> b h s d', h=self.num_heads)\n        k = rearrange(k, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        v = rearrange(v, 'b s (h d) -> b h s d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        if self.num_heads > self.num_heads_kv:\n            k = torch.repeat_interleave(k, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n            v = torch.repeat_interleave(v, repeats=self.num_heads // self.\n                num_heads_kv, dim=1)\n        context = self.chunk_attention(q, k, v)\n        head_weights = torch.sigmoid(self.head_router(X))\n        head_weights = rearrange(head_weights, 'b s h -> b h s 1')\n        context = context * head_weights\n        context = rearrange(context, 'b h s d -> b s (h d)')\n        output = self.out_proj(context)\n        gate = torch.sigmoid(self.copy_gate(X).clamp(-5, 5))\n        output = gate * output + (1 - gate) * X\n        return output, Z\n\n    def reset_parameters(self):\n        \"\"\"Initialize parameters with stable values\"\"\"\n        nn.init.xavier_uniform_(self.qkv_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.out_proj.weight, gain=1 / math.sqrt(2))\n        nn.init.xavier_uniform_(self.geo_proj.weight, gain=0.1)\n        if self.qkv_proj.bias is not None:\n            nn.init.zeros_(self.qkv_proj.bias)\n        if self.out_proj.bias is not None:\n            nn.init.zeros_(self.out_proj.bias)\n        nn.init.zeros_(self.copy_gate.weight)\n        if self.copy_gate.bias is not None:\n            nn.init.constant_(self.copy_gate.bias, 1.0)\n        nn.init.xavier_uniform_(self.head_router.weight, gain=0.1)\n        if self.head_router.bias is not None:\n            nn.init.zeros_(self.head_router.bias)\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'causal': True,\n    'num_heads_kv': None, 'head_dim': None, 'qkv_proj_bias': True,\n    'out_proj_bias': True, 'softmax_scale': None, 'chunk_size': 1024}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}